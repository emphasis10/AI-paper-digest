# Visual Instruction Tuning
## TL;DR
## Summary
- [https://arxiv.org/pdf/2304.08485.pdf](https://arxiv.org/pdf/2304.08485.pdf)

이 연구에서는 GPT-4와 같은 언어 모델만을 사용하여 다중모달(언어와 이미지를 모두 포함하는) 지시사항을 따르는 데이터를 생성하는 새로운 방법을 제안합니다. 이를 통해 LLaVA(Large Language and Vision Assistant)라는 새로운 다중모달 모델을 개발하였으며, 시각적 인코더와 언어 모델을 연결하여 일반적인 시각 및 언어 이해를 위한 모델을 구축합니다. 실험을 통해 LLaVA가 다양하고 도전적인 과제에서 인상적인 다중모달 대화 능력을 보여주었고, 특히 Science QA에서 새로운 최고 기록을 세웠습니다.

### 주요 기여
- **다중모달 지시사항 따르기 데이터:** 언어 모델을 사용하여 시각-언어 지시사항을 따르는 데이터를 생성하는 새로운 방법을 소개합니다. 이 데이터는 다양한 이미지와 관련된 질문과 대답을 포함하여, 모델이 시각적 컨텍스트를 이해하고 관련된 언어적 응답을 생성할 수 있도록 합니다.
- **LLaVA 모델:** 시각적 인코더와 언어 모델을 결합하여, 일반적인 시각 및 언어 이해를 목표로 하는 다중모달 모델을 개발했습니다. 이 모델은 생성된 다중모달 지시사항 따르기 데이터에 기반하여 훈련되었습니다.
- **성능 평가 벤치마크:** 시각적 지시사항을 따르는 모델의 능력을 평가하기 위한 두 가지 새로운 벤치마크를 제시합니다. 이를 통해 LLaVA가 기존의 다른 모델들에 비해 우수한 성능을 보임을 입증하였습니다.
- **오픈 소스 자료:** 연구를 촉진하기 위해, 생성된 데이터, 모델 코드, 모델 체크포인트, 그리고 시각적 대화 데모를 공개하였습니다.

### 종합 요약
LLaVA는 언어 모델만을 사용하여 생성된 새로운 형태의 다중모달 지시사항 따르기 데이터에 기반한, 시각적 인코더와 언어 모델을 연결한 최초의 모델입니다. 이 모델은 다양한 시각 및 언어 이해 과제에서 뛰어난 성능을 보여주었으며, 특히 Science QA에서는 새로운 최고 기록을 달성하였습니다. 연구 결과는 다중모달 인공지능 분야의 발전에 기여할 것으로 기대됩니다.