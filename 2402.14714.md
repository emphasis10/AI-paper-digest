# Efficient and Effective Vocabulary Expansion Towards Multilingual Large Language Models
## TL;DR
## Summary
- [https://arxiv.org/pdf/2402.14714.pdf](https://arxiv.org/pdf/2402.14714.pdf)

### 1. 각 섹션의 요약

#### 1.1 소개
이 보고서는 EEVE-Korean-v1.0, 한국어에 대한 대규모 언어 모델의 적용을 소개하며, 영어 중심의 LLMs, 예를 들어 SOLAR-10.7B와 Phi-2와 같은 모델들이 비영어 텍스트를 비효율적으로 처리하는 문제를 해결하기 위한 효율적이고 효과적인 어휘 확장(EEVE) 방법을 제시합니다. 이 방법은 파라미터 동결과 서브워드 초기화를 포함하며, 새로운 임베딩이 수조 개의 훈련 토큰을 필요로 한다는 기존의 생각과 달리, 단 20억 개의 토큰으로 비영어 능력을 크게 향상시킬 수 있음을 보여줍니다.

#### 1.2 효율적이고 효과적인 어휘 확장
이 섹션에서는 영어 중심의 언어 모델을 비영어 언어로 확장하는 새로운 방법론을 소개합니다. 이 방법은 파라미터 동결과 서브워드 기반의 임베딩 초기화를 결합하여 새로운 언어적 토큰을 효과적으로 통합하고 적응시키며, 다양한 언어적 맥락에서의 적용 가능성을 향상시킵니다. 여기서는 서브워드 기반 임베딩 초기화에 대한 세부 사항과 7단계 훈련 과정이 자세히 설명됩니다.

#### 1.3 구현 세부 사항
이 섹션은 한국어 코퍼스에 대한 새로운 토크나이저의 훈련, 고품질의 사전 훈련 코퍼스 구축을 위한 데이터셋 준비, 그리고 EEVE-Korean 모델들의 훈련 과정에 대해 설명합니다. SOLAR-10.7B와 Phi-2와 같은 기존의 영어 중심 LLMs를 바탕으로 한 EEVE-Korean 모델들이 한국어와 영어 언어 작업 모두에 대해 어떻게 평가되었는지에 대한 세부 사항이 포함되어 있습니다.

#### 1.4 평가
이 섹션은 EEVE-Korean 모델들이 한국어 및 영어 LLM 벤치마크에서 어떻게 평가되었는지를 자세히 설명합니다. EEVE-Korean 모델들이 한국어 작업에서 최신 성능을 달성하는 동시에 영어 작업에서도 강력한 기능을 유지함으로써, 어휘 확장 방법의 효율성과 효과성을 입증합니다.

#### 1.5 결론 및 향후 작업
EEVE-Korean-v1.0은 효율적이고 효과적인 어휘 확장 방법을 사용하여 한국어 텍스트 처리 기능을 크게 향상시킨 대규모 언어 모델의 한국어 적용 사례를 제시합니다. 이 프로젝트는 더 포괄적이고 효율적인 언어 처리 기술 개발에 기여할 것을 목표로 하며, 향후 다양한 언어로의 어휘 확장 방법론 적용 가능성과 다양한 작업에서의 추론 및 생성 능력 평가를 탐구할 계획입니다.

### 2. 전체 요약
EEVE-Korean-v1.0은 영어 중심의 대규모 언어 모델을 한국어로 확장하기 위한 새로운 접근법을 제시합니다. 이 방법은 파라미터 동결과 서브워드 초기화를 통해 새로운 언어적 토큰을 효과적으로 통합하고, 단 20억 개의 토큰으로 비영어 능력을 크게 향상시킵니다. EEVE-Korean-10.8B-v1.0 모델은 한국어 언어 작업에서 최고 성능을 달성하며, 영어 작업에서도 강력한 성능을 유지함으로써, 어휘 확장 방법의 성공을 입증합니다. 이 프로젝트는 더 포괄적이고 효율적인 언어 처리 기술 개발을 위해 커뮤니티에 기여할 것이며, 향후 추가 언어로의 확장과 다양한 작업에 대한 모델의 성능을 탐구할 계획입니다.