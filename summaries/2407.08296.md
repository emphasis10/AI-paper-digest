# Q-GaLore: Quantized GaLore with INT4 Projection and Layer-Adaptive Low-Rank Gradients
## TL;DR
## Summary
- [https://arxiv.org/pdf/2407.08296.pdf](https://arxiv.org/pdf/2407.08296.pdf)

### Section Summaries

#### Abstract
이 논문에서는 Q-GaLore라는 새로운 방법론을 소개합니다. 이 방법론은 큰 언어 모델(LLM)의 훈련 시 메모리 사용량을 크게 줄이는 동시에 성능을 유지합니다. Q-GaLore는 양자화(quantization)와 저랭크 투영(low-rank projection)을 결합하여, 메모리 효율성을 극대화하고 훈련 속도를 향상시킵니다. 실험 결과, Q-GaLore는 기존 방법들보다 메모리 사용을 최대 50% 절감하면서도 성능을 유지하거나 보다 나은 결과를 보였습니다.

#### Introduction
한계로 인해 대부분의 연구 그룹은 LLM을 훈련하는 데 어려움을 겪고 있습니다. GaLore는 저랭크 투영을 통해 메모리 사용량을 줄였지만, 여전히 자주 발생하는 SVD 연산 때문에 시간이 많이 소요됩니다. 이를 해결하기 위해 Q-GaLore는 INT4 포맷을 사용하여 메모리 효율성을 더욱 향상시키고, 적응형 업데이트를 통해 SVD 연산을 최소화합니다.

#### Methodology
Q-GaLore는 주로 두 가지 혁신적인 부분으로 구성되어 있습니다. 첫째, 저랭크 투영을 통해 메모리 사용을 크게 줄이는 것입니다. 둘째, 양자화된 프로젝션 행렬을 사용하여 메모리 효율성을 높이고 훈련 속도를 향상시킵니다. 또한, stochastically rounding (SR)을 사용하여 저정밀도 가중치 값을 유지하면서도 고정밀도의 경사 값을 보존합니다.

#### Experiments
Q-GaLore의 성능을 평가할 때, 여러 모델과 데이터셋을 사용하여 실험을 진행했습니다. 실험 결과, Q-GaLore는 메모리 사용량을 줄이면서도 기존 방법과 동등하거나 더 나은 성능을 보였습니다. 특히, 훈련 시 메모리 사용량을 최대 50% 줄일 수 있었습니다.

#### Conclusion
Q-GaLore는 큰 언어 모델의 훈련 시 필요한 메모리 사용량을 획기적으로 줄이는 방법입니다. 양자화와 저랭크 투영 기술을 결합하여 메모리 사용량을 줄이고, 훈련 속도를 향상시키면서도 성능을 유지합니다. 앞으로도 더욱 다양한 모델에 적용하여 효율성을 검증할 예정입니다.

### Overall Summary
Q-GaLore는 큰 언어 모델의 훈련 시 메모리 효율성을 극대화하고자 설계된 혁신적인 방법론입니다. 기존의 GaLore 방법론에서 개선된 부분은 다음과 같습니다:

1. **저랭크 투영과 양자화 결합**: 가중치를 INT8로, 프로젝션 행렬을 INT4로 저장하여 메모리 사용량을 크게 줄였습니다.
2. **적응형 업데이트**: 경사 하강법을 적용하는 동안 SVD 연산을 최소화하여 훈련 시간을 단축하였습니다.
3. **stochastically rounding (SR)**: 저정밀도 가중치 값을 유지하면서도 고정밀도 경사값을 보존하여 훈련 성능을 유지했습니다.

실험 결과, Q-GaLore는 메모리 사용량을 최대 50% 줄이면서도 기존 방법과 비교해 높은 성능을 보여주었습니다. 이러한 결과는 큰 언어 모델을 더 적은 리소스로 효과적으로 훈련할 수 있는 가능성을 제시합니다.