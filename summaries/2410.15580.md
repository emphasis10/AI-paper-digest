# Language Models are Symbolic Learners in Arithmetic
## TL;DR
## Summary
- [https://arxiv.org/pdf/2410.15580.pdf](https://arxiv.org/pdf/2410.15580.pdf)

1. 각 섹션 요약:

- **서론**: 현대 수학 벤치마크는 최첨단 언어 모델 (LLM)의 발전으로 인해 빠르게 포화상태에 이르고 있습니다. 이러한 모델들은 기본 산술 연산에서 어려움을 겪고 있으며, 이는 수치 계산과 언어 모델링의 근본적인 차이에서 기인할 수 있다고 주장합니다.

- **부분 곱(Partial Products) 연구**: LLM이 산술 학습 중 부분 곱을 인식할 수는 있지만, 이러한 방식이 실제 계산에 활용되지는 않는다는 사실을 발견했습니다. 이는 LLM이 계산을 순수하게 상징적으로 접근함을 시사합니다.

- **언어 모델의 상징적 관찰자 역할**: LLM이 산술 작업을 부분 곱 대신 서브그룹을 통해 접근하는 방식에 대해 조사했습니다. 서브그룹 복잡성의 고정 상태에서, LLM은 다양한 산술 작업을 비슷하게 처리함을 보였습니다.

- **학습의 곤란점**: LLM이 학습할 때 하위그룹을 선택하는 방법은 '쉬운 것에서 어려운 것'으로 진행되는 경향이 있습니다.

- **제한점 및 윤리적 고찰**: 연구는 CoT 방법론이나 자연 언어 인지가 포함된 설정에서는 적용되지 않았으며, 이는 미래 연구의 중요한 기회로 남아 있음을 설명합니다.

2. 전체 요약:

이 논문에서 연구자들은 LLM이 산술 계산을 할 때 실제 계산 대신 상징적인 패턴 매칭을 사용한다는 것을 확인했습니다. 산술 학습의 어려움은 서브그룹의 복잡성 및 선택과 관련이 있으며, LLM의 학습은 '쉬운 것에서 어려운 것'으로 진행됨을 발견했습니다. 이는 LLM이 복잡한 수학적 문제를 해결하는 능력을 이해하기 위해 서브그룹 수준의 양적 접근이 중요함을 시사합니다. 이 연구는 LLM의 심볼릭 학습 능력을 탐구하는 미래 연구의 기반이 될 것입니다.