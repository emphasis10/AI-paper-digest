# Perplexed by Perplexity: Perplexity-Based Data Pruning With Small Reference Models
## TL;DR
## Summary
- [https://arxiv.org/pdf/2405.20541.pdf](https://arxiv.org/pdf/2405.20541.pdf)

### 논문의 주요 내용 요약

#### 1. 서론
- **내용 요약**:
  대형 언어 모델(LLM)의 성능을 향상시키면서 훈련 비용을 줄이는 방법을 탐구합니다. 본 연구에서는 사전 학습 데이터의 품질을 개선하여 LLM의 성능을 향상시키는 방법에 초점을 맞춥니다. 특히, 소형 모델이 대형 모델의 데이터를 가지치기(불필요한 데이터를 제거) 할 수 있는지와 이로 인해 발생하는 성능 향상을 조사합니다. 

#### 2. 기여 내용
- **주요 기여**:
  - 작은 참조 모델이 훨씬 큰 모델의 사전 학습 데이터를 효과적으로 가지치기 할 수 있음을 보여줌.
  - 도메인 구성에 따른 데이터 가지치기의 민감성을 강조.
  - 과학적 분석 및 데이터 제한 환경에서의 가지치기 효과를 탐구.

#### 3. 방법론
- **내용 요약**:
  데이터셋을 두 부분으로 나누어 참조 모델을 훈련시키고, 해당 모델을 사용하여 각 샘플의 혼란도(perplexity)를 계산합니다. 그런 다음, 혼란도 범위에 따라 데이터셋을 가지치기 합니다. 낮은, 중간, 높은 혼란도를 기준으로 샘플을 선택합니다.

#### 4. 실험
- **실험 설정**:
  - 다양한 도메인 구성의 데이터셋(Pile, Dolma)을 사용하여 실험.
  - 참조 모델은 1억 2천5백만 개의 매개변수로 구성, 최종 모델은 10억 및 30억 개의 매개변수로 구성.
  - LLM의 성능을 여러 질문-답변 과제를 통해 평가.
  
- **주요 결과**:
  - 가지치기 된 데이터셋으로 훈련된 모델이 원본 데이터셋으로 훈련된 모델보다 성능이 우수함.
  - 가지치기 된 데이터는 훈련 효율성을 높여 더 적은 단계로도 동일한 성능을 달성.

#### 5. 결론
- **내용 요약**:
  혼란도 기반 데이터 가지치기가 모델 성능 및 훈련 효율성을 크게 향상시킬 수 있음을 입증하였으며, 이는 다양한 도메인 구성과 훈련 환경에서도 일관된 성과를 보였습니다.

### 전반적인 요약

이 논문은 소형 언어 모델을 사용하여 대형 언어 모델의 사전 학습 데이터를 가지치기 함으로써 데이터 품질을 개선하고, 이를 통해 대형 모델의 성능을 향상시키는 방법을 제안합니다. 다양한 도메인 구성 및 훈련 환경에서도 혼란도 기반 가지치기가 효과적임을 보여줍니다. 주요 기여로는 소형 모델의 효율적 가지치기 가능성 입증, 도메인 구성의 민감성 강조, 비표준 훈련 환경에서의 가지치기 효과 탐구 등이 있습니다. 이 연구는 향후 LLM의 데이터 가지치기 기술 발전에 중요한 기여를 할 것입니다.

## Similar Papers
- [LAB: Large-Scale Alignment for ChatBots](2403.01081.md)
- [LoRA Learns Less and Forgets Less](2405.09673.md)
- [LLM-jp: A Cross-organizational Project for the Research and Development of Fully Open Japanese LLMs](2407.03963.md)
- [When Scaling Meets LLM Finetuning: The Effect of Data, Model and Finetuning Method](2402.17193.md)
- [Towards Robust Evaluation: A Comprehensive Taxonomy of Datasets and Metrics for Open Domain Question Answering in the Era of Large Language Models](2406.13232.md)
- [DenseFormer: Enhancing Information Flow in Transformers via Depth Weighted Averaging](2402.02622.md)
- [Pre-training Small Base LMs with Fewer Tokens](2404.08634.md)
- [Scalable MatMul-free Language Modeling](2406.02528.md)
- [BigCodeBench: Benchmarking Code Generation with Diverse Function Calls and Complex Instructions](2406.15877.md)
