# Training Noise Token Pruning
## TL;DR
## Summary
- [https://arxiv.org/pdf/2411.18092.pdf](https://arxiv.org/pdf/2411.18092.pdf)

### 1. 각 섹션의 주요 내용 요약

- **Introduction (소개)**  
  본 논문은 트랜스포머의 계산 부담을 줄이기 위한 토큰 프루닝(Token Pruning) 방법을 제안하고 있습니다. 이 방법은 사용자 정의 노이즈를 추가하여 부드러운 최적화를 가능하게 하며, 배포 설정에서는 계산적 이점을 유지합니다. 이 연구는 주로 이미지넷 데이터셋을 사용한 실험을 통해, 비전 트랜스포머 모델의 효율성을 높이는 방법을 소개합니다.

- **Methodology (방법론)**  
  토큰 드랍핑을 정보 보틀넥 문제로 재구성하여 프루닝을 최적화합니다. 정보 전송을 제한하는 조건을 추가해 최적화를 수행하고, 트랜스포머 구조 내의 노이즈 투여를 통해 효율성을 높입니다.

- **Empirical Evaluations (실증 평가)**  
  다양한 모델에서 실험을 통해 제안 방법의 유용성을 확인하였습니다. 실험에서는 이미지넷 데이터셋을 사용하며, 프루닝 비율 및 레이어 구성에 따라 성능을 비교합니다. 특히, 제안된 방법은 낮은 퍼센트의 토큰 유지율에서도 높은 정확도를 유지하고 있습니다.

- **Conclusion (결론)**  
  이 연구를 통해 정보 보틀넥 프레임워크 내에서 새로운 토큰 프루닝 방법이 제안되었습니다. 이 방법은 리소스 제약이 있는 환경에서 비전 트랜스포머의 효율성을 크게 향상시킬 수 있으며, 동일한 모델에서 계산 부담을 크게 줄일 수 있습니다.

### 2. 전체 요약

이 논문은 비전 트랜스포머의 계산량을 줄이고 성능을 최적화하기 위한 새로운 토큰 프루닝 방법을 제안합니다. 기존의 스토캐스틱 드랍핑 및 주의 기반 히어리스틱을 넘어서는 정보 보틀넥 접근 방식을 사용하여, 연속적인 최적화를 통해 더 나은 정확도와 줄어든 계산적 부담을 동시에 달성합니다. 본 방법론은 주로 이미지 처리 모델에서 매우 높은 퍼포먼스를 보여주었으며, 리소스가 제한된 환경에서도 매우 효율적으로 사용할 수 있습니다.