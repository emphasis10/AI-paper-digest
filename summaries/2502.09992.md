# Large Language Diffusion Models
## TL;DR
## Summary
- [https://arxiv.org/pdf/2502.09992.pdf](https://arxiv.org/pdf/2502.09992.pdf)

### Section 별 중요 내용 요약

1. **도입 (Introduction)**:
   본 논문은 대규모 언어 모델(LLM)의 기초가 되고 있는 자회귀 모델(ARM)에 대한 새로운 접근을 제안합니다. 기존의 자회귀 방식이 아닌 확산 방식을 활용하여 LLaDA라는 모델을 소개하며, 이는 새로운 방식으로 대규모 언어 모델이 가지는 능력을 확장하고 있습니다.

2. **모델 설명 (Model Description)**:
   LLaDA는 확산 모델을 사용하여, 자회귀 방식의 다음 단어 예측이 아닌 전체 문맥에서 토큰을 예측하는 방식을 취합니다. 이는 기존의 자회귀 모델이 가진 제약을 극복할 수 있는 가능성을 제시합니다.

3. **학습 과정 (Training Process)**:
   LLaDA는 데이터 준비, 사전 훈련, 감독된 미세 조정(SFT), 평가의 표준 파이프라인을 따르고 있습니다. 특히, 2.3조개의 토큰을 학습하고 다양한 작업에서 경쟁력 있는 성능을 보였습니다.

4. **주요 기여 및 성과 (Key Contributions and Achievements)**:
   LLaDA는 확장성, 문맥 학습 및 지시 사항 준수 능력에서 두각을 나타냅니다. 이는 기존 ARM 방식과 비교하여 대등하거나 우수한 성능을 보이며, 특히 수학 및 중국어 작업에서 두드러집니다.

5. **결론 및 토론 (Conclusion and Discussion)**:
   LLaDA는 기존 자회귀 모델의 한계를 넘어 새로운 가능성을 제시하며, 특히 확장성과 지시 사항 준수 능력에서 확산 모델의 잠재성을 입증하였습니다. 이러한 결과는 확산 모델이 자회귀 모델의 대안으로서 유망하다는 것을 보여줍니다.

### 전체 요약

이 논문은 기존의 대규모 언어 모델(LLM)의 자회귀 기반 접근 방식을 확산 기반의 새로운 방식으로 전환하여, 대규모 언어 모델의 효율성과 성능을 향상시키고자 합니다. LLaDA 모델은 새로운 확산 모델을 사용하여 문맥 내의 토큰을 예측함으로써 다양한 작업에서 대등하거나 우수한 성능을 입증하였습니다. 특히 확장성과 지시 사항을 따르는 능력에서 독보적이며, 자회귀 모델의 제약을 극복하는 데 큰 역할을 합니다. 이러한 연구는 확산 모델이 LLM의 한계를 넘어 새로운 가능성을 제시할 수 있음을 보여줍니다.