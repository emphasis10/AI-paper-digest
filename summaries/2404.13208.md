# The Instruction Hierarchy: Training LLMs to Prioritize Privileged Instructions
## TL;DR
## Summary
- [https://arxiv.org/pdf/2404.13208.pdf](https://arxiv.org/pdf/2404.13208.pdf)

이 논문은 AI 기술의 발전과 관련하여, 특히 라지 랭귀지 모델(LLMs)이 사용자의 명령을 우선시하도록 훈련시키는 방법에 초점을 맞추고 있습니다. 여기에는 몇 가지 주요 섹션이 포함되어 있습니다.

1. **도입부**: 이 섹션에서는 현대 LLM들이 직면한 주요 위협 중 하나인 '프롬프트 인젝션' 공격에 대해 설명합니다. 이러한 공격은 사용자 또는 제3자가 LLM에 악의적인 명령을 주입하여 원래의 명령을 덮어쓸 수 있게 합니다.

2. **배경**: LLM이 어떻게 다양한 메시지 타입을 처리하는지, 그리고 이러한 메시지들이 어떻게 다른 우선순위를 갖는지 설명합니다. 이는 시스템 메시지, 사용자 메시지, 모델 출력 등을 포함합니다. 또한, LLM을 대상으로 하는 다양한 공격 유형에 대해서도 설명합니다.

3. **명령 계층**: 이 섹션에서는 명령의 우선순위를 설정하여 모델이 충돌하는 명령 사이에서 어떻게 행동해야 하는지에 대한 체계를 제안합니다. 이는 LLM이 더 높은 우선순위의 명령을 따르도록 하며, 낮은 우선순위의 명령은 무시하거나 거부합니다.

4. **훈련 데이터 생성**: 여러 공격에 대비하기 위한 훈련 데이터를 생성하는 방법을 설명합니다. 이는 ‘상황 합성’과 ‘상황 무시’의 두 가지 주요 방법을 포함합니다.

5. **주요 결과**: 향상된 모델의 성능을 보여주는 실험 결과를 제공합니다. 이는 명령 계층을 사용하여 훈련된 모델이 다양한 공격 유형에 대해 더 강력한 견고성을 보여줌을 강조합니다.

6. **토론 및 관련 연구**: 명령 계층을 통한 방어 전략과 이와 관련된 다른 연구들과의 비교를 다룹니다.

7. **결론 및 향후 연구**: 연구의 결론과 앞으로의 연구 방향을 제시합니다.

각 섹션은 LLMs의 안전성과 효율성을 향상시키기 위한 방법론을 자세히 설명하며, 이를 바탕으로 실제 AI 시스템에 적용할 수 있는 구체적인 전략을 제공합니다. 이 논문은 LLM을 사용하는 애플리케이션 개발자들에게 중요한 지침을 제공하여, 사용자의 데이터를 보호하고 악의적인 공격으로부터 시스템을 안전하게 유지할 수 있는 방법을 제시합니다.