# Inference-Time Scaling for Diffusion Models beyond Scaling Denoising Steps
## TL;DR
## Summary
- [https://arxiv.org/pdf/2501.09732.pdf](https://arxiv.org/pdf/2501.09732.pdf)

1. 각 섹션의 요약 및 핵심 기여:

- **도입부**: 생성 모델은 학습 시 데이터 볼륨, 계산 자원 및 모델 크기를 증가시키면서 변혁적인 성과를 가져왔습니다. 이러한 확장은 주로 '확장 법칙'에 의해 예측됩니다. 최근 대형 언어 모델(LLM)은 추론 시간 확장을 통해 추가적인 성능 향상을 이루고 있으며, 이는 더욱 높은 품질의 응답을 가능하게 합니다.

- **배경 및 동기**: 확산 모델은 연속적인 데이터 도메인에서 잡음을 제거하는 방식으로 작동하며, 이미지, 오디오, 비디오 등 다양한 분야에서 우수한 성능을 발휘합니다. 이들은 추론 시 계산 할당을 조정할 유연성을 제공합니다.

- **방법론**: 우리는 열거된 잡음을 통해 더 나은 잡음을 식별하는 문제를 구조화했습니다. 검색 프레임워크는 피드백을 제공하는 검증기와 더 나은 잡음 후보를 찾는 알고리즘으로 구성됩니다. 이러한 접근은 다양한 생성 작업에서 성능을 크게 향상시킬 수 있음을 보여줍니다.

- **실험 결과**: 제안된 프레임워크가 클래스 조건 이미지 생성과 텍스트 조건 이미지 생성에서 뛰어난 샘플 품질을 제공합니다. 또한 복잡한 자연의 이미지와 텍스트 조건이 주어진 경우, 컴포넌트 조합을 통해 특정 응용 시나리오에 맞추어야 함이 밝혀졌습니다.

- **결론**: 우리는 확산 모델에서 추론 시 확장을 위한 프레임워크를 제시하고, 모든 작업에 대해 보편적인 솔루션은 없으며, 특정 Generation 작업에 맞춘 정확한 설계가 필요함을 강조합니다.

2. 전체 요약:

이 논문은 생성 모델, 특히 확산 모델이 훈련 시뿐만 아니라 추론 시에도 어떻게 계산 자원을 효과적으로 사용할 수 있는지를 탐구합니다. 기존의 잡음 제거 단계에서 벗어나, 검색을 통한 계산 배분이 성능 향상을 가능하게 하는 방법론을 제안하고 실험합니다. 이러한 접근은 다양한 작업에서 높은 품질의 샘플을 생성할 수 있어, 미래의 AI 개발에 있어서 중요한 방향성을 제공합니다.