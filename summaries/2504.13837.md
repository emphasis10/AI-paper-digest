# Does Reinforcement Learning Really Incentivize Reasoning Capacity in LLMs Beyond the Base Model?
## TL;DR
## Summary
- [https://arxiv.org/pdf/2504.13837.pdf](https://arxiv.org/pdf/2504.13837.pdf)

1. 섹션별 요약:

- **서론**:
  연구는 강화학습의 검증 가능한 보상(RLVR)을 사용하여 대형 언어 모델(LLMs)의 추론 능력을 높이는 방법을 살펴봅니다. 이 방법이 기존 모델의 한계를 넘어서기 위해 어떻게 작용하는지, 그리고 새로운 추론 능력을 실제로 도입하는지를 비판적으로 조사합니다.

- **연구 목표**:
  연구의 주요 질문은 RLVR이 실제로 새로운 추론 패턴을 도입했는지, 혹은 기본 모델이 이미 가지고 있는 것을 샘플링하는지의 여부를 평가하는 것입니다. 이를 위해 연구는 다양한 벤치마크와 함께 여러 개의 모델과 RLVR 알고리즘을 사용하여 모델의 추론 능력 경계를 평가합니다.

- **결과**:
  연구 결과, RLVR은 추론 능력을 확장하는 것보다 샘플링 효율성을 개선하는 데 더 기여한다는 것을 발견했습니다. 강화학습을 통한 모델은 작은 k 값에서 우수한 성과를 보이지만 k 값이 클수록 기본 모델이 우위에 있습니다.

- **주요 기여 및 혁신**:
  본 논문은 RLVR이 새로운 방식으로 모델의 추론 능력을 강화하지 않는다는 점, 오히려 샘플링 효율성만 개선한다는 점을 강조합니다. 또한, 증류(distillation) 방법이 새로운 추론 패턴을 도입할 수 있으며, 이는 모델의 추론 영역을 확장할 수 있다는 점을 밝혔습니다.

2. 전체 요약:

이 논문은 대형 언어 모델의 추론 능력을 강화하는 데 RLVR의 한계를 비판적으로 탐구합니다. 연구 결과, RLVR은 샘플링 효율성을 개선하지만 새로운 추론 능력을 도입하지는 못한다는 것이 밝혀졌습니다. 대신, 증류가 모델에 새로운 지식을 도입하여 추론 능력을 확장할 수 있는 잠재력을 가집니다. 이는 기존 강화 학습 패러다임의 한계를 보여주며, 새로운 접근 방식의 필요성을 제시합니다.