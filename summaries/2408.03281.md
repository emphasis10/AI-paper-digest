# StructEval: Deepen and Broaden Large Language Model Assessment via Structured Evaluation
## TL;DR
## Summary
- [https://arxiv.org/pdf/2408.03281.pdf](https://arxiv.org/pdf/2408.03281.pdf)

### 1. 각 섹션 요약

#### 서론 (Introduction)

이 논문은 현재 대형 언어 모델(LLM)의 평가가 단일 항목 평가 패러다임에 의존하여 모델이 요구되는 역량을 정확히 보유하고 있는지 판단하는 것이 어렵다는 문제점을 지적합니다. 이를 해결하기 위해 StructEval이라는 새로운 평가 프레임워크를 제안합니다. StructEval은 원자적인 테스트 목표를 시작으로, 다중 인지 수준 및 중요한 개념에 걸친 구조적 평가를 통해 LLM의 평가를 심화하고 확장합니다. 이를 통해 LLM의 역량에 대해 보다 신뢰할 수 있고 일관된 결론을 도출할 수 있습니다.

#### 본문 (Main Content)
StructEval은 Bloom의 인지 분류에 따라 여섯 가지 인지 수준과 중요한 개념에 대해 다중 테스트 인스턴스를 생성하여 평가를 수행합니다. 이 방법은 단일 항목 평가에 비해 모델이 여러 인지 레벨에서 지식을 보여주어야 하므로 LLM의 성능을 보다 공정하고 정확하게 평가할 수 있습니다. StructEval의 주요 기여는 다음과 같습니다:

1. **심화 및 확장된 평가**: Bloom의 분류에 따른 다중 인스턴스 생성.
2. **데이터 오염 저항성**: 데이터 오염 환경에서도 일관된 평가 결과 제공.
3. **자동 생성된 대규모 벤치마크**: 구조적 평가를 위한 새로운 벤치마크 데이터셋 자동 생성.
4. **기존 개선 전략 초과 성능**: 이전의 증강 기반 전략보다 뛰어난 성능을 보여줌.

#### 실험 결과 (Experimental Results)

StructEval이 기존의 여러 벤치마크(MMLU, ARC, OpenBook QA)에서 높은 성능을 발휘하는 것을 실험을 통해 입증했습니다. StructEval은:

1. 대규모 벤치마크 데이터셋을 자동 생성할 수 있습니다.
2. 데이터 오염 환경에서도 높은 평가 일관성과 정확성을 유지합니다.
3. 이전의 증강 기반 전략(단어 방해, 백번역)보다 뛰어난 성능을 보여줍니다.

#### 결론 및 한계 (Conclusion and Limitations)

StructEval은 대형 언어 모델의 종합적이고 일관된 평가를 가능하게 하며, 데이터 오염의 영향을 줄입니다. 다만, 현재 GPT-3.5를 사용한 제한된 생성 사례가 있어, 향후 더 강력한 모델(GPT-4)과 사람의 평가를 포함하여 테스트 인스턴스의 질을 더욱 향상시킬 예정입니다.

### 2. 전체 요약

이 논문은 대형 언어 모델(LLM)의 현 평가 방식을 개선하고자 StructEval이라는 새로운 평가 프레임워크를 제안합니다. StructEval은 Bloom의 인지 분류에 기반하여 여러 인지 수준과 중요한 개념을 평가하는 구조적 평가 방법으로, 보다 공정하고 정확한 성능 평가를 가능하게 합니다. 또한, 데이터 오염 환경에서도 일관된 결과를 보여주며, 기존의 증강 기반 전략보다 뛰어난 성능을 입증했습니다. 앞으로 StructEval은 더 강력한 모델(GPT-4)과 사람의 평가를 포함하여 테스트 인스턴스의 질을 향상시키고, 대형 언어 모델 평가 프로토콜의 원칙을 정립하는 데 기여할 것입니다.

## Similar Papers
- [Towards Scalable Automated Alignment of LLMs: A Survey](2406.01252.md)
- [SPIQA: A Dataset for Multimodal Question Answering on Scientific Papers](2407.09413.md)
- [Leave No Document Behind: Benchmarking Long-Context LLMs with Extended Multi-Doc QA](2406.17419.md)
- [Speculative RAG: Enhancing Retrieval Augmented Generation through Drafting](2407.08223.md)
- [Poisoned LangChain: Jailbreak LLMs by LangChain](2406.18122.md)
- [MMEvalPro: Calibrating Multimodal Benchmarks Towards Trustworthy and Efficient Evaluation](2407.00468.md)
- [Searching for Best Practices in Retrieval-Augmented Generation](2407.01219.md)
- [AutoRAG-HP: Automatic Online Hyper-Parameter Tuning for Retrieval-Augmented Generation](2406.19251.md)
- [Large Language Monkeys: Scaling Inference Compute with Repeated Sampling](2407.21787.md)
