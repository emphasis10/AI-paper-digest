# FuseChat: Knowledge Fusion of Chat Models
## TL;DR
## Summary
- [https://arxiv.org/pdf/2402.16107.pdf](https://arxiv.org/pdf/2402.16107.pdf)

이 연구 논문은 "FUSECHAT: Knowledge Fusion of Chat Models"이라는 제목으로, 대규모 언어 모델의 지식 융합을 활용한 채팅 모델의 개선 방법을 제시합니다. 이 연구는 기존의 대규모 언어 모델들을 효율적으로 융합하여 새로운 채팅 모델을 생성하는 두 단계 접근법을 도입합니다.

### 주요 내용 요약

1. **서론 및 배경**:
   - 대규모 언어 모델(LLM)의 훈련은 비용이 많이 들고 시간이 오래 걸리는 과정입니다. 따라서 기존의 LLM들을 융합하여 새로운 모델을 생성하는 지식 융합 방법이 제안됩니다.

2. **FUSECHAT의 구조**:
   - FUSECHAT는 먼저 다양한 구조와 규모의 소스 LLM들로부터 지식을 융합하여 동일한 구조와 크기의 여러 타겟 LLM을 생성합니다. 이후 이 타겟 LLM들을 파라미터 공간에서 융합하여 최종적인 융합 LLM을 생성합니다.

3. **모델 융합 방법**:
   - VARM(Variation Ratio Merge)이라는 새로운 방법을 도입하여 미세 조정 전후의 파라미터 행렬의 변동 비율을 기반으로 융합 가중치를 결정합니다. 이 방법은 추가적인 훈련 노력 없이 세밀한 가중치를 자동으로 계산할 수 있습니다.

4. **성능 평가**:
   - 다양한 채팅 도메인에서 NH2-Mixtral-8x7B, NH2-Solar-10.7B, OpenChat-3.5-7B와 같은 대표적인 오픈 소스 채팅 LLM을 사용하여 FUSECHAT의 효과를 검증합니다. 실험 결과는 FUSECHAT이 모든 소스 LLM과 미세 조정된 기준선을 능가하는 성능을 보여줍니다.

### 혁신적인 부분
FUSECHAT의 혁신성은 다양한 구조와 규모의 LLM을 효율적으로 융합하여 하나의 강력한 채팅 LLM을 생성할 수 있는 능력에 있습니다. 특히, 파라미터 융합을 위한 VARM 방식은 지식 융합을 위한 새로운 패러다임을 제시하며, 다양한 모델의 강점을 효과적으로 통합합니다.

이 연구는 대규모 언어 모델의 지식 융합을 통해 채팅 모델의 성능을 향상시키는 새로운 방법론을 제시하며, 이는 다양한 대화 시스템 개발에 중요한 기여를 할 것입니다.

## Similar Papers
- [PAS: Data-Efficient Plug-and-Play Prompt Augmentation System](2407.06027.md)
- [OLoRA: Orthonormal Low-Rank Adaptation of Large Language Models](2406.01775.md)
- [MedFuzz: Exploring the Robustness of Large Language Models in Medical Question Answering](2406.06573.md)
- [Tx-LLM: A Large Language Model for Therapeutics](2406.06316.md)
- [Sheared LLaMA: Accelerating Language Model Pre-training via Structured Pruning](2310.06694.md)
- [HuatuoGPT-Vision, Towards Injecting Medical Visual Knowledge into Multimodal LLMs at Scale](2406.19280.md)
- [ChuXin: 1.6B Technical Report](2405.04828.md)
- [AI Hospital: Benchmarking Large Language Models in a Multi-agent Medical Interaction Simulator](2402.09742.md)
- [Xmodel-LM Technical Report](2406.02856.md)
