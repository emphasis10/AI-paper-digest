# FuseChat: Knowledge Fusion of Chat Models
## TL;DR
## Summary
- [https://arxiv.org/pdf/2402.16107.pdf](https://arxiv.org/pdf/2402.16107.pdf)

**[1장 소개]**
인공지능 대형 언어 모델(LLM)들은 다양한 자연어 처리 작업에서 뛰어난 성과를 보여주고 있습니다. 이 모델들을 처음부터 훈련시키는 것은 큰 비용이 들지만, 기존의 LLM들을 결합하여 새로운 모델을 만드는 '지식 융합(Knowledge Fusion)' 방법이 비용을 절감하고 여러 모델의 강점을 활용할 수 있는 대안이 될 수 있습니다. 이 연구에서는 다양한 구조와 규모를 가진 채팅 LLM들을 융합하는 새로운 프레임워크인 FUSECHAT을 소개합니다.

**[2장 관련 연구]**
기존의 모델 융합 방법들은 주로 결과물을 직접 결합하거나 파라미터 공간에서 모델들을 통합하는 방식을 사용했습니다. 그러나 이런 방법들은 여러 모델을 동시에 운용해야 하는 비효율성이 있거나 동일한 아키텍처의 모델들에만 적용 가능했습니다. 이에 비해 FUSELLM 방법은 다양한 구조의 LLM들의 지식을 타깃 LLM으로 전달하는 새로운 접근법을 제공합니다.

**[3장 채팅 모델의 지식 융합]**
FUSECHAT은 두 단계로 구성됩니다. 첫 번째로, 다양한 소스 LLM들의 지식을 타깃 LLM으로 전달하고, 이를 통해 동일한 구조와 크기를 가진 여러 타깃 LLM을 생성합니다. 두 번째로, 이러한 타깃 LLM들을 파라미터 공간에서 결합하여 최종적인 융합된 LLM을 생성합니다. 특히, 파라미터 변경 비율을 기반으로 결합 가중치를 결정하는 VARM 방법론을 소개합니다.

**[4장 실험]**
실험에서는 FUSECHAT의 유효성을 검증하기 위해 NH2-Mixtral-8x7B, NH2-Solar-10.7B, OpenChat-3.5-7B 등 세 가지 대표적인 채팅 LLM을 사용하였습니다. 결과적으로 FUSECHAT은 기존 모델들과 조정된 베이스라인들을 초월하는 성능을 보였습니다.

**[5장 결론]**
이 연구에서는 채팅 LLM들의 지식과 강점을 통합하여 보다 강력한 채팅 LLM을 생성하는 FUSECHAT 프레임워크를 제안합니다. 이는 다양한 구조와 규모를 가진 모델들을 유연하고 효율적으로 통합할 수 있는 가능성을 보여줍니다.

이 연구는 채팅 모델을 융합하여 더 강력하고 효율적인 모델을 개발하는 새로운 방법론을 제시하며, 이는 향후 다양한 채팅 LLM의 개발에 기여할 수 있을 것입니다.