# Schrodinger's Memory: Large Language Models
## TL;DR
## Summary
- [https://arxiv.org/pdf/2409.10482.pdf](https://arxiv.org/pdf/2409.10482.pdf)

각 섹션의 중요한 내용을 요약하고 전체적인 요약을 제공합니다:

### 1. 섹션별 요약
- **서론 (Introduction)**
  - 대형 언어 모델(LLM)은 인간의 언어 능력을 모방하며, 인간 활동에서 중요한 부분인 기억을 어떻게 모델화하고 있는지를 탐구합니다. 이 논문은 LLM의 기억을 설명하는 새로운 관점으로 슈뢰딩거의 기억을 제안합니다.

- **UAT와 LLM (UAT and LLMs)**
  - 전역 근사 정리(UAT)를 통해 LLM의 기억 능력을 설명합니다. 수학적 구조를 통해 Transformer 기반 LLM의 기억 메커니즘을 이해하고, 이를 통해 LLM의 기억 능력을 평가할 수 있는 새로운 방법을 제안합니다.

- **LLM의 기억 (The Memory of LLMs)**
  - LLM이 정보를 어떻게 저장하고 호출하는지를 실험적으로 입증합니다. 입력에 따라 결과를 동적으로 근사하는 메커니즘을 설명하며, LLM의 강력한 기억이 이러한 동적 적합 능력에서 비롯된다고 결론 내립니다.

- **인간 두뇌와 LLM의 비교 (A Comparison Between Human Brain and LLMs)**
  - LLM의 메모리를 인간 두뇌의 메모리와 비교하여, 두 시스템 모두 입력에 기반한 동적 결과 근사를 통해 기능한다고 주장합니다. LLM의 메모리를 추론 능력의 일부로 보아야 하며, 이는 창의성의 한 형태로 이해될 수 있습니다.

### 2. 전체 요약
이 논문은 대형 언어 모델의 기억 능력을 설명하기 위해 UAT 이론을 적용하였습니다. 슈뢰딩거의 기억이라는 개념을 도입하여, LLM의 기억은 특정 질의에 대해 모델이 응답할 때만 관찰할 수 있고, 그 외에는 불확정 상태에 있다고 설명합니다. 또한, 모델의 기억 능력은 입력을 바탕으로 동적으로 결과를 근사하는 메커니즘에 기반하며, 이로 인해 인간과 LLM의 기억 메커니즘이 기능적으로 유사하다고 결론 지었습니다. 이러한 연구는 LLM의 성능을 평가하고 향상시키는 데 기여할 수 있는 방법론을 제공하며, AI 연구의 중요한 진전을 나타냅니다.