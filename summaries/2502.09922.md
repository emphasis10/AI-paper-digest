# λScale: Enabling Fast Scaling for Serverless Large Language Model Inference
## TL;DR
## Summary
- [https://arxiv.org/pdf/2502.09922.pdf](https://arxiv.org/pdf/2502.09922.pdf)

### 섹션별 요약

1. **서론:**
   이 문서는 클라우드 기반의 머신 러닝 추론 서비스를 위한 𝜆Scale 라는 새로운 서버리스 추론 시스템을 소개합니다. 이 시스템은 빠른 모델 확장을 실현하기 위해 고속 RDNA 네트워크를 활용합니다.

2. **배경 및 동기:**
   - LLM 추론: 대규모 언어 모델(LLM)은 점점 더 대용량화됨에 따라 서버리스 환경에서의 시작 지연 문제를 극복하기 어렵습니다.
   - 서버리스의 필요성: 서버리스 컴퓨팅은 사용자가 실제 자원 사용에만 비용을 지불하게 하여 경제적 부담을 줄입니다. 하지만, 시작 지연과 같은 문제들이 기존 서버리스 플랫폼의 비효율성을 초래하고 있습니다.

3. **𝜆Scale 개요:**
   효율적인 교차 노드 통신을 통해 빠른 모델 스케일링을 구현합니다. 이 시스템은 학습 및 추론 환경에서 파이프라인을 효율적으로 관리하여 빠른 모델 설치 및 실행을 지원합니다.

4. **파이프라인 설계:**
   - 적응형 모델 멀티캐스트: 이 기술은 다양한 확장 시나리오에서 모델을 신속히 분산시키고, 실행 파이프라인 생성을 가속화하도록 돕습니다.
   - 파이프라인 추론 실행: 동적 파이프라인 구성 및 실행을 통해 전체적인 중계 성능을 향상시키며, 상대할 수 없는 요청의 대기시간도 줄여줍니다.

5. **효율적인 모델 관리:**
   여러 스토리지 계층(GPU 메모리, 호스트 메모리)에서 모델을 효율적으로 관리할 수 있도록 지원합니다. 이러한 관리 전략은 모델 확장을 신속히 돕습니다.

6. **스케일 구현 및 평가:**
   𝜆Scale의 구현 세부사항 및 성능 평가 결과를 제시하며, 이 시스템이 실질적인 요구를 충족시키는 효과적인 솔루션으로 기능함을 증명합니다.

### 전체 요약

논문은 𝜆Scale 이라는 서버리스 추론 시스템을 제안하며, 이는 대규모 언어 모델과 같은 대형 모델들의 시작 지연 문제를 완화하기 위한 솔루션을 제공합니다. 고속 RDMA 네트워크를 활용하여 모델의 멀티캐스트 및 동적 파이프라인 실행을 지원하며, 이는 모델의 빠른 확장과 낮은 대기시간을 실현합니다. 𝜆Scale은 여러 GPU와 호스트 메모리에서 효율적인 모델 관리를 가능하게 하여, 대규모 언어 모델 추론 서비스의 실질적인 성능 향상과 비용 절감을 달성합니다.