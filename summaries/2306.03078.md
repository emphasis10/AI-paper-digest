# SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression
## TL;DR
## Summary
- [https://arxiv.org/pdf/2306.03078.pdf](https://arxiv.org/pdf/2306.03078.pdf)

### 논문 요약: SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression

#### 1. 서론
최근 대형 언어 모델(LLM)의 사전 학습이 빠르게 발전하면서 높은 성능을 보이고 있다. LLM을 3-4 비트로 양자화하면 랩탑이나 모바일 기기와 같은 메모리 제한 장치에서도 사용할 수 있다. 그러나 이러한 낮은 비트 양자화는 정확도 손실을 초래한다. SpQR(Sparse-Quantized Representation)은 LLM을 거의 손실 없이 압축하면서도 기존 방법과 유사한 압축 수준을 달성한다.

#### 2. 관련 연구
기존 연구들은 주로 이미지 모델 또는 소규모 언어 모델에 대한 양자화 방법을 다루고 있다. SpQR은 대규모 LLM을 대상으로 한 최초의 양자화 방법으로, 높은 정확도와 낮은 메모리 사용량을 동시에 달성한다.

#### 3. SpQR의 혁신
- **이상치 가중치 처리**: 큰 양자화 오류를 유발하는 가중치를 고정밀도로 저장하고, 나머지 가중치는 3-4 비트로 압축한다.
- **이중 양자화 그룹화**: 작은 그룹 크기(예: 16개 요소)로 양자화하여 양자화 스케일 자체를 3비트로 표현한다.

#### 4. 구현 방법
SpQR은 사전 학습된 LLM의 가중치를 이중 단계로 양자화한다. 먼저, 큰 오류를 유발하는 이상치를 감지하고, 나머지 가중치를 낮은 비트로 압축한다. 이를 통해 메모리 사용량을 약 3.4배 줄이면서도 정확도를 유지한다.

#### 5. 실험 결과
- **압축 효율성**: SpQR은 LLaMA 및 Falcon 모델에서 기존의 16비트 모델과 비교해도 1% 미만의 정확도 손실을 보인다.
- **속도 향상**: 16비트 모델보다 20-30% 빠른 추론 속도를 보인다.

#### 6. 논의 및 한계
SpQR은 높은 정확도를 유지하면서 메모리 사용량을 크게 줄일 수 있는 방법이다. 그러나 생성 품질에 대한 평가가 부족하며, 미래 연구에서는 생성 품질을 평가할 필요가 있다.

### 전체 요약
SpQR은 대형 언어 모델을 3-4 비트로 양자화하여 메모리 사용량을 크게 줄이면서도 거의 손실 없는 정확도를 유지하는 혁신적인 방법이다. 이 방법은 메모리 제한 장치에서도 높은 성능을 제공할 수 있어, 개인화된 LLM 사용을 가능하게 한다. SpQR은 특히 LLaMA와 Falcon 모델에서 뛰어난 성능을 보여주었으며, 기존 양자화 방법보다 더 효율적이다. 이 연구는 LLM의 실용성을 크게 향상시킬 수 있는 중요한 기여를 한다.