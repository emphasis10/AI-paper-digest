# OpenVid-1M: A Large-Scale High-Quality Dataset for Text-to-video Generation
## TL;DR
## Summary
- [https://arxiv.org/pdf/2407.02371.pdf](https://arxiv.org/pdf/2407.02371.pdf)

### 섹션별 요약 및 주요 기여 내용

#### Abstract
**요약:** 
문장은 OpenVid-1M이라는 고품질 데이터셋을 소개하고, 텍스트-영상(T2V) 생성에서의 주요 두 가지 문제를 해결하려고 한다고 설명합니다. 첫 번째는 정밀한 고품질 데이터셋의 부족이며, 두 번째는 텍스트 정보의 충분한 활용 부족입니다.

**주요 기여 내용:**
1. OpenVid-1M이라는 100만 개 이상의 고해상도 동영상과 텍스트 페어를 가진 데이터셋을 소개.
2. 다중 모달 동영상 확산 트랜스포머(MVDiT)를 제안.
3. 광범위한 실험과 비교 실험을 통해 데이터셋과 모델의 효과를 입증.


#### Introduction
**요약:** 
텍스트-영상(T2V) 생성은 텍스트 설명을 기반으로 비디오 시퀀스를 생성하는 기술로 주목받고 있습니다. 그러나 이전의 데이터셋은 낮은 품질이거나 너무 커서 연구 기관에서 사용하기 어렵다는 문제와 텍스트 정보를 충분히 활용하지 않는 모델 디자인의 문제를 가지고 있습니다.

**주요 기여 내용:**
1. OpenVid-1M 데이터셋의 필요성과 이에 따른 주요 기여를 설명.
2. OpenVid-1M 데이터셋의 특성과 품질, 그리고 고해상도 동영상 생성을 위한 OpenVidHD-0.4M의 소개.
3. MVDiT 모델을 통해 동영상 품질과 텍스트-영상 일치를 개선.


#### Related Work
**요약:** 
텍스트-영상 생성에 사용되는 데이터셋과 모델들에 대해 논의. 기존 데이터셋은 특정 시나리오에 국한되거나 낮은 품질이라는 한계를 가지며, 모델 디자인은 유넷 기반과 디퓨전 기반 두 가지로 나뉩니다.

**주요 기여 내용:**
1. 기존 텍스트-영상 훈련 데이터셋과의 비교 설명.
2. 기존 모델의 디자인과 최근 연구 동향 소개.


#### Methodology
**요약:** 
MVDiT 모델의 구조와 기능 소개. 시각적 토큰과 텍스트 토큰을 동시에 처리하여 텍스트와 생성 영상 간의 일치를 높임. 다중 모달 자기 주의 모듈(MMSA), 다중 모달 시간 주의 모듈(MMTA), 다중 헤드 교차 주의 모듈(CAL) 등을 포함.

**주요 기여 내용:**
1. MVDiT의 다중 모달 자체 주의 모듈과 시간 주의 모듈을 통한 템포럴 일관성 향상.
2. 다중 헤드 교차 주의 모듈을 통한 텍스트 정보와 시각적 정보의 직접적 통합.


#### Experiments
**요약:** 
OpenVid-1M 데이터셋과 MVDiT 모델의 성능 실험 결과 분석. 다양한 성능 지표를 사용하여 영상 품질, 텍스트-영상 일치도 및 템포럴 일관성을 평가함.

**주요 기여 내용:**
1. 고해상도 및 고품질 데이터셋이 성능 향상에 기여함을 입증.
2. MVDiT 모델이 기존 모델보다 뛰어난 성능을 발휘함을 확인.
3. 데이터 처리 단계별 효과 분석을 통해 가장 높은 성능을 달성.


#### Conclusion
**요약:** 
OpenVid-1M과 MVDiT의 주요 성과를 요약하며, 향후 연구 방향으로 물리적 세계 모델링의 한계를 극복하는 과제 제시.

**주요 기여 내용:**
1. 고해상도 동영상 데이터셋의 필요성과 이를 통해 텍스트-영상 생성 연구의 진전을 도모.
2. MVDiT 모델의 효과적인 성능 입증으로 텍스트-영상 생성 모델의 진화를 촉진.
3. 데이터셋과 모델의 한계와 미래 연구 방향 제시.


### 전체 요약

이 논문은 텍스트-영상(T2V) 생성 분야에서 큰 도약을 이루기 위해 제안된 OpenVid-1M 데이터셋과 Multi-modal Video Diffusion Transformer(MVDiT) 모델을 다룹니다. OpenVid-1M은 100만 개 이상의 고해상도 동영상과 자세한 설명을 담고 있어, 기존 데이터셋이 가진 품질 문제를 해결합니다. MVDiT는 시각적 토큰과 텍스트 토큰의 병렬 처리를 통해 텍스트와 생성된 비디오 간의 일관성을 높입니다. 실험 결과, OpenVid-1M과 MVDiT는 기존 모델 및 데이터셋보다 우수한 성능을 보였으며, 특히 영상 품질과 텍스트-영상 일치도에서 큰 향상을 보였습니다. 논문은 또한 향후 연구에서 물리적 세계 모델링의 한계를 해결할 방향을 제시합니다.

## Similar Papers
- [VIMI: Grounding Video Generation through Multi-modal Instruction](2407.06304.md)
- [MiraData: A Large-Scale Video Dataset with Long Durations and Structured Captions](2407.06358.md)
- [VidGen-1M: A Large-Scale Dataset for Text-to-video Generation](2408.02629.md)
- [Openstory++: A Large-scale Dataset and Benchmark for Instance-aware Open-domain Visual Storytelling](2408.03695.md)
- [StoryDiffusion: Consistent Self-Attention for Long-Range Image and Video Generation](2405.01434.md)
- [CamCo: Camera-Controllable 3D-Consistent Image-to-Video Generation](2406.02509.md)
- [EasyAnimate: A High-Performance Long Video Generation Method based on Transformer Architecture](2405.18991.md)
- [Noise Calibration: Plug-and-play Content-Preserving Video Enhancement using Pre-trained Video Diffusion Models](2407.10285.md)
- [CamViG: Camera Aware Image-to-Video Generation with Multimodal Transformers](2405.13195.md)
