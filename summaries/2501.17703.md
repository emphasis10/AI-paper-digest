# Critique Fine-Tuning: Learning to Critique is More Effective than Learning to Imitate
## TL;DR
## Summary
- [https://arxiv.org/pdf/2501.17703.pdf](https://arxiv.org/pdf/2501.17703.pdf)

1. **중요 내용 요약**  
   - **서론**: 이 논문은 기존의 감독된 미세 조정(Supervised Fine-Tuning, SFT) 접근 방식을 도전하고, 비슷한 학습을 넘어서 비판 기반 학습인 비판 미세 조정(Critique Fine-Tuning, CFT)을 제안합니다. CFT는 학습자가 단순 모방이 아닌, 비판적 사고를 통해 학습할 수 있도록 합니다. 
   
   - **방법론 및 데이터셋**: CFT의 효과를 검증하기 위해, 웹에서 수집한 교육 자료를 기초로 한 50K 샘플 비판 데이터셋(WebInstruct)을 생성했습니다. 이 데이터셋을 활용해 CFT를 적용하여 수학적 문제 해결 능력을 포함한 여러 과제를 평가하였습니다.
   
   - **결과**: CFT 모델은 기존 SFT 모델들보다 4-10% 향상된 수학적 성능을 보여주었으며, 적은 데이터로도 강력한 결과를 도출해 냈습니다. 예를 들어, CFT로 훈련된 Qwen2.5-Math-CFT 모델은 50K 예제로 강력한 성능을 달성하여 2M 이상의 데이터를 사용하는 경쟁 모델들보다 뛰어난 성능을 보였습니다.
   
   - **비교**: CFT 모델은 전통적인 강화 학습(Reinforcement Learning, RL) 기반 모델들과 비교할 때, 140배 이상의 컴퓨팅 자원을 사용하지 않고도 비슷한 성능에 도달했습니다. 
   
   - **결론**: CFT는 기존 SFT 방식보다 데이터 효율성이 뛰어나며, 언어 모델의 추론 능력을 향상할 수 있는 더 효율적인 접근법으로 자리잡았습니다. 또한, 이 모델은 수학적 문제 해결뿐만 아니라 STEM 분야의 다양한 과제에서도 우수한 성능을 보여줍니다.

2. **전체 요약**  
   이 논문에서는 비판 미세 조정(CFT)이라는 새로운 학습 접근법을 제안하며, 이는 기존의 감독된 미세 조정(SFT)에 비해 더 효율적이고 효과적인 방법임을 보여줍니다. CFT는 모델이 단순히 정답을 모방하는 것이 아닌, 주어진 응답을 비판하고 분석함으로써 더 깊은 이해와 향상된 추론 능력을 발전시킬 수 있도록 돕습니다. 실험 결과, CFT는 기존 SFT보다 4-10%의 성능 향상을 보였으며, 적은 양의 데이터(50K)를 사용하면서도 경쟁 모델들과 유사한 성과를 낼 수 있음을 확인하였습니다. 이러한 결과는 언어 모델 훈련의 효율성을 높이고, 데이터와 컴퓨팅 자원 요구 사항을 줄이는 데 기여할 것으로 기대됩니다.