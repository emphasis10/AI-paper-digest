# Long Code Arena: a Set of Benchmarks for Long-Context Code Models
## TL;DR
## Summary
- [https://arxiv.org/pdf/2406.11612.pdf](https://arxiv.org/pdf/2406.11612.pdf)

### 요약: 각 섹션의 주요 내용 및 논문 전체 요약

#### 1. 서론 (Introduction)
ML4SE(소프트웨어 공학을 위한 머신러닝) 분야는 최근 몇 년간 빠르게 발전했습니다. 그러나 대부분의 기존 벤치마크는 짧은 코드 조각이나 한 파일 내의 맥락만을 다룹니다. 이 논문은 풀 프로젝트 맥락을 기반으로 하는 Long Code Arena라는 새로운 벤치마크 세트를 소개합니다. 이를 통해 ML4SE 모델이 프로젝트 수준에서 긴 맥락을 처리할 수 있는지를 평가합니다  .

#### 2. Long Code Arena
Long Code Arena는 다음 여섯 개의 과제를 포함합니다: 
- 라이브러리 기반 코드 생성
- CI 빌드 수리
- 프로젝트 수준 코드 완성
- 커밋 메시지 생성
- 버그 위치 확인
- 모듈 요약

각 과제는 프로젝트 모듈 또는 전체 프로젝트의 정보를 사용해야 성공적으로 수행할 수 있습니다. 모든 데이터는 오픈 소스 리포지토리에서 수집되었으며, 엄격하게 필터링되고 수동으로 확인되어 높은 데이터 품질을 보장합니다 .

#### 3. 관련 작업 (Related Work)
기존 벤치마크는 대부분 짧은 맥락을 다루지만, Long Code Arena는 프로젝트 전체를 고려합니다. SWE-bench와 같은 최근 벤치마크는 실제 프로그래밍 프로젝트의 이슈 해결을 요구하며, Long Code Arena는 더 다양한 과제를 다룹니다 .

#### 4. 한계 및 미래 작업 (Limitations and Future Work)
현재 벤치마크는 오픈 소스 데이터를 기반으로 하므로 데이터 오염 가능성이 있습니다. 그러나 선택한 과제들은 모델이 훈련 데이터를 암기할 가능성이 낮습니다. 또한, 벤치마크의 일반화를 위해 더 많은 프로그래밍 언어로 데이터셋 확장 등의 추가 작업이 필요합니다  .

#### 5. 결론 (Conclusion)
Long Code Arena는 ML 기반 소프트웨어 엔지니어링의 현실적 과제를 해결하는 연구를 촉진하는 것을 목표로 합니다. 여섯 개의 벤치마크 과제를 신중하게 설계하고 평가 데이터와 기준 솔루션을 제공합니다. 본 논문은 프로젝트 수준의 맥락을 고려하여 ML4SE 모델의 성능을 평가하는 새로운 벤치마크입니다  .

### 논문의 전체 요약
본 논문은 소프트웨어 엔지니어링을 위한 머신러닝(Machine Learning for Software Engineering, ML4SE) 모델을 평가하기 위한 Long Code Arena라는 새로운 벤치마크 세트를 소개합니다. 대부분의 기존 ML4SE 벤치마크는 짧은 맥락만을 다루지만, Long Code Arena는 프로젝트 전체의 맥락을 고려합니다. 이 벤치마크는 여섯 개의 주요 과제를 포함하며, 각 과제는 오픈소스 리포지토리에서 수집된 데이터를 사용해 신중하게 설계되었습니다. Long Code Arena는 ML4SE와 NLP(자연어 처리) 커뮤니티의 연구자들이 ML 기반 소프트웨어 엔지니어링 분야를 발전시킬 수 있도록 돕기 위한 것입니다. 다양한 모델을 평가하고 벤치마크 데이터를 통한 추가 연구를 장려합니다  .