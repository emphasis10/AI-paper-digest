# Pairwise RM: Perform Best-of-N Sampling with Knockout Tournament
## TL;DR
## Summary
- [https://arxiv.org/pdf/2501.13007.pdf](https://arxiv.org/pdf/2501.13007.pdf)

### 1. 각 섹션 요약 및 주요 기여점

**1. 소개**
이 논문은 대형 언어 모델(LLM)의 테스트 시 확대를 위한 새로운 전략으로, 기존의 보상 모델이 임의적이고 일관성 없는 점수를 매겨 제한적이었던 문제를 해결하기 위해 '페어와이즈 보상 모델(Pairwise RM)'과 '녹아웃 토너먼트'를 제시합니다. 이 접근법은 수학 문제에 대해 두 개의 후보 해답을 동시에 평가하므로 임의의 점수를 피할 수 있습니다.

**2. 선수지**  
수학 문제 해결에서 사용되는 'Best-of-N 샘플링'과 기존 보상 모델의 한계를 설명하며, 결과 보상 모델과 과정 보상 모델의 차이점을 소개합니다.

**3. 페어와이즈 보상 모델 및 녹아웃 토너먼트**  
페어와이즈 RM은 두 후보 해답의 정답 여부를 동시에 확인하며, 이 과정에서 체인의 사고 방식을 사용해 전개합니다. 녹아웃 토너먼트는 후보를 비교해 최종 답안을 선택하는 과정으로, 문제 해결 과정에서 두 후보를 경쟁시킵니다.

**4. PAIRWISE-443K 데이터셋 수집**  
443,000개의 쌍별 비교 데이터셋인 PAIRWISE-443K 구축 과정을 설명합니다. 이 데이터셋은 수학문제 해결을 위한 후보의 정답 여부 비교 학습에 사용됩니다.

**5. 실험 설정 및 결과**  
MATH-500 및 Olympiad Bench 데이터셋을 사용해 BoN 샘플링을 평가했습니다. 페어와이즈 보상 모델은 DAsalami2023Princeubon의 RLHFlow-8B-Deepseek Data와 같은 최신 결과 보상 모델을 능가한다는 것을 보여줍니다. 특히 도전적인 문제에서 40% 이상 향상된 결과를 나타냅니다.

**6. 비평가 모델과의 비교**  
비평가 모델은 LLM의 계산 동안 생긴 응답의 질을 평가하는 또 다른 보상 모델입니다. 이 논문은 정답 검증 측면에서 페어와이즈 RM이 비평가 모델보다 더 효과적이라는 것을 보여줍니다.

**7. 결론**  
이 연구는 수학 문제 해결에서 보상 모델이 LLM에게 최적의 해답을 선택하도록 도울 수 있는 새로운 방법을 제시하며 나아가 다른 추론 과제에도 적용할 수 있음을 시사합니다.

### 2. 전체 요약
이 논문은 수학 문제 해결 과정에서 대형 언어 모델의 성능을 강화하기 위한 새로운 보상 모델인 '페어와이즈 보상 모델'을 제안합니다. 이 모델은 후보 해답을 쌍으로 비교해 녹아웃 방식으로 최적의 답을 찾는 방법을 제공하며, 443,000개의 데이터셋을 바탕으로 뛰어난 성능 개선을 입증합니다. 이는 다양한 추론 문제에 대한 적용 가능성을 열어두고 있으며, 기존 모델의 제한을 극복할 가능성을 보여줍니다.