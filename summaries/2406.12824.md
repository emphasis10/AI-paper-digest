# From RAGs to rich parameters: Probing how language models utilize external knowledge over parametric information for factual queries
## TL;DR
## Summary
- [https://arxiv.org/pdf/2406.12824.pdf](https://arxiv.org/pdf/2406.12824.pdf)

### 1. 섹션별 요약
#### Introduction (소개)
이 논문에서는 증가하는 산업적 활용 사례에서 대규모 언어 모델(LMs)의 활용을 위한 검색 증강 생성(RAG)의 필요성을 강조하며, 이 기술이 사실적 질문에 대한 추론 능력을 향상시키는 방법을 설명합니다. 연구 목표는 RAG가 LMs의 내부 지식에 비해 얼마나 유용한지를 탐구하는 것입니다. 저자들은 LMs가 내부의 파라메트릭 메모리를 거의 사용하지 않는다는 사실을 밝혔습니다.

#### Related Work (관련 연구)
RAG 시스템은 많은 자연어 처리 응용 프로그램에서 널리 사용되고 있으며, 외부 문맥을 통합하여 성능을 향상시키는 것으로 알려져 있습니다. 그러나 모델 내부 지식과 외부 문맥 간의 상호작용에 대한 체계적인 탐구는 거의 이루어지지 않았습니다.

#### Probing Mechanisms (탐침 메커니즘)
저자들은 원인 중재 분석, 주의 제거 및 기여 메커니즘 등 세 가지 기법을 사용하여 LLMs와 SLMs의 사실적 추론에 대한 지식 기여를 해부했습니다. 이 방법들은 모델의 내부 상태를 조사하여 사실적 예측을 도출하는 데 중요한 부분을 식별합니다.

#### Empirical Results (경험적 결과)
- **Finding 1**: 언어 모델은 문맥이 제공될 때 파라메트릭 메모리를 거의 사용하지 않습니다. 이를 통해 RAG를 사용하였을 때 LMs가 내부의 파라메트릭 지식보다는 외부 문맥에 크게 의존하는 것을 발견했습니다.
- **Finding 2**: 최종 토큰의 잔여 스트림은 쿼리의 주제 토큰보다는 문맥에서 더 많은 정보를 얻습니다. 이로 인해 RAG 문맥이 주제 토큰의 주의 기여보다 더 중요한 역할을 합니다.

#### Discussion and Conclusions (토론 및 결론)
이 논문은 세 가지 다른 탐침 기법을 사용하여 RAG 문맥이 어떻게 모델의 파라메트릭 지식을 보완할 수 있는지 분석한 첫 번째 연구입니다. 연구 결과는 RAG가 모델의 사실적 회상에 있어 모델의 내부 메모리의 중요성을 줄이는 데 기여한다고 결론짓습니다.

#### Limitations and Future Work (제한점 및 미래 연구)
이 연구의 제한 사항으로는 RAG 기반 문맥의 길이를 짧게 유지하는 데 따른 제한점이 있습니다. 더 긴 문맥과 관련된 연구는 미래 연구 과제입니다. 또한, 실제 데이터의 노이즈와 회수기 및 하이퍼파라미터의 품질에 민감한 retrieved outputs의 분석도 필요한 과제입니다.

### 2. 전체 요약
이 논문은 대규모 언어 모델(LMs)에서 검색 증강 생성(RAG)이 사실적 질문에 어떻게 영향을 미치는지에 대한 기계적인 이해를 제공하는 것을 목표로 합니다. 저자들은 LMs가 내부의 파라메트릭 메모리를 거의 사용하지 않고, 주로 제공된 RAG 문맥에 의존하여 질문에 답변한다는 사실을 발견했습니다. 이를 위해 원인 중재 분석, 주의 제거, 주의 기여 메커니즘을 활용하여 LMs와 SLMs의 사실적 추론 과정을 분석했습니다. 주요 발견은 RAG 문맥이 주제 토큰보다 중요한 정보를 제공하며, 이를 통해 LMs가 "shortcut" 메커니즘을 통해 외부 문맥을 내부 지식보다 우선시한다는 것입니다. 이 연구는 RAG 문맥이 모델의 사실적 회상에 있어 중요한 보완 역할을 한다는 결론을 내렸습니다. 앞으로는 더 긴 문맥과 실제 데이터의 노이즈 문제를 포함한 연구가 필요합니다.

## Similar Papers
- [Searching for Best Practices in Retrieval-Augmented Generation](2407.01219.md)
- [Estimating Knowledge in Large Language Models Without Generating a Single Token](2406.12673.md)
- [Luna: An Evaluation Foundation Model to Catch Language Model Hallucinations with High Accuracy and Low Cost](2406.00975.md)
- [Never Miss A Beat: An Efficient Recipe for Context Window Extension of Large Language Models with Consistent "Middle" Enhancement](2406.07138.md)
- [From Local to Global: A Graph RAG Approach to Query-Focused Summarization](2404.16130.md)
- [Time Sensitive Knowledge Editing through Efficient Finetuning](2406.04496.md)
- [Can Small Language Models Help Large Language Models Reason Better?: LM-Guided Chain-of-Thought](2404.03414.md)
- [Unsupervised Information Refinement Training of Large Language Models for Retrieval-Augmented Generation](2402.18150.md)
- [THEANINE: Revisiting Memory Management in Long-term Conversations with Timeline-augmented Response Generation](2406.10996.md)
