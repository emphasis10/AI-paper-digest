# FP8-LM: Training FP8 Large Language Models
## TL;DR
## Summary
- [https://arxiv.org/pdf/2310.18313.pdf](https://arxiv.org/pdf/2310.18313.pdf)

### 논문 요약

#### 1. Introduction (소개)

이 논문에서는 FP8 저비트 데이터 포맷을 사용하여 대형 언어 모델(LLM)을 효율적으로 훈련하는 방법을 탐구합니다. 주요 통찰력은 대부분의 변수(예: 기울기 및 옵티마이저 상태)가 낮은 정밀도 데이터 포맷을 사용해도 모델 정확도에 영향을 미치지 않는다는 것입니다. 이를 위해 FP8 자동 혼합 정밀도 프레임워크를 제안하여 LLM 훈련을 위해 세 가지 수준의 FP8 활용을 제공합니다. 실험 결과, FP8 혼합 정밀도 훈련 프레임워크는 메모리 사용량을 39% 줄이고, BF16 프레임워크보다 75% 빠르게 실행되었습니다.

#### 2. FP8 LLMs

FP8 혼합 정밀도 훈련 프레임워크는 FP8의 효율성을 최대한 활용하기 위해 기울기 및 옵티마이저 상태와 같은 요소를 낮은 정밀도로 처리합니다. FP8의 도입은 2배의 속도 향상, 50-75%의 메모리 비용 절감, 50-75%의 통신 비용 절감을 이끌어낼 수 있습니다. 하지만 FP8 훈련은 수치 불안정성과 같은 문제가 있어, 이를 해결하기 위해 정밀도 분리 및 자동 스케일링 기법을 제안합니다.

#### 3. Experiment

FP8 저비트 프레임워크의 유효성을 확인하기 위해 GPT 스타일 모델 훈련에 적용했습니다. 실험 결과, 메모리 사용량은 29-39% 감소하고, 통신 오버헤드는 63-65% 감소했습니다. 또한 FP8 혼합 정밀도는 모델 성능을 유지하면서 훈련 시간을 37% 단축했습니다.

#### 4. Related Work

혼합 정밀도 훈련은 현대 딥러닝에서 비용 절감을 위해 널리 사용됩니다. FP16과 BF16이 주로 사용되었으나, FP8은 새로운 저비트 데이터 타입으로 주목받고 있습니다. Nvidia의 Hopper GPU 아키텍처가 등장하면서 FP8의 실용성이 증가했습니다.

#### 5. Conclusion

FP8 혼합 정밀도 훈련 프레임워크는 대형 모델 훈련에 있어서 메모리와 통신 비용을 크게 절감할 수 있습니다. 향후 연구에서는 FP8 GPT 모델의 규모와 훈련 단계를 확장하고, FP8 스키마를 멀티모달 대형 모델 훈련에 적용할 계획입니다.

### 전체 요약

이 논문은 대형 언어 모델(LLM) 훈련을 위해 FP8 저비트 데이터 포맷을 사용하는 새로운 혼합 정밀도 프레임워크를 제안합니다. FP8을 사용하면 메모리 사용량과 통신 비용을 크게 줄이면서도 모델 성능을 유지할 수 있습니다. 주요 기여는 FP8 기울기, 옵티마이저 상태, 분산 훈련을 점진적으로 통합하는 것입니다. 실험 결과는 FP8이 기존 BF16 프레임워크보다 메모리 효율적이고 빠른 훈련이 가능함을 보여줍니다. 이 연구는 LLM 훈련의 비용 절감을 목표로 하며, 향후 다양한 모델과 훈련 설정에 FP8을 적용할 계획입니다.