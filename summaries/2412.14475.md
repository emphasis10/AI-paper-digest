# MegaPairs: Massive Data Synthesis For Universal Multimodal Retrieval
## TL;DR
## Summary
- [https://arxiv.org/pdf/2412.14475.pdf](https://arxiv.org/pdf/2412.14475.pdf)

1. 논문 섹션 요약:

   - **소개 (Introduction)**:
     이 논문은 다중 모달 검색(multi-modal retrieval)의 문제를 다룹니다. 글과 이미지 같은 다양한 데이터 모달리티에 대한 정보 요구를 충족시키는 것을 목표로 하며, 다양한 실제 상황에서 다중 모달 검색이 활용되고 있는 것을 강조합니다. 기존의 모델들이 가진 한계를 극복하기 위해 더 보편적인 다중 모달 검색기를 개발할 필요성을 제시합니다.

   - **MegaPairs 데이터 합성 방법**:
     MegaPairs라는 새로운 데이터 합성 방법을 소개합니다. 이 기법은 개방형 도메인 이미지와 공개 비전-언어 모델(VLMs)을 활용하여 대규모 합성 데이터를 생성합니다. 이러한 방법을 통해 높은 품질의 데이터를 생성하고, 이를 통해 70배 더 적은 데이터로도 기존 모델을 능가하는 성과를 거두었습니다.

   - **MMRet 모델**:
     MMRet라는 새로운 모델을 소개하며, 클립 기반과 MLLM 기반의 두 가지 비전-언어 모델 아키텍처를 통합하여 보편적 다중 모달 임베딩을 제공합니다. 이 모델은 다양한 다중 모달 검색 작업에서 탁월한 성능을 발휘합니다.

   - **다중 모달 대조 학습**:
     CLIP 및 MLLM 모델을 변형하여 다양한 다중 모달 검색 작업을 지원하는 MMRet 모델을 만들기 위해 다중 모달 대조 학습을 사용합니다.

   - **실험 결과 및 평가**:
     MegaPairs를 통해 생성된 데이터를 사용하여 MMRet 모델을 학습시킨 결과, 네 가지 주요 복합 이미지 검색 벤치마크 및 36개의 데이터셋에서 최상의 성과를 거두었습니다. 이 모델은 다운스트림 미세 조정에서도 우수한 성능을 보이며, 전반적으로 강력한 일반화 능력을 보여주었습니다.

2. 전체 요약:
   이 논문은 다중 모달 검색의 발전을 위해 MegaPairs라는 데이터 합성 방법과 MMRet 모델을 소개합니다. MegaPairs는 개방형 도메인 이미지와 세 가지 종류의 유사성 모델을 사용하여 고품질의 다양성과 확장성을 겸비한 데이터를 생성합니다. 이를 통해 26백만 개 이상의 새로운 데이터 인스턴스를 생성하고, MegaPairs에 기반한 MMRet 모델을 학습시켜 다양한 벤치마크에서 최상의 성과를 달성했습니다. 이 연구는 다중 모달 임베딩 작업의 진보와 관련 분야의 발전을 위해 데이터셋, 모델, 데이터 생산 파이프라인을 공개할 계획입니다.