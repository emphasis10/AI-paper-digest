# Friends-MMC: A Dataset for Multi-modal Multi-party Conversation Understanding
## TL;DR
## Summary
- [https://arxiv.org/pdf/2412.17295.pdf](https://arxiv.org/pdf/2412.17295.pdf)

1. **논문 각 섹션 요약:**

- **소개 및 데이터셋 설명:**
  이 논문은 다중 모달 다자대화(Multi-modal multi-party conversation, MMC)라는 새로운 연구 분야를 제안합니다. 이를 위해 "Friends-MMC"라는 데이터셋을 개발하였으며, 이는 TV 시리즈 Friends에서 대화를 수집한 것입니다. 이 데이터셋은 각 발언이 비디오 컨텍스트와 연결되어 있으며, 발언자 및 얼굴 인식 정보를 포함하고 있습니다.

- **모델 및 방법론:**
  논문은 시각, 오디오, 텍스트, 얼굴 트랙을 포함하여 다양한 모달리티를 통합하는 기본 방법론을 제시합니다. 이 메서드는 M1 (인셉션 모델, TalkNet)과 M2 (DeBERTa-v3) 모델과 같은 다양한 모듈로 구성되어 각각 얼굴 예측 및 텍스트 기반 발언자 판단 기능을 제공합니다.

- **실험 결과:**
  다양한 모달리티 조합을 통해 실험을 진행하였으며, 합리적인 결과를 얻었습니다. 특히, 본 논문은 다중 모달리티가 발언자 식별 정확도를 크게 향상시킬 수 있음을 보여줍니다. 시간대가 가까운 여러 회차를 세션으로 만들어 텍스트와 시각 정보를 기반으로 발언자를 예측하는 방법을 사용했습니다.

- **결론:**
  이 논문은 MMC 분야에서 실질적인 발전을 이루었으며, 다중 모달 다자대화의 연구 방향을 설정하는 데 기여하였습니다. 발언자 식별 및 대화 응답 예측이라는 두 가지 새로운 과제를 제시하고, 모델 성능을 검증하여 발언자 정보를 활용하는 이점을 분석했습니다.

2. **전체 요약:**
   이 논문은 다중 모달 다자대화의 연구를 위한 새로운 데이터셋인 "Friends-MMC"를 소개하며, 이 분야의 발전을 위한 이론적 및 실천적 기반을 마련했습니다. TV 시리즈 Friends를 기반으로 한 이 데이터셋은 다자 대화 환경에서, 발언자 식별과 대화 응답 예측을 개선하기 위한 실험들을 통해 그 가능성을 입증했습니다. 다자대화에서 발언자 정보의 중요성을 강조하고, 다중 모달리티 접근법이 제공하는 이점을 실험적으로 분석했습니다.