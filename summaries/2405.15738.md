# ConvLLaVA: Hierarchical Backbones as Visual Encoder for Large Multimodal Models
## TL;DR
## Summary
- [https://arxiv.org/pdf/2405.15738.pdf](https://arxiv.org/pdf/2405.15738.pdf)

### 섹션별 요약

#### 1. Introduction

최근 몇 년간 대형 멀티모달 모델(LMMs)은 이미지 및 비디오 이해, 디지털 에이전트 개발, 로봇공학 등의 다양한 도메인에서 놀라운 성과를 거두었습니다. 그러나 비전 트랜스포머(ViT)는 높은 해상도 작업에서 과도한 시각 토큰과 이차적인 시각 복잡성 문제를 겪고 있습니다. ConvLLaVA는 이러한 문제를 해결하기 위해 ConvNeXt라는 계층적 백본을 사용하여 시각 정보를 압축함으로써 과도한 시각 토큰 생성을 방지합니다.

#### 2. Related Work

기존의 연구들은 ViT의 시각 복잡성 문제를 해결하기 위해 다양한 방법을 제안했지만, 시각 토큰의 중복성을 해결하지 못했습니다. 반면, ConvNeXt는 시각 정보를 압축하여 효율성을 높이며, 임의의 해상도와 비율의 이미지를 처리할 수 있습니다. 이를 통해 ConvLLaVA는 기존 모델보다 우수한 성능을 발휘합니다.

#### 3. ConvLLaVA

ConvLLaVA는 5단계 ConvNeXt를 사용하여 고해상도 이미지를 정보가 풍부한 시각 특징으로 압축합니다. 이를 통해 시각 토큰의 중복성을 줄이고, 다양한 해상도와 비율의 이미지를 효율적으로 처리할 수 있습니다. ConvLLaVA는 최신 벤치마크에서 우수한 성능을 보였으며, 기존 모델보다 성능이 향상되었습니다.

##### 3.1 ConvNeXt as Standalone Visual Encoder
ConvNeXt는 비전 트랜스포머(ViT)보다 시각 복잡성이 낮고, 시각 토큰의 수를 줄여 언어 모델의 계산 부하를 줄입니다. 또한, ConvNeXt는 선형 시각 복잡성과 정보 압축 절차를 통해 고해상도 입력을 효과적으로 처리합니다.

##### 3.2 Updating ConvNeXt is Essential
ConvNeXt를 업데이트하면 일반 벤치마크에서 ViT와 유사한 성능을 발휘하며, 세부적인 벤치마크에서는 더 나은 성능을 보입니다. 이는 ConvNeXt가 높은 해상도에서도 세부 정보를 더 많이 포함하고 있음을 나타냅니다.

##### 3.3 Training with Stage 5 Scales up Resolution to 1536
ConvNeXt의 5단계를 추가하여 시각 정보를 더 많이 압축함으로써 높은 해상도를 처리할 수 있습니다. 이를 통해 1536 해상도에서도 효율적인 처리와 성능 향상을 달성합니다.

#### 4. Experiments

ConvLLaVA는 최신 벤치마크에서 경쟁력 있는 성능을 보였으며, 특히 고해상도 입력에서 더 높은 성능을 발휘합니다. 실험 결과, ConvLLaVA는 문서 인식(OCR) 및 기타 세부적인 작업에서 우수한 성능을 보였습니다.

#### 5. Conclusion

ConvLLaVA는 높은 해상도의 시각 정보를 효율적으로 처리할 수 있는 계층적 백본을 사용하여 시각 토큰의 중복성을 줄였습니다. 이를 통해 다양한 해상도와 비율의 이미지를 효율적으로 처리할 수 있으며, 최신 벤치마크에서 우수한 성능을 발휘했습니다.

### 전체 요약

이 논문은 ConvLLaVA라는 대형 멀티모달 모델을 제안하며, 이는 ConvNeXt라는 계층적 백본을 사용하여 시각 정보를 압축함으로써 기존 비전 트랜스포머(ViT)의 문제를 해결합니다. ConvLLaVA는 시각 토큰의 중복성을 줄여 계산 부하를 감소시키고, 다양한 해상도와 비율의 이미지를 효율적으로 처리할 수 있습니다. 실험 결과, ConvLLaVA는 최신 벤치마크에서 우수한 성능을 발휘하였으며, 특히 문서 인식(OCR) 및 세부적인 작업에서 탁월한 성과를 보였습니다. 이를 통해 ConvLLaVA는 대형 멀티모달 모델의 시각 정보 처리에 있어 중요한 진전을 이루었습니다.

## Similar Papers
- [EVLM: An Efficient Vision-Language Model for Visual Understanding](2407.14177.md)
- [ML-Mamba: Efficient Multi-Modal Large Language Model Utilizing Mamba-2](2407.19832.md)
- [MG-LLaVA: Towards Multi-Granularity Visual Instruction Tuning](2406.17770.md)
- [Xmodel-VLM: A Simple Baseline for Multimodal Vision Language Model](2405.09215.md)
- [Beyond LLaVA-HD: Diving into High-Resolution Large Multimodal Models](2406.08487.md)
- [Improved Baselines with Visual Instruction Tuning](2310.03744.md)
- [MiniGPT4-Video: Advancing Multimodal LLMs for Video Understanding with Interleaved Visual-Textual Tokens](2404.03413.md)
- [INF-LLaVA: Dual-perspective Perception for High-Resolution Multimodal Large Language Model](2407.16198.md)
- [LLaVA-PruMerge: Adaptive Token Reduction for Efficient Large Multimodal Models](2403.15388.md)
