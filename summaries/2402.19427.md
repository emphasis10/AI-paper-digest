# Griffin: Mixing Gated Linear Recurrences with Local Attention for Efficient Language Models
## TL;DR
## Summary
- [https://arxiv.org/pdf/2402.19427.pdf](https://arxiv.org/pdf/2402.19427.pdf)

이 논문은 기존 RNN 모델의 한계를 극복하고자 하는 새로운 접근법인 "Hawk"와 "Griffin"을 제안하고 있습니다. 이 두 모델은 게이트가 있는 선형 순환 유닛과 지역적 주의 기능을 혼합하여, 효율적인 언어 모델을 구축하는 것을 목표로 합니다.

**요약:**
1. **서론**: RNN이 초기 딥러닝과 NLP 연구에서 중요한 역할을 했으나, 트랜스포머 아키텍처에 의해 주류에서 밀려났습니다. 그러나 RNN은 긴 시퀀스에 대해 효율적으로 확장할 수 있는 잠재력을 가지고 있습니다.
2. **모델 아키텍처**: 'Hawk'는 게이트가 있는 선형 순환 블록을 사용하며, 'Griffin'은 이를 지역적 주의 기능과 혼합한 하이브리드 모델입니다. 이들은 모두 성능 향상과 효율적인 분산 훈련을 위해 설계되었습니다.
3. **훈련과 추론 성능**: 'Hawk'와 'Griffin'은 트랜스포머 모델과 유사하거나 더 나은 성능을 보이면서도, 더 적은 데이터로 훈련됩니다. 추론 시에는 더 낮은 지연시간과 더 높은 처리량을 달성합니다.
4. **긴 문맥 모델링**: 이 모델들은 훈련보다 긴 시퀀스에서도 효과적으로 작동하며, 복사 및 검색 작업에 뛰어난 성능을 보입니다.

**혁신적인 부분:**
- 'Hawk'와 'Griffin'은 기존 RNN과 트랜스포머의 장점을 결합하여 긴 시퀀스와 대규모 언어 모델링 문제에 효과적으로 대응합니다.
- 이들 모델은 특히 긴 문맥에서의 정보 처리 능력이 뛰어나며, 효율적인 하드웨어 활용으로 높은 처리량과 낮은 지연시간을 제공합니다.

**결론:**
이 논문은 RNN과 트랜스포머의 장점을 통합한 새로운 접근 방식을 통해 언어 모델의 효율성과 확장성을 크게 향상시키고 있습니다. 이는 향후 긴 시퀀스를 다루는 다양한 NLP 작업에 중요한 기여를 할 것입니다.

## Similar Papers
- [Mamba: Linear-Time Sequence Modeling with Selective State Spaces](2312.00752.md)
- [Better & Faster Large Language Models via Multi-token Prediction](2404.19737.md)
- [HGRN2: Gated Linear RNNs with State Expansion](2404.07904.md)
- [Contextual Position Encoding: Learning to Count What's Important](2405.18719.md)
- [RecurrentGemma: Moving Past Transformers for Efficient Open Language Models](2404.07839.md)
- [You Only Cache Once: Decoder-Decoder Architectures for Language Models](2405.05254.md)
- [GoldFinch: High Performance RWKV/Transformer Hybrid with Linear Pre-Fill and Extreme KV-Cache Compression](2407.12077.md)
- [Layer-Condensed KV Cache for Efficient Inference of Large Language Models](2405.10637.md)
- [Megalodon: Efficient LLM Pretraining and Inference with Unlimited Context Length](2404.08801.md)
