# Language Models are Hidden Reasoners: Unlocking Latent Reasoning Capabilities via Self-Rewarding
## TL;DR
## Summary
- [https://arxiv.org/pdf/2411.04282.pdf](https://arxiv.org/pdf/2411.04282.pdf)

### 1. 각 섹션의 요약

- **서론(Introduction)**
  LaTent Reasoning Optimization(LatRO)은 기존의 LLMs(대규모 언어 모델)의 한계를 극복하고자 하는 방법으로, 언어 모델의 추론 능력을 동시에 증진시키고 평가할 수 있게 해주는 메커니즘을 제안합니다. 이는 외부 피드백이나 보상 모델 없이 가능하며, 여러 모델 아키텍처와 추론 작업에서 성과를 거두고 있습니다.

- **관련 연구(Related Work)**
  기존 연구에서는 사소한 문제 해결을 위해 Chain-of-Thought(CoT) 같은 방법이 도입되었습니다. 하지만, LatRO는 더욱 공고한 이론적 기반을 가지며, 이러한 틀 내에서 자체 보상 메커니즘을 사용하여 모델의 추론 품질을 개선하는 접근을 보입니다.

- **방법론(Methodology)**
  LatRO는 잠재 분포에서의 샘플링을 통해 추론을 공식화하고, 변이 프레임워크를 통해 이를 최적화합니다. 이 방법론은 추가적인 특별한 예시나 보상 모델이 필요하지 않으며, 모델이 추론 가능성을 최적화하기 위한 매우 체계적인 접근법입니다.

- **실험결과(Results)**
  실험을 통해 LatRO는 다른 모델들과 비교할 때 뛰어난 성능을 보이며, 특히 여러 추론 작업에서 두드러진 성과를 기록하였습니다. GSM8K 데이터셋에서도 높은 정확도를 보였으며, Phi-3.5, Mistral-7B 등의 모델과 비교했을 때 평균 12.5% 정도 성능 향상을 나타냈습니다.

- **논의 및 한계(Discussion and Limitations)**
  LatRO는 매우 유망한 결과를 보여주었으나, 초대형 모델의 경우 이 방식의 컴퓨팅 비용이 상당할 수 있는 한계가 있습니다. 향후 연구 방향은 이러한 비용을 줄이는 효율적 샘플링 기술을 탐구하는 것입니다.

### 2. 전체 요약

라탠트 추론 최적화(LatRO)는 언어 모델의 내재된 잠재 추론 능력을 발휘시키고 여러 난해한 문제 해결에 본질적인 추론 역량을 강화하는 데 중점을 둡니다. 이 방법은 외부 리소스 없이 자체 보상 메커니즘을 통해 학습할 수 있도록 설계되었으며, 대규모 언어 모델들 간의 다양한 아키텍처와 조건에서 일관되게 성능을 향상시킵니다. LatRO는 기존의 표준적 학습 방식과는 달리, 추론 과정을 비약적으로 최적화하였습니다. 