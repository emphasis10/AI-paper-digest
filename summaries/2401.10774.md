# Medusa: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads
## TL;DR
## Summary
- [https://arxiv.org/pdf/2401.10774.pdf](https://arxiv.org/pdf/2401.10774.pdf)

이 논문은 "MEDUSA: 간단한 LLM 추론 가속 프레임워크와 다중 디코딩 헤드"에 대한 연구로, 대규모 언어 모델(LLM)의 추론 과정에서 자동 회귀 디코딩 과정의 병렬성 부재로 인해 발생하는 한계를 극복하기 위한 새로운 접근법을 제안합니다. MEDUSA는 추가적인 디코딩 헤드를 도입하여 복수의 후속 토큰을 동시에 예측할 수 있도록 함으로써, 각 디코딩 단계에서 다수의 후보 연속을 동시에 검증할 수 있는 트리 기반 주의 메커니즘을 사용합니다. 이 방법은 병렬 처리를 활용하여 단일 단계의 지연 시간에 최소한의 오버헤드만을 도입하면서 필요한 디코딩 단계 수를 크게 줄일 수 있습니다.

### 1. 소개

- 대규모 언어 모델의 성능은 모델 크기가 커질수록 향상되지만, 추론 지연 시간도 증가하는 문제가 있습니다. 이는 LLM 추론이 메모리 대역폭에 제한되기 때문입니다. 이 문제를 해결하기 위해 다중 디코딩 헤드를 사용하여 추론 속도를 높이는 새로운 방법을 제시합니다.

### 2. 관련 연구

- LLM 추론 가속화와 샘플링 스키마에 관한 기존 연구를 소개하며, 본 연구의 차별점을 설명합니다.

### 3. MEDUSA

- **주요 구성 요소**: MEDUSA는 추가적인 디코딩 헤드와 트리 기반 주의 메커니즘을 도입합니다.
- **훈련 전략**: MEDUSA-1(고정된 백본 모델)과 MEDUSA-2(백본 모델과 함께 학습)의 두 가지 미세 조정 절차를 제안합니다.
- **확장**: 일반적인 수용 스키마, 자체 증류, 최적화된 트리 구성 탐색 등의 확장 기능을 소개합니다.

### 4. 실험

- 다양한 크기의 모델과 훈련 절차에 대한 MEDUSA의 성능을 평가합니다. MEDUSA-1은 생성 품질을 손상시키지 않으면서 2.2배 이상의 속도 향상을 달성하고, MEDUSA-2는 속도를 2.3-3.6배까지 추가로 향상시킵니다.

### 5. 토론 및 결론

- MEDUSA는 추가적인 디코딩 헤드를 통해 LLM의 추론 속도를 획기적으로 향상시키는 동시에 생성 품질을 유지하거나 개선할 수 있는 유망한 방법입니다. 이 연구는 향후 대규모 언어 모델을 더 효율적으로 활용하는 방법에 대한 새로운 방향을 제시합니다.

### 전반적 요약

MEDUSA는 대규모 언어 모델의 추론 과정을 가속화하기 위한 새로운 프레임워크입니다. 추가적인 디코딩 헤드와 트리 기반 주의 메커니즘을 통해 복수의 토큰을 동시에 예측함으로써 추론 속도를 크게 향상시킬 수 있습니다. 이는 효율적인 추론 속도 향상과 함께 생성 품질을 유지하거나 개선할 수 있는 강력한 방법을 제공합니다.