# Characterizing Prompt Compression Methods for Long Context Inference
## TL;DR
## Summary
- [https://arxiv.org/pdf/2407.08892.pdf](https://arxiv.org/pdf/2407.08892.pdf)

### 요약: Characterizing Prompt Compression Methods for Long Context Inference

1. **서론 (Introduction)**:
   현대 대규모 언어 모델(LLM)은 긴 문맥을 처리하는데 많은 컴퓨팅 자원과 메모리를 요구합니다. 이러한 문제를 해결하기 위해 여러 압축 방법들이 제안되었으나, 다양한 태스크에 걸쳐 표준화된 분석이 부족하여 일관되지 않은 결과가 발생했습니다. 이 논문에서는 여러 프롬프트 압축 방법들(추출적 압축, 요약 기반 압축, 토큰 삭제법)을 종합적으로 평가하고 비교했습니다. 이 연구의 주요 결과는 추출적 압축이 다른 방법들보다 우수하며, 최대 10배 압축 시에도 정확도 저하가 최소화된다는 것입니다.

2. **관련 연구 (Related Work)**:
   기존 연구에서는 LLM의 길어진 문맥 창과 그로 인한 시스템적 과제들을 다뤘으며, 특히 드문 간격의 압축과 양자화 기술을 통해 효율성을 높이고자 했습니다. 여러 가지 프롬프트 압축 기법이 제안되었으나, 이 연구에서는 그 다양한 기법들을 표준화된 방식으로 분류하고 평가하였습니다.

3. **프롬프트 압축 방법론 (Prompt Compression Methodologies)**:
   - **추출적 압축 (Extractive Compression)**: 주어진 문서에서 큰 문장 단위로 중요한 부분을 추출하는 방식으로, 일반적으로 가장 강력한 성능을 보였습니다. 특히 언어 모델이 문법 구조를 유지하면서 클러스터 단위의 압축을 수행할 수 있게 합니다.
   - **요약 기반 압축 (Abstractive Compression)**: 모델이 요약을 생성하는 방식으로, 종종 정보 유실이나 사실 왜곡이 발생할 수 있습니다. 특히 요약 작업에서 이 방식은 다른 방법보다 성능이 낮았습니다.
   - **토큰 삭제법 (Token Pruning)**: 덜 중요한 토큰을 제거하는 방식으로, 일반적으로 문법 및 문장 이해 방해로 인해 성능이 저하되는 경향이 있습니다. 그러나 특정 데이터셋(예: GovReport 및 QMSum)에서는 더 높은 압축률에서도 유의미한 성능을 보이기도 했습니다.

4. **실험 결과 (Experimental Results)**:
   다양한 태스크에 걸쳐 각 방법론의 성능을 비교했을 때, 추출적 압축 방법이 전반적으로 가장 우수한 결과를 나타냈습니다. 예를 들어, 2WikiMultihopQA 데이터셋에서 7.75배 압축 후 정확도가 7.89포인트 증가했습니다. 반대로 요약 기반 압축은 종종 중요한 정보를 누락하거나 왜곡할 가능성이 높아 성능이 떨어졌습니다.

5. **결론 (Conclusion)**:
   이 연구에서는 다양한 프롬프트 압축 방법을 비교 평가하여 추출적 압축이 가장 효과적이라는 결론을 도출했습니다. 이는 긴 문맥 추론이 필요한 시스템에서 최대 10배 압축률을 달성하면서도 정확도 저하를 최소화할 수 있음을 의미합니다. 

   추가적으로, 향후 연구 방향으로는 특정 애플리케이션에 최적화된 토큰 삭제 방법 개발 및 다른 언어에 대한 프롬프트 압축 방법의 차이 분석이 제안되었습니다.

---

### 전체 요약:

이 논문은 긴 문맥 추론을 위해 다양한 프롬프트 압축 방법을 비교 및 평가한 연구입니다. 추출적 압축, 요약 기반 압축, 토큰 삭제법을 종합적으로 분석한 결과, 추출적 압축이 정보 유실 없이 최대 10배까지 압축률을 높이면서도 가장 정확도 높은 성능을 보였습니다. 본 연구는 추출적 압축이 긴 문맥 추론 시스템에 가장 효과적임을 강조하며, 향후 연구 방향으로 애플리케이션 특화 토큰 삭제 방법 개발 및 다양한 언어에 대한 분석을 제안하고 있습니다.

본 분석은 AI와 머신러닝 시스템의 프롬프트 효율성을 극대화하는 데 기여할 수 있는 중요한 정보를 제공합니다.

## Similar Papers
- [LLoCO: Learning Long Contexts Offline](2404.07979.md)
- [LLMLingua-2: Data Distillation for Efficient and Faithful Task-Agnostic Prompt Compression](2403.12968.md)
- [NeedleBench: Can LLMs Do Retrieval and Reasoning in 1 Million Context Window?](2407.11963.md)
- [Finch: Prompt-guided Key-Value Cache Compression](2408.00167.md)
- [An LLM Compiler for Parallel Function Calling](2312.04511.md)
- [Context Embeddings for Efficient Answer Generation in RAG](2407.09252.md)
- [SampleAttention: Near-Lossless Acceleration of Long Context LLM Inference with Adaptive Structured Sparse Attention](2406.15486.md)
- [Compressing LLMs: The Truth is Rarely Pure and Never Simple](2310.01382.md)
- [SPEED: Speculative Pipelined Execution for Efficient Decoding](2310.12072.md)
