# Efficient World Models with Context-Aware Tokenization
## TL;DR
## Summary
- [https://arxiv.org/pdf/2406.19320.pdf](https://arxiv.org/pdf/2406.19320.pdf)

### 1. 섹션별 주요 내용 요약

#### 1.1. 소개 (Introduction)
심층 강화 학습(RL) 방법이 최근 몇 년 동안 전통적인 벤치마크에서 인상적인 결과를 보여주었으며, 복잡한 환경에서 에이전트를 훈련하는 데 더 높은 데이터 요구사항과 모델의 표현력을 요구합니다. 이러한 문제를 해결하기 위해 모델 기반 강화 학습(MBRL)이 제안되었고, 이는 RL 문제를 생성 모델링 문제로 전환시켜 정책 훈련을 크게 단순화합니다.

#### 1.2. 관련 배경: IRIS
IRIS 에이전트는 이미지를 토큰으로 변환하는 이산 오토인코더와 시퀀스 모델링 문제로 다이나믹스를 학습하는 자동회귀 트랜스포머로 구성된 세계 모델을 사용합니다. 그러나 IRIS는 시각적으로 복잡한 환경에서 많은 수의 토큰을 요구하여 확장에 한계를 보입니다.

#### 1.3. 제안된 방법: ∆-IRIS
∆-IRIS는 시간 단계 사이의 확률적 변화(델타)를 인코딩하는 이산 오토인코더와 현재 상태를 연속적인 토큰으로 요약하여 미래의 델타를 예측하는 자동회귀 트랜스포머로 구성된 새로운 에이전트입니다. 이를 통해 ∆-IRIS는 토큰 수를 크게 줄이며, 학습 속도를 기존 방법보다 10배 향상시킵니다.

#### 2. 방법론 (Method)
∆-IRIS는 부분 관측 마르코프 의사 결정 프로세스(POMDP)를 사용하여 이미지를 토큰으로 변환하고, 이 토큰을 기반으로 미래 상태를 예측합니다. 이를 통해 환경의 확률적 및 결정론적 역학을 분리하여 더 효율적인 학습을 가능하게 합니다.

#### 3. 실험 (Experiments)
Crafter 벤치마크와 Atari 100k 게임을 통해 ∆-IRIS의 성능을 평가합니다. 실험 결과, ∆-IRIS는 복잡한 환경에서 높은 프레임 예산에도 불구하고 뛰어난 학습 효율성과 속도를 보였습니다.

#### 4. 관련 연구 (Related Work)
이 연구는 이전의 세계 모델 및 상상 기반 학습 방법을 발전시킨 것으로, 다양한 환경에서 에이전트가 효율적으로 학습할 수 있도록 돕습니다. 특히, Dreamer 시리즈와 마찬가지로, ∆-IRIS는 복잡한 환경에서 더 강력한 성능을 발휘합니다.

#### 5. 결론 (Conclusions)
∆-IRIS는 확률적 역학과 결정론적 역학을 분리하여 복잡한 환경에서도 효율적으로 학습할 수 있는 새로운 에이전트를 제안합니다. 향후 연구에서는 현재 상황에 따라 다양한 수의 토큰을 동적으로 예측하여 성능을 더욱 향상시킬 계획입니다.

### 2. 전체 요약

이 논문에서는 강화 학습 에이전트의 학습 효율성을 높이기 위해 제안된 ∆-IRIS 모델에 대해 설명합니다. ∆-IRIS는 시퀀스 모델링과 이산 오토인코더를 사용하여 시간 단계 사이의 확률적 변화를 인코딩하고, 미래의 상태를 예측하는 새로운 접근 방식을 도입했습니다. 이로 인해 ∆-IRIS는 기존 방법보다 더 적은 토큰으로 더 정확한 환경 모델을 구축할 수 있으며, 학습 속도도 개선되었습니다. 실험 결과를 통해, 제안된 모델이 복잡한 환경에서 높은 성능을 발휘함을 확인하였으며, 향후 동적 토큰 예측을 통한 성능 향상이 기대됩니다.