# Grounding DINO 1.5: Advance the "Edge" of Open-Set Object Detection
## TL;DR
## Summary
- [https://arxiv.org/pdf/2405.10300.pdf](https://arxiv.org/pdf/2405.10300.pdf)

### 주요 내용 요약

1. **서론 및 배경**:
   - 이 논문은 **Grounding DINO 1.5**라는 새로운 개방형 객체 탐지 모델 세트를 소개합니다. 이 모델은 강력한 일반화 능력을 가진 **Grounding DINO 1.5 Pro**와 엣지 디바이스에서 빠른 속도를 위해 최적화된 **Grounding DINO 1.5 Edge**로 구성되어 있습니다. Grounding DINO 1.5 Pro는 모델 아키텍처를 확장하고, 향상된 비전 백본을 통합하며, 2000만 개 이상의 이미지로 훈련되어 풍부한 의미 이해를 제공합니다. Grounding DINO 1.5 Edge는 기능 규모를 줄이고 엣지 디바이스의 요구 사항에 맞게 최적화되었습니다.

2. **방법론**:
   - **모델 아키텍처**: Grounding DINO 1.5 시리즈는 이중 인코더-단일 디코더 구조를 유지하며, Pro 모델과 Edge 모델 각각에 최적화된 특성을 추가했습니다. Pro 모델은 ViT-L 아키텍처를 사용하여 깊은 초기 융합 전략을 통해 언어와 이미지 특징을 결합합니다. Edge 모델은 고수준 이미지 특징만을 사용하여 효율적인 특징 향상을 통해 엣지 디바이스에서 빠른 추론을 가능하게 합니다.
   - **훈련 데이터**: Grounding DINO 1.5는 Grounding-20M이라고 불리는 2000만 개의 이미지로 사전 훈련되었으며, 이 데이터는 공개된 소스에서 수집된 것입니다. 고품질의 주석을 보장하기 위해 일련의 주석 파이프라인과 후처리 규칙을 개발했습니다.

3. **실험**:
   - **성능 평가**: Grounding DINO 1.5 Pro는 COCO 및 LVIS 데이터셋에서 이전 모델을 능가하는 성능을 보여주었으며, COCO zero-shot 전이 벤치마크에서 54.3 AP를 기록했습니다. LVIS-minival 및 LVIS-val zero-shot 전이 벤치마크에서도 각각 55.7 AP와 47.6 AP를 기록하여 새로운 기록을 세웠습니다. Edge 모델은 TensorRT 최적화로 75.2 FPS의 속도를 달성했으며, LVIS-minival 벤치마크에서 36.2 AP를 기록했습니다.
   - **다운스트림 데이터셋 미세 조정**: Pro 모델을 다양한 다운스트림 데이터셋에서 미세 조정한 결과, LVIS-minival에서 68.1 AP, LVIS-val에서 63.5 AP를 기록하여 zero-shot 설정보다 각각 12.4 AP와 15.9 AP 향상된 성능을 보였습니다. ODinW35와 ODinW13 벤치마크에서도 새로운 기록을 세웠습니다.

4. **결론**:
   - Grounding DINO 1.5는 개방형 객체 탐지 분야에서 새로운 기준을 세우는 모델입니다. Pro 모델은 뛰어난 탐지 성능을 제공하며, Edge 모델은 다양한 응용 분야에서 실시간 객체 탐지를 가능하게 합니다. 이 모델들은 COCO, LVIS, ODinW와 같은 벤치마크에서 높은 성능을 입증하였으며, 엣지 디바이스에서도 효과적으로 동작할 수 있도록 최적화되었습니다.

### 혁신적인 부분
Grounding DINO 1.5의 혁신성은 고성능과 빠른 추론 속도를 동시에 제공하는 개방형 객체 탐지 모델을 제시한 데 있습니다. Pro 모델은 확장된 아키텍처와 대규모 데이터셋을 통해 뛰어난 성능을 발휘하며, Edge 모델은 효율적인 특징 향상을 통해 엣지 디바이스에서도 실시간 탐지를 가능하게 합니다. 이러한 접근 방식은 다양한 실제 응용 분야에서 객체 탐지 모델의 실용성을 크게 향상시킬 수 있습니다.

## Similar Papers
- [TAPTRv2: Attention-based Position Update Improves Tracking Any Point](2407.16291.md)
- [EVF-SAM: Early Vision-Language Fusion for Text-Prompted Segment Anything Model](2406.20076.md)
- [Qwen2 Technical Report](2407.10671.md)
- [INF-LLaVA: Dual-perspective Perception for High-Resolution Multimodal Large Language Model](2407.16198.md)
- [Xmodel-VLM: A Simple Baseline for Multimodal Vision Language Model](2405.09215.md)
- [COCONut: Modernizing COCO Segmentation](2404.08639.md)
- [An Introduction to Vision-Language Modeling](2405.17247.md)
- [AgentGym: Evolving Large Language Model-based Agents across Diverse Environments](2406.04151.md)
- [InternLM-XComposer-2.5: A Versatile Large Vision Language Model Supporting Long-Contextual Input and Output](2407.03320.md)
