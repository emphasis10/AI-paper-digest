# Physics of Language Models: Part 2.2, How to Learn From Mistakes on Grade-School Math Problems
## TL;DR
## Summary
- [https://arxiv.org/pdf/2408.16293.pdf](https://arxiv.org/pdf/2408.16293.pdf)

### 섹션 요약

#### 1. 서론
이 논문은 언어 모델이 자가 수정(self-correction)을 통해 논리적 오류를 교정할 수 있는지 검토합니다. 주로 오류-수정(training with retry data)의 유용성을 탐구하며, 해당 데이터를 사전 학습 단계에 포함시키는 방법을 제안합니다. 이 방법은 오류가 포함된 데이터를 기반으로 모델의 추론 정확도를 높이는 것을 목표로 합니다.

#### 2. 기존 연구에서 사용하는 수학 데이터
기존 연구에서 사용된 iGSM 데이터셋을 사용하여, 초등학교 수준의 수학 문제를 통해 오류 및 교정을 실험합니다. 이는 GPT-4 같은 대형 언어 모델이 수학 문제를 풀 때 자주 발생하는 논리적 오류를 포함합니다.

#### 3. 모델 재시도 (Result 0-1: Language Models Can Retry Upon Regret)
이 섹션에서는 모델이 잘못된 추론을 인식하고 수정하는 방법을 설명합니다. "후회 후 재시도(retry upon regret)"라는 개념을 소개하며, 오류 감지 후 이전 단계에서 다시 시작하여 해결하는 방법을 제안합니다. 이 방법은 비록 특정 상황에서 효과가 있지만 모든 상황에서 보장되지는 않으며, 높은 정확도의 오류 감지기가 필요합니다.

#### 4. 재학습 데이터로 사전 학습 (Result 2-6: Pretrain with Retry Data)
이 섹션에서는 사전 학습 데이터에 오류와 그 교정을 포함시키는 방법을 논의합니다. 잘못된 단계와 그 즉각적인 교정 데이터를 포함하여 사전 학습을 수행하면, 모델이 오류를 식별하고 교정하는 능력을 향상시킬 수 있습니다. 특히 iGSM 데이터에서는 오류 비율이 높을수록 모델의 추론 정확도가 높아지는 것으로 나타났습니다.

#### 5. 재시도 데이터로 미차별 학습 (Result 7: Finetune with Retry Data)
오류 없는 데이터로 사전 학습된 모델을 추가로 재시도 데이터를 사용해 미차별 학습(finetuning)할 경우, 모델의 정확성이 크게 향상되지 않는 과제를 다룹니다. 이는 오류 교정이 처음부터 학습되어야 하는 독립적인 기술임을 시사합니다.

#### 6. 가짜 재시도 데이터 준비 (Result 8: Pretrain with Fake Mistakes)
오류 데이터를 생성하기 어려운 현실적인 측면을 고려하여, 잘못된 단계를 인위적으로 삽입하고 그 후에 교정을 추가하는 "가짜 재시도 데이터"를 만드는 방법을 제안합니다. 이러한 방법은 모델이 불필요한 단계를 건너뛰지 않도록 훈련시킬 수 있습니다.

#### 7. 결론
이 논문에서는 언어 모델이 오류를 포함한 데이터를 통해 학습함으로써 추론 능력을 향상시키는 방법이 유용함을 입증하였습니다. 오류 교정 능력은 사전 학습 단계에서 학습되어야 하며, 이는 오류 없는 데이터로만 학습한 모델이 후속 미차별 학습에서 개선되기 어렵다는 점을 강조합니다. 또한, 상업적 LLM 훈련에서 오류 교정 데이터를 포함시키는 것이 실질적임을 제안합니다.

### 전체 요약
이 논문은 언어 모델의 오류 교정 능력을 향상시키기 위해 사전 학습 단계에서 오류와 그 즉각적인 교정이 포함된 데이터를 사용하는 방법을 제안합니다. iGSM 데이터셋을 이용하여 모델을 실험한 결과, 오류 포함 데이터를 사용한 사전 학습이 오류 없는 데이터보다 모델의 추론 정확도를 크게 향상시킬 수 있음을 보여주었습니다. 또한, 잘못된 단계를 인위적으로 생성하는 "가짜 재시도 데이터" 방법을 통해 현실적인 데이터 준비 방법을 탐구했습니다. 연구 결과는 상업적 언어 모델 훈련에서 오류 교정 데이터를 포함시키는 것이 중요함을 시사합니다.