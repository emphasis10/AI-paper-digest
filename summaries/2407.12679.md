# Goldfish: Vision-Language Understanding of Arbitrarily Long Videos
## TL;DR
## Summary
- [https://arxiv.org/pdf/2407.12679.pdf](https://arxiv.org/pdf/2407.12679.pdf)

### 논문 요약

#### 1. 개요

1.1. 도입부
이 논문은 현재 비디오 이해를 위한 대형 언어 모델(LLM)이 몇 분 동안의 비디오만 처리할 수 있는 한계를 지적합니다. 특히, 긴 비디오에서 발생하는 "노이즈와 중복성" 문제와 "메모리 및 계산" 문제를 강조합니다. 이를 해결하기 위해 Goldfish라는 기법을 소개하며, 이 방법은 TVQA-long 벤치마크에서 모델 성능을 평가합니다.

1.2. 혁신성
- **Goldfish 프레임워크**: 효율적인 검색 메커니즘을 통해 긴 비디오의 상위 k개의 관련 클립만 질문에 답변하는 데 사용합니다.
- **MiniGPT4-Video**: 단일 이미지에서 다프레임 처리로 확장된 VLM 모델로, 감지된 프레임을 언어 토큰으로 변환하여 사용자 질의에 맞춰 훈련되었습니다.
- **TVQA-long 벤치마크**: 긴 비디오 이해 모델을 평가하기 위해 새로운 벤치마크를 제안하며, 시각 및 텍스트 콘텐츠 모두를 이해하도록 요구합니다.

#### 2. 관련 연구

2.1. LLM 기반 짧은 비디오 이해
최근 Video-LLM(Video-LLaMA, VideoChat 등) 모델이 비디오 임베딩을 추출하기 위해 오디오 및 시각 신호를 사용하여 짧은 비디오를 처리하고 있습니다. 그러나 이들은 긴 비디오에서는 한계가 있습니다.

2.2. LLM 기반 긴 비디오 이해
긴 비디오의 이해는 MovieChat, LLaMA-VID 등에서 메모리 모듈을 활용하는 등 다양한 시도가 있었으나 여전히 노이즈 및 중복성 문제를 해결하지 못했습니다. Goldfish는 상위 k개의 관련 클립만을 사용하여 이 문제를 완화합니다.

#### 3. Goldfish 방법론

3.1. 검색 기반 긴 비디오 이해
비디오를 작은 클립으로 나누고, 각 클립에 대한 요약을 생성한 다음, 텍스트 인코더를 통해 임베딩합니다. 사용자의 질의도 같은 텍스트 인코더로 임베딩한 후, 검색 모듈이 가장 관련 있는 k개의 클립을 찾아 답변 모듈로 전송합니다.

3.2. 학습 파이프라인
- 대규모 이미지-텍스트 페어 사전학습: EVA-CLIP 비전 인코더를 사용하여 텍스트 공간에 시각적 특징을 맞춥니다.
- 대규모 비디오-텍스트 페어 사전학습: 짧은 비디오 프레임을 입력으로 사용하여 모델을 학습합니다.
- 비디오 질문-응답 학습: 고품질의 비디오 질문-응답 데이터 세트를 사용하여 모델의 입력 비디오 해석 능력을 향상시킵니다.

#### 4. 실험

4.1. 데이터셋
- **학습 데이터셋**: Condensed Movies Video Captions(CMD), WebVid, Video Instruction Dataset을 사용하여 사전학습과 학습을 수행합니다.
- **벤치마크**: LLama-Vid, MovieChat, Movie QA, TVQA-Long에서 SOTA 성과를 달성하였습니다.

4.2. 성과
- **짧은 비디오**: MSVD, MSRVTT, TGIF, TVQA 벤치마크에서 기존 방법을 능가하는 성능을 보여주었습니다.
- **긴 비디오**: TVQA-Long 벤치마크에서 무작위 정확도 기준선인 20%를 상회하며, 검색 모듈을 사용하지 않을 경우와 비교했을 때 성능이 크게 향상되었습니다.

#### 5. 결론
Goldfish 방법론은 긴 비디오 이해의 주요 문제를 해결하고 상위 k개의 관련 클립을 사용하여 질문에 효율적으로 답변하는 메커니즘을 제공합니다. MiniGPT4-Video와 함께 Goldfish는 긴/짧은 비디오에서 뛰어난 성능을 보여주며, TVQA-Long 벤치마크에서 14.94% 향상된 결과를 기록했습니다.

---

### 전체 요약

이 논문은 긴 비디오의 이해를 위해 Goldfish라는 새로운 프레임워크를 제안했습니다. Goldfish는 효율적인 검색 메커니즘을 통해 긴 비디오에서 필요한 상위 k개의 관련 클립만을 사용하여 질문에 답하는 방식으로 노이즈와 중복성을 줄이고 계산 및 메모리 문제를 해결합니다. 또한 단일 이미지에서 다수의 프레임을 처리하는 MiniGPT4-Video 모델을 개발하여 비디오 내용 해석 능력을 향상시켰습니다. 새로운 TVQA-long 벤치마크를 통해 그 효용성을 검증한 결과, 기존 방법을 크게 초과하는 성능을 입증했습니다. Goldfish와 MiniGPT4-Video는 현재 및 미래의 긴 비디오 이해 연구에 큰 기여를 할 것입니다.

## Similar Papers
- [MiniGPT4-Video: Advancing Multimodal LLMs for Video Understanding with Interleaved Visual-Textual Tokens](2404.03413.md)
- [LLM-AD: Large Language Model based Audio Description System](2405.00983.md)
- [MA-LMM: Memory-Augmented Large Multimodal Model for Long-Term Video Understanding](2404.05726.md)
- [Visual Haystacks: Answering Harder Questions About Sets of Images](2407.13766.md)
- [VideoTetris: Towards Compositional Text-to-Video Generation](2406.04277.md)
- [Sparser is Faster and Less is More: Efficient Sparse Attention for Long-Range Transformers](2406.16747.md)
- [MovieDreamer: Hierarchical Generation for Coherent Long Visual Sequence](2407.16655.md)
- [Pegasus-v1 Technical Report](2404.14687.md)
- [OpenVid-1M: A Large-Scale High-Quality Dataset for Text-to-video Generation](2407.02371.md)
