# Unlocking Continual Learning Abilities in Language Models
## TL;DR
## Summary
- [https://arxiv.org/pdf/2406.17245.pdf](https://arxiv.org/pdf/2406.17245.pdf)

### Section Summaries

#### 1. Introduction
본 논문은 언어 모델(Language Models, LMs)이 놀라운 성능을 보이지만, 지속적인 학습(Continual Learning, CL)에서 발생하는 치명적인 망각(catastrophic forgetting) 문제를 여전히 겪고 있음을 밝혀냅니다. 이를 해결하기 위해 기존 접근 방식들은 과거 과업 데이터를 포함하거나 과업 정보를 사용하는데, 이는 데이터 확보가 어려운 상황에서 문제를 발생시킵니다. 이 논문은 "MIGU" (MagnItude-based Gradient Updating)라는 새로운 방법을 소개하여, 과업 레이블 없이도 모델의 지속적인 학습 능력을 향상시킵니다.

#### 2. Related Work
이어지는 관련 연구 부분에서는 지속적인 학습을 위해 주요하게 사용되는 세 가지 접근 방식을 설명합니다:
1. 복습(rehearsal) 기반: 새로운 과업 데이터를 소량의 과거 데이터와 혼합.
2. 아키텍처 기반: 새로운 모듈을 추가하여 새로운 과업을 통합.
3. 파라미터 기반: 중요한 파라미터의 변화를 규제하거나, 과업별로 파라미터의 그라디언트를 직교하는 서브 스페이스로 분리.

#### 3. Method - MIGU
MIGU는 LMs의 선형 레이어에서 출력의 크기 분포의 차이를 활용하여, 과업 레이블 없이 지속적인 학습을 가능하게 합니다. 
- Forward Propagation: Linear 레이어의 출력을 L1-정규화하여 저장.
- Backward Propagation: 지정된 임계치 이상의 큰 값만 업데이트.

이는 과업 간의 그라디언트 충돌을 완화하며, 모델의 내재적인 지속 학습 능력을 최대화합니다.

#### 4. Evaluation
MIGU 방법은 로버타(RoBERTa), T5, Llama2 등 세 가지 주요 언어 모델 아키텍처와 네 가지 지속 학습 데이터셋에서 그 성능이 입증되었습니다. 
- 예를 들어, T5 모델에서 15개의 과업을 연속으로 학습시키는 실험에서 평균 정확도가 15.2% 향상되었습니다.

#### 5. Conclusion
본 연구는 언어 모델의 지속 학습 능력을 크게 향상시킬 수 있는 단순하면서도 효과적인 방법을 제안합니다. MIGU는 기존의 지속 학습 접근 방식과 쉽게 통합될 수 있으며, 과거 과업 데이터나 과업 레이블 없이도 높은 성능을 발휘합니다.

### Overall Summary
본 논문은 언어 모델의 치명적인 망각 문제를 해결하기 위해 "MIGU" 방법을 소개합니다. 이는 선형 레이어의 출력 크기 분포 차이를 이용하여 과업 레이블 없이도 지속 학습을 가능하게 하며, 로버타, T5, Llama2 등의 주요 모델 아키텍처에서 그 성능을 입증했습니다. MIGU는 기존의 지속 학습 방법과 통합될 수 있으며, 과거 데이터 없이도 높은 정확도를 제공합니다.