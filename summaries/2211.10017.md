# Who Says Elephants Can't Run: Bringing Large Scale MoE Models into Cloud Scale Production
## TL;DR
## Summary
- [https://arxiv.org/pdf/2211.10017.pdf](https://arxiv.org/pdf/2211.10017.pdf)

## 섹션별 요약

### Abstract
혼합 전문가(Mixture of Experts, MoE) 모델은 선택적으로 활성화되는 희소층을 통해 더 많은 수의 매개변수를 사용하여 훈련할 수 있게 해줍니다. 이러한 모델은 기계 번역 등 여러 자연어 처리 작업에서 더 나은 성능을 보였습니다. 하지만, 큰 메모리 요구사항과 비효율적인 추론으로 인해 실생활에서의 배포는 어려웠습니다. 이 연구에서는 희소 모델의 계산을 가속화하고 메모리 소비를 크게 줄이는 고효율 추론 프레임워크를 소개합니다. 이를 통해 최대 26배의 처리량 증가와 모델 크기 8분의 1 감소를 달성했으며, 이를 통해 136배 더 큰 모델을 27% 적은 비용으로 배포할 수 있었습니다.

### 1. Introduction
트랜스포머 모델은 점점 더 커지고 좋아지고 있습니다. Mixture of Experts(MoE) 모델은 희소하게 활성화되는 계산을 사용하여 모델 크기를 더 비용 효율적으로 확장할 수 있는 방법을 제공합니다. MoE 모델은 더 큰 매개변수를 사용하지만 계산 비용은 선형 이하로 유지됩니다. 이 연구에서는 MoE 모델의 추론 최적화에 중점을 둡니다.

### 2. Challenges and Contributions
#### 2.1 MoE Inference Challenge
MoE 모델은 이론적으로 더 적은 계산을 요구하지만, 실제로는 토큰 라우팅 및 통신 비용이 추가되어 훈련 처리량에 큰 영향을 미칩니다. 추론 시, MoE 모델은 동일한 임베딩 및 숨겨진 차원을 가진 밀집 모델보다 최대 30배 느릴 수 있습니다. 따라서 추론 비용을 낮추는 것이 중요합니다.

#### 2.2 Inference Optimization Contributions
본 연구에서는 단일 GPU에서 가능한 가장 큰 모델을 배포하기 위해 메모리 요구 사항을 줄이고, 효율적인 라우팅 및 배치 가지치기(batch pruning)를 구현합니다. 이를 통해 NVIDIA의 FasterTransformer 프레임워크를 확장하여 실제 배포 시나리오에서 MoE 모델 아키텍처를 지원합니다.

### 3. MoE Inference Optimizations
#### 3.1 Model Architecture
이 연구에서는 깊은 인코더와 얕은 디코더 아키텍처를 사용하는 기계 번역을 위한 인코더-디코더 모델을 훈련합니다. Top-1 게이팅 알고리즘을 사용하여 MoE 레이어를 효율적으로 실행합니다.

#### 3.2 Multilingual Machine Translation Model
전통적인 기계 번역 배포 방식은 여러 교사 모델을 작은 학생 모델로 증류하여 CPU에 배포합니다. 이는 확장성이 떨어지고 지식 공유가 어려운 문제를 발생시킵니다. 이 연구에서는 다국어 번역 시스템을 단일 모델로 대체하여 여러 언어 쌍에 사용할 수 있도록 훈련합니다.

#### 3.3 Optimized GPU Kernel Design
효율적인 병렬 알고리즘을 사용하여 토큰 라우팅 및 행렬 곱셈을 수행합니다. GPU 친화적인 래딕스 정렬을 사용하여 토큰을 전문가에게 라우팅하고, Grouped GEMM을 사용하여 병렬로 행렬 곱셈을 수행합니다.

#### 3.4 Expert Quantization with 4-bit and 8-bit
MoE 가중치를 양자화하여 모델 크기를 줄이고 메모리 대역폭 문제를 해결합니다. 양자화된 가중치를 FP16으로 디양자화하여 부동 소수점 연산을 수행합니다.

#### 3.5 MoE Batch Pruning
배치 가지치기 알고리즘을 구현하여 번역이 완료된 문장을 배치에서 동적으로 제거함으로써 MoE 레이어의 속도를 높입니다.

### 4. Results and Discussion
실험은 NVIDIA V100 GPU에서 수행되었으며, 여러 배치 크기와 양자화 방식에 따른 성능을 비교합니다. 최적화된 MoE 모델은 PyTorch 기반 모델 대비 최대 26배의 속도 향상을 달성했습니다.

### 5. Conclusions and Future Work
본 연구는 단일 GPU에서 대규모 MoE 모델을 비용 효율적으로 사용할 수 있는 방법을 설명합니다. 향후 연구에서는 더 큰 모델을 분산 추론 환경에서 비용 효율적으로 배포하는 방법을 탐구할 계획입니다.

## 전체 요약
이 논문은 대규모 Mixture of Experts(MoE) 모델을 단일 GPU에서 효율적으로 추론할 수 있는 방법을 제시합니다. MoE 모델은 희소하게 활성화되는 레이어를 사용하여 많은 매개변수를 처리할 수 있으나, 실생활에서의 추론은 큰 메모리 요구사항과 비효율적인 성능으로 어려움이 있었습니다. 본 연구에서는 NVIDIA의 FasterTransformer 프레임워크를 확장하여 MoE 모델의 메모리 요구사항을 줄이고, 효율적인 라우팅 및 배치 가지치기를 구현하였습니다. 이를 통해 최대 26배의 처리량 증가와 모델 크기 8분의 1 감소를 달성했습니다. 또한, 양자화를 통해 더 큰 모델을 낮은 비용으로 배포할 수 있음을 보여주었습니다. 이러한 최적화는 기계 번역뿐만 아니라 다른 자연어 처리 작업에도 적용될 수 있습니다. 향후 연구에서는 더 큰 모델을 분산 추론 환경에서 효율적으로 배포하는 방법을 탐구할 예정입니다.

## Similar Papers
- [QMoE: Practical Sub-1-Bit Compression of Trillion-Parameter Models](2310.16795.md)
- [PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU](2312.12456.md)
- [ZeroQuant-FP: A Leap Forward in LLMs Post-Training W4A8 Quantization Using Floating-Point Formats](2307.09782.md)
- [SparseGPT: Massive Language Models Can Be Accurately Pruned in One-Shot](2301.00774.md)
- [AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration](2306.00978.md)
- [A Closer Look into Mixture-of-Experts in Large Language Models](2406.18219.md)
- [FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning](2307.08691.md)
- [SpecInfer: Accelerating Generative Large Language Model Serving with Tree-based Speculative Inference and Verification](2305.09781.md)
- [ZeroQuant: Efficient and Affordable Post-Training Quantization for Large-Scale Transformers](2206.01861.md)
