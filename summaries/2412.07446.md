# Causal World Representation in the GPT Model
## TL;DR
## Summary
- [https://arxiv.org/pdf/2412.07446.pdf](https://arxiv.org/pdf/2412.07446.pdf)

### 섹션 요약

#### 1. 서론
이 논문은 생성적 사전 학습 변환기(GPT) 모델이 단순히 다음 토큰을 예측하기 위해 학습되는지, 아니면 시퀀스가 한 토큰씩 생성되는 세계 모델을 암묵적으로 학습하는지를 탐구합니다.

#### 2. 기본 개념
GPT의 주의(attention) 메커니즘과 구조적 인과 모델(SCM)의 개념을 설명합니다. 주의 메커니즘은 주어진 입력 시퀀스의 상관관계를 추론하여 토큰의 시퀀스에서 암시적인 인과 구조를 형성합니다.

#### 3. GPT의 인과 해석
이 섹션에서는 GPT의 마스크된 주의 메커니즘을 인과 관계를 학습하는 방법으로 제안하여 SCM 기반 세계 모델과의 관계를 설명합니다. 이를 통해 특정 상황에서 제로샷으로 인과 구조를 학습할 수 있는 능력을 강조합니다.

#### 4. 실험 및 결과
오델로(Othello) 보드 게임의 규칙에 따른 실험을 수행하여 GPT가 주의 메커니즘을 통해 인과 구조를 어떻게 학습하는지를 증명합니다. 또, 모델이 게임 규칙을 따르는 합법적인 이동을 생성했을 때와 그럴 수 없었던 구조적인 불확실성 간의 상관관계를 조사합니다.

#### 5. 결론
GPT는 인과적 메커니즘을 암묵적으로 학습하며, 이는 AI 모델의 '환상' 현상을 설명할 수 있는 새로운 시각을 제공할 가능성을 제시합니다.

### 전체 요약
이 논문은 GPT 모델의 주의 메커니즘을 통해 인과 구조를 학습하는 방법을 제시하며, 이 과정에서 제로샷 인과 구조 학습의 가능성을 탐구합니다. 오델로 게임의 사례를 통해 GPT가 세계 모델을 형성하는 과정을 실험적으로 검증하고, 인과 구조가 명확하게 형성되지 않는 경우 법칙에 따라 예측할 수 없는 한계를 드러냅니다. 이 연구는 AI 모델의 생기는 능력에 대한 '환상' 현상을 이해하는 방향성도 제안합니다.