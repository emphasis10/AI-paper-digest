# OLMoE: Open Mixture-of-Experts Language Models
## TL;DR
## Summary
- [https://arxiv.org/pdf/2409.02060.pdf](https://arxiv.org/pdf/2409.02060.pdf)

### 1. 각 섹션 요약

#### 1. 서론
서론에서는 대형 언어 모델(LM)이 다양한 과제에서 큰 발전을 이루었지만, 성능과 비용 간의 분명한 트레이드오프가 존재한다고 설명합니다. 이러한 문제를 해결하기 위해 희소 활성화 Mixture-of-Experts(MoE) 기법을 사용합니다. MoE는 각 레이어에 여러 전문가를 두고 일부만 활성화하기 때문에 비슷한 숫자의 파라미터를 가진 밀집 모델(dense model)보다 훨씬 효율적입니다. 주요 목표는 OLMOE라는 완전 오픈 소스의 최첨단 언어 모델을 소개하는 것입니다.

#### 2. 문헌 검토
이 섹션에서는 최신 MoE 연구와 다양한 오픈 소스 모델들을 검토합니다. MoE는 몇 가지 새로운 라우팅 기법과 전문가 분할 기법을 통해 크게 개선되었습니다. 대부분의 MoE 모델은 여전히 비공개이지만, OLMOE는 이러한 문제를 해결하며 완전한 오픈 소스 모델을 제공합니다.

#### 3. 방법론
OLMOE 모델을 설명하며, 7억 개의 파라미터를 가지지만 입력 토큰당 1억 개만 사용합니다. 5조 개의 토큰으로 사전 훈련되었고, 이후 'OLMOE-1B-7B-INSTRUCT'로 추가 적응되었습니다. 이 모델은 비슷한 파라미터 수를 가진 다른 모델들을 능가합니다.

#### 4. 실험
다양한 실험을 통해 MoE 훈련에 대한 통찰을 제공합니다. 또한 라우팅 및 전문가 상호작용을 분석하며, 많은 실험 데이터를 통해 성능을 최적화하는 방법을 탐구합니다.

#### 5. 결과
OLMOE 모델이 기존의 밀집 모델과 MoE 모델들을 성능 면에서 능가하는지 보여줍니다. 특히, 모델의 전문화된 라우팅과 효율적인 토큰 사용을 통해 성능이 개선되었습니다.

#### 6. 결론
이 연구는 OLMOE 모델이 기존 모델들에 비해 비용 효율성과 성능 면에서 우월하다는 결론을 내립니다. 또한, 이 모델을 오픈 소스로 제공함으로써 학계와 산업계 모두에게 기여할 수 있는 가능성을 열었습니다.

### 2. 전체 요약
이 연구의 주된 기여는 완전 오픈 소스인 최첨단 언어 모델 OLMOE를 개발한 것입니다. 이 모델은 희소 활성화 기법인 Mixture-of-Experts(MoE)를 활용해 성능과 비용 효율성을 크게 개선했습니다. 5조개의 토큰으로 사전 훈련되었으며, 이후 추가 적응을 통해 성능을 극대화했습니다. OLMOE는 비슷한 파라미터 수를 가진 다른 모델들을 능가하며, 다양한 실험을 통해 모델의 라우팅과 전문가 상호작용을 분석하고 최적화한 것이 주요 혁신입니다. 이 모델은 기존 밀집 모델들보다 3배 적은 토큰을 사용하면서도 동일한 성능을 달성합니다.
 
이 연구는 이 모델을 오픈 소스로 제공하여 학계와 산업계 모두에서 과학적 연구와 혁신을 촉진하는 것을 목표로 하고 있습니다. OLMOE는 완전 공개된 모델 가중치, 데이터, 코드 및 로그를 통해 연구자들이 모델을 이해하고 개선할 수 있는 기회를 제공합니다.

**주요 혁신 포인트:**
1. **Mixture-of-Experts(MoE)의 효율성:** 여러 전문가 중 일부만 활성화하여 기존 밀집 모델보다 훈련 효율성을 크게 향상.
2. **완전 오픈 소스:** 모델 가중치, 데이터, 코드 및 로그를 모두 공개하여 투명성을 제공.
3. **높은 성능:** 기존의 밀집 모델과 비교하여 성능을 유지하면서도 비용 절감.