# A Survey of Multi-Tenant Deep Learning Inference on GPU
## TL;DR
## Summary
- [https://arxiv.org/pdf/2203.09040.pdf](https://arxiv.org/pdf/2203.09040.pdf)

"다중 테넌트 딥러닝 추론에 대한 GPU 활용" 논문은 GPU에서 다중 테넌트 딥러닝 추론 최적화의 새로운 도전과 기회를 종합적으로 조사합니다. 이 논문은 다양한 최적화 레벨(서비스 레벨, 그래프 레벨, 런타임 레벨, 커널 레벨, 자원 관리 레벨)에서 단일 테넌트 추론과 다중 테넌트 추론의 차이점을 분석하고, 이를 통해 GPU의 자원 활용도와 처리량을 향상시키는 방법을 제시합니다.

논문의 주요 내용은 다음과 같습니다:

1. **서론**: 딥러닝 모델이 인지 작업에서 뛰어난 성능을 보이고 있으며, GPU와 같은 컴퓨팅 하드웨어가 세대별로 계속해서 성능이 향상되고 있다고 언급합니다.

2. **다중 테넌트 컴퓨팅의 도전 및 기회**:
   - 단일 테넌트 최적화와 비교하여 다중 테넌트 딥러닝 추론이 직면한 주요 도전과 기회를 소개합니다.
   - 다중 테넌트 환경에서는 고유의 병렬 처리 및 자원 공유가 가능하며, 이로 인해 GPU 자원 활용과 전력 효율성이 개선될 수 있습니다.

3. **다중 테넌트 컴퓨팅 최적화**:
   - 서비스 레벨 조정, 그래프 및 런타임 레벨 스케줄링, 커널 레벨 자동 조정, 자원 레벨 관리 등 다양한 레벨에서의 최적화 전략을 제안합니다.
   - 다중 테넌트 환경에서 효율적인 자원 관리와 동시 실행을 위한 기술적 접근 방법을 설명합니다.

4. **대규모 딥러닝 컴퓨팅으로의 발전**:
   - 하드웨어와 소프트웨어의 공동 스케줄링을 통해 더 높은 작업 스케줄링 유연성과 엄격한 자원 격리를 달성할 수 있는 방안을 모색합니다.
   - '전체 스택을 루프에 포함'하는 아키텍처 설계를 통해 최적의 성능과 유연성을 추구합니다.

5. **결론**:
   - 다중 테넌트 DL 추론이 GPU를 통한 처리량, 자원 활용도 및 전력 효율성을 향상시킬 수 있는 유망한 컴퓨팅 패러다임으로 부상하고 있음을 강조합니다.
   - 이 논문이 미래의 대규모 DL 시스템 최적화에 대한 새로운 관점과 혁신적인 작업을 이끌어낼 수 있기를 기대합니다.