# Leave No Document Behind: Benchmarking Long-Context LLMs with Extended Multi-Doc QA
## TL;DR
## Summary
- [https://arxiv.org/pdf/2406.17419.pdf](https://arxiv.org/pdf/2406.17419.pdf)

### 1. 각 섹션 요약

#### 서론 (Introduction)
이 논문에서는 장문 맥락 이해 능력을 평가하기 위해 여러 개의 긴 문서를 처리하는 LLMs (대형 언어 모델)의 효율성을 검토합니다. 현재 벤치마크들은 실제 시나리오와 동떨어진 노이즈 텍스트를 이용해 테스트 사례의 길이를 인위적으로 확장하고 있는 점을 지적합니다. 이에 따라 긴 맥락의 다중 문서 질의응답 시나리오와 일치하며 데이터 오염이 없는 새 벤치마크인 Loong을 제안합니다.

#### 배경 및 관련 연구 (Background and Related Work)
LLMs의 장문 맥락 이해 능력을 평가하기 위해 기존의 벤치마크들과 새로운 접근 방법인 Retrieval Augmented Generation (RAG)을 비교합니다. RAG는 외부 지식을 이용하여 성능을 향상시킬 수 있지만, 이는 여전히 특정 작업에 맞춤화되기에는 한계가 있음을 지적합니다.

#### Loong 벤치마크 개요 (Loong Benchmark Overview)
Loong 벤치마크는 실제 시나리오에 맞춰 재구성된 1600개의 테스트 케이스로 구성됩니다. 각 문서는 금융 보고서, 학술 논문, 법률 사례 등의 세 가지 주요 도메인에서 수집되었으며, 질문-응답 형식으로 제공됩니다. 이 벤치마크는 네 가지 평가 작업(Spotlight Locating, Comparison, Clustering, Chain of Reasoning)으로 구성되어 있고, 각 작업은 여러 문서에 걸쳐 분산된 증거를 찾는 것을 목표로 합니다.

#### 평가 작업 설명 (Evaluation Task Description)
각 작업은 다중 문서 간의 의미적 관계를 평가하며, 실제 시나리오와의 정렬에 중점을 둡니다. 예를 들어, Spotlight Locating 작업에서는 여러 문서 중 특정 문서에만 포함된 정보를 찾는 능력을 평가합니다. 각 작업의 구체적인 테스트 사례와 프롬프트는 논문의 부록에서 자세히 설명되어 있습니다.

#### 실험 설정 (Experimental Setup)
여섯 개의 최신 LLMs (예: GPT-4o, Gemini-Pro1.5 등)을 다양한 문맥 길이(128K-1000K)에서 테스트하여 성능을 평가합니다. 주요 평가 기준은 정확성과 환각 등입니다. 특히 GPT-4는 사람의 평가와 높은 일관성을 보여, 이를 기준으로 모델의 출력을 평가합니다.

#### 실험 결과 (Results)
실험 결과, Gemini-Pro1.5가 가장 뛰어난 성능을 보였으며, 긴 문맥 처리에서도 우수한 결과를 나타냈습니다. 그러나 공개 소스 모델들은 닫힌 소스 모델에 비해 성능이 떨어졌으며, 이는 모델의 파라미터와 맥락 창 크기의 중요성을 강조합니다. 또한, RAG 모듈을 추가했을 때 성능이 향상되었지만, 이는 길이 한계를 해결하지 못하며 장문 맥락 모델링 능력의 지속적인 개선이 필요하다는 결론을 제시합니다.

#### 결론 (Conclusion)
Loong 벤치마크는 다중 문서 시나리오에서 LLMs의 장문 맥락 이해 능력을 평가하는 데 중요한 기여를 합니다. 실험을 통해 기존 모델들이 여전히 개선의 여지가 많음을 확인했으며, RAG와 맥락 크기 스케일링에 대한 심층 분석이 이루어졌습니다.

### 2. 전체 요약
이 논문은 LLMs의 긴 맥락 이해 능력을 보다 현실적인 시나리오에 맞춰 평가하기 위한 새 벤치마크 Loong을 제안합니다. Loong은 금융 보고서, 학술 논문, 법률 사례 등의 세 가지 주요 도메인에서 수집된 다중 문서를 활용하여 네 가지 평가 작업을 수행합니다. 주요 실험 결과에서는 Gemini-Pro1.5가 가장 뛰어난 성능을 보였으나, 다른 모델들은 여전히 개선이 필요하다는 한계를 드러냈습니다. Retrieval Augmented Generation (RAG) 모듈 도입은 성능 향상에 기여했지만, 이는 길이 제한 문제를 완전히 해결하지 못했습니다. 나아가, 더 강력한 훈련 방법과 긴 텍스트에 대한 효과적인 훈련이 필요함을 강조합니다.

## Similar Papers
- [MIBench: Evaluating Multimodal Large Language Models over Multiple Images](2407.15272.md)
- [A Careful Examination of Large Language Model Performance on Grade School Arithmetic](2405.00332.md)
- [StructEval: Deepen and Broaden Large Language Model Assessment via Structured Evaluation](2408.03281.md)
- [Searching for Best Practices in Retrieval-Augmented Generation](2407.01219.md)
- [Video-MME: The First-Ever Comprehensive Evaluation Benchmark of Multi-modal LLMs in Video Analysis](2405.21075.md)
- [cosFormer: Rethinking Softmax in Attention](2202.08791.md)
- [AI Hospital: Benchmarking Large Language Models in a Multi-agent Medical Interaction Simulator](2402.09742.md)
- [LLoCO: Learning Long Contexts Offline](2404.07979.md)
- [Codecfake: An Initial Dataset for Detecting LLM-based Deepfake Audio](2406.08112.md)
