# Language Models are Super Mario: Absorbing Abilities from Homologous Models as a Free Lunch
## TL;DR
## Summary
- [https://arxiv.org/pdf/2311.03099.pdf](https://arxiv.org/pdf/2311.03099.pdf)

### Section Summaries

#### 1. 서론 및 개요

**서론**
인류는 영화와 게임과 같은 매체를 통해 능력을 흡수하고 강화하는 꿈을 꾸어왔습니다. 이 논문에서는 언어 모델(LM)이 재학습이나 GPU 없이도 다른 동형 모델의 파라미터를 흡수하여 새로운 능력을 획득할 수 있다는 것을 발견했습니다. 이를 위해 DARE(Drop And REscale) 방법을 도입하여, 대부분의 델타 파라미터(미세 조정된 파라미터와 사전 학습된 파라미터 간의 차이)를 0으로 설정해도 능력에 영향을 미치지 않는다는 것을 보여줍니다.

**개요**
DARE는 특정 델타 파라미터를 무작위로 0으로 설정하고 남은 파라미터를 1/(1-p)의 비율로 재조정하여 원래의 임베딩을 대체합니다. 실험 결과, 각 레이어의 임베딩과 매우 유사한 값을 제공하여 성능에 큰 영향을 미치지 않음을 보여줍니다. 이를 통해 여러 동형 모델을 결합하여 다양한 능력을 가진 하나의 모델을 만드는 데 성공했습니다.

#### 2. 관련 연구

**Supervised Fine-Tuning (SFT)**
SFT는 사전 학습된 모델에 특정 과제를 수행할 수 있는 능력을 부여하는 가장 널리 사용되는 전략입니다. 논문에서는 다양한 SFT 모델의 델타 파라미터가 극도로 중복된다는 것을 보여주고, DARE를 통해 90% 또는 99%의 델타 파라미터를 제거할 수 있음을 증명했습니다.

**Network Pruning**
네트워크 프루닝 기술은 불필요한 파라미터를 제거하여 모델의 성능을 유지하면서도 계산 비용을 줄이는 것을 목표로 합니다. DARE는 델타 파라미터에 집중하고, 재학습이나 추가 데이터 없이도 잘 작동한다는 점에서 기존 프루닝 기법과 차별화됩니다.

**Model Merging**
모델 병합은 여러 과제별 모델을 하나로 결합하여 다양한 능력을 가진 단일 모델을 만드는 것이 목표입니다. DARE는 기존 모델 병합 기법에 플러그 앤 플레이 모듈로 적용 가능하여, 중복 파라미터를 줄이면서도 잘 작동함을 보여주었습니다.

#### 3. DARE 방법 및 실험 결과

**DARE의 작동 원리**
DARE는 Drop-And-Rescale의 약자로, 델타 파라미터의 값을 무작위로 0으로 설정한 뒤 남은 파라미터를 재조정하는 간단한 방법입니다. 실험을 통해, 90% 이상의 델타 파라미터를 제거해도 모델 성능이 거의 영향을 받지 않음을 확인했습니다.

**실험 설정 및 결과**
폭넓은 실험을 통해 DARE가 SFT의 델타 파라미터 중복성을 줄이고 모델 병합 성능을 향상시킴을 입증했습니다. 특히, Task Arithmetic 및 TIES-Merging과 같은 모델 병합 기법을 사용한 실험에서 DARE가 성능을 향상시키는 것을 확인했습니다.

### 전체 요약

이 논문은 DARE라는 혁신적인 방법을 소개하여, 미세 조정된 언어 모델의 델타 파라미터를 극적으로 줄이는 데 집중합니다. DARE는 대부분의 델타 파라미터를 0으로 설정하고 남은 파라미터를 재조정하여 원래 임베딩을 근사합니다. 실험 결과, DARE는 최대 99%의 델타 파라미터를 제거해도 성능 저하가 거의 없고, 여러 모델을 결합하여 다양한 능력을 가진 하나의 모델을 만들 수 있음을 보여줍니다. 이 방법은 모델 병합 기법의 성능을 향상시키는 데도 유효하며, 낮은 비용으로 다양한 기능을 가진 모델을 생성할 수 있는 가능성을 열어줍니다.

## Similar Papers
- [DELLA-Merging: Reducing Interference in Model Merging through Magnitude-Based Sampling](2406.11617.md)
- [On LLMs-Driven Synthetic Data Generation, Curation, and Evaluation: A Survey](2406.15126.md)
- [Cost-Effective Hallucination Detection for LLMs](2407.21424.md)
- [Superposition Prompting: Improving and Accelerating Retrieval-Augmented Generation](2404.06910.md)
- [Understanding and Diagnosing Deep Reinforcement Learning](2406.16979.md)
- [Fast Feedforward Networks](2308.14711.md)
- [Position: Foundation Agents as the Paradigm Shift for Decision Making](2405.17009.md)
- [Benchmarking Mental State Representations in Language Models](2406.17513.md)
- [Probing the 3D Awareness of Visual Foundation Models](2404.08636.md)
