# HackerRank-ASTRA: Evaluating Correctness & Consistency of Large Language Models on cross-domain multi-file project problems
## TL;DR
## Summary
- [https://arxiv.org/pdf/2502.00226.pdf](https://arxiv.org/pdf/2502.00226.pdf)

1. **각 섹션의 주요 내용 요약:**

   - **소개 (Introduction):**  
     대규모 언어 모델(LLM)의 신속한 발전은 소프트웨어 개발에서 코드 생성과 버그 수정과 같은 기능을 가능하게 하였다. 하지만 이러한 모델의 실제 효과를 평가하는 것은 도전 과제이다. 기존 평가 기준은 주로 단일 언어 또는 단일 파일 작업에 집중되어 있다는 점을 언급함. 

   - **HackerRank-ASTRA (HackerRank-ASTRA):**  
     HackerRank-ASTRA는 프로젝트 기반의 코딩 문제를 평가하는 프레임워크로, 주로 프론트엔드 개발에 중점을 두며, 다양한 스킬 도메인을 포함한다. 초기 평가 결과는 LLM이 직면하는 주요 과제를 강조하며, 주요 기술과 하위 기술의 분류도 제공한다.  

   - **실험 설정 (Experiment Setup):**  
     모델의 성능을 평가하기 위해 문제 설명 및 관련 소스 파일을 입력으로 사용하여 코드를 생성하도록 요구한다. 결과는 여러 번 실행하여 일관성을 평가한다. 

   - **결과 (Results):**  
     여러 모델을 평가한 결과, o1, o1-preview, Claude-3.5-Sonnet-1022가 비슷한 평균 점수(약 75%)를 기록했으며, 가장 높은 일관성을 보인 모델은 Claude-3.5-Sonnet-1022였다. 이는 실질적인 소프트웨어 개발에서의 신뢰성과 일관성을 강조한다.  

   - **결론 (Conclusions):**  
     HackerRank-ASTRA 벤치마크는 LLM의 실제 소프트웨어 개발 능력을 평가하는 데 기여하며, 모델의 신뢰성과 성능에 대한 더 깊은 통찰력을 제공한다. LLM이 특정 기술 및 하위 기술에서 상당한 도전에 직면한다는 초기 발견이 이루어졌다.  

2. **전반적인 요약:**  
   HackerRank-ASTRA는 대규모 언어 모델의 실제 소프트웨어 개발에 대한 평가를 가능하게 하는 프로젝트 기반의 벤치마크로, 모델의 정확성과 일관성을 중심으로 한 메트릭스를 사용하여 실질적인 개발 작업에서의 성과를 평가한다. LLM은 다양한 기술 요구 사항을 가지고 있으며 특히 주요 기술에서 상당한 개선이 필요함을 보여준다. 이 연구는 실제 소프트웨어 개발 시나리오를 반영한 보다 포괄적인 평가 기준을 통해 이러한 모델들이 해결해야 할 중대한 과제를 드러낸다.