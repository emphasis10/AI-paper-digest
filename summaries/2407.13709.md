# Understanding Reference Policies in Direct Preference Optimization
## TL;DR
## Summary
- [https://arxiv.org/pdf/2407.13709.pdf](https://arxiv.org/pdf/2407.13709.pdf)

### 1. 각 섹션 요약 및 메인 기여와 혁신 부분 설명

---

#### 1. 서론 (Introduction)

**요약:**
최근 표준 지도 학습 미세 조정(SFT) 알고리즘의 한계를 극복하기 위해 다양한 학습 알고리즘이 제안되었습니다. 그 중 하나는 강화학습(RL)과 연관된 Direct Preference Optimization (DPO)입니다. DPO는 입력 텍스트(x)와 출력 텍스트(y) 사이의 관계를 학습할 때 참조 정책(pref)과의 KL-발산 내에서 보상 함수(r)를 최대화합니다.

**메인 기여 및 혁신:**
DPO는 강화학습 프레임워크를 이용하여 언어 모델이 보다 선호되는 출력을 생성하도록 학습하는 방법입니다.

---

#### 2. 관련 연구 (Related Work)

**요약:**
DPO 이외에도 다양한 선호 학습 알고리즘이 제안되었습니다. 특히 대조 학습은 모델이 예측한 확률을 품질 점수로 해석합니다. 이들 방법은 특정 참조 모델이 필요하지 않지만, 추가적인 정규화가 필요합니다.

**메인 기여 및 혁신:**
참조 모델이 필요 없는 학습 알고리즘들이 제안되었으며, DPO는 이들에 비해 이론적으로 잘 정립된 배경을 가지고 있습니다.

---

#### 3. DPO의 실험 적용 및 결과 (DPO for Instruction Fine-tuning)

**요약:**
본 연구는 UltraFeedback 데이터셋을 사용하여 DPO를 통해 대규모 언어 모델을 미세 조정합니다. 이 과정에서 두 가지 LLM(대규모 언어 모델)이 사용되었습니다: 미스트랄-7b와 툴루-2-7b.

**메인 기여 및 혁신:**
DPO를 사용한 모델이 SFT 모델보다 더 나은 성능을 보이며, 평가 지표로 AlpacaEval2를 사용하여 평가하였습니다. DPO가 모델의 오버피팅 문제를 해결함을 시사합니다.

---

#### 4. 최적의 KL 제약 강도 탐색 (Optimal KL Constraint Strength)

**요약:**
DPO에서 참조 정책의 KL 제약 강도는 모델 성능에 큰 영향을 미칩니다. 제약이 너무 작으면 성능이 저하되고, 적절한 강도가 필요합니다.

**메인 기여 및 혁신:**
참조 정책의 제약 강도를 최적화하는 것이 중요하다는 것을 밝혔습니다.

---

#### 5. 참조 정책의 필요성 탐구 (Necessity of Reference Policies)

**요약:**
참조 정책이 없이도 실험을 진행하였으며, 참조 정책이 모델의 안정성에 기여함을 확인하였습니다.

**메인 기여 및 혁신:**
참조 정책이 없는 학습 알고리즘도 성능 향상을 이룰 수 있지만, 안정성 측면에서 참조 정책의 중요성을 재확인하였습니다.

---

#### 6. 강력한 참조 정책의 혜택 탐구 (Benefit from Stronger Reference Policies)

**요약:**
더 강력한 참조 정책을 사용하여 실험을 진행했으며, 특정 상황에서 성능이 향상됨을 확인했습니다.

**메인 기여 및 혁신:**
모델과의 적합성이 높은 참조 정책이 성능을 추가로 개선시킬 수 있음을 발견했습니다.

---

### 2. 전체 요약

이 논문은 Direct Preference Optimization (DPO)이 대규모 언어 모델의 지도 학습 미세 조정 방식의 한계를 극복하는 효과적인 방법임을 제안합니다. DPO는 입력과 출력 관계를 학습할 때 참조 모델과의 KL-발산 내에서 보상 함수를 최적화하는 알고리즘입니다. 연구 결과, DPO가 참조 정책의 제약 강도에 민감하며, 최적의 제약 강도가 필요함을 확인했습니다. 또한, 참조 정책이 없는 환경에서도 모델 안정성에 중요한 역할을 한다는 것을 발견했습니다. 마지막으로, 더욱 강력한 참조 정책을 도입하여 DPO의 성능 향상을 확인하였습니다. 이 연구는 AI와 머신 러닝 분야에서 선호 학습 알고리즘의 새로운 가능성을 탐구하는 중요한 기여를 합니다.