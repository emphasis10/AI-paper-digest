# QMoE: Practical Sub-1-Bit Compression of Trillion-Parameter Models
## TL;DR
## Summary
- [https://arxiv.org/pdf/2310.16795.pdf](https://arxiv.org/pdf/2310.16795.pdf)

### 1. 섹션별 요약 및 세부설명

#### 소개 (Introduction)
소개 섹션에서는 대규모 언어 모델(LLM)의 고성능과 함께 발생하는 높은 추론 비용 문제를 해결하는 방법으로 Mixture-of-Experts(MoE) 아키텍처를 소개합니다. 특히, SwitchTransformer와 같은 모델의 파라미터 수가 1.6조에 달해 실용적인 배포 및 연구가 어렵다는 점을 강조합니다.

#### 배경 (Background)
이 섹션에서는 MoE 모델과 데이터 의존 양자화(Quantization) 기술에 대한 배경을 설명합니다. MoE 모델은 네트워크의 일부 구성 요소를 여러 번 복제하여 입력 토큰을 부분 집합으로만 처리함으로써 모델링 능력을 향상시키는 방법입니다. 이를 통해 계산 비용을 일정하게 유지하면서 모델의 성능을 높일 수 있습니다. 데이터 의존 양자화는 모델 가중치를 낮은 숫자 정밀도로 변환하여 메모리 비용을 줄이는 가장 효과적인 전략입니다.

#### 기여 (Contribution)
이 논문의 주요 기여는 QMoE라는 프레임워크를 통해 대규모 MoE 모델을 정확하게 압축하고 빠르게 추론하는 방법을 제시하는 것입니다. 특히, 1.6조 파라미터 SwitchTransformer-c2048 모델의 크기를 3.2TB에서 160GB로 줄이고도 정확도를 크게 해치지 않습니다. 또한, 커스텀 압축 형식과 고성능 GPU 디코딩 커널을 설계하여 저렴한 컴퓨팅 자원을 사용하여도 원활한 추론이 가능하게 합니다.

#### 데이터 의존 양자화를 대규모 모델에 적용하기 (Scaling Data-dependent Quantization to Trillion Parameter MoEs)
이 섹션에서는 대규모 MoE 모델을 압축하는 데에 필요한 데이터 의존 양자화 기법을 설명합니다. 주요 과제로는 메모리 요구량, GPU 활용도, 신뢰성 문제 등이 있으며, 이를 해결하기 위한 시스템 디자인 및 최적화 기법을 다룹니다. 또한, 압축 중간 결과를 효율적으로 처리하기 위한 최적화된 활성화 오프로드 기법에 대해서도 설명합니다.

#### 관련 연구 (Related Work)
여러 관련 연구들을 요약하며, 기존 모델 압축 기법과의 비교를 통해 QMoE의 우수성을 강조합니다. 특히, MoE 모델의 양자화와 관련된 연구들과 이를 개선하는 새로운 접근 방식들에 대해 논의합니다.

### 2. 전체 요약

이 논문은 QMoE라는 새로운 프레임워크를 제시하여 대규모 모델, 특히 1.6조 파라미터 SwitchTransformer와 같은 MoE 모델의 실용적인 배포를 가능하게 합니다. QMoE는 모델의 크기를 3.2TB에서 160GB로 줄여도 정확도를 유지하며, 저렴한 컴퓨팅 자원으로도 빠른 추론을 할 수 있는 방법을 제공합니다. 이를 통해 대규모 모델의 추론 비용 문제를 해결하고, 더 많은 연구와 실용적인 응용을 가능하게 합니다.

이 논문의 핵심 기여는 세 가지입니다:
1. 대규모 MoE 모델에 대한 고도로 확장 가능한 압축 알고리즘 구현.
2. 커스텀 압축 형식과 빠른 디코딩을 위한 맞춤형 GPU 커널 설계.
3. 압축 기술 적용 후에도 모델의 정확도를 거의 해치지 않고 소형 하드웨어에서 실행 가능.

QMoE의 이러한 혁신적인 접근 방식은 대규모 모델의 실용성을 높여 AI 연구 및 응용의 발전에 기여할 것입니다.

## Similar Papers
- [Extreme Compression of Large Language Models via Additive Quantization](2401.06118.md)
- [EdgeMoE: Fast On-Device Inference of MoE-based Large Language Models](2308.14352.md)
- [Who Says Elephants Can't Run: Bringing Large Scale MoE Models into Cloud Scale Production](2211.10017.md)
- [SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression](2306.03078.md)
- [AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration](2306.00978.md)
- [SparseGPT: Massive Language Models Can Be Accurately Pruned in One-Shot](2301.00774.md)
- [LLM in a flash: Efficient Large Language Model Inference with Limited Memory](2312.11514.md)
- [PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU](2312.12456.md)
- [Efficient LLM Inference on CPUs](2311.00502.md)
