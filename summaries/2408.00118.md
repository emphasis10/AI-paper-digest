# Gemma 2: Improving Open Language Models at a Practical Size
## TL;DR
## Summary
- [https://arxiv.org/pdf/2408.00118.pdf](https://arxiv.org/pdf/2408.00118.pdf)

### 1. 섹션별 요약

#### 1. Introduction
Gemma 2는 다양한 규모 (2억~27억 파라미터)를 가지며, 특히 작은 모델들의 성능을 개선하기 위한 다양한 기법을 도입한 오픈 모델이다. 이 논문은 대규모 언어 모델(LLM)이 최근 여러 언어 이해, 생성, 추론 능력을 보여주며, 모델의 크기가 증가할수록 성능도 향상된다는 점을 기반으로 한다. Gemma 2는 지식 증류(knowledge distillation) 방식을 사용하여 학습된 모델로, 작은 모델에서도 우수한 성능을 보인다. 모델 학습 시 각 토큰 예측 대신 좀 더 풍부한 정보 분포를 사용한다는 점이 주요 혁신점이다.

#### 2. Model Architecture
Gemma 2는 기존 Gemma 모델들과 유사하게 디코더 전용 Transformer 아키텍처를 기반으로 한다. 주된 특징으로는 글로벌 및 로컬 어텐션 레이어를 결합한 벨타지(Beltagy) 기법과 그룹-쿼리 어텐션(GQA)을 적용하였다. 이로 인해 모델의 성능을 개선하고, 파라미터 수를 줄인 상태에서도 높은 성능을 유지할 수 있다.

#### 3. Pre-training
사전 학습 단계에서 대규모 데이터셋을 사용해 모델의 이해도를 높였으며, 이를 통해 사전 훈련된 여러 언어적 맥락을 잘 학습할 수 있게 하였다. 여기서 중요한 점은 훈련 데이터가 모델의 최종 성능에 결정적인 영향을 미친다는 것이다.

#### 4. Training Data
훈련 데이터는 다양한 언어와 문서로부터 수집하여 모델이 다양한 상황에서도 잘 작동할 수 있도록 하였다. 또한 데이터 증강 기술을 통해 모델의 일반화 능력을 향상시켰다.

#### 5. Knowledge Distillation
지식 증류는 큰 모델의 출력 분포를 활용해 작은 모델을 훈련시키는 방식을 의미한다. 이를 통해 작은 모델에서도 큰 모델의 성능을 일부 이어받을 수 있다. 또한 이 기법을 통해 훈련 시간을 단축시키고, 더 적은 데이터로도 높은 성능을 낼 수 있게 한다.

#### 6. Compute Infrastructure
높은 계산 자원을 효율적으로 사용하기 위해 분산 학습 및 고성능 컴퓨팅 인프라를 구축하였다. 이를 통해 대규모 데이터와 모델을 효과적으로 처리할 수 있었다.

#### 7. Ablations
여러 실험을 통해 다양한 설정에서의 성능 변화를 관찰하였다. 예를 들어, 지식 증류 기법을 사용한 모델은 동일한 파라미터 크기의 모델보다 성능이 뛰어나며, 다층 어텐션 기법을 사용한 모델도 성능 향상을 보인다.

#### 8. Evaluation
모델은 여러 벤치마크 테스트를 통해 평가되었으며, 다양한 언어 이해 과제에서 높은 성능을 보인다. 특히 인간 평가와 자동 평가 모두에서 우수한 결과를 얻었다.

#### 9. Discussion and Conclusion
논문에서는 Gemma 2 모델의 지식 증류 기법이 기존의 토큰 예측 방식보다 우수하다는 점을 다시 한번 강조하고 있다. 이러한 모델을 커뮤니티에 공개함으로써 연구와 개발의 새로운 물결을 이끌기를 기대하고 있다.

### 2. 전체 요약
Gemma 2는 다양한 규모와 개선된 성능을 가진 다목적 언어 모델로, 특히 작은 모델에서도 뛰어난 성능을 냄으로써 현재의 대규모 언어 모델이 가진 한계를 극복하고자 한다. 주요 혁신점은 지식 증류(knowledge distillation) 기법을 통해 작은 모델에서도 큰 모델의 성능을 구현하는 것에 있다. 이를 통해 학습 시간과 자원을 절약하면서도, 높은 수준의 언어 이해와 생성 능력을 보인다. 이 논문은 여러 실험과 평가를 통해 Gemma 2 모델의 성능, 안전성, 효율성을 입증하며, 향후 연구와 개발에 큰 영향을 미칠 것으로 기대된다.