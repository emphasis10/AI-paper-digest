# LoRA: Low-Rank Adaptation of Large Language Models
## TL;DR
## Summary
- [https://arxiv.org/pdf/2106.09685.pdf](https://arxiv.org/pdf/2106.09685.pdf)

#### 섹션별 주요 내용 요약:

1. **소개 및 문제 상태 (1 INTRODUCTION & 2 PROBLEM STATEMENT)**
   - 이 섹션에서는 자연어 처리(NLP)에서 대규모 사전 훈련된 모델을 특정 작업이나 도메인에 적용할 필요성에 대해 설명합니다. 특히 GPT-3와 같이 매우 큰 모델을 전체적으로 미세 조정하는 것의 비효율성을 지적하며, 저장 공간 및 계산 비용을 절약하면서도 효과적인 적응 방식을 찾는 것이 주된 도전 과제입니다. **LoRA(Low-Rank Adaptation)** 방식을 제안하여, 사전 훈련된 모델의 가중치를 고정하고 각 레이어에 저랭크 분해 행렬을 삽입함으로써, 다운스트림 작업에 대한 훈련 가능한 매개변수의 수를 크게 줄입니다. 이 방식은 저장 요구 사항과 GPU 메모리 사용을 상당히 줄이면서도 모델 품질을 유지하거나 개선할 수 있음을 주장합니다.

#### 주요 기여 및 혁신 부분:

LoRA는 대규모 언어 모델의 적응에서 저랭크 분해를 사용하여 파격적으로 적은 수의 매개변수로 효율적인 fine-tuning을 실현합니다. 이는 기존의 fine-tuning 방식과 비교할 때 저장 공간, 계산 비용을 대폭 절감하면서, 모델 효율성과 작업 전환 효율성을 크게 향상시키는 혁신적인 접근 방식입니다. 특히, 이 방법은 추가적인 추론 지연 없이 기존의 fine-tuning 모델과 동일한 수준의 성능을 제공한다는 점에서 주목할 만합니다. 또한, LoRA는 다양한 NLP 작업에서의 모델 품질을 향상시키면서도 GPT-3 같은 대규모 모델에 적용할 경우, 훈련 가능한 매개변수 수를 10,000배까지 줄일 수 있음을 실험을 통해 입증했습니다.

#### 전체 요약:

본 논문에서 제안하는 **LoRA(Low-Rank Adaptation)**는 대규모 사전 훈련된 언어 모델을 효율적으로 다운스트림 작업에 적응시키는 새로운 접근 방법입니다. 특히, 각 트랜스포머 레이어에 저랭크 분해 행렬을 삽입하고 사전 훈련된 모델의 가중치를 고정함으로써, 다운스트림 작업에 필요한 훈련 가능한 매개변수의 수를 현저히 줄이는 동시에 저장 공간 및 계산 비용을 절감하는 방법을 제시합니다. 이 방법은 추가적인 추론 지연 없이 우수한 모델 품질을 유지하면서, 대규모 언어 모델을 다양한 NLP 작업에 적용하기 위한 새로운 표준을 제시한다는 점에서 큰 의의를 지닙니다. LoRA는 GPT-3, GPT-2, RoBERTa, DeBERTa 등과 같은 대표적인 언어 모델에 대한 실험을 통해 그 효율성과 효과를 입증했습니다. 이는 향후 NLP 연구 및 어플리케이션 개발에서 더욱 효율적이고 효과적인 모델 적응 방법으로 활용될 수 있는 가능성을 열어줍니다.

## Similar Papers
- [MultiLoRA: Democratizing LoRA for Better Multi-Task Learning](2311.11501.md)
- [AdaMix: Mixture-of-Adaptations for Parameter-efficient Model Tuning](2205.12410.md)
- [Grass: Compute Efficient Low-Memory LLM Training with Structured Sparse Gradients](2406.17660.md)
- [Tied-Lora: Enhancing parameter efficiency of LoRA with weight tying](2311.09578.md)
- [The Unreasonable Ineffectiveness of the Deeper Layers](2403.17887.md)
- [DoRA: Weight-Decomposed Low-Rank Adaptation](2402.09353.md)
- [PiSSA: Principal Singular Values and Singular Vectors Adaptation of Large Language Models](2404.02948.md)
- [VeRA: Vector-based Random Matrix Adaptation](2310.11454.md)
- [Your Transformer is Secretly Linear](2405.12250.md)
