# Your Transformer is Secretly Linear
## TL;DR
## Summary
- [https://arxiv.org/pdf/2405.12250.pdf](https://arxiv.org/pdf/2405.12250.pdf)

### 섹션 요약

#### 1. 서론
이 연구는 트랜스포머 디코더에서 거의 완벽한 선형 특성을 발견했습니다. Procrustes 유사성 분석을 통해 연속 층 간 임베딩 변환이 거의 선형(유사성 점수 0.99)을 보임을 확인했습니다. 선형성은 잔차(residual) 성분을 제거할 때 감소합니다. 일부 선형 블록을 제거하거나 선형적으로 근사해도 모델 성능에 큰 영향을 주지 않았습니다. 소규모 모델에서 코사인 유사성 기반 정규화를 도입하여 성능을 개선했습니다.

#### 2. 관련 연구
스파시티를 활용한 모델 프루닝은 기계 학습에서 중요한 주제입니다. 여러 연구가 LLM에서 스파시티를 통한 효율성 향상을 탐구했습니다. WANDA는 단일 포워드 패스로 프루닝을 구현하며, 이 연구는 선형적 특성을 활용하여 트랜스포머 모델을 프루닝하는 여러 기술을 조사합니다.

#### 3. 사전 훈련된 아키텍처 분석
임베딩의 선형성을 평가하기 위해 일반화된 Procrustes 유사성 지표를 사용했습니다. 모든 테스트된 트랜스포머 디코더 층의 선형성 점수가 1에 가까웠으며, 잔차 스트림의 각 블록 기여도가 낮았습니다. 선형 블록의 조합이 비선형 결과를 초래할 수 있음을 발견했습니다.

#### 4. 선형성을 개선하는 정규화 사전 훈련
Mistral 아키텍처를 사용하여 정규화된 사전 훈련 실험을 수행했습니다. 연속 층 간 임베딩의 관계를 조정하기 위해 MSE 및 코사인 유사성 정규화 항을 도입했습니다. 코사인 유사성 기반 접근 방식이 가장 유망한 결과를 보였으며, 이를 통해 모델 성능이 향상되었습니다.

#### 5. 선형성을 활용한 프루닝
가장 선형적인 층을 제거하는 프루닝 전략을 탐구했습니다. 선형 근사 및 증류 손실을 통합하여 성능 저하를 최소화했습니다. 이 방법은 성능을 유지하면서 모델 크기를 줄이는 데 효과적이었습니다.

#### 6. 결론
트랜스포머 디코더의 선형성에 대한 심층 탐구를 통해 모델 최적화와 효율성을 향상시키는 새로운 기회를 열었습니다. 프루닝 및 증류 기술을 제안하여 모델 성능을 저해하지 않고도 개선할 수 있음을 보였습니다.

### 전체 요약
이 연구는 트랜스포머 디코더에서 거의 완벽한 선형 특성을 발견하고, 이를 활용한 다양한 최적화 기술을 제안했습니다. 선형적 특성을 활용한 프루닝 및 증류 기법은 모델 성능을 유지하면서도 효율성을 극대화할 수 있음을 보여주었습니다. 코사인 유사성 기반 정규화는 모델의 임베딩 표현력을 향상시켰습니다. 이 연구는 트랜스포머 아키텍처의 선형적 작동 방식을 재고하게 하며, 보다 효율적인 모델 개발에 기여할 수 있습니다.