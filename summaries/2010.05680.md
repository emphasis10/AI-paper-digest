# TurboTransformers: An Efficient GPU Serving System For Transformer Models
## TL;DR
## Summary
- [https://arxiv.org/pdf/2010.05680.pdf](https://arxiv.org/pdf/2010.05680.pdf)

### 1. 각 섹션별 요약

#### Introduction
이 논문에서는 Transformer 모델을 GPU 서버에서 효율적으로 배포하는 시스템인 TurboTransformers를 소개합니다. Transformer는 NLP에서 가장 중요한 알고리즘 혁신으로, 긴 시퀀스에서의 정확도를 높이기 위해 병렬 처리가 가능합니다. 그러나 Transformer의 구조는 더 많은 계산을 필요로 하며, 가변 길이의 입력 데이터를 처리하는 문제로 인해 메모리 관리 및 최적화가 어렵습니다.

#### Backgrounds
Transformer 모델의 핵심은 자기 주의 메커니즘으로, 시퀀스의 다양한 위치에 주의를 기울여 해당 시퀀스의 표현을 계산합니다. 이 모델은 RNN이나 CNN 대신 자기 주의 레이어 스택을 사용하여 가변 길이 입력 데이터를 처리합니다. 이는 배칭(batching) 처리 시 짧은 시퀀스와 긴 시퀀스를 효율적으로 처리하는 데 어려움을 초래합니다.

#### Design Overview
TurboTransformers는 추론(runtime)과 서빙 프레임워크로 구성되어 있습니다. 서빙 시스템은 네트워크 요청을 받아와 Transformer 모델을 사용해 결과를 처리합니다. 이 논문은 주로 추론(runtime)과 서빙 프레임워크 각각의 최적화 방법에 대해 다룹니다.

#### Inference Runtime
추론(runtime)에서는 계산 효율성을 높이고 메모리 할당을 최적화하는 것에 집중합니다. 이를 위해 병렬 알고리즘과 가변 길이 입력을 처리할 수 있는 메모리 할당 알고리즘, 그리고 동적 프로그래밍을 이용한 배치 스케줄러를 도입했습니다.

#### Computational Optimizations
TurboTransformers는 GPU를 위한 병렬 알고리즘을 제안하여 Softmax와 LayerNorm 같은 주요 연산의 효율성을 극대화합니다. 이를 통해 짧은 시퀀스 요청에서 높은 성능을 발휘합니다.

#### Memory Manager
고정 길이 입력에 비해 가변 길이 입력의 메모리 할당 효율성과 메모리 풋프린트를 동시에 최적화하는 것은 복잡한 문제입니다. TurboTransformers는 그래프-토폴로지-감지 메모리 할당 알고리즘을 사용하여 중간 텐서 메모리 사용을 최적화합니다.

#### Serving Framework
서빙 프레임워크는 네트워크 요청을 받아 캐싱 및 배칭 최적화를 통해 처리합니다. TurboTransformers는 새로운 배치 스케줄러를 도입하여 가변 길이 요청 처리 시 최적의 처리량을 달성합니다.

#### Experimental Results
TurboTransformers는 PyTorch 및 onnxruntime 대비 짧은 요청에서 뛰어난 성능을 보입니다. BERT, ALBERT, 그리고 DistilBERT 모델에서 TurboTransformers는 평균적으로 1.13배에서 1.25배의 속도 향상을 보였습니다.

#### Conclusion
이 논문은 TurboTransformers라는 시스템을 통해 가변 길이 입력을 효율적으로 처리하고 GPU 서버 환경에서 Transformer 모델을 최적화할 수 있는 방법을 제안합니다. 이 시스템은 최소한의 코드 변경으로 PyTorch 코드에 쉽게 통합될 수 있습니다.

### 2. 전체 요약

TurboTransformers는 Transformer 모델을 GPU 서버에서 효율적으로 배포하기 위한 시스템으로, 추론(runtime)과 서빙 프레임워크 두 가지 주요 구성 요소로 이루어져 있습니다. 이 시스템은 특히 다음 세 가지 혁신적인 기능을 통해 뛰어난 성능을 발휘합니다:

1. GPU 기반 병렬 알고리즘을 이용한 연산 최적화.
2. 가변 길이 입력 상황에서도 효율적인 메모리 할당 알고리즘.
3. 동적 프로그래밍을 활용한 새로운 배치 스케줄러.

TurboTransformers는 특히 짧은 시퀀스 요청에서 PyTorch 대비 최고 2.44배의 성능을 보이며, BERT, ALBERT, 그리고 DistilBERT 모델에서 전반적으로 높은 성능을 보였습니다. 이를 통해 현실적인 네트워크 서비스에서 Transformer 모델의 적용을 용이하게 만들 수 있습니다.

이 논문은 Transformer 모델을 현실 세계의 애플리케이션에 효과적으로 사용할 수 있도록 돕기 위한 혁신적인 접근 방법을 제안하며, 이는 AI와 머신러닝의 발전에 중요한 기여를 할 것입니다.

## Similar Papers
- [Reducing shared memory footprint to leverage high throughput on Tensor Cores and its flexible API extension library](2308.15152.md)
- [ByteTransformer: A High-Performance Transformer Boosted for Variable-Length Inputs](2210.03052.md)
- [FlashDecoding++: Faster Large Language Model Inference on GPUs](2311.01282.md)
- [AUITestAgent: Automatic Requirements Oriented GUI Function Testing](2407.09018.md)
- [Efficient Memory Management for Large Language Model Serving with PagedAttention](2309.06180.md)
- [Talaria: Interactively Optimizing Machine Learning Models for Efficient Inference](2404.03085.md)
- [Matting by Generation](2407.21017.md)
- [SpecInfer: Accelerating Generative Large Language Model Serving with Tree-based Speculative Inference and Verification](2305.09781.md)
- [LogoMotion: Visually Grounded Code Generation for Content-Aware Animation](2405.07065.md)
