# Generalist Reward Models: Found Inside Large Language Models
## TL;DR
## Summary
- [https://arxiv.org/pdf/2506.23235.pdf](https://arxiv.org/pdf/2506.23235.pdf)

### 1. 논문의 각 섹션 요약

**1. 서론**
이 논문은 대규모 언어 모델(LLM)을 인간의 복잡한 가치(예: 도움과 정직)에 맞게 조정하는 것은 AI 개발에서 중요한 문제임을 서술합니다. 기존의 접근법은 주로 인간의 피드백으로부터 보상을 학습하는 강화 학습(RLHF)에 의존하고 있으며, 이 과정은 많은 비용이 드는 고품질의 인간 선호 데이터세트에 의존하는 문제를 안고 있습니다.

**2. 기초**
LLM은 다음 토큰을 예측하는 방식으로 작동하며, 이 과정은 MDP(마르코프 결정 과정)로 형식화됩니다. 이러한 모델은 웹 데이터로 사전학습되거나, 고품질의 인간 반응 데이터로 세밀 튜닝됩니다.

**3. 이론적 발전**
논문은 내재적 보상 모델이 이론적으로 오프라인 역강화학습(IRL)와 같다는 것을 입증하며, 이 보상 모델이 LLM의 내재된 특성에서 도출될 수 있음을 이론적으로 증명합니다. 이 접근법은 기존 방법보다 성능이 뛰어나다고 실험적으로도 입증되었습니다.

**4. 혁신적 강화 학습**
내재적 보상을 통한 강화 학습은 전통적인 보상 모델 단계가 불필요하게 되며, 이는 RLHF 파이프라인을 더 간단하고 효율적으로 만듭니다. 또한 이미 훈련된 모델의 능력을 작은 모델에 전이하는 지식 증류의 새로운 방식도 제안합니다.

**5. 다모드 강화 학습으로의 확장**
이 내재적 보상 메커니즘은 텍스트에 한정되지 않으며, 이미지 생성, 비디오 합성, 음악 작곡 모델에도 적용될 수 있어, 다중 모달 AI의 질과 제어 가능성을 향상시킬 수 있습니다.

**6. 결론**
논문은 외부 보상 모델이 필요 없는 새로운 내재적 보상 개념을 도입하며, 이는 AI 모델을 조정하는 혁신적인 방법을 제시합니다.

### 2. 전체 요약
이 논문은 LLM의 조정을 위한 새로운 패러다임을 제시하며, 내재적 보상 메커니즘을 통해 기존의 복잡한 RLHF 파이프라인을 단순화하고 더 효율적인 방법을 제공합니다. 내재적인 보상 모델을 이용하면, 고비용의 외부 보상 모델 없이도 기존 모델의 능력을 활용할 수 있으며, 강화학습을 통해 더 높은 성능을 확보할 수 있습니다. 이 접근법은 향후 다중 모달 AI에까지 적용할 수 있는 가능성을 열어, AI 맞춤화의 새로운 지평을 여는 중요한 기여를 합니다.