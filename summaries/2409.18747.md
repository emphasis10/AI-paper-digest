# Cottention: Linear Transformers With Cosine Attention
## TL;DR
## Summary
- [https://arxiv.org/pdf/2409.18747.pdf](https://arxiv.org/pdf/2409.18747.pdf)

### 논문 요약 및 분석

이 논문은 Cosine Attention을 활용한 새로운 주의 메커니즘을 소개하며 Transformer 모델의 성능 향상을 논의합니다. 아래는 각 섹션별 주요 내용과 기여점입니다.

#### 1. 서론

- **내용 요약:** Transformer 모델은 자연어 처리(NLP)와 컴퓨터 비전(CV) 분야에서 큰 성공을 거두었지만, 기존의 소프트맥스 주의 메커니즘은 시퀀스 길이에 따라 메모리와 계산 복잡도가 증가하는 문제가 있습니다.
- **주요 기여:** 메모리 효율성과 계산의 선형 복잡성을 유지하면서 기존의 소프트맥스 주의를 대체할 수 있는 Cosine Attention 메커니즘을 제안합니다.

#### 2. 관련 연구

- **내용 요약:** 이 섹션은 소프트맥스 주의 메커니즘과 다양한 서브쿼드(at) 주의 메커니즘에 대한 연구를 리뷰합니다.
- **주요 기여:** 다양한 주의 메커니즘의 장단점을 분석하며, 소프트맥스 주의를 대체할 수 있는 접근 방식의 필요성을 강조합니다.

#### 3. 알고리즘

- **내용 요약:** 코사인 유사도와 이를 활용한 Cosine Attention 메커니즘을 설명하고, 이를 안정화할 수 있는 방법을 논의합니다.
- **주요 기여:** 기존의 소프트맥스 연산을 코사인 유사도로 대체하여 계산 효율성을 높이는 방식을 제안하며, 안정적인 동작을 위한 정상화 기법을 제안합니다.

#### 4. 서브쿼드(at) 코사인 주의

- **내용 요약:** 서브쿼드(at) 코사인 주의 메커니즘을 소개하고 이를 RNN 형태로 재구성하는 방법을 논의합니다.
- **주요 기여:** 코사인 주의가 RNN 구조로 변환될 수 있음을 보여줌으로써 시퀀스 길이와 상관없이 일정한 메모리 사용을 가능하게 합니다.

#### 5. 결과

- **내용 요약:** BERT와 GPT-J 모델을 사용해 Cosine Attention의 성능을 평가하고, 소프트맥스 주의와 비교하여 메모리 사용량과 시간 복잡도를 분석합니다.
- **주요 기여:** 코사인 주의가 소프트맥스 주의와 유사한 성능을 유지하면서도 메모리와 계산 효율성을 크게 향상시킴을 실험적으로 증명합니다.

#### 6. 결론 및 향후 연구

- **내용 요약:** 코사인 주의의 성능을 평가하며, 향후 연구 방향으로 CUDA 커널 최적화, 대규모 모델 적용, 다양한 정상화 기법 탐색 등을 제안합니다.
- **주요 기여:** 실험 결과를 토대로 코사인 주의의 가능성을 확인하고, 다양한 응용 분야에서의 추가 연구 필요성을 강조합니다.

### 전체 요약

이 논문은 Transformer 모델의 주의 메커니즘에서 발생하는 메모리와 계산 복잡도 문제를 해결하기 위해 Cosine Attention을 제안합니다. 주요 내용은 현대의 NLP 및 CV 응용에서 필수적인 주의 메커니즘을 더 효율적으로 만들기 위한 새로운 접근 방식을 제시하는 것입니다. 특히, 코사인 유사도를 활용하여 기존의 소프트맥스 주의를 대체함으로써 메모리 사용량을 줄이고 계산 복잡도를 선형으로 유지하는 방법을 보여줍니다. 실험 결과는 BERT와 GPT-J 모델을 활용한 사례를 통해 이를 입증하고 있으며, 향후 CUDA 커널 최적화와 대규모 모델 적용 등의 연구를 제안합니다.

이 논문은 주의 메커니즘의 새로운 가능성을 탐구하고, 현재의 제한점을 극복하기 위한 중요한 발판을 제공합니다.