# Self-Improving Robust Preference Optimization
## TL;DR
## Summary
- [https://arxiv.org/pdf/2406.01660.pdf](https://arxiv.org/pdf/2406.01660.pdf)

## 개요
이 논문은 **Self-Improving Robust Preference Optimization (SRPO)**라는 새로운 오프라인 강화 학습 프레임워크를 제안합니다. 기존의 강화 학습 방법들은 특정 작업에 매우 종속적이어서, 작업이 바뀔 때 성능이 저하되는 문제를 겪습니다. SRPO는 이러한 문제를 해결하기 위해 자기 개선 프로세스를 활용하여 인간의 선호도로부터 학습하는 방법을 제시합니다. SRPO는 자기 개선 정책과 생성 정책을 적대적으로 최적화하여, 훈련 작업에 독립적인 최적의 솔루션을 제공합니다. 이는 보상 모델이나 온라인 추론 없이도 대규모로 표준 감독 최적화 기법을 사용하여 최적화할 수 있습니다.

## 1. 도입
### 주요 내용
- 인간의 피드백을 통한 강화 학습(RLHF)이 대형 언어 모델(LLM)의 정렬을 위해 널리 사용됨.
- 기존 방법들은 훈련 작업에 강하게 의존, 작업 분포가 변할 때 성능 저하.
- SRPO는 작업 분포의 변화에 견고한 솔루션을 제공하기 위해 제안됨.

## 2. 자기 개선 정책 학습
### 주요 내용
- 인간의 선호 데이터는 더 선호되는 완성도를 향상시키는 정보를 제공.
- 기존 RLHF 방법과 달리, 자기 개선 모델은 하위 완성도를 상위 완성도로 개선하는 규칙을 학습함.
- SRPO는 자기 개선 정책을 최적화하여 LLM의 출력을 반복적으로 향상시킴.

## 3. SRPO 목표
### 주요 내용
- SRPO의 목표는 맥락에서 자기 개선 모델을 학습하는 것.
- 주어진 맥락과 완성도를 기반으로 개선된 완성도를 생성.
- 이 문제는 KL-정규화된 형태로 표현되어 표준 감독 최적화 방식으로 해결 가능.

## 4. 오프라인 SRPO 최적화 솔루션
### 주요 내용
- SRPO 최적화 문제는 두 단계의 적대적 최적화 문제로 해결.
- 첫 번째 단계는 최적의 자기 개선 정책을 학습.
- 두 번째 단계는 생성 정책을 학습하여 최소한의 개선이 필요한 완성도를 생성.

## 5. SRPO의 견고성
### 주요 내용
- SRPO는 행동 정책에 독립적인 견고한 솔루션을 제공.
- 기존의 방법들과 달리, SRPO는 행동 정책의 변화에 영향을 받지 않음.
- 실험을 통해 SRPO가 OOD(Out-Of-Distribution) 상황에서도 높은 성능을 유지함을 확인.

## 6. 관련 연구
### 주요 내용
- 기존의 오프라인 선호 최적화 연구들은 RLHF와 달리 행동 정책의 변화에 취약.
- SRPO는 이러한 문제를 해결하기 위해 제안됨.
- 자기 개선 모델을 학습하는 새로운 접근 방식을 제시.

## 7. 실험
### 주요 내용
- SRPO의 성능을 다양한 요약 작업에서 평가.
- TL;DR 요약 데이터셋과 XSum 데이터셋을 사용하여 실험.
- SRPO는 OOD 상황에서 DPO보다 높은 성능을 보임.

## 8. 토론 및 한계
### 주요 내용
- SRPO는 행동 정책의 변화에 강건한 솔루션을 제공함을 실험적으로 입증.
- 향후 연구에서는 더 복잡한 멀티 태스크 벤치마크에 SRPO를 적용할 계획.
- SRPO의 한계는 보다 복잡한 작업에 대한 성능 평가가 필요함.

## 전반적인 요약
SRPO는 인간의 선호도를 학습하여 대형 언어 모델을 정렬하는 새로운 오프라인 강화 학습 방법입니다. SRPO는 자기 개선 프로세스를 통해 작업 분포의 변화에 강건한 솔루션을 제공하며, 이는 보상 모델이나 온라인 추론 없이도 대규모로 최적화할 수 있습니다. 실험 결과, SRPO는 OOD 상황에서 기존 방법들보다 뛰어난 성능을 보였으며, 향후 더 복잡한 작업에 대한 적용 가능성을 가지고 있습니다.