# An LLM Compiler for Parallel Function Calling
## TL;DR
## Summary
- [https://arxiv.org/pdf/2312.04511.pdf](https://arxiv.org/pdf/2312.04511.pdf)

### 1. 각 섹션의 주요 내용 요약

#### 서론
최근 대형 언어 모델(LLM)의 추론 능력은 외부 함수 호출을 통해 자체적인 한계를 극복할 수 있게 되었습니다. 이 논문에서 소개된 "LLMCompiler"는 함수들을 병렬로 호출하여 효율성을 극대화하는 방법을 제안합니다. 이를 위해 실행 계획을 세우는 Function Calling Planner, 함수 호출 작업을 디스패치하는 Task Fetching Unit, 그리고 이 작업을 병렬로 실행하는 Executor라는 세 가지 주요 구성 요소가 도입되었습니다.

#### 관련 연구
기존 연구들은 모델 설계 최적화와 시스템 수준의 효율성을 중점적으로 다루었으나, LLMCompiler는 응용 프로그램 수준에서 효율성을 높이는 방법을 제시합니다. 병렬 실행의 이점을 활용하여 효율성을 극대화합니다.

#### 방법론
LLMCompiler의 구조는 세 가지 핵심 요소로 나뉩니다:
1. **Function Calling Planner** - 호출해야 할 함수들 간의 의존성을 분석하고 실행 계획을 세웁니다.
2. **Task Fetching Unit** - 중간 결과를 기반으로 다음 실행할 작업을 선택합니다.
3. **Executor** - 실제 작업을 병렬로 실행합니다.

#### 실험
LLMCompiler는 여러 작업에 대해 기존 ReAct 접근법보다 최대 3.7배 빠른 성능을 보였으며, 비용 효율성도 최대 6.7배 향상되었습니다. 또한 정확도도 약 9% 향상되었습니다. 다양한 복잡한 패턴에서 LLMCompiler의 높은 다이나믹 재계획 능력을 입증하였으며, 더 복잡한 문제 해결에서 뛰어난 성능을 보여주었습니다.

#### 토론
LLMCompiler는 병렬 함수 호출을 통해 LLM의 효율성을 크게 향상시키며, 사용자 정의와 오픈소스 모델에서 특히 유용합니다. 이를 통해 큰 규모의 복잡한 작업도 보다 신속하게 처리할 수 있게 되었습니다.

#### 결론
LLMCompiler는 병렬 함수 호출을 통해 대형 언어 모델의 효율성을 크게 향상시킵니다. 이 논문은 그 효율성을 입증하며, 향후 LLM 기반 애플리케이션의 개발 방향을 제시합니다.

### 2. 전체 요약
LLMCompiler는 대형 언어 모델(LLM)의 병렬 함수 호출을 최적화하는 혁신적인 방법론을 제안합니다. 기존의 순차적 함수 호출 방식의 비효율성을 극복하기 위해, 이 논문은 실행 계획을 세우고, 작업을 배분하며, 이를 병렬로 실행하는 구조를 제시합니다. 실험 결과, LLMCompiler는 속도, 비용 효율성, 및 정확도에서 기존 방법들보다 우수한 성과를 보였습니다. 이러한 병렬 처리는 특히 오픈소스 모델과 사용자 정의 모델에 적용 가능하여, LLM 기반 애플리케이션의 성능을 크게 향상시킬 수 있습니다.

## Similar Papers
- [Tree of Thoughts: Deliberate Problem Solving with Large Language Models](2305.10601.md)
- [SPEED: Speculative Pipelined Execution for Efficient Decoding](2310.12072.md)
- [Full Stack Optimization of Transformer Inference: a Survey](2302.14017.md)
- [Item-Language Model for Conversational Recommendation](2406.02844.md)
- [WILBUR: Adaptive In-Context Learning for Robust and Accurate Web Agents](2404.05902.md)
- [Characterizing Prompt Compression Methods for Long Context Inference](2407.08892.md)
- [FlexGen: High-Throughput Generative Inference of Large Language Models with a Single GPU](2303.06865.md)
- [MMAU: A Holistic Benchmark of Agent Capabilities Across Diverse Domains](2407.18961.md)
- [Demystifying Chains, Trees, and Graphs of Thoughts](2401.14295.md)
