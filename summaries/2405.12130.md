# MoRA: High-Rank Updating for Parameter-Efficient Fine-Tuning
## TL;DR
## Summary
- [https://arxiv.org/pdf/2405.12130.pdf](https://arxiv.org/pdf/2405.12130.pdf)

### 요

이 논문은 "MoRA: High-Rank Updating for Parameter-Efficient Fine-Tuning"이라는 제목으로, 대형 언어 모델(LLM)의 파라미터 효율적인 미세 조정을 위해 새로운 방법론을 제안합니다. MoRA는 기존의 LoRA 방식이 가지는 한계를 극복하기 위해 고차원 업데이트 방식을 도입했습니다.

#### 1. 소개
- 대형 언어 모델(LLM)이 다양한 작업에서 뛰어난 성능을 발휘하지만, 모든 파라미터를 업데이트하는 완전 미세 조정(FFT)은 메모리와 계산 비용이 많이 듭니다.
- 파라미터 효율적인 미세 조정(PEFT)은 적은 수의 파라미터만을 업데이트하여 약슷한 성능을 얻는 방법으로, LoRA가 대표적입니다.
- 본 논문은 LoRA의 저차원 업데이트가 새로운 지식을 학습하고 기억하는 데 한계가 있음을 지적하고, 이를 극복하기 위해 MoRA를 제안합니다.

#### 2. 관련 연구
- LoRA는 저차원 행렬을 사용하여 파라미터 업데이트를 수행하는 방법으로, 다양한 PEFT 방법 중에서도 널리 사용됩니다.
- 기존 연구들은 LoRA의 성능을 향상시키기 위해 다양한 접근법을 시도했지만, 대부분은 명령어 튜닝이나 텍스트 분류 작업에서만 효율성을 검증했습니다.

#### 3. 방법론
1. **고차원 업데이트**:
   - MoRA는 저차원 행렬 대신 정사각 행렬을 사용하여 더 높은 차원의 업데이트를 수행합니다.
   - 입력 차원을 줄이고 출력 차원을 늘리는 비파라미터 연산자를 도입하여 업데이트 행렬의 차원을 조정합니다.
   - 이로 인해 MoRA는 더 많은 지식을 학습하고 기억할 수 있습니다.

2. **압축 및 디컴프레션 연산자**:
   - MoRA는 입력 차원을 줄이고 출력 차원을 늘리는 비파라미터 연산자를 사용하여 정사각 행렬을 활용합니다.
   - 다양한 방법으로 압축 및 디컴프레션 연산자를 구현하여 성능을 최적화했습니다.

#### 4. 실험
- MoRA의 성능을 다양한 작업에서 평가했습니다.
- UUID 페어를 기억하는 작업, 명령어 튜닝, 수학적 추론, 지속적 사전 훈련 등의 작업에서 LoRA와 비교했습니다.
- MoRA는 특히 기억 집약적인 작업에서 LoRA보다 우수한 성능을 보였으며, 다른 작업에서도 비교 가능한 성능을 보였습니다.

#### 5. 결론
- MoRA는 저차원 업데이트의 한계를 극복하기 위해 고차원 업데이트를 도입한 새로운 방법론입니다.
- 다양한 실험을 통해 MoRA의 우수성을 입증했으며, 이는 새로운 지식을 학습하고 기억하는 데 효과적입니다.
- 향후 연구에서는 더 높은 품질의 훈련 데이터와 지식 증류, 인간 선호도 최적화와 같은 훈련 전략을 도입하여 성능을 더욱 향상시킬 계획입니다.

### 전체 요약
이 논문은 대형 언어 모델의 파라미터 효율적인 미세 조정을 위해 MoRA라는 새로운 방법론을 제안합니다. MoRA는 기존의 LoRA 방식이 가지는 저차원 업데이트의 한계를 극복하기 위해 고차원 업데이트 방식을 도입했습니다. 이를 위해 정사각 행렬과 비파라미터 연산자를 사용하여 입력과 출력 차원을 조정했습니다. 다양한 실험을 통해 MoRA는 특히 기억 집약적인 작업에서 우수한 성능을 보였으며, 다른 작업에서도 비교 가능한 성능을 보였습니다. MoRA는 파라미터 효율적인 미세 조정 방법으로, 대형 언어 모델이 새로운 지식을 학습하고 기억하는 데 효과적인 솔루션을 제공합니다.