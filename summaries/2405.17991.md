# VeLoRA: Memory Efficient Training using Rank-1 Sub-Token Projections
## TL;DR
## Summary
- [https://arxiv.org/pdf/2405.17991.pdf](https://arxiv.org/pdf/2405.17991.pdf)

### 논문의 중요 내용 요약

#### 1. 초록
이 논문은 대규모 언어 모델(LLMs)의 훈련과 미세 조정 과정에서 발생하는 메모리와 계산 비용을 감소시키기 위한 새로운 알고리즘, VeLoRA(Vector projected LoRA)를 제안합니다. 이 알고리즘은 중간 활성화 값을 효과적으로 압축하여 원본 성능을 유지하면서 메모리 사용량을 줄입니다. VeLoRA는 다수의 최첨단 메모리 효율적인 훈련 방법들과 비교하여 낮은 GPU 메모리를 요구합니다.

#### 2. 서론
대규모 언어 모델은 높은 성능을 보이지만 훈련과 미세 조정 과정에서 많은 메모리와 계산 자원이 필요합니다. 기존의 다양한 방법들은 메모리 요구사항을 줄이기 위해 개발되었지만, 이들 방법은 종종 추가적인 계산 오버헤드를 유발하거나 메모리 절약의 한계를 가집니다. VeLoRA는 이러한 문제를 해결하면서도 높은 효율성을 자랑합니다.

#### 3. 관련 연구
메모리 효율적인 훈련 방법에는 Gradient Checkpointing, Adafactor, LOMO, GaLore 등이 있습니다. 그러나 이들은 모두 계산 오버헤드를 유발하거나 메모리를 완전히 절약하지 못하는 한계가 있습니다. PeFT(파라미터 효율적인 미세 조정)는 적은 수의 학습 가능한 파라미터를 사용하여 대규모 모델을 효율적으로 미세 조정하는 방법입니다. VeLoRA는 이 분야의 다양한 다른 방법들과도 상호 보완적으로 작용합니다.

#### 4. 메서드
VeLoRA는 중간 활성화 값을 압축하여 훈련 과정에서 메모리 사용량을 줄이며, 고정된 1차원 벡터를 사용하여 압축된 활성화 값을 다시 재구성합니다. 이 방법은 기존의 Gradient Checkpointing이나 SVD를 사용하는 방법에 비해 덜 비싸며, 메모리를 현저히 줄일 수 있습니다. 또한 다른 PEFT 방법과도 효과적으로 결합될 수 있습니다.

#### 5. 실험
다양한 벤치마크 실험에서 VeLoRA는 다른 최첨단 방법들과 비교하여 메모리 사용량을 줄이면서도 높은 성능을 보였습니다. 예를 들어, RoBERTa 근거 GLUE 벤치마크에서 미세 조정 없이도 비슷한 성능을 보였습니다.

#### 6. 결론
VeLoRA는 훈련 과정에서 중간 활성화 값을 효과적으로 압축하고, 최첨단 메모리 효율적인 훈련 방법들과 잘 결합되는 방법입니다. 이 방법은 중규모 비전 트랜스포머와 대규모 언어 모델에서 모두 효과적인 것으로 입증되었습니다. 다양한 벤치마크에서 높은 성능을 보이며 메모리 요구사항을 크게 줄였습니다.

### 전체 요약

VeLoRA는 대규모 언어 모델의 훈련과 미세 조정에서 메모리 사용량을 크게 줄이는 새로운 방법입니다. 이 방법은 중간 활성화 값을 압축하고, 다시 재구성하는 프로세스를 통해 GPU 메모리 사용량을 현저히 줄입니다. 기존의 다양한 메모리 효율적인 방법들에 비해 계산 오버헤드가 적으며, GLUE 벤치마크 및 다른 여러 벤치마크에서 높은 성능을 보였습니다. VeLoRA는 미래에 더 큰 모델들이 효율적으로 훈련될 수 있도록 하는 중요한 기술적 진보를 이뤄냅니다.

논문의 핵심 기여와 혁신적인 부분은 메모리 효율성을 중시한 새로운 알고리즘 VeLoRA의 개발 및 이를 통한 실제 벤치마크에서의 성능 입증입니다. 이 방법은 특히 메모리 제한이 있는 환경에서 큰 모델을 훈련하고 미세 조정하는 데 유용합니다.

## Similar Papers
- [MultiLoRA: Democratizing LoRA for Better Multi-Task Learning](2311.11501.md)
- [Q-GaLore: Quantized GaLore with INT4 Projection and Layer-Adaptive Low-Rank Gradients](2407.08296.md)
- [Grass: Compute Efficient Low-Memory LLM Training with Structured Sparse Gradients](2406.17660.md)
- [LoRA: Low-Rank Adaptation of Large Language Models](2106.09685.md)
- [OLoRA: Orthonormal Low-Rank Adaptation of Large Language Models](2406.01775.md)
- [Conformer-Based Speech Recognition On Extreme Edge-Computing Devices](2312.10359.md)
- [2BP: 2-Stage Backpropagation](2405.18047.md)
- [PiSSA: Principal Singular Values and Singular Vectors Adaptation of Large Language Models](2404.02948.md)
- [Training Neural Networks from Scratch with Parallel Low-Rank Adapters](2402.16828.md)
