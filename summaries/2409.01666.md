# In Defense of RAG in the Era of Long-Context Language Models
## TL;DR
## Summary
- [https://arxiv.org/pdf/2409.01666.pdf](https://arxiv.org/pdf/2409.01666.pdf)

### 논문 주요 내용 요약 및 설명

#### 1. 각 섹션 내용 요약

**Abstract**
이 논문은 길이 제한이 있는 기존의 언어 모델(LLM)을 보완하기 위해 도입된 검색 보강 생성(RAG)의 성능을 다시 평가합니다. 최근의 장문맥 LLM의 등장으로 RAG의 필요성이 줄어들고 있지만, 본 연구는 지나치게 긴 문맥이 정보의 집중도를 떨어뜨릴 수 있음을 지적하며 새로운 `순서 보존 RAG` 메커니즘을 제안합니다. 이 메커니즘은 긴 문맥 QA 응용에서의 성능을 크게 개선하며, 적절한 문맥 길이를 통해 적으면서도 더 양질의 답변을 도출할 수 있음을 보여줍니다.

**Introduction**
초기 LLM의 제한된 문맥 길이 문제를 해결하기 위해 RAG가 도입되었지만, 최근 등장한 장문맥 LLM이 더 긴 텍스트 시퀀스를 처리할 수 있게 되면서 RAG의 중요성이 감소했습니다. 본 연구는 이러한 변화 속에서도 RAG의 효용성을 재검토하고, 지나치게 긴 문맥이 답변의 질을 떨어뜨릴 수 있다고 주장합니다. 이를 해결하기 위해 순서 보존 RAG(OP-RAG)를 제안하며, 이를 통해 적은 수의 토큰으로 높은 질의 답변을 얻을 수 있음을 실험으로 입증합니다.

**Related Work**
기존 연구들은 RAG와 장문맥 LLM의 성능을 비교했으며, 다수의 연구에서 LLM이 RAG보다 우수하다는 결과를 도출했습니다. 그러나 본 연구는 RAG의 성능 향상을 위해 순서 보존 메커니즘을 제안하고, 이를 통해 RAG가 LLM보다 우수한 성능을 발휘할 수 있음을 보여줍니다.

**Order-Preserve RAG**
이 메커니즘은 긴 텍스트를 여러 청크로 나누고, 각 청크의 순서를 보존하여 적절한 청크를 검색하는 방식입니다. 청크의 관련성 점수를 계산하여 상위 청크를 선택하고, 원래 문서 내 순서를 유지하여 배치합니다. 이를 통해 기존의 RAG 방식보다 더 높은 질의 답변을 도출할 수 있습니다.

**Experiments**
EN.QA와 EN.MC 데이터셋을 사용하여 실험을 진행했으며, 실험 결과 순서 보존 RAG가 기존의 RAG와 LLM보다 우수한 성능을 보였음을 확인했습니다. 이 메커니즘은 적은 수의 토큰으로도 높은 질의 답변을 제공할 수 있음을 입증했습니다.

**Conclusion**
RAG의 효용성을 재검토하며, 긴 문맥이 꼭 좋은 답을 제공하지는 않는다는 점을 강조합니다. 순서 보존 RAG(OP-RAG) 메커니즘을 통해 효율적인 검색과 집중된 문맥 활용이 지나치게 긴 문맥을 처리하는 방식보다 효과적임을 실험으로 증명했습니다.

#### 2. 전체 요약

이 논문은 과거 제한된 문맥 길이를 가진 LLM의 문제를 보완하기 위해 도입된 RAG의 성능을 재평가하며, 최근 등장한 장문맥 LLM의 효용성에 대한 의문을 제기합니다. 본 연구는 지나치게 긴 문맥이 정보 집중도를 떨어뜨려 답변의 질을 저하시킬 수 있음을 지적합니다. 이를 해결하기 위해 제안된 순서 보존 RAG(OP-RAG) 메커니즘은, 긴 문맥 내에서도 관련 정보를 효율적으로 검색하고 적절히 활용하여 높은 질의 답변을 제공할 수 있도록 합니다. 실험 결과, OP-RAG는 적은 수의 토큰으로도 기존 RAG와 장문맥 LLM보다 우수한 성능을 발휘함을 입증했습니다. 이로써 효율적인 검색과 문맥 활용이 중요한 연구 과제로 부각되었음을 알 수 있습니다.