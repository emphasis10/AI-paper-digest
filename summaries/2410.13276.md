# SeerAttention: Learning Intrinsic Sparse Attention in Your LLMs
## TL;DR
## Summary
- [https://arxiv.org/pdf/2410.13276.pdf](https://arxiv.org/pdf/2410.13276.pdf)

### 1. 논문 각 섹션 요약

#### 서론
이 논문은 Transformer 기반의 대규모 언어 모델(LLM)에서 핵심적인 역할을 하는 주의(attention) 메커니즘의 복잡성을 해결하고자 하는 내용을 담고 있습니다. 고정된 스케줄이나 패턴을 사용하는 기존의 희소성(스패시티)을 개선하여, 주의의 희소성(sparsity)을 학습할 것을 주장합니다. 이를 위해 SeerAttention이라는 새로운 메커니즘을 제안하며, 이는 학습 가능한 게이트를 통해 주의 맵 안에서 중요한 블록을 선택하여, 나머지를 희소한 것으로 간주합니다.

#### 배경 및 동기
주의 메커니즘은 Transformer 아키텍처에서 중요한 역할을 하지만, 이로 인해 발생하는 메모리 및 시간 복잡성이 모델의 확장성과 효율성을 제한하는 문제점을 드러냅니다. 이를 해결하기 위해 많은 연구에서 대안적인 주의 메커니즘을 탐색해 왔으나, 이러한 대체 방법은 큰 모델이나 긴 문맥을 다룰 때 전체 주의 메커니즘을 능가하기 어렵습니다.

#### SeerAttention 메커니즘
SeerAttention은 블록 희소성을 학습하는 방식으로 주의의 희소성을 식별하고, 이를 통해 LLM에서 효율적인 추론을 수행합니다. 학습 가능한 게이트(Attention Gate)를 통해 주의 맵에서 중요한 블록의 위치를 식별하여, 이를 기반으로 후속 주의 계산이 블록 희소한 FlashAttention 커널을 사용하여 성능을 향상시킬 수 있습니다.

#### 실험
SeerAttention의 효율성과 정확성을 다양한 설정에서 평가했습니다. 실험 결과, 기존 주의 방법인 Minference 및 MoA보다 뛰어난 성능을 보였으며, 특히 긴 문맥 확장 미세조정에서 높은 희소성 수준에서도 거의 손실 없는 정확도를 유지했습니다.

### 2. 전체 요약

이 논문은 대규모 언어 모델의 효율성을 높이기 위해 주의 메커니즘의 희소성을 학습하는 SeerAttention을 제안합니다. 기존의 정형화된 희소성 접근법과 달리, SeerAttention은 학습 가능한 게이트를 통해 동적이고 본질적인 희소성을 포착하여 주의 메커니즘의 효율성을 극대화합니다. 실험 결과, SeerAttention은 기존의 주의 방법보다 우수한 성능과 적응력을 보여주며, 다양한 문맥 길이와 희소성 비율에서도 거의 손실 없는 정확도를 유지합니다. 이는 AI 모델의 확장성을 높이고, 계산 자원을 절감하는 데 기여할 것으로 기대됩니다.