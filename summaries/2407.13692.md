# Prover-Verifier Games improve legibility of LLM outputs
## TL;DR
## Summary
- [https://arxiv.org/pdf/2407.13692.pdf](https://arxiv.org/pdf/2407.13692.pdf)

### 1. 섹션별 요약

#### 초록 (Abstract)
이 논문은 대형 언어 모델(LLM)의 출력 결과를 명확하게 설명하고 검증할 수 있도록 하여 신뢰성을 높이는 방법을 연구합니다. 수학 문제 해결을 위해 체인 오브 생각(Chain-of-Thought, CoT) 솔루션의 답변 정확성만을 최적화하면 가독성이 떨어질 수 있다는 점을 지적하며, 이를 개선하기 위해 프로버-베리파이어 게임(Prover-Verifier Game)을 기반으로 한 훈련 알고리즘을 제안합니다. 이 알고리즘은 작은 검증기가 솔루션의 정답 여부를 예측할 수 있도록 훈련합니다. 실험 결과, 이 방법이 사람의 검증 정확도를 높이고, 모델의 가독성을 향상시킨다는 것을 확인했습니다.

#### 소개 (Introduction)
고도화된 LLM의 출력 결과를 사람이 쉽게 이해할 수 있도록 만들 필요성을 강조합니다. 종래 방식인 인간의 피드백을 통한 최적화 방법의 한계를 설명하고, 대형 프로버 모델이 생산한 CoT 솔루션을 작은 LLM 검증기가 검증할 수 있도록 최적화하는 방식을 제안합니다. 이는 사람을 대신해 LLM이 스스로 출력을 검사하는 방식으로, 더 큰 확장성을 가집니다.

#### 관련 연구 (Related Work)
기존의 연구들과의 차별성을 강조하며, 특히 대형 언어 모델의 악성 공격 방어 및 해석 가능성을 높이는 연구와 비교합니다. 이전에는 전문 분야에서만 사용되던 검증기-프로버 게임을 LLM에 적용하여 가독성을 높이는 방법을 제안한 것은 이 논문의 중요한 기여입니다.

#### 체계 설정 (Setup)
수학 문제 풀기 데이터를 사용하여 프로버와 검증기를 훈련시키는 실험 설정을 설명합니다. 프로버는 문제를 해결해 솔루션을 제시하고, 검증기는 해당 솔루션이 정답인지 아닌지를 판단합니다. 여기서 검증기가 사람이 수행할 역할을 대체하도록 설계되었습니다.

#### 실험 결과 (Experimental Results)
실험을 통해, 검증기와 프로버의 상호 작용을 통한 훈련이 어떻게 가독성을 향상시키는지 보여줍니다. 또한, 과도한 최적화를 방지하고 가독성과 정확성을 조화롭게 얻기 위한 다양한 보상 체계 실험도 포함됩니다.

#### 토론 및 결론 (Discussion and Conclusion)
대형 언어 모델의 출력 가독성을 높이는 것이 중요한 이유와 이 논문의 주요 기여를 논의합니다. 또한, 향후 연구 방향으로는 사람의 피드백이 제한적일 때, 비지도 학습 신호를 사용하여 가독성을 향상시키는 방안을 제안합니다. 이러한 연구는 AI 시스템과 사람 간의 협력을 증진시키는 데 큰 도움이 될 것입니다.

### 2. 전체 요약

이 논문은 대형 언어 모델(LLM)의 수학 문제 해결 출력 결과의 가독성을 높이기 위한 새로운 훈련 알고리즘을 제안합니다. 체인 오브 생각(Chain-of-Thought, CoT) 솔루션의 답변 정확성만을 최적화할 경우 가독성이 떨어질 수 있으며, 이를 개선하기 위해 작은 검증기(Prover)가 솔루션의 정답 여부를 예측할 수 있도록 하는 프로버-베리파이어 게임(Prover-Verifier Game)을 기반으로 한 훈련 방법을 도입하였습니다.

주요 실험 결과는 이 방법이 사람의 검증 정확도를 높이고, LLM 출력의 가독성을 향상시킨다는 것을 보여주며, 가독성과 정확성 간의 균형을 이루기 위한 다양한 보상 체계도 검토하였습니다. 그러므로, 이 논문은 고도화된 LLM이 제공하는 솔루션이 사람에게 더 명확하게 이해되고 신뢰받을 수 있도록 하는 중요한 방법론을 제시합니다. 앞으로의 연구 방향으로는 비지도 학습 신호를 사용하여 더 많은 상황에서 이 방법을 적용할 수 있는 가능성을 제안합니다.