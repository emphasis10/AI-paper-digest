# Following Length Constraints in Instructions
## TL;DR
## Summary
- [https://arxiv.org/pdf/2406.17744.pdf](https://arxiv.org/pdf/2406.17744.pdf)

### 1. 섹션별 요약

#### 소개 (Introduction)
이 논문에서는 대형 언어 모델들이 사람의 지시를 따를 때 자주 발생하는 '길이 편향(length bias)' 문제를 다룹니다. 길이 편향이란, 모델이 응답 길이를 늘려 더 좋은 평가를 받으려는 경향을 말합니다. 이를 해결하기 위해 논문은 길이 지시(length instruction)를 포함한 벤치마크와 새로운 훈련 방법(LIFT: Length-Instruction Fine-Tuning)을 제안합니다.

#### 관련 연구 (Related Work)
길이 편향 문제는 기존 연구에서도 제기되어 왔으며, 일부 연구에서는 길이 페널티(length penalty)를 도입해 이에 대응하려 했습니다. 이 논문은 이러한 접근법이 충분하지 않다고 보고 길이를 명시적으로 지시하는 방법을 제안합니다.

#### 새로운 길이 지시 벤치마크 (AlpacaEval-LI & MT-Bench-LI: New Length-Instructed Benchmarks)
길이 지시에 따른 모델의 성능 평가를 위해 AlpacaEval-LI와 MT-Bench-LI라는 새로운 벤치마크를 소개합니다. 이 벤치마크는 기존의 지시 따르기 작업(instruction-following tasks)에 길이 제한을 추가한 것입니다. 이를 통해 모델이 주어진 길이 제한을 얼마나 잘 따르는지 평가합니다.

#### 길이 지시 미세 조정 (Length-Instruction Fine-Tuning, LIFT)
길이 지시를 따르도록 훈련 데이터를 강화하여 모델을 미세 조정하는 방법을 제안합니다. 기존의 DPO(Direct Preference Optimization) 방법을 개선한 LIFT-DPO를 통해 모델의 응답 길이를 제어하면서도 높은 품질의 응답을 유지할 수 있습니다.

#### 실험 설정 (Experimental Setup)
다양한 Llama 모델을 대상으로 DPO, R-DPO(Regularized DPO), LIFT-DPO를 사용하여 실험을 진행합니다. 실험에는 AlpacaEval-LI와 MT-Bench-LI 벤치마크를 사용해 길이 지시 성능을 평가합니다.

#### 실험 결과 (Experimental Results)
LIFT-DPO는 기존 DPO 훈련 방법에 비해 길이 지시를 훨씬 잘 따르며, 동시에 응답의 품질도 향상시킵니다. 예를 들어, Llama2-70B-Base 모델의 경우 표준 DPO 훈련에서 65.8%의 위반률이 나타났지만, LIFT-DPO를 적용하면 7.1%로 감소하였습니다.

#### 결론 (Conclusion)
이 논문은 길이 지시를 통한 모델 평가와 훈련의 중요성을 강조합니다. 제안된 LIFT-DPO 방법은 길이 지시를 잘 따르며, 이는 실제 사용자 시나리오에서의 모델 제어에 유용합니다. 향후 연구는 다양한 길이 지시 상황과 인간의 선호도를 고려한 모델 평가로 확장될 수 있습니다.

### 2. 전체 요약
이 논문은 AI와 머신 러닝 모델들이 사람의 지시를 따를 때 발생하는 길이 편향 문제를 해결하고자 길이 지시 벤치마크와 새로운 훈련 방법인 LIFT-DPO를 제안합니다. 길이 지시를 통해 모델의 응답 길이를 명시적으로 제한함으로써 모델의 성능을 공정하게 평가하고, 사용자 요구에 맞춘 응답을 제공할 수 있습니다. 실험 결과, LIFT-DPO는 기존 방법들에 비해 길이 지시를 잘 따르면서도 높은 응답 품질을 유지하는 것으로 나타났습니다. 이러한 접근법은 모델 평가와 훈련의 새로운 기준을 제시하며, AI의 실제 적용에서 더 나은 사용자 경험을 제공할 수 있습니다.