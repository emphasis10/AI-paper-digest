# Extreme Compression of Large Language Models via Additive Quantization
## TL;DR
## Summary
- [https://arxiv.org/pdf/2401.06118.pdf](https://arxiv.org/pdf/2401.06118.pdf)

### 1. 각 섹션의 요약 및 중요한 내용

#### 1. 도입부 (Introduction)
이 논문은 대규모 언어 모델(LLM)의 압축을 위한 새로운 알고리즘인 AQLM을 제안합니다. AQLM은 기존의 다중 코드북 양자화(MCQ) 기술을 기반으로 하며, 특히 LLAMA 2와 같은 정확한 오픈 LLM을 대상으로 매우 낮은 비트 수(2-3 비트 이상)의 압축을 다룹니다. 두 가지 주요 혁신은 다음과 같습니다:
1. 입력에 따라 가중치를 적응적으로 학습하는 적응형 양자화.
2. 여러 레이어 블록을 걸쳐 코드북 매개변수를 공동으로 최적화하는 방법.

#### 2. 배경 및 관련 연구 (Background & Related Work)
양자화는 모델의 메모리 효율성과 정확도를 동시에 유지하기 위한 중요한 기술입니다. 초기 연구는 직접 라운딩(RTN) 및 데이터 인식 방법(GPTQ) 등을 사용했으며, 이후에는 가중치 외의 활성화 양자화, 희소-양자화 포맷 등으로 발전해왔습니다.

#### 3. AQLM: 대규모 언어 모델을 위한 가산 양자화 (AQLM: Additive Quantization for LLMs)
AQLM은 전형적인 포스트 트레이닝 양자화 문제를 해결하면서, 레이어 입력 및 출력 활성화를 고려하여 양자화 매개변수를 최적화합니다. 또한, 여러 레이어 동안 코드북을 공동으로 최적화하는 기능 역시 포함되어 있습니다. 이로 인해 매우 낮은 비트 수로도 높은 정확도를 유지할 수 있습니다.

#### 4. 실험 (Experiments)
LLAMA 2와 Mistral 모델에 대해 AQLM의 양자화 효과를 테스트한 결과, 다른 최첨단 방법보다 뛰어난 결과를 보였습니다. 특히, 2비트 양자화의 경우 다른 방법들보다 큰 향상을 보였으며, 인퍼런스 속도에서도 최소한 FP16과 같은 수준을 유지하였습니다.

#### 5. 결론 및 향후 연구 (Conclusion & Future Work)
AQLM은 2-3 비트/파라미터 범위에서 매우 효율적인 양자화를 달성했으며, CPU 및 GPU에서 실행 가능한 효율적인 구현도 제공하였습니다. 향후 연구는 더 나은 파인 튜닝 전략 및 다른 양자화 시나리오에 AQLM의 적응 적용 등을 포함합니다.

### 2. 전체 요약

이 논문은 대규모 언어 모델(LLM)의 극단적인 압축을 위해 새로운 형태의 가산 양자화 알고리즘인 AQLM을 제안합니다. AQLM은 기존의 다중 코드북 양자화를 확장하여, 특히 LLAMA 2와 같은 정확한 오픈 LLM을 대상으로 합니다. 이를 통해 2-3 비트 수준의 낮은 비트 수로도 높은 정확도를 유지할 수 있으며, 이를 위해 입력 적응형 양자화 및 여러 레이어 블록 동안의 공동 최적화 방법을 사용합니다.

실험 결과, AQLM은 다른 최첨단 방법보다 뛰어난 성능을 보였으며, 특히 2비트 양자화에서 큰 향상을 보였습니다. 또한, AQLM은 CPU 및 GPU에서 효율적으로 실행될 수 있는 구현 방식을 제공하여, 실시간 응용에 적합합니다.

향후 연구에서는 더 나은 파인 튜닝 전략 및 다른 양자화 시나리오에의 적용 가능성을 탐구하며, 이를 통해 더욱 효율적이고 정확한 양자화 방법을 개발할 계획입니다. 

이 논문은 AI와 머신러닝의 발전을 위해 중요한 기여를 하였으며, 대규모 언어 모델을 보다 효율적으로 사용할 수 있는 가능성을 열어줍니다.