# Agent-SafetyBench: Evaluating the Safety of LLM Agents
## TL;DR
## Summary
- [https://arxiv.org/pdf/2412.14470.pdf](https://arxiv.org/pdf/2412.14470.pdf)

### 요약

1. **섹션별 요약**

   - **서론**: 대형 언어 모델(LLM)의 보급이 증가함에 따라 안전성 문제가 부각되고 있으며, 기존 연구는 주로 콘텐츠 안전성에 초점을 맞췄으나, 본 논문은 LLM이 다양한 도구를 사용하여 상호작용하는 환경에서의 새로운 안전성 문제를 다루고 있다.
   
   - **AGENT-SAFETYBENCH 소개**: LLM 에이전트의 안전성을 평가하기 위한 종합적인 벤치마크를 제안하였으며, 349개의 상호작용 환경과 2,000개의 테스트 케이스를 포함한다. 8개의 안전 위험 범주와 10개의 일반적인 실패 모드를 평가하며, 16개의 대표적인 LLM 에이전트를 테스트하여 안전 점수가 모두 60% 미만이라는 우려스러운 결과를 발견하였다.
   
   - **안전 평가의 필요성**: LLM의 툴 사용이 증가하면서 안전성 결함의 두 가지 주요 원인으로는 툴 사용의 견고성 부족과 위험 인식 부족이 지적되었다. 단순한 방어 프롬프트로는 이러한 문제를 해결하기에 충분하지 않으며, 더 발전된 안전 전략의 필요성이 강조된다.

2. **주요 기여 및 혁신적 부분**

   - AGENT-SAFETYBENCH의 제안은 LLM에이전트의 안전성을 체계적으로 평가할 수 있는 새로운 방법론을 제공하며, 이는 현재 안전성 향상을 위한 중요한 초석이 되어줄 것이다. LLM 에이전트의 두 가지 주된 안전 결함을 드러내어 이를 해결할 수 있는 잠재적 방안을 시사한다.

### 전체 요약

이 논문은 대형 언어 모델(LLM) 에이전트의 안전성을 평가하기 위한 고유한 벤치마크를 제안하고, 16개의 대표 LLM 에이전트를 평가하여 안전 결함을 식별한다. 이는 에이전트의 견고성과 위험 인식에 대한 부족을 해결하고, LLM 에이전트의 안전성을 개선하기 위한 중요하고 새로운 접근을 제시한다.