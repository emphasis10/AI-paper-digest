# Turbo Sparse: Achieving LLM SOTA Performance with Minimal Activated Parameters
## TL;DR
## Summary
- [https://arxiv.org/pdf/2406.05955.pdf](https://arxiv.org/pdf/2406.05955.pdf)

### 1. Section-by-Section Summary

#### Abstract
이 연구는 대규모 언어 모델(LLM)의 추론 속도를 향상시키기 위해 제안된 dReLU 함수와 고품질 데이터 학습 방법론을 소개합니다. dReLU는 활성화 희소성을 높여 성능 저하 없이 속도를 가속화합니다. 이 연구에서는 기존 모델 대비 2-5배의 속도 향상을 보였고, 모바일에서도 높은 효율을 달성했습니다.

#### Introduction
대규모 언어 모델은 자연어 처리에서 혁신을 이뤘지만, 모든 파라미터를 활성화함으로써 비효율적입니다. 이 문제를 해결하기 위해 MoE(전문가 혼합)와 ReLU 기반의 활성화 희소성을 활용한 방법론이 제안되었습니다.

#### Related Work and Background
효율적인 LLM 추론을 위한 기존 연구들을 소개하며, 활성화 희소성 문제를 해결하기 위한 알고리즘과 시스템적 접근법을 통합하는 필요성을 강조합니다.

#### Analysis
기존의 ReLUfication 접근법을 분석하고, 부족한 부분을 보완하기 위해 dReLU를 제안합니다. 이를 통해 활성화 희소성이 90%까지 증가하면서도 성능 저하 없이 모델을 학습할 수 있습니다.

#### dReLU 및 Sparsification
dReLU 함수의 정의와 이를 활용한 모델의 학습 방법을 설명합니다. dReLU는 두 개의 up- 및 gate-projections 후에 ReLU를 적용하여 희소성을 극대화합니다. 또한, 다양한 다운스트림 작업에서 높은 성능을 유지함을 확인했습니다.

#### Experiments Results
다양한 벤치마크 테스트를 통해 TurboSparse-Mistral-7B 및 TurboSparse-Mixtral-47B 모델의 성능을 검증한 결과, 기존 모델보다 훨씬 높은 효율성을 보였습니다.

#### Practical Inference Speedup Evaluation
GPU-CPU 혼합 환경과 모바일 폰에서 TurboSparse 모델의 추론 속도를 평가한 결과, 상당한 속도 향상을 보였습니다. 특히, 모바일에서 22.2배의 속도 향상을 기록했습니다.

#### Conclusion
본 연구는 대규모 언어 모델의 활성화 희소성을 극대화하는 효율적인 방법론을 제안하며, 이를 통해 대규모 모델의 실용성을 크게 개선했습니다. 이 방법은 향후 AI 기술 발전에 중요한 역할을 할 것입니다.

### 2. Overall Summary

이 논문은 대규모 언어 모델(LLM)의 추론 속도를 높이는 dReLU라는 새로운 활성화 함수와 이를 활용하는 방법을 소개합니다. 주요 내용으로는 LLM의 활성화 희소성을 90%까지 증가시키며 성능을 유지하는 방법론을 개발한 것입니다. 이를 통해 MoE 모델에서 활성화 희소성을 극대화하며, TurboSparse-Mistral-7B 및 TurboSparse-Mixtral-47B 모델을 통해 기존 모델보다 성능을 개선하고 추론 속도를 2-5배까지 향상시켰습니다. 특히, 모바일 환경에서도 뛰어난 추론 성능을 발휘하여, AI 기술의 실용성을 크게 높였습니다.

이 연구는 향후 AI 및 자연어 처리 기술 발전에 중요한 기여를 할 것이며, 다양한 환경에서 더 효율적인 큰 모델을 사용할 수 있게 합니다.

## Similar Papers
- [ReLU Strikes Back: Exploiting Activation Sparsity in Large Language Models](2310.04564.md)
- [PowerInfer-2: Fast Large Language Model Inference on a Smartphone](2406.06282.md)
- [Q-Sparse: All Large Language Models can be Fully Sparsely-Activated](2407.10969.md)
- [PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU](2312.12456.md)
- [ReLU$^2$ Wins: Discovering Efficient Activation Functions for Sparse LLMs](2402.03804.md)
- [TernaryLLM: Ternarized Large Language Model](2406.07177.md)
- [T-MAC: CPU Renaissance via Table Lookup for Low-Bit LLM Deployment on Edge](2407.00088.md)
- [ShiftAddLLM: Accelerating Pretrained LLMs via Post-Training Multiplication-Less Reparameterization](2406.05981.md)
- [Yuan 2.0-M32: Mixture of Experts with Attention Router](2405.17976.md)
