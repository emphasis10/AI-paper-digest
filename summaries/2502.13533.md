# Train Small, Infer Large: Memory-Efficient LoRA Training for Large Language Models
## TL;DR
## Summary
- [https://arxiv.org/pdf/2502.13533.pdf](https://arxiv.org/pdf/2502.13533.pdf)

1. **세션 요약**

   - **도입부**  
     이 논문은 대규모 언어 모델(LLM)의 미세 조정을 위한 메모리 효율적인 로우 랭크 적응(LoRA) 방법론을 제안하고 있다. LoRA는 원본 모델의 가중치 값은 그대로 유지하고, 가벼운 저급 랭크의 어댑터 행렬만을 학습하여 새로운 작업에 적응시킨다. 그러나 기존 LoRA가 차지하는 메모리의 대부분은 기존 모델의 가중치 때문이다.

   - **메모리 효율적인 로라 트레이닝**  
     LORAM 방식은 잘라낸(다듬어진) 모델을 훈련한 후, 원본 모델로 복구하여 사용하는 방법이다. 이는 훈련 중 모델 파라미터로 인해 발생하는 메모리 초과를 줄이고, 인퍼런스 시에는 전체 파라미터를 활용하여 퍼포먼스를 증가시킨다.

   - **효과적인 정렬 전략**  
     훈련 모델과 원본 모델 간에 발생하는 지식 불일치를 해소하기 위해, 잘라낸 모델을 작은 데이터셋에서 일반적인 코퍼스로 훈련시켜 정렬을 수행한다.

   - **확장 실험 평가**  
     LORAM의 유효성을 다양한 잘라내기 알고리즘, 모델 크기, 도메인별 작업들에 걸쳐 실험적으로 검증했다. 특히, QLORAM을 통해 메모리 사용 비용을 대폭 줄였다.

   - **복구 및 정렬의 필요성**  
     복구가 된 모델들은 비복구 모델에 비해 더 낮은 perplexity를 가지며, 이는 LORAM의 전체 성능을 향상시킨다. 추가로, 정렬 과정을 거친 모델은 성능이 더 뛰어나다.

2. **전체 요약**

   이 논문은 대규모 언어 모델의 미세 조정을 위한 메모리 효율성을 극대화하는 LORAM 방법을 제안한다. LORAM은 메모리 초과 문제를 줄이고도 효율적인 퍼포먼스를 제공하도록 설계되었다. 이를 위해 작게 다듬어진 모델을 훈련하고 원본 모델로 재조합하는 방식의 저랭크 행렬 구조를 사용한다. 논문은 이 과정을 통해 모델의 메모리 활용도를 크게 향상시키는 방법을 다수의 실험을 통해 규명하였다. LORAM은 특히 다양한 잘라내기 및 정량화 스키마와 통합하여 메모리 오버헤드를 줄이고, 모델 평활성을 유지하면서 성능 저하를 방지한다는 점에서 혁신적이다.