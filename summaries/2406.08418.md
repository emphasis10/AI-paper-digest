# OmniCorpus: A Unified Multimodal Corpus of 10 Billion-Level Images Interleaved with Text
## TL;DR
## Summary
- [https://arxiv.org/pdf/2406.08418.pdf](https://arxiv.org/pdf/2406.08418.pdf)

#### Introduction

이 논문은 세계 최대 규모의 멀티모달 데이터셋 OmniCorpus를 소개합니다. 이 데이터셋은 86억 개의 이미지와 1696억 개의 텍스트 토큰을 포함하며, 주로 Common Crawl, 중국 웹사이트 및 동영상 플랫폼에서 데이터를 수집하였습니다. 이 데이터셋은 특히 대규모 언어 모델(LLM) 및 멀티모달 언어 모델(MLLM)의 학습을 향상시키는 데 중점을 둡니다.

#### Related Works

이 절에서는 기존의 이미지-텍스트 데이터셋과 비전-언어 모델에 대해 논의합니다. 기존 데이터셋의 크기와 다양성이 제한적이며, 이는 모델의 성능에 부정적인 영향을 미칩니다. OmniCorpus는 이러한 제한을 극복하기 위해 고안되었습니다.

#### Data Engine

데이터 엔진은 5단계로 구성되며 주요 내용 추출, 텍스트 필터링, 중복 문서 제거, 이미지 필터링 및 인간 피드백 기반 텍스트 필터링이 포함됩니다. 이 절에서는 이러한 각 단계를 자세히 설명합니다.

#### Exploring OmniCorpus

이 절에서는 OmniCorpus 데이터셋의 탐색 및 핵심 통계 정보를 제공합니다. 데이터셋의 품질과 다양성을 평가하고, OmniCorpus의 고유한 특징을 강조합니다.

#### Experiments

실험에서는 OmniCorpus가 멀티모달 모델의 성능을 어떻게 향상시키는지에 대해 다룹니다. 다양한 벤치마크 테스트를 통해 데이터셋의 효과를 입증하고, 몇 가지 새로운 발견 사항도 제시합니다.

#### Conclusion & Limitation

이 논문은 OmniCorpus의 출시와 함께, 데이터셋이 미래의 멀티모달 모델 연구에 중요한 토대를 마련할 것이라고 주장합니다. 그러나 현재 필터링 프로세스는 모델 성능에 제한적인 개선만을 제공합니다. 향후 연구에서는 필터링의 구체적 요인이 모델에 어떻게 영향을 미치는지 탐구할 예정입니다.

### Overall Summary

이 논문은 OmniCorpus라는 세계 최대 규모의 멀티모달 데이터셋을 소개하며, 이를 통해 멀티모달 언어 모델(MLLM)의 성능을 어떻게 향상시킬 수 있는지 설명합니다. OmniCorpus는 86억 개의 이미지와 1696억 개의 텍스트 토큰을 포함하여 기존의 데이터셋보다 훨씬 더 크고 다양합니다. 데이터셋의 고유한 특징은 다소 어려운 기존의 멀티모달 학습 문제를 해결하는 데 중점을 둡니다. 또한 실험을 통해 데이터셋의 효율성과 품질을 입증하였으며, 이를 통해 멀티모달 모델의 성능을 크게 향상시킬 수 있음을 보여줍니다. 향후 연구에서는 데이터 필터링 과정의 개선을 통해 더욱 높은 성능을 기대할 수 있습니다.

## Similar Papers
- [Task Me Anything](2406.11775.md)
- [How Far Are We to GPT-4V? Closing the Gap to Commercial Multimodal Models with Open-Source Suites](2404.16821.md)
- [Needle In A Multimodal Haystack](2406.07230.md)
- [Video-MME: The First-Ever Comprehensive Evaluation Benchmark of Multi-modal LLMs in Video Analysis](2405.21075.md)
- [Ferret-v2: An Improved Baseline for Referring and Grounding with Large Language Models](2404.07973.md)
- [InternLM-XComposer2-4KHD: A Pioneering Large Vision-Language Model Handling Resolutions from 336 Pixels to 4K HD](2404.06512.md)
- [InternLM-XComposer-2.5: A Versatile Large Vision Language Model Supporting Long-Contextual Input and Output](2407.03320.md)
- [AlignGPT: Multi-modal Large Language Models with Adaptive Alignment Capability](2405.14129.md)
- [Prism: A Framework for Decoupling and Assessing the Capabilities of VLMs](2406.14544.md)
