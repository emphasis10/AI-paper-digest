# LoLDU: Low-Rank Adaptation via Lower-Diag-Upper Decomposition for Parameter-Efficient Fine-Tuning
## TL;DR
## Summary
- [https://arxiv.org/pdf/2410.13618.pdf](https://arxiv.org/pdf/2410.13618.pdf)

1. 논문 각 섹션의 주요 내용 요약

- **서론**: 대규모 모델의 파인 튜닝은 컴퓨팅과 스토리지 요구가 높습니다. 이를 해결하기 위해 Parameter-Efficient Fine-Tuning(PEFT) 기법들이 개발되었습니다. 이 논문에서 소개하는 LoLDU는 LDU 분해법을 사용하여 이러한 어려움을 극복하고 효율적인 모델 커스터마이징을 가능하게 합니다.

- **방법**: LoLDU는 Lower-Diag-Upper (LDU) 분해를 활용하여 사전 학습된 가중치의 변화를 학습합니다. 기존의 무작위 초기화 방법과 달리, LDU는 초기화에 사용되어 모델의 기본 구조를 보존하고 트레이닝의 안정성을 높입니다. 점진적인 가중치 업데이트로 모델의 메모리 사용을 효율적으로 줄입니다.

- **최적화 과정**: LoLDU의 미세 조정 단계에서 대각 행렬과 스케일링 요소를 최적화하여 파라미터 업데이트의 세밀한 제어를 지원합니다. 이로써 사전 훈련된 지식을 보존하면서도 더 나은 성능을 이끌어냅니다.

- **결과 및 성능 평가**: 실험 결과, LoLDU는 기존의 완전 파인 튜닝 방식보다 매우 적은 수의 파라미터만으로도 비슷하거나 더 나은 성능을 달성했습니다. 이는 로버타 기반의 NLU와 다양한 이미지 분류 작업에서도 잘 드러났습니다.

논문은 LDU 분해법을 사용하여 대규모 언어 모델의 효과적인 미세 조정 방안을 제시하며, 이를 통해 모델의 파라미터를 극적으로 줄이면서도 여전히 높은 성능을 유지할 수 있음을 보여줍니다.

2. 전체 요약

이 논문은 LDU 분해를 활용한 Parameter-Efficient Fine-Tuning(PEFT) 방법, LoLDU를 제시합니다. LDU는 낮은 수의 파라미터 변경을 통해 모델의 성능을 유지할 수 있는 효율성을 제공합니다. LoLDU는 사전 학습된 가중치의 변경을 효과적으로 학습하며, 파라미터 수를 0.00025%까지 줄일 수 있습니다. 여러 실험을 통해 LoLDU의 강점과 잠재력을 입증했으며, 특히 다양한 모델 아키텍처와 규모에서 우수한 성능을 보여줍니다.