# Understanding Dimensional Collapse in Contrastive Self-supervised Learning
## TL;DR
## Summary
- [https://arxiv.org/pdf/2110.09348.pdf](https://arxiv.org/pdf/2110.09348.pdf)

문서를 바탕으로 AI와 머신러닝에 대한 논문을 요약하였습니다.

### 1. 각 섹션 요약 (Main Contribution 및 혁신적 부분 설명 포함)

**1) 서론 (Introduction):**
자기 지도 학습은 인간의 주석 없이 유용한 데이터 표현을 학습하는 방법입니다. 본 논문에서는 이미지의 다양한 뷰로부터 임베딩 벡터 사이의 일치도를 최대화시켜 표현을 학습하는 방법을 설명합니다. 명확한 목표 벡터로 모든 입력을 변환할 수 있는 해결책을 제안하며, 대조 학습(Contrastive Learning)을 통해 이를 방지하는 방법을 소개합니다. 대조 학습은 '양성' 및 '음성' 샘플 쌍을 정의하여 손실 함수에서 다르게 취급합니다.

**2) 관련 연구 (Related Works):**
본 논문은 대조 학습 및 비대조 학습 방법들의 이론적 이해를 다룹니다. 비대조 학습은 음성 샘플 쌍에 의존하지 않으며, 대조 학습이 어떻게 유용한 표현을 학습하고 하위 작업에 적용될 수 있는지를 설명합니다.

**3) 차원 붕괴 (Dimensional Collapse):**
자기 지도 학습 방법에서 임베딩 벡터는 저차원 하위 공간에 축소되는 차원 붕괴가 발생합니다. 이러한 붕괴는 강력한 데이터 증강과 은닉 레이어 간의 암묵적 정규화(interplay of weight matrices)로 인해 발생하며, 네트워크가 여러 층을 가지고 있을 때 발생합니다.

**4) 강한 증강에 의한 차원 붕괴 (Dimensional Collapse caused by Strong Augmentation):**
대조 학습에서 무작위 증강이 입력 정보보다 강하면 임베딩 차원 붕괴가 발생할 수 있습니다. 대조 손실은 서로 다른 입력 이미지의 임베딩 벡터를 멀리 밀어내어 완전 붕괴를 방지합니다. 그러나, 이러한 과정에서도 차원 붕괴는 여전히 발생합니다.

**5) 암묵적 정규화에 의한 차원 붕괴 (Dimensional Collapse caused by Implicit Regularization):**
과적합된 선형 네트워크에서는 저차원 솔루션을 찾는 경향 때문에 차원 붕괴가 발생합니다. 다층 네트워크 및 비선형 설정으로 이 이론을 확장할 수 있습니다.

**6) DirectCLR**
새로운 대조 학습 방법론인 DirectCLR을 제안합니다. 이 방법은 명시적인 학습 가능한 프로젝터를 사용하지 않고 표현 공간을 직접 최적화합니다. 실험 결과, DirectCLR은 표준 대조 학습 방법보다 뛰어난 성능을 보여줍니다.

### 2. 전체 요약

이 논문에서는 자기 지도 학습의 대조 학습 방법이 특정 차원에 축소되는 차원 붕괴 문제를 겪는다는 것을 보여줍니다. 저자들은 강력한 증강과 암묵적 정규화가 이러한 붕괴의 두 가지 주요 원인이며, 이를 방지하기 위해 DirectCLR이라는 새로운 학습 방법을 제안했습니다. DirectCLR은 명시적인 프로젝터 없이 표현 공간을 직접 최적화하여 기존의 방법보다 더 나은 성능을 발휘합니다. 이 연구는 대조 학습의 차원 붕괴에 대한 이론적 이해를 심화시키고, 차세대 자기 지도 학습 알고리즘 개발에 기여할 수 있습니다.