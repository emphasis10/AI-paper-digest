# IMAGDressing-v1: Customizable Virtual Dressing
## TL;DR
## Summary
- [https://arxiv.org/pdf/2407.12705.pdf](https://arxiv.org/pdf/2407.12705.pdf)

### 1. 논문의 각 섹션 요약

#### Introduction
논문의 도입부에서는 현재 가상 체험(VTON) 기술이 소비자에게 많은 혜택을 제공하지만, 상인의 입장에서 옷을 유연하게 보여주기 위해 얼굴, 포즈, 배경 등을 조절할 필요가 있다고 지적합니다. 이를 해결하기 위해 Virtual Dressing (VD)라는 새로운 과제를 제안하고 Comprehensive Affinity Metric Index (CAMI)를 설계하여 생성된 이미지와 참조 의류 간의 일관성을 평가합니다.

#### Related Work
가상 체험 관련 기존 연구를 둘러보며, 초기 VTON 기술은 주로 Generative Adversarial Networks (GANs)를 사용해 의류를 맞추고 인간 모델에 합치는 방식으로 발전해왔습니다. Diffusion 모델은 세밀한 의류 특징을 잘 보존하면서 고품질 이미지 생성에 있어서 중요한 발전을 이루었습니다.

#### Methodology
논문에서 제안된 IMAGDressing-v1 모델은 두 가지 주요 요소로 구성됩니다:
1. **Garment UNet**: CLIP으로부터 의미론적 특징, VAE로부터 텍스처 특징을 동시 포착.
2. **Denoising UNet**: 고정된 Self-Attention과 학습 가능한 Cross-Attention 모듈을 포함하여 의류 특징을 통합. 이를 통해 사용자들은 텍스트 프롬프트를 통해 다른 장면을 제어할 수 있습니다.

#### Experiments
(CGPair 데이터셋을 이용한 실험에서는 IGPair 데이터셋을 기반으로 모델을 학습시켰고, 이 모델이 다른 최첨단 방법들과 비교하여 뛰어난 성능을 보였습니다.)
IMAGDressing-v1 모델은 여러 최첨단 모델과 비교 실험을 통해 더욱 세부적인 의류 특징 포착과 텍스트 편집 및 생성 능력을 유지하면서도 우수한 성능을 보였습니다. VTON 작업을 위해 ControlNet-Inpaint와 결합하여 가상 체험(VTON)을 구현할 수 있습니다.

#### Conclusion
논문에서는 최신 VTON 기술이 상인을 위한 옷을 전시하는데 한계가 있음을 지적하며, 이를 해결하기 위해 IMAGDressing-v1 모델을 개발했습니다. 이 모델은 다양한 플러그인을 지원하며, 확장성과 제어 가능한 이미지 생성을 제공합니다. 이와 함께 30만 개 이상의 의류와 착용 이미지가 포함된 IGPair 데이터셋을 공개하여 커뮤니티 연구를 촉진합니다.

### 2. 전체 요약
이 논문은 상인을 위한 새로운 가상 드레싱 과제(VD)를 정의하고, 기존 가상 시뮬레이션 기술의 한계를 극복하기 위해 IMAGDressing-v1 모델을 제안합니다. IMAGDressing-v1 모델은 두 개의 UNet 구조와 하이브리드 어텐션 모듈을 사용하여 세밀한 의류 특징과 유연한 장면 제어 기능을 제공합니다. 또한 데이터 부족 문제를 해결하기 위해 IGPair라는 대규모 데이터셋을 공개합니다. 논문에서 제안된 모델은 다양한 평가 지표에서 최첨단 방법들에 비해 우수한 성능을 보였으며, 상인들이 보다 다양하고 제어 가능한 방식으로 의류를 전시할 수 있도록 도와줍니다.