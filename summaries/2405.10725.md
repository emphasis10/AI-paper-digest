# INDUS: Effective and Efficient Language Models for Scientific Applications
## TL;DR
## Summary
- [https://arxiv.org/pdf/2405.10725.pdf](https://arxiv.org/pdf/2405.10725.pdf)

### 요약

#### 1. 소개 (Introduction)
이 논문은 범용 대형 언어 모델(LLM)이 자연어 처리(NLP) 작업에서 뛰어난 성능을 보여주었으나, 도메인별 특화 모델이 특정 작업에서 더 나은 성능을 발휘함을 설명합니다. 이를 바탕으로 지구과학, 생물학, 물리학, 태양 물리학, 행성 과학 및 천체 물리학 등 다양한 과학 분야를 위한 특화 언어 모델인 INDUS를 개발했습니다. INDUS는 과학적 말뭉치로 학습된 인코더 모델, 정보 검색을 위한 텍스트 임베딩 모델, 그리고 지연 시간 및 자원 제약이 있는 응용 프로그램을 위한 소형 모델을 포함합니다. 또한 CLIMATE-CHANGE NER, NASA-QA, NASA-IR 등의 새로운 과학 벤치마크 데이터 세트를 만들었습니다.

#### 2. 데이터 (Data)
모델 성능 향상을 위해 고품질의 도메인별 말뭉치를 사용했습니다. 주요 데이터 소스로는 SAO/NASA ADS, PubMed Central(PMC), American Meteorological Society(AMS), American Geophysical Union(AGU), NASA CMR, 그리고 일반 영어 학습을 위한 Wikipedia가 있습니다.

#### 3. 인코더 모델 (Encoder Models)
INDUS는 바이트 쌍 인코딩(BPE) 알고리즘을 사용하여 도메인별 말뭉치로부터 INDUSBPE 토크나이저를 만들었습니다. 이 토크나이저는 일반 토크나이저보다 계산 비용이 적게 들고 과학 용어를 더 잘 처리합니다. 인코더 모델은 마스크드 언어 모델링(MLM) 목표로 학습되었습니다. 또한, 효율적인 인코더 모델을 위해 INDUSBASE 모델을 교사로 사용하여 소형 모델 INDUSSMALL을 지식 증류 기술을 통해 학습했습니다.

#### 4. 문장 임베딩 모델 (Sentence Embedding Models)
텍스트 임베딩 모델은 효율적인 검색 시스템을 위해 저차원 벡터로 텍스트를 표현합니다. INDUS-RETRIEVERBASE와 INDUS-RETRIEVERSMALL 모델은 대조 학습 목표로 학습되었으며, 소형 모델도 지식 증류를 통해 학습되었습니다.

#### 5. 벤치마크 생성 (Creating Benchmarks)
기존 벤치마크 데이터셋이 부족한 과학 분야를 위한 새로운 벤치마크 데이터셋을 만들었습니다. CLIMATE-CHANGE NER, NASA-QA, NASA-IR의 세 가지 벤치마크를 통해 모델의 성능을 평가했습니다.

#### 6. 실험 결과 (Experimental Results)
INDUS 모델은 CLIMATE-CHANGE NER, NASA-QA, NASA-IR 및 기존의 도메인별 벤치마크에서 일반 목적의 모델인 RoBERTa와 도메인 특화 모델인 SCIBERT를 능가했습니다. 특히 소형 모델인 INDUSSMALL은 지연 시간을 크게 줄이면서도 뛰어난 성능을 보였습니다.

#### 7. 결론 (Conclusions)
INDUS 모델은 과학 도메인을 위한 고품질 인코더 모델 및 문장 임베딩 모델을 제공하며, 소형 모델도 지식 증류를 통해 효율성을 유지합니다. 이러한 모델과 벤치마크 데이터셋을 공개하여 과학 커뮤니티에 기여할 예정입니다.

### 전체 요약
INDUS는 지구과학, 생물학, 물리학 등 다양한 과학 분야를 위한 특화 언어 모델로, 도메인별 말뭉치와 INDUSBPE 토크나이저를 사용해 학습되었습니다. 대조 학습을 통해 문장 임베딩 모델도 개발되었으며, 소형 모델은 지식 증류를 통해 효율성을 높였습니다. 또한 CLIMATE-CHANGE NER, NASA-QA, NASA-IR의 새로운 벤치마크 데이터셋을 통해 모델의 성능을 평가하였습니다. INDUS 모델은 일반 및 도메인 특화 모델을 능가하는 성능을 보였으며, 과학 커뮤니티의 연구를 가속화할 것으로 기대됩니다.

## Similar Papers
- [Piccolo2: General Text Embedding with Multi-task Hybrid Loss Training](2405.06932.md)
- [BM25S: Orders of magnitude faster lexical search via eager sparse scoring](2407.03618.md)
- [BiomedRAG: A Retrieval Augmented Large Language Model for Biomedicine](2405.00465.md)
- [RE-AdaptIR: Improving Information Retrieval through Reverse Engineered Adaptation](2406.14764.md)
- [AGRaME: Any-Granularity Ranking with Multi-Vector Embeddings](2405.15028.md)
- [Towards a Personal Health Large Language Model](2406.06474.md)
- [CodecLM: Aligning Language Models with Tailored Synthetic Data](2404.05875.md)
- [T-FREE: Tokenizer-Free Generative LLMs via Sparse Representations for Memory-Efficient Embeddings](2406.19223.md)
- [MS MARCO Web Search: a Large-scale Information-rich Web Dataset with Millions of Real Click Labels](2405.07526.md)
