# Mediator: Memory-efficient LLM Merging with Less Parameter Conflicts and Uncertainty Based Routing
## TL;DR
## Summary
- [https://arxiv.org/pdf/2502.04411.pdf](https://arxiv.org/pdf/2502.04411.pdf)

1. **섹션별 요약**

   - **서론**: 본 논문에서는 대형 언어 모델(LLM)을 다양한 작업에 맞게 미세 조정하고 이를 통합하여 하나의 강력한 모델로 만드는 방법을 제안합니다. 이 과정에서 발생하는 매개변수 충돌 문제를 해결하고자 합니다.

   - **기존 연구의 한계**: 기존의 모델 병합 방식은 성능이 떨어지기 쉽고 매개변수 충돌로 인해 성능 저하 문제가 발생할 수 있습니다. 모델 라우팅 기법을 사용하면 일부 충돌 문제를 해결할 수 있으나, 이는 또한 과도한 저장 및 계산 비용을 초래할 수 있습니다.

   - **제안하는 방법**: 본 연구에서는 매개변수 충돌이 적은 레이어를 평균화하고 충돌이 큰 레이어에 대해서는 전문가 라우팅을 이용합니다. 이는 스토리지 비용을 줄임과 동시에 성능을 유지할 수 있도록 합니다.

   - **실험 및 결과**: 실험 결과, LLaMA 및 Qwen 등 다양한 모델과 실제 추론 작업에서 제안된 방법이 일관되게 성능을 향상시킴을 확인하였습니다. 이는 기존 방법 대비 시스템 비용을 줄이는 동시에 높은 성능을 유지합니다.

   - **결론**: 본 연구는 매개변수 충돌을 적응적으로 해결하고, 다양한 입력에 대한 적응성을 높여 성능을 개선하는 대형 언어 모델 병합 방법을 제시합니다. 이를 통해 최첨단 언어 모델들이 더욱 접근 가능하고 비용 효율적이도록 만들었습니다.

2. **논문의 전체 요약**

   본 논문은 대형 언어 모델 병합 시 발생하는 매개변수 충돌 문제와 관련된 성능 저하를 해결하는 방법을 제시합니다. 주요 기여는 매개변수 충돌이 적은 레이어와 충돌이 큰 레이어를 구별하여 각각 평균화 또는 전문가 라우팅을 수행하는 방식으로 모델 병합의 효율성을 높인 것입니다. 제안된 방법은 실험을 통해 성능 향상을 보였고, 저장 비용을 줄이면서도 높은 성능을 유지하는 결과를 얻었습니다.