# ReFT: Representation Finetuning for Language Models
## TL;DR
## Summary
- [https://arxiv.org/pdf/2404.03592.pdf](https://arxiv.org/pdf/2404.03592.pdf)

이 문서는 인공지능과 기계학습에 관한 연구로, 특히 "ReFT: Representation Finetuning for Language Models"에 초점을 맞춥니다.

1. **서론(Introduction) 및 관련 작업(Related Work)**:
    - 이 연구는 기존의 매개변수 효율적인 파인튜닝 방법(PEFT)이 전체 모델 가중치를 업데이트하는 대신 일부 가중치만 업데이트하여 대규모 모델을 적은 데이터로 새로운 도메인이나 작업에 적응시키는 방식을 개선하고자 한다.
    - "Representation Finetuning(ReFT)" 방법을 소개하며, 이 방법은 고정된 기본 모델 위에서 작업별 특정 개입을 학습하여 숨겨진 표현을 조작하고, 이를 통해 추론 시 모델 행동을 지정된 작업에 맞게 유도한다.
    - 인터넷의 예시로는 LoRA와 같은 최신 PEFT가 가중치 대신 표현을 수정함으로써 얻을 수 있는 잠재력을 탐구한다.

2. **ReFT(ReFT):**
    - ReFT는 가중치 기반의 PEFT와는 대조적으로, 모델의 숨겨진 표현을 직접 조작하는 새로운 접근 방식을 도입한다. 이는 표현에 대한 개입을 통해 모델의 더 풍부한 의미 정보를 활용하고자 하는 아이디어에서 출발한다.
    - ReFT의 강력한 사례인 "Low-rank Linear Subspace ReFT(LoReFT)"는 표현에 대한 개입을 더욱 효율적으로 만든다. LoReFT는 낮은 차원의 투영 행렬을 사용하여 숨겨진 표현에 대해 구현된다.
    - LoReFT는 기존 PEFTs보다 훨씬 더 매개변수 효율적이며, 다양한 벤치마크에서 최신 PEFTs를 거의 항상 능가한다.

3. **실험(Experiments):**
    - 다양한 공통지식 추리, 산술 추리, 지시 따르기, 자연 언어 이해 작업에서 LoReFT를 평가한다. 이는 LoReFT가 LoRA에 비해 더 적은 매개변수를 사용하면서도 최상의 성능을 달성한다는 것을 보여준다.

**전반적 요약(Overall Summary)**:
이 연구는 기존 PEFT 방법을 개선하고자 하며, 모델의 숨겨진 표현에 개입하는 새로운 접근 방식인 ReFT를 제안한다. 특히, LoReFT는 매우 매개변수 효율적이면서 여러 벤치마크에서 높은 성능을 보여주어, 가중치 기반 PEFT 대신 사용될 수 있는 효과적인 대안임을 시사한다. 이 연구는 AI와 기계학습 분야에서의 새로운 가능성을 열며, 향후 더 많은 탐구를 위한 출발점을 제공한다.

## Similar Papers
- [Xmodel-LM Technical Report](2406.02856.md)
- [MultiLoRA: Democratizing LoRA for Better Multi-Task Learning](2311.11501.md)
- [MoRA: High-Rank Updating for Parameter-Efficient Fine-Tuning](2405.12130.md)
- [OpenBezoar: Small, Cost-Effective and Open Models Trained on Mixes of Instruction Data](2404.12195.md)
- [VeRA: Vector-based Random Matrix Adaptation](2310.11454.md)
- [JetMoE: Reaching Llama2 Performance with 0.1M Dollars](2404.07413.md)
- [QLoRA: Efficient Finetuning of Quantized LLMs](2305.14314.md)
- [Adapting LLMs to Hebrew: Unveiling DictaLM 2.0 with Enhanced Vocabulary and Instruction Capabilities](2407.07080.md)
- [LoRA Learns Less and Forgets Less](2405.09673.md)
