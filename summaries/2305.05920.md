# Fast Distributed Inference Serving for Large Language Models
## TL;DR
## Summary
- [https://arxiv.org/pdf/2305.05920.pdf](https://arxiv.org/pdf/2305.05920.pdf)

### 1. 각 섹션 요약

#### Introduction (서론)
대규모 언어 모델(LLM)은 대화형 AI 애플리케이션의 새 시대를 열었습니다. LLM 추론 작업은 기존의 완전 처리(run-to-completion) 방식의 처리로 인해 병목 현상과 긴 작업 완료 시간(JCT)을 겪습니다. FastServe는 LLM의 자회귀 패턴을 활용하여 출력 토큰 단위로 선점할 수 있게 하여 JCT를 최소화하는 분산 추론 제공 시스템입니다.

#### Background and Motivation (배경 및 동기)
LLM 추론은 ResNet과 같은 다른 딥러닝 모델 추론과는 다르게 각 출력 토큰을 생성하는 여러 반복(iteration)으로 구성됩니다. 이러한 특성으로 인해 실행 시간이 예측 불가능하며, 이는 효율적인 스케줄링을 어렵게 만듭니다. FastServe는 이러한 문제를 해결하기 위해 설계되었습니다.

#### Desired Properties (목표 속성)
FastServe의 주요 목표는 다음 세 가지입니다:
1. **낮은 작업 완료 시간:** 사용자들이 빠른 응답을 기대하기 때문에 LLM 추론 작업의 JCT를 최소화해야 합니다.
2. **효율적인 GPU 메모리 관리:** LLM의 모델 파라미터와 중간 상태를 저장하는 데 많은 GPU 메모리가 필요하므로 이를 효율적으로 관리해야 합니다.
3. **확장 가능한 분산 실행:** LLM 추론 작업을 여러 GPU에 분산하여 확장 가능해야 합니다.

#### Overall Architecture (전체 아키텍처)
FastServe의 아키텍처는 작업 풀이 있으며, 스킵-조인 다단계 피드백 큐(skip-join MLFQ) 스케줄러가 초기 우선 순위를 결정합니다. 스케줄러는 작업을 GPU 클러스터에서 실행하고 분산된 키-값 캐시와 상호 작용합니다. GPU 메모리 제한 문제를 해결하기 위해 키-값 캐시 관리자가 우선 순위가 낮은 작업의 데이터를 호스트 메모리로 오프로드합니다.

#### FastServe Design (FastServe 디자인)
FastServe는 다음과 같은 세 가지 주요 디자인 요소로 구성됩니다:
1. **스킵-조인 다단계 피드백 큐 스케줄러:** 작업의 우선 순위를 결정하여 JCT를 최소화합니다.
2. **프로액티브 키-값 캐시 관리:** GPU 메모리 용량 제한 문제를 해결하기 위해 데이터를 호스트 메모리로 오프로드하고 업로드하는 메커니즘입니다.
3. **분산 LLM 제공 지원:** LLM 추론 작업을 여러 GPU에 분산하여 확장성을 보장합니다.

#### Skip-Join MLFQ Scheduler (스킵-조인 MLFQ 스케줄러)
전통적인 MLFQ 스케줄링 방법은 작업 크기를 알 수 없기 때문에 LLM 추론에 적합하지 않습니다. FastServe는 작업의 첫 번째 반복 시간에 따라 적절한 우선 순위 큐에 작업을 배정하여 선점을 효율적으로 관리합니다.

#### Proactive Key-Value Cache Management (프로액티브 키-값 캐시 관리)
MLFQ 스케줄링은 더 많은 GPU 메모리를 필요로 합니다. FastServe는 키-값 텐서를 필요할 때만 GPU 메모리에 유지하고, 필요하지 않을 때는 호스트 메모리로 오프로드하여 메모리 사용을 최적화합니다.

#### Support for Distributed LLM Serving (분산 LLM 제공 지원)
대규모 LLM 모델은 여러 개의 GPU에 분산하여 처리해야 합니다. FastServe는 텐서 병렬 처리와 파이프라인 병렬 처리를 지원하여 분산 추론을 효과적으로 수행합니다.

#### Implementation (구현)
FastServe는 NVIDIA FasterTransformer를 기반으로 시스템 프로토타입을 구현했습니다. 다양한 GPT 모델과 작업 부하를 사용하여 FastServe의 성능을 평가했습니다.

#### Evaluation (평가)
FastServe는 최첨단 솔루션인 Orca에 비해 평균 및 꼬리 JCT를 최대 5.1배 및 6.4배까지 개선했습니다. FastServe의 설계 선택과 확장성을 평가하여 GPT-3 175B 모델의 엔드 투 엔드 성능을 분석했습니다.

#### Conclusion (결론)
FastServe는 LLM 추론 작업을 효율적으로 관리하여 JCT를 최소화하고 GPU 메모리 사용을 최적화합니다. 분산된 LLM 추론 제공을 지원하여 대규모 모델을 효율적으로 처리할 수 있습니다.

### 2. 전체 요약
FastServe는 대규모 언어 모델(LLM)의 분산 추론 제공을 최적화하는 시스템으로, 작업 완료 시간을 최소화하고 GPU 메모리를 효율적으로 관리하는 것을 목표로 합니다. 스킵-조인 MLFQ 스케줄러와 프로액티브 키-값 캐시 관리 메커니즘을 도입하여 효율성을 극대화하고, 분산된 LLM 추론을 지원합니다. FastServe는 NVIDIA FasterTransformer 기반으로 구현되었으며, 다양한 GPT 모델에서 성능을 검증하여 최첨단 솔루션에 비해 뛰어난 성능을 입증했습니다.