# LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale
## TL;DR
## Summary
- [https://arxiv.org/pdf/2208.07339.pdf](https://arxiv.org/pdf/2208.07339.pdf)

### 요약

#### 1. 서론
대규모 사전학습 언어 모델은 많은 메모리를 필요로 합니다. 이 논문은 성능 저하 없이 다중 억개 매개변수 트랜스포머를 8비트로 양자화하는 방법을 제안합니다. 기존 방법들은 350M 매개변수 이하의 모델에만 적용 가능했으나, 이 연구는 175B 매개변수까지 확장할 수 있습니다.

#### 2. 배경
양자화 기술은 주로 절대 최대값(absolute maximum) 양자화와 제로포인트(zeropoint) 양자화로 나뉩니다. 절대 최대값 양자화는 데이터 타입의 전체 비트 범위를 활용하지 못해 정밀도가 떨어집니다. 제로포인트 양자화는 이를 개선하지만 실용적인 제약이 있습니다.

#### 3. 방법
LLM.int8() 방법은 두 가지 핵심 요소로 구성됩니다:
1. **벡터 단위 양자화**: 행과 열마다 다른 정규화 상수를 사용하여 행렬 곱셈의 정밀도를 높입니다.
2. **혼합 정밀도 분해**: 99.9%의 값은 8비트로, 나머지 중요한 값은 16비트로 처리하여 메모리 사용량을 절반으로 줄입니다.

#### 4. 주요 결과
- **성능 평가**: 125M부터 13B까지의 모델에서 LLM.int8()이 성능 저하 없이 8비트로 작동함을 확인했습니다.
- **제로샷 성능**: 125M부터 175B까지의 모델에서 LLM.int8()이 16비트 성능을 유지했습니다.
- **중요한 특징**: 트랜스포머의 모든 층에서 큰 크기의 특징이 나타나며, 이들은 모델의 성능에 중요합니다.

#### 5. 관련 연구
- **언어 모델의 큰 특징**: 이전 연구에서는 레이어 정규화와 토큰 빈도 분포가 이러한 큰 특징과 관련이 있다고 보고했습니다.
- **대규모 트랜스포머 양자화**: nuQmm과 ZeroQuant와 같은 방법들은 벡터 단위 양자화보다 더 정밀한 그룹 단위 양자화를 사용합니다. 그러나 이 연구는 175B 매개변수까지 성능 저하 없이 확장 가능합니다.

#### 6. 결론
LLM.int8() 방법은 175B 매개변수까지 성능 저하 없이 8비트 양자화를 가능하게 합니다. 이는 대규모 언어 모델의 접근성을 높이며, 더 많은 연구와 응용이 가능하게 합니다.

### 전체 요약
이 논문은 트랜스포머 모델의 메모리 사용량을 줄이기 위해 8비트 양자화를 적용하는 방법을 제안합니다. LLM.int8() 방법은 벡터 단위 양자화와 혼합 정밀도 분해를 결합하여, 175B 매개변수까지 성능 저하 없이 모델을 운영할 수 있습니다. 이 연구는 대규모 언어 모델의 접근성을 높여, 연구와 응용의 가능성을 확장합니다.