# BigCodeBench: Benchmarking Code Generation with Diverse Function Calls and Complex Instructions
## TL;DR
## Summary
- [https://arxiv.org/pdf/2406.15877.pdf](https://arxiv.org/pdf/2406.15877.pdf)

## 섹션별 요약

### 1. 서론
이 논문은 자동화된 소프트웨어 엔지니어링, 특히 코드 생성 및 수리에 대한 관심이 증가하는 배경에서 시작합니다. 대규모 언어 모델(LLM)이 문법적으로 올바른 기능성 코드를 생성하는 능력이 뛰어나다고 인정받고 있으나, 기존의 벤치마크가 현실적인 소프트웨어 엔지니어링 문제를 충분히 반영하지 못하고 있다고 지적합니다. 본 연구는 LLM이 더 현실적이고 도전적인 프로그래밍 작업을 해결할 수 있는지 평가하기 위해 고품질의 실행 기반 벤치마크(BigCodeBench)를 제안합니다.

### 2. BigCodeBench의 구성
이 섹션에서는 LLM과 인간 전문가의 협력을 통해 BigCodeBench를 구성하는 방법을 설명합니다. 논문은 LLM을 이용해 프로그래밍 작업을 수집하고, 프로그램을 리팩터링하며, 테스트 케이스를 추가하는 방법을 자세히 다룹니다. BigCodeBench는 복잡한 소프트웨어 개발 작업을 시뮬레이션하기 위해 설계된 1,140개의 풍부한 컨텍스트를 가진 프로그래밍 작업을 포함합니다.

### 3. 결과
논문은 다양한 LLM들을 평가한 결과, 많은 모델들이 여전히 개선의 여지가 큼을 보여줍니다. 특히, 도구 사용 및 복잡한 지시 사항 따르기 능력에서 모델들이 겪는 어려움을 강조합니다. 또한, 벤치마크로부터 도출된 몇 가지 주요 결과와 모델 성능을 향상시킬 수 있는 잠재적 방법들을 제시합니다.

### 4. 결론 & 향후 방향
본 연구는 BigCodeBench 벤치마크를 통해 도구 사용 능력 및 복잡한 지시 사항을 따르는 능력을 평가하는 중요성을 강조합니다. 연구는 빅코드 커뮤니티의 지속적인 기여를 요청하며, LLM이 소프트웨어 엔지니어링 작업을 더 잘 해결할 수 있도록 벤치마크를 지속적으로 확장할 계획을 논의합니다.

## 혁신적 부분 및 주요 기여
- **BigCodeBench 벤치마크**: 기존의 벤치마크가 단순한 알고리즘적 프로그래밍 작업에 중점을 둔 것과 달리, BigCodeBench는 현실적이고 도전적인 소프트웨어 엔지니어링 문제에 초점을 맞추었습니다.
- **협력적 구성 방법**: LLM과 인간 전문가가 협력하여 벤치마크 작업을 구성하고 평가하는 새로운 접근 방식을 도입했습니다.
- **다양한 프로그래밍 작업**: 1,140개의 복합적인 프로그램 작업을 포함하여 모델이 다양한 시나리오에서 평가될 수 있도록 설계되었습니다.

## 종합 요약
이 논문은 대규모 언어 모델(LLM)이 현실적이고 복잡한 소프트웨어 엔지니어링 작업을 해결하는 능력을 평가하는 새로운 벤치마크, BigCodeBench를 소개합니다. BigCodeBench는 LLM과 인간 전문가가 협력하여 구성되었으며, 다양한 도구 사용 및 복잡한 지시 사항을 수행하는 능력을 평가합니다. 연구 결과, 현재의 LLM들이 여전히 많은 개선이 필요함을 보여주었으며, 벤치마크를 기반으로 모델 성능을 향상시킬 수 있는 여러 방안을 제시합니다. BigCodeBench는 기존의 단순한 벤치마크를 넘어서 현실적인 문제 해결 능력을 평가하는 중요한 기여를 합니다.

## Similar Papers
- [Perplexed by Perplexity: Perplexity-Based Data Pruning With Small Reference Models](2405.20541.md)
- [OpenDevin: An Open Platform for AI Software Developers as Generalist Agents](2407.16741.md)
- [CodexGraph: Bridging Large Language Models and Code Repositories via Code Graph Databases](2408.03910.md)
- [Applying RLAIF for Code Generation with API-usage in Lightweight LLMs](2406.20060.md)
- [CodeEditorBench: Evaluating Code Editing Capability of Large Language Models](2404.03543.md)
- [AutoCoder: Enhancing Code Large Language Model with \textsc{AIEV-Instruct}](2405.14906.md)
- [OpenBezoar: Small, Cost-Effective and Open Models Trained on Mixes of Instruction Data](2404.12195.md)
- [MuChoMusic: Evaluating Music Understanding in Multimodal Audio-Language Models](2408.01337.md)
- [DevEval: A Manually-Annotated Code Generation Benchmark Aligned with Real-World Code Repositories](2405.19856.md)
