# MultiLoRA: Democratizing LoRA for Better Multi-Task Learning
## TL;DR
## Summary
- [https://arxiv.org/pdf/2311.11501.pdf](https://arxiv.org/pdf/2311.11501.pdf)

### 섹션별 요약

#### 1. 서론 (Introduction)
- **내용 요약**:
  - 최근 대규모 언어 모델(LLM)은 다양한 자연어 처리 작업에서 뛰어난 성능을 보여주었습니다. 그러나 LLM을 특정 작업에 맞추기 위한 전체 파라미터 미세 조정은 많은 메모리를 필요로 하며, 이는 비용이 많이 듭니다. 이를 해결하기 위해 파라미터 효율적 미세 조정(PEFT) 방법이 제안되었습니다. PEFT 방법은 모델 파라미터의 일부만 최적화하여 VRAM 사용을 줄입니다. 
  - LoRA는 PEFT 방법 중 하나로, 낮은 랭크 행렬을 사용하여 추가 파라미터를 최소화하면서 성능을 유지합니다. 그러나 LoRA는 복잡한 다중 작업 시나리오에서는 성능이 제한될 수 있습니다. 이를 해결하기 위해 MultiLoRA가 제안되었습니다. MultiLoRA는 LoRA의 단점을 극복하기 위해 모듈을 수평으로 확장하고 파라미터 초기화를 변경하여 더 균형 잡힌 유니터리 서브스페이스를 만듭니다.

#### 2. 관련 연구 (Related Work)
- **내용 요약**:
  - PEFT 방법은 하드웨어 요구 사항을 줄이기 위해 모델 미세 조정의 일부 파라미터만 최적화합니다. 대표적인 PEFT 방법으로는 어댑터, p-튜닝, IA3, LoRA 등이 있습니다. LoRA는 모듈성, 병합 가능한 가중치, 제로 추론 오버헤드 등의 장점을 가지고 있습니다.
  - PEFT 방법을 다중 작업 학습에 적용한 연구도 있지만, 이들 방법은 주로 자연어 이해(NLU) 작업에 초점을 맞추고 있어 생성 LLM의 관심 작업을 충분히 반영하지 못합니다.

#### 3. 방법론 (Method)
- **내용 요약**:
  - MultiLoRA는 LoRA의 문제를 해결하기 위해 제안되었습니다. LoRA는 특정 상위 특이 벡터에 의존하여 유니터리 변환의 기여도를 극대화하지만, MultiLoRA는 이를 완화하기 위해 모듈을 수평으로 확장하고 파라미터 초기화를 변경합니다.
  - LLaMA 모델의 스택된 디코더 레이어 구조를 설명하고, LoRA의 낮은 랭크 적응 방식을 소개합니다.
  - LoRA와 전체 파라미터 미세 조정의 차이를 분석하기 위해 SVD를 사용하여 가중치 업데이트 행렬의 특이 값 분포를 비교합니다.

#### 4. 실험 (Experiments)
- **내용 요약**:
  - MultiLoRA의 성능을 평가하기 위해 메모리 프로파일, 처리량, 다운스트림 성능을 측정합니다. 실험은 LLaMA 시리즈 모델(7B~65B)을 사용하여 수행됩니다.
  - MultiLoRA는 단일 데이터셋 설정과 혼합 데이터셋 설정 모두에서 LoRA 및 전체 파라미터 미세 조정과 비교하여 더 나은 성능을 보입니다. 특히 작은 모델에서 뛰어난 성능 안정성을 보입니다.
  - 리소스 및 처리량 분석 결과, MultiLoRA는 LoRA와 유사한 처리량을 유지하지만, VRAM 사용량은 병렬 LoRA 모듈 수에 따라 선형적으로 증가합니다.

#### 5. MultiLoRA 이해 (Understanding MultiLoRA)
- **내용 요약**:
  - SVD를 사용하여 가중치 업데이트 행렬을 분석한 결과, MultiLoRA는 LoRA보다 전체 파라미터 미세 조정과 더 유사한 서브스페이스를 나타냅니다. 또한, 특이 값 분포를 비교한 결과, MultiLoRA는 LoRA에 비해 더 민주적인 유니터리 변환 기여도를 보입니다.
  - 병렬 LoRA 모듈 수를 증가시키면 더 세밀한 피팅이 가능해집니다.

#### 6. 결론 (Conclusion)
- **내용 요약**:
  - MultiLoRA는 복잡한 다중 작업 시나리오에서 성능을 향상시키기 위해 LoRA의 상위 특이 벡터 지배를 완화합니다. 수평 확장 및 파라미터 초기화를 통해 더 균형 잡힌 유니터리 서브스페이스를 생성합니다.
  - 광범위한 실험을 통해 MultiLoRA는 단일 LoRA보다 우수하며, 여러 벤치마크와 모델 규모에서 전체 파라미터 미세 조정과 비슷한 성능을 달성합니다. 특히 작은 모델에서 안정적인 다중 작업 적응을 제공합니다.

### 전체 요약
MultiLoRA는 기존 LoRA의 단점을 보완하여 복잡한 다중 작업 시나리오에서 더 나은 성능을 제공합니다. LoRA는 특정 상위 특이 벡터에 의존하여 복잡한 작업에서 성능이 제한될 수 있지만, MultiLoRA는 모듈을 수평으로 확장하고 파라미터 초기화를 변경하여 더 민주적인 유니터리 서브스페이스를 만듭니다. 이를 통해 다양한 벤치마크와 모델 규모에서 더 안정적이고 우수한 성능을 발휘합니다. MultiLoRA는 LoRA와 비교하여 VRAM 사용량이 증가하지만, 처리량은 거의 동일합니다. 결론적으로, MultiLoRA는 다중 작업 적응에 효율적이고 효과적인 솔루션을 제공합니다.