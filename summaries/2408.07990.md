# FuseChat: Knowledge Fusion of Chat Models
## TL;DR
## Summary
- [https://arxiv.org/pdf/2408.07990.pdf](https://arxiv.org/pdf/2408.07990.pdf)

### 1. 각 섹션 요약

#### 1. 소개 (Introduction)
이 논문은 대규모 언어 모델(LLMs)을 처음부터 개발하는 데 소요되는 높은 비용과 시간 문제를 해결하기 위해 다양한 구조와 기능을 가진 기존의 LLMs를 결합하여 더 강력한 LLM을 만드는 "지식 융합(knowledge fusion)"이라는 개념을 소개합니다. 이를 통해 개발 비용을 절감하고 모델의 강점을 통합할 수 있습니다.

#### 2. 관련 연구 (Related Works)
LLMs의 합성 접근법으로는 앙상블 메서드와 파라미터 공간에서 여러 신경망을 병합하는 방법 등이 있습니다. 지식 융합은 이들 모델과 달리 다양한 아키텍처의 LLMs를 하나로 통합하여 더 강력한 LLM을 만듭니다.

#### 3. FUSECHAT 접근법 (FUSECHAT Approach)
- **3.1 개요 (Overview)**: FUSECHAT는 두 주요 단계로 이루어져 있습니다. 첫번째는 '퓨즈' 단계로, 다양한 구조와 규모의 소스 LLMs 간에 쌍별 지식 융합을 수행하여 동일한 구조와 크기의 타겟 LLMs를 만듭니다. 두번째는 '병합' 단계로, 타겟 LLMs를 파라미터 공간에서 병합합니다.
- **3.2 준비 작업 (Preliminaries)**: 훈련 데이터셋에서 각 명령과 그에 대한 응답을 모델에 입력하여 예측 확률 분포 행렬을 생성합니다.
- **3.3 쌍별 지식 융합 (Pairwise Knowledge Fusion)**: 다양한 LLMs로부터 얻은 확률 분포 행렬을 융합하여 타겟 LLM을 훈련합니다. 이를 통해 다양한 소스 LLM의 지식을 통합합니다.

#### 4. 실험 (Experiments)
FUSECHAT는 OpenChat, Starling-LM, NH2-SOLAR, InternLM, Mixtral, Qwen 등의 다양한 아키텍처와 규모를 가진 여섯 개의 대표적인 LLMs를 사용하여 실험을 진행했습니다. 최종 FUSECHAT-7B 모델은 다양한 벤치마크에서 기존 여러 모델보다 우수한 성능을 보였습니다.

#### 5. 결론 (Conclusion)
지식 융합을 통해 FUSECHAT는 다양한 아키텍처와 규모를 가진 LLMs를 단일 LLM으로 통합하여 더 강력한 모델을 만듭니다. FUSECHAT는 모델 크기와 관계 없이 효율적으로 동작하며, 특히 모델 크기가 고려사항일 때 매우 효율적입니다.

#### 6. 한계 및 미래 연구 (Limitations and Future Work)
본 연구는 다양한 도메인을 아우르는 지식 융합 데이터셋을 구성하는 것에 중점을 두었습니다. 이는 데이터 엔지니어링에 많은 노력을 필요로 하며, 확장성에 제약이 있습니다. 따라서 향후 연구는 더 효율적인 데이터 합성 기법을 개발하는 것에 집중해야 합니다.

### 2. 전체 요약
이 논문은 "지식 융합"이라는 혁신적 접근법을 통해 다양한 구조와 기능을 가진 기존 LLMs를 통합하여 더 강력한 LLM을 만드는 방법을 제안합니다. FUSECHAT는 다양한 소스 모델 간의 쌍별 융합과 병합 단계를 거쳐 최종 모델을 생성합니다. 실험 결과, FUSECHAT는 다양한 벤치마크에서 기존 여러 모델에 비해 우수한 성능을 보였습니다. 이 접근법은 모델 크기를 효율적으로 관리할 수 있는 점에서 특히 뛰어나며, 데이터 엔지니어링에 많은 노력이 필요하지만, 향후 연구를 통해 더 효율적인 데이터 합성 기법이 개발될 것으로 기대됩니다.