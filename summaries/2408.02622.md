# Language Model Can Listen While Speaking
## TL;DR
## Summary
- [https://arxiv.org/pdf/2408.02622.pdf](https://arxiv.org/pdf/2408.02622.pdf)

### 1. 각 섹션 요약 및 주요 기여 내용 설명

#### 서론 (Introduction)
본 논문은 대화형 음성 언어 모델(iSLM)에서 실시간 상호작용을 강화하기 위해 전체 이중 모델링(FDM)을 도입하는 문제를 다룹니다. 이는 머신이 사람과 대화하는 동안 동시에 듣고 말할 수 있도록 하는 기술입니다. 이를 구현하기 위해 듣고-말하는 동안 언어 모델(LSLM)을 제안하며, 이는 음성 생성과 실시간 오디오 입력을 동시에 처리할 수 있습니다.

#### 관련 연구 (Related Work)
Simplex와 Half Duplex 음성 언어 모델은 단방향 또는 교대로 말하는 모델로 제한됩니다. 그러나 이 연구에서는 Full Duplex 음성 언어 모델(SLM)을 탐구하여 인간이 기계와 실시간 상호작용할 수 있는 능력을 설명합니다. 기존 연구들은 텍스트 중심의 언어 모델에서 이러한 기능을 발전시켰지만, 본 연구는 FDM 기능을 내장한 모델을 제안합니다.

#### Full Duplex Modeling (FDM)
FDM은 모델이 동시에 말하고 들을 수 있어야 함을 의미합니다. 이는 예측 단계에서 양쪽 채널에서 실시간으로 받은 입력을 활용하여 다음 토큰을 예측하는 방식으로 동작합니다. 이 접근 방식은 다중 채널 정보를 효과적으로 활용하여 상호작용의 정확성과 유창함을 향상시키는 데 도움이 됩니다.

#### 제안된 LSLM (Proposed LSLM)
본 논문은 FDM을 구현하기 위해 듣고-말하는 동안 언어 모델(LSLM)을 제안합니다. LSLM은 토큰 기반 디코더 전용 텍스트 음성 변환(TTS) 모델과 스트리밍 자가 지도 학습(SSL) 인코더를 사용하여 실시간으로 오디오 입력을 처리합니다. LSLM은 초기 융합, 중간 융합 및 후반 융합의 세 가지 전략을 사용하여 두 채널을 융합합니다. 실험 결과, 중간 융합이 가장 우수한 성능을 보였습니다.

#### 실험 및 결과 (Experiments and Results)
본 실험에서는 LSLM의 명령 기반 및 음성 기반 FDM 성능을 평가합니다. TTS 성능 평가에서는 단어 오류율(WER)을, 상호작용 성능 평가에서는 정밀도(Precision), 재현율(Recall), F1 점수를 사용합니다. 실험 결과, 중간 융합 LSLM은 다양한 환경에서 안정적으로 우수한 성능을 보여주었습니다.

#### 결론 (Conclusion)
본 논문에서는 실시간 상호작용을 강화하기 위해 전체 이중 모델링(FDM)을 도입한 듣고-말하는 동안 언어 모델(LSLM)을 제안합니다. LSLM은 토큰 기반 디코더 전용 TTS와 스트리밍 SSL 인코더를 통합하여 실시간 턴테이킹을 가능하게 합니다. 다양한 실험을 통해 LSLM의 우수한 성능을 입증하였으며, 이는 실생활 애플리케이션에 대한 가능성을 제시합니다.

### 2. 전체 요약

본 논문은 실시간 인간-기계 상호작용을 개선하기 위해 Full Duplex Modeling(FDM)을 도입한 대화형 음성 언어 모델(LSLM)을 제안합니다. LSLM은 동시 듣기와 말하기를 가능하게 하여 턴테이킹 문제를 해결합니다. 중간 융합 전략이 가장 우수한 성능을 보였으며, 이를 통해 음성 생성과 실시간 상호작용을 동시에 수행할 수 있음을 입증하였습니다. 본 연구의 결과는 실생활 응용 프로그램에서의 자연스러운 인간-기계 상호작용 가능성을 한층 높였습니다.

## Similar Papers
- [Efficient Audio Captioning with Encoder-Level Knowledge Distillation](2407.14329.md)
- [Investigating Decoder-only Large Language Models for Speech-to-text Translation](2407.03169.md)
- [FlashSpeech: Efficient Zero-Shot Speech Synthesis](2404.14700.md)
- [LiveSpeech: Low-Latency Zero-shot Text-to-Speech via Autoregressive Modeling of Audio Discrete Codes](2406.02897.md)
- [Multimodal Large Language Models with Fusion Low Rank Adaptation for Device Directed Speech Detection](2406.09617.md)
- [Qwen2-Audio Technical Report](2407.10759.md)
- [Sibyl: Simple yet Effective Agent Framework for Complex Real-world Reasoning](2407.10718.md)
- [VALL-E 2: Neural Codec Language Models are Human Parity Zero-Shot Text to Speech Synthesizers](2406.05370.md)
- [E2 TTS: Embarrassingly Easy Fully Non-Autoregressive Zero-Shot TTS](2406.18009.md)
