# Projected Language Models: A Large Model Pre-Segmented Into Smaller Ones
## TL;DR
## Summary
- [https://openreview.net/pdf/348dad9ca10fe95c3d41d9078db9149a02d522d8.pdf](https://openreview.net/pdf/348dad9ca10fe95c3d41d9078db9149a02d522d8.pdf)

### 1. 각 섹션의 요약

#### 1. 서론 (Introduction)
대형 언어 모델(LLM)은 다양한 언어 작업을 처리할 수 있지만, 높은 추론 비용 때문에 사용에 제약이 있습니다. 반면 소형 언어 모델(SLM)은 효율적이지만, 성능이 제한적이며 특정 도메인에만 적합합니다. 본 논문은 사전 학습 중 전문화 데이터를 알 수 없는 상황에서도 높은 전문화 정확도를 가진 소형 언어 모델을 얻는 방법을 탐구합니다. 새로운 아키텍처인 "프로젝티드 네트워크(PN)"를 제안하며, 이는 높은 용량의 네트워크로, 매개변수를 작은 네트워크로 선형 투영하여 미세 조정할 수 있습니다.

#### 2. 프로젝티드 언어 모델 (Projected Language Models)
프로젝티드 네트워크(PN)는 소형 모델들의 집합을 공동으로 학습합니다. 각 모델은 큰 매개변수의 특정 선형 투영을 통해 구현됩니다. 대용량 PN 네트워크를 사전 학습하고, 전문화 데이터가 제공되면, 해당 데이터를 통해 미세 조정을 수행합니다. PN 모델은 3개의 하이퍼 파라미터(h, k, m)로 구성되며, 이는 전체 모델 용량, 전문가 수, 각 전문가에게 할당된 작은 용량을 조절합니다. PN은 다층 퍼셉트론(MLP) 레이어에만 적용되며, 이는 모델 매개변수의 대부분을 차지합니다.

#### 3. 실험 및 결과 (Experiments & Results)
본 연구는 제한된 전문화 데이터와 제한된 추론 예산을 가진 언어 모델을 목표로 합니다. 다양한 사전 학습 및 전문화 비용에 따른 성능을 비교하였습니다. 주요 성능 지표는 전문화 데이터의 퍼플렉시티로 측정됩니다. SLM, SLM-믹스(SLM-mix), SLM-프로젝티드(SLM-pn) 등의 모델을 비교한 결과, 사전 학습 후 미세 조정을 통해 퍼플렉시티가 크게 향상되었으며, SLM-pn 모델이 다른 모델보다 더 좋은 성능을 보였습니다.

#### 4. 결론 (Conclusions)
본 논문은 도메인 데이터 부족과 제한된 추론 예산이라는 제약을 고려한 언어 모델을 제안합니다. 제안된 프로젝티드 네트워크(PN)는 여러 소형 모델을 공동으로 학습하고, 각 모델은 독립적으로 사용할 수 있어 새로운 도메인에서 미세 조정이 가능합니다. 실험 결과는 다양한 도메인, 학습 예산, 학습 데이터 크기에서의 성능 향상을 보여줍니다. PN은 사전 학습 데이터 없이도 전문화할 수 있어 민감한 데이터에도 적용이 가능합니다.

### 2. 전체 요약
본 논문은 대형 언어 모델(LLM)의 높은 추론 비용 문제를 해결하기 위해, 사전 학습 시 전문화 데이터를 알 수 없는 상황에서도 높은 전문화 정확도를 가진 소형 언어 모델(SLM)을 얻는 방법을 제안합니다. 제안된 프로젝티드 네트워크(PN)는 높은 용량의 네트워크를 소형 네트워크로 선형 투영하여, 효율적인 미세 조정을 가능하게 합니다. 다양한 도메인과 학습 예산에서의 실험을 통해, PN 모델이 다른 기존 방법들보다 우수한 성능을 보임을 입증하였습니다.

### 3. 논문의 주요 목적
본 논문의 주요 목적은 제한된 추론 예산과 제한된 도메인 데이터의 상황에서, 높은 성능을 유지하는 소형 언어 모델을 개발하는 것입니다. 이를 위해, 높은 용량의 네트워크를 소형 네트워크로 선형 투영하여 미세 조정할 수 있는 프로젝티드 네트워크(PN) 아키텍처를 제안하였습니다. PN은 사전 학습 시 많은 데이터를 사용하여 일반적인 성능을 확보하고, 필요한 경우 각 도메인에 맞춰 효율적으로 미세 조정할 수 있습니다. 이는 특히 전문화 데이터가 제한된 응용 프로그램에서 효과적이며, 데이터 소유자가 민감한 데이터를 가지고 있을 때도 적용 가능합니다.