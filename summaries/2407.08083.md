# MambaVision: A Hybrid Mamba-Transformer Vision Backbone
## TL;DR
## Summary
- [https://arxiv.org/pdf/2407.08083.pdf](https://arxiv.org/pdf/2407.08083.pdf)

### 1. 섹션별 요약

#### Introduction
이 논문은 비전 애플리케이션을 위해 특별히 고안된 새로운 하이브리드 Mamba-Transformer 백본 'MambaVision'을 제안합니다. 주요 공헌으로는 Mamba 구조를 재설계하여 시각적 특징을 효율적으로 모델링할 수 있게 한 점입니다. 또한, Vision Transformer(ViT)과 Mamba를 통합하는 가능성에 대해 종합적인 연구를 진행했습니다. 결과적으로, Mamba 구조에 여러 개의 자기 주목(Self-Attention) 블록을 추가하여 장거리 공간 의존성을 캡처하는 능력을 크게 향상시켰습니다.

#### Related Work
비전 트랜스포머(ViT)은 자기 주목 층을 활용해 수용 영역을 확장하여 CNN의 대안으로 떠올랐습니다. 그러나 CNN이 가지는 귀납적 편향과 변환 불변성 등의 장점이 부족하고, 대규모 데이터셋을 필요로 했습니다. 이에 대한 한계점을 극복하고자 다수의 연구가 제안되었습니다. Mamba 기반의 여러 연구들은 비전 태스크에서도 그 장점을 활용하려는 시도를 해왔습니다.

#### Methodology
MambaVision은 4개의 다른 스테이지로 구성된 계층적 아키텍처를 가지고 있으며, 높은 입력 해상도에서는 빠른 특징 추출을 위한 CNN 기반 레이어를, 낮은 해상도에서는 제안된 MambaVision과 Transformer 블록을 포함합니다. 제안된 구조는 CNN 기반 빠른 특징 추출과 함께 짧고 긴 거리의 공간 의존성을 동시에 캡처할 수 있는 능력을 갖추고 있습니다. 또한, 다양한 디자인 기준을 만족시키는 MambaVision 모델들의 패밀리를 소개합니다.

#### Experiments
논문에서 수행된 실험들은 MambaVision의 성능을 검증하기 위해 ImageNet-1K, MS COCO, ADE20K 데이터셋에서 이미지 분류, 객체 인식, 의미론적 분할 태스크 등에 대해 진행되었습니다. MambaVision 모델은 성능 및 이미지 처리 속도에서 기존 모델들보다 우수한 성능을 달성했습니다.

#### Results
MambaVision의 결과는 ImageNet-1K에서 새로운 SOTA(최고의) 성능을 달성했으며, 특히 상위-1 정확도와 이미지 처리 속도에서 탁월한 성능을 보였습니다. MS COCO와 ADE20K 데이터셋에서도 객체 인식과 의미론적 분할 태스크에서 MambaVision 백본을 사용한 모델이 비교 가능한 크기의 다른 백본을 사용한 모델보다 우수한 성능을 나타냈습니다.

#### Conclusion
논문은 Mamba와 Transformer를 결합한 최초의 하이브리드 비전 백본을 소개하였으며, 다양한 디자인 통합 패턴에 대한 종합적인 연구를 제시했습니다. 연구 결과, MambaVision 백본은 탁월한 성능을 제공하며, 향후 하이브리드 비전 모델의 새로운 클래스의 기초가 될 수 있음을 희망합니다.

### 2. 전체 요약
이 논문은 비전 태스크를 개선하기 위한 하이브리드 Mamba-Transformer 백본인 MambaVision을 제안합니다. 구조적으로, MambaVision은 빠른 특징 추출을 위해 높은 해상도에서는 CNN 기반 레이어를, 낮은 해상도에서는 자기 주목 블록을 포함한 제안된 MambaVision 블록을 결합하여 짧고 긴 거리의 공간 의존성을 동시에 캡처합니다. 실험 결과, MambaVision은 ImageNet-1K, MS COCO, ADE20K 등 다양한 데이터셋에서 최고 성능을 기록하며, 이미지 처리 속도 면에서도 뛰어난 결과를 보였습니다. 이로써 MambaVision 모델은 향후 하이브리드 비전 모델 설계에 중요한 기여를 할 수 있을 것으로 보입니다.

## Similar Papers
- [VSSD: Vision Mamba with Non-Causal State Space Duality](2407.18559.md)
- [POA: Pre-training Once for Models of All Sizes](2408.01031.md)
- [Alleviating Distortion in Image Generation via Multi-Resolution Diffusion Models](2406.09416.md)
- [Diffusion-RWKV: Scaling RWKV-Like Architectures for Diffusion Models](2404.04478.md)
- [TAPTRv2: Attention-based Position Update Improves Tracking Any Point](2407.16291.md)
- [Bytes Are All You Need: Transformers Operating Directly On File Bytes](2306.00238.md)
- [Sparser is Faster and Less is More: Efficient Sparse Attention for Long-Range Transformers](2406.16747.md)
- [COCONut: Modernizing COCO Segmentation](2404.08639.md)
- [Xmodel-VLM: A Simple Baseline for Multimodal Vision Language Model](2405.09215.md)
