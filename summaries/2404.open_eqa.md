# OpenEQA: Embodied Question Answering in the Era of Foundation Models
## TL;DR
## Summary
- [https://open-eqa.github.io/assets/pdfs/paper.pdf](https://open-eqa.github.io/assets/pdfs/paper.pdf)

**1. 개요**

본 논문은 인간의 기억을 활용하거나 주변 환경을 적극적으로 탐색하는 두 가지 방법을 통해 자연어로 질문에 답할 수 있는 새로운 벤치마크 데이터셋인 OpenEQA를 소개합니다. 이는 실제 환경에서 다양한 모달리티와 질문 유형을 포함하는 특성으로 인해, 최신 AI 모델의 한계를 시험하고 연구를 촉진하는 데 목적이 있습니다.

**2. OpenEQA 벤치마크**

이 벤치마크는 1600개가 넘는 질문을 포함하며, 이는 사람이 생성한 다양한 질문 유형으로 구성되어 있습니다. 각 질문은 질문자가 주변 환경에 대한 이해를 바탕으로 할 수 있는 질문입니다. 이 데이터셋은 특히 에피소드 메모리(EM-EQA)와 액티브 탐색(A-EQA) 두 가지 시나리오를 지원합니다.

**3. 평가 방법 및 기술**

평가 메트릭은 LLM-Match라는 새로운 방법을 사용하여 인간의 평가와 높은 상관관계를 가지며, 다양한 AI 모델을 평가하는데 사용됩니다. 이는 OpenEQA가 실제 환경을 반영한 효과적인 평가 도구로 사용될 수 있음을 시사합니다.

**4. 연구 결과 및 분석**

여러 최신 AI 모델이 이 벤치마크에서 테스트되었고, 인간의 수행 능력에는 못 미치지만, GPT-4V와 같은 모델이 상대적으로 높은 성능을 보였습니다. 분석을 통해 모델이 질문에 대한 시각적 맥락을 얼마나 잘 이해하고 활용하는지에 대한 인사이트를 제공합니다.

**5. 결론**

OpenEQA는 AI의 현실 세계 적용 가능성을 탐구하고, 다양한 연구를 촉진하기 위한 중요한 도구입니다. 이는 AI 모델이 실제 환경에서 어떻게 작동할 수 있는지를 평가하는 데 큰 도움이 될 것입니다.

이러한 요약을 바탕으로 보다 전체적인 요약을 제공하겠습니다.

본 논문에서 소개된 OpenEQA는 실제 환경을 반영한 1600개 이상의 다양한 자연어 질문을 포함하는 벤치마크 데이터셋입니다. 이 데이터셋은 인간이 만든 질문으로 구성되어 있으며, 에피소드 메모리와 액티브 탐색이라는 두 가지 시나리오를 지원합니다. 평가 방법으로는 LLM-Match라는 새로운 메트릭을 도입하여 인간의 판단과 높은 상관관계를 보이는 것으로 나타났습니다. 실험 결과 다양한 AI 모델이 테스트되었으며, 특히 GPT-4V가 높은 성능을 보였지만 여전히 인간의 수준에는 미치지 못했습니다. 이 연구는 AI 기술이 실제 환경에서 어떻게 활용될 수 있는지 이해하는 데 중요한 기여를 하며, 향후 AI 모델의 개선과 적용을 위한 중요한 토대를 마련합니다.