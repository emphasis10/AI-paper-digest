# Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement
## TL;DR
## Summary
- [https://arxiv.org/pdf/2412.04003.pdf](https://arxiv.org/pdf/2412.04003.pdf)

### 1. 논문 섹션별 요약

#### 도입부
마르코-LLM(Marco-LLM)은 다국어 자연어 처리(NLP) 작업에서 성능 격차를 해소하기 위해 저자원 언어에 중점을 두고 다국어로 훈련된 언어 모델입니다. 이 모델은 Qwen2 모델을 기반으로 막대한 양의 다국어 데이터를 수집하여 지속적인 다국어 사전 학습과 사후 학습을 수행합니다.

#### 관련 연구
대규모 언어 모델(LLM)은 최근 몇 년간 NLP의 연구 패러다임을 혁신했습니다. 특히 GPT-3, GPT-4와 같은 모델은 고자원 언어에 주로 초점을 맞추어 개발되었으므로 저자원 언어에서는 성능 문제가 지속되고 있습니다.

#### 마르코-LLM의 개발
이 연구에서는 다국어 친화적으로 설계된 Qwen2 모델을 기반으로 마르코-LLM을 개발합니다. 데이터 혼합 및 학습률은 중요한 하이퍼파라미터이며, 저자원 언어로 일반 지식을 효과적으로 전이하고자 두 단계의 지속적 사전 학습 방법론을 제안합니다.

#### 지속적인 사전 학습 및 평가
연구에서는 지속적 사전 학습을 통해 1.5B, 7B, 72B 모델 사이의 성능을 비교합니다. 고품질 데이터는 모델의 성능을 크게 향상시키며, 학습률이 능력 내에서 잊혀지는 것을 방지하기 위해 중요한 역할을 합니다.

#### 다국어 후학습
마르코-LLM은 Supervised Fine-Tuning과 Direct Preference Optimization 단계를 거칩니다. 이는 모델의 다국어 능력을 활성화하고, 다양한 도메인에서의 성능을 향상시키기 위한 것입니다.

#### 결론 및 미래 연구 방향
연구자는 앞으로 데이터의 품질과 다양성을 향상시키고, 합성 데이터를 확장하여 모델의 자기 개선 능력을 활용할 계획입니다.

### 2. 전체 요약
이 논문의 주요 기여는 저자원 언어를 포함한 다양한 언어에 대해 성능을 크게 향상시킨 다국어 언어 모델, 마르코-LLM의 개발입니다. 지속적인 사전 학습과 다국어 후학습을 통해 다양한 언어 환경에서의 효과적인 능력 강화 방법을 제시합니다. 이 모델은 특히 고자원과 저자원 언어 간 성능 격차를 줄이고, 추후 더 높은 품질의 데이터와 자기 개선을 이용해 발전할 계획입니다.