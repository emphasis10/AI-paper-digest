# Does Math Reasoning Improve General LLM Capabilities? Understanding Transferability of LLM Reasoning
## TL;DR
## Summary
- [https://arxiv.org/pdf/2507.00432.pdf](https://arxiv.org/pdf/2507.00432.pdf)

1. 각 섹션의 주요 내용 요약:

- 서론: 이 논문은 대규모 언어 모델(LLM)에서의 추론 능력 전이를 탐구하며, RL(강화 학습) 기반 튜닝이 일반적인 도메인에서 더 높은 전이를 보인다는 점을 강조합니다. SFT(지도 학습 기반 미세 조정)는 특정 도메인에 특화되지만, 다른 도메인에 대한 전이 성능이 제한적임을 지적합니다.

- 2.1 실험 결과: 수학적 추론 과제에서 RL로 튜닝된 모델들이 SFT 기반 모델보다 더 나은 성능을 보입니다. RL 기반 모델들은 모든 벤치마크에서 기반 모델보다 성능이 뛰어났지만, SFT 기반 모델들은 비추론 평가에서 성능 저하를 겪었습니다.

- 2.2 통제 연구: RL과 SFT간의 차이를 연구하기 위해 동일한 데이터셋을 사용한 통제된 실험이 진행되었으며, RL 기반 모델들이 SFT 모델들보다 더욱 효과적으로 일반 도메인 문제에 대한 성능을 유지하고 있는 것으로 나타났습니다.

- 3. 잠재 표현 변화: PCA 분석을 통해 RL은 백본 표현에서 최소한의 이동만을 유도하며, SFT는 큰 잠재적 이동을 초래하고 있음을 밝힙니다. 결과적으로 RL이 보다 목표 지향적인 최적화를 달성함을 보여줍니다.

- 결론: RL 기반 튜닝은 수학적 추론에서 큰 이득을 얻으면서 다른 도메인에도 긍정적인 전이를 유지하며, SFT는 비추론 벤치마크에서 종종 부정적인 전이를 가져옵니다. RL은 특성 안정성을 유지하는 데 있어 매우 효과적이며, 모델의 일반 도메인 표현을 더 잘 보존합니다.

2. 전체 요약:

이 논문은 대규모 언어 모델에서의 추론 능력을 개선하기 위한 새로운 학습 방식을 탐구합니다. 특히, 강화 학습(RL) 방식을 통해 다양한 도메인에서의 일반화를 유지하면서 특정 도메인에서도 향상된 성능을 달성하는 방법을 제시합니다. RL 모델은 지도 미세 조정(SFT) 모델보다 여러 도메인에 걸쳐 안정적인 성능을 보여주며, 잠재적인 표현 공간을 보다 일관되게 보존하여 다른 도메인으로의 지식 전이가 가능하다는 것을 입증합니다. 이러한 결과는 AI의 다양한 분야에 적용할 수 있는 강력한 모델을 개발하는 데 기여할 수 있습니다.