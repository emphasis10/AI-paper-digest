# Is Your Model Really A Good Math Reasoner? Evaluating Mathematical Reasoning with Checklist
## TL;DR
## Summary
- [https://arxiv.org/pdf/2407.08733.pdf](https://arxiv.org/pdf/2407.08733.pdf)

### 논문의 주요 내용 요약

#### 1. 서론
이 논문은 수학적 추론 능력을 대규모 언어 모델(LLMs)의 중요한 지표로 보고, 이를 평가하는 새로운 벤치마크 시스템인 MATHCHECK을 소개합니다. MATHCHECK은 다양한 수학적 추론 과제를 포함해 모델의 문제 해결 능력뿐 아니라 일반화 및 견고성을 테스트합니다. 이를 통해 모델의 진정한 수학적 능력을 보다 정확하게 평가할 수 있습니다.

#### 2. 관련 연구
수학적 추론 능력을 평가하기 위한 기존의 여러 벤치마크가 소개됩니다. 이 벤치마크들은 주로 문제 해결에 초점을 맞추고 있으며, 여러 단계의 학습 과정(사전 학습, 사후 학습)에서 수학적 추론 능력 향상 방법을 연구합니다. 논문에서는 다양한 벤치마크가 제안되었지만, 모델의 진정한 수학적 능력을 보여주기에는 한계가 있다고 지적합니다.

#### 3. MATHCHECK: 종합 평가 시스템
MATHCHECK는 모델의 과제 일반화 및 추론 견고성을 테스트하기 위해 설계된 체크리스트 기반 벤치마크 시스템입니다. MATHCHECK-GSM과 MATHCHECK-GEO로 나누어 각각 텍스트와 시각적 맥락에서 수학적 추론 능력을 평가합니다. MATHCHECK-GSM은 기존의 GSM8k와 같은 벤치마크를 향상시켰고, MATHCHECK-GEO는 GeoQA, UniGeo, Geometry3K 및 기타 시각적 수학적 추론 과제를 포함합니다.

#### 4. 모델 평가
논문에서는 20개 이상의 LLMs와 11개의 MLLMs의 수학적 추론 능력을 평가했습니다. MATHCHECK를 통해 모델 행동 분석도 수행하여 모델의 상세한 문제 해결 과정을 평가할 수 있습니다. 실험 결과, GPT-4와 같은 일부 선도적 LLM은 높은 성능을 보였지만, 다른 많은 모델들은 성능이 저하되는 것으로 나타났습니다. 이는 기존의 수학 벤치마크보다 MATHCHECK가 진정한 수학적 능력을 더 잘 반영한다는 것을 보여줍니다.

#### 5. 결론
모델이 진정으로 문제를 이해한다면 다양한 과업에서 견고하게 적용될 수 있어야 하며, 이를 바탕으로 MATHCHECK를 도입해 모델의 수학적 추론 능력을 더욱 정밀하게 평가할 수 있음을 보여줍니다. 연구자들이 보다 전반적인 평가 체계를 사용해 AI 시스템의 수학적 추론 능력을 평가하기를 권장합니다. 이 연구는 수학적 추론 능력 평가에 있어 중요한 발전을 이루는 한 걸음이 될 것입니다.

### 전반적인 요약
이 논문은 대규모 언어 모델의 수학적 추론 능력을 종합적으로 평가하기 위해 MATHCHECK이라는 새로운 벤치마크 시스템을 도입했습니다. MATHCHECK는 문제 해결 능력 외에도 모델의 과제 일반화 및 견고성을 평가하도록 설계되어 있습니다. MATHCHECK-GSM과 MATHCHECK-GEO로 분류된 벤치마크를 통해 20개 이상의 LLMs와 11개의 MLLMs를 평가한 결과, 기존 벤치마크보다 실제 수학적 추론 능력을 더 잘 반영하는 것으로 나타났습니다. 이는 AI 시스템의 수학적 추론 능력을 평가하는 데 있어 중요한 발전을 이룬 것입니다.

## Similar Papers
- [AI-Assisted Generation of Difficult Math Questions](2407.21009.md)
- [ChartMimic: Evaluating LMM's Cross-Modal Reasoning Capability via Chart-to-Code Generation](2406.09961.md)
- [MIRAI: Evaluating LLM Agents for Event Forecasting](2407.01231.md)
- [MAVIS: Mathematical Visual Instruction Tuning](2407.08739.md)
- [MuirBench: A Comprehensive Benchmark for Robust Multi-image Understanding](2406.09411.md)
- [Prover-Verifier Games improve legibility of LLM outputs](2407.13692.md)
- [A Careful Examination of Large Language Model Performance on Grade School Arithmetic](2405.00332.md)
- [AlphaMath Almost Zero: process Supervision without process](2405.03553.md)
- [TC-Bench: Benchmarking Temporal Compositionality in Text-to-Video and Image-to-Video Generation](2406.08656.md)
