# Can Large Reasoning Models Self-Train?
## TL;DR
## Summary
- [https://arxiv.org/pdf/2505.21444.pdf](https://arxiv.org/pdf/2505.21444.pdf)

1. **요약: 섹션별 중요 내용 요약**

- **초록**:
  대형 언어 모델의 성능 확대는 인간의 감독 의존도를 줄이는 방법에 점점 더 의존하고 있습니다. 자동 검증을 통한 강화 학습이 대안이 될 수 있으나, 이 또한 인간이 설계한 검증기에 대한 의존성으로 인해 확장성에 한계가 있습니다. 이 논문에서는 모델의 스스로의 판단력을 활용하여 감독 신호를 제공하고, 일반적인 정답 태그 없이 수학적 추론 작업을 수행하는 온라인 자기 학습 알고리즘을 제안합니다.

- **도입부**:
  대형 신경망은 인간이 큐레이션한 방대한 데이터셋을 통해 일반적인 언어 처리 기능을 갖추게 되었습니다. 그러나 인간이 생성한 데이터의 가용성은 점점 더 제한적인 요인으로 작용하고 있으며, 이는 모델 능력 확장에 한계를 초래하고 있습니다. 이 연구에서는 자기 개선능력을 갖춘 모델이 외부 피드백 없이 자체적으로 정답을 평가하고 수정을 통한 개선을 가능하게 하는 방법을 모색합니다.

- **주요 기여 및 혁신 부분**:
  이 논문에서는 스스로 생성한 대응 점수를 활용하여 감독 신호를 제공하며, 이러한 점수들이 초기에는 정확성과 긍정적으로 상관관계가 있지만 시간이 지남에 따라 부정확함을 유도할 수 있음을 밝힙니다. 이를 해결하기 위해 'Self-Rewarded Training(SRT)'이라는 새로운 학습 방법론을 소개하고, 모델 자체의 일관성을 감독 신호로 활용하며, 외부 레이블 없이 성능을 지속적으로 향상시키는 방안을 제안합니다.

2. **전체 요약**

이 논문은 대형 언어 모델의 성능을 지속적으로 강화하는 새로운 강화 학습 방법론인 Self-Rewarded Training(SRT)을 제안합니다. 이 방법은 모델의 자기 일관성을 활용하여 외부 레이블 없이도 지속적인 성능 향상을 가능하게 합니다. 특히, 수학적 추론 작업에서 고성능을 달성하며 강화 학습 초기에 인간 감독 없이도 경쟁력 있는 결과를 보여주었습니다. 하지만, 자기 생성된 보상이 초기에 긍정적 상관관계를 가지더라도 시간이 지나면 부정확성을 야기할 가능성이 있어, 보상 해킹을 방지하기 위한 몇 가지 완화 전략을 제공합니다. 이러한 새로운 접근은 자기 개선 모델의 미래 가능성을 열어주며, 지속적인 연구와 발전이 필요합니다.