# MobileCLIP: Fast Image-Text Models through Multi-Modal Reinforced Training
## TL;DR
## Summary
- [https://arxiv.org/pdf/2311.17049.pdf](https://arxiv.org/pdf/2311.17049.pdf)

이 연구는 모바일 장치에서의 빠른 이미지-텍스트 모델 실행을 목표로 하는 MobileCLIP를 제안합니다. 기존의 CLIP 모델과 비교하여, MobileCLIP는 실행 시간 성능을 최적화하면서도 효율적인 학습 방법을 통해 정확도를 개선합니다. 이를 위해 다중모달 강화 학습이라는 새로운 효율적인 학습 접근법을 도입하여, 이미지 캡셔닝 모델과 강력한 CLIP 인코더 집합으로부터의 지식 전달을 활용합니다. MobileCLIP은 여러 데이터셋에서 뛰어난 정확도-지연 시간 트레이드오프를 달성하며, 특히 제로샷 분류 및 검색 작업에서 새로운 최고 기록을 세웁니다.

### 연구의 주요 기여
- **MobileCLIP 모델:** 모바일 장치에 최적화된 새로운 이미지-텍스트 모델 패밀리로, 구조적 재매개변수화 및 컨볼루셔널 토큰 믹싱과 같은 여러 아키텍처 설계 기법을 활용하여 이미지 및 텍스트 인코더의 크기와 지연 시간을 줄입니다.
- **다중모달 강화 학습:** 이미지 캡셔닝 모델과 강력한 CLIP 모델 집합으로부터의 지식 전달을 통해 학습 효율성을 개선하는 새로운 학습 전략입니다. 이를 통해 기존 CLIP 학습 대비 10배에서 1000배의 학습 효율성 개선을 달성합니다.
- **DataCompDR 데이터셋:** 기존의 DataComp 데이터셋을 강화한 버전으로, 합성 캡션 및 강력한 이미지 증강을 포함하여 모델의 학습을 지원합니다.

### 종합 요약
MobileCLIP은 모바일 장치에 적합하도록 설계된 새로운 이미지-텍스트 모델 패밀리입니다. 이 연구는 다중모달 강화 학습을 통해 모델의 학습 효율성을 크게 향상시키며, 다양한 데이터셋에서 뛰어난 성능을 보입니다. 특히, 제로샷 분류 및 검색 작업에서의 성능 개선은 MobileCLIP이 효과적인 이미지-텍스트 모델로서의 가능성을 보여줍니다. 연구 결과는 모바일 장치에서의 AI 모델 배포와 관련된 도전 과제를 해결하는 데 기여할 것으로 기대됩니다.

## Similar Papers
- [CatLIP: CLIP-level Visual Recognition Accuracy with 2.7x Faster Pre-training on Web-scale Image-Text Data](2404.15653.md)
- [Knowledge Transfer from Vision Foundation Models for Efficient Training of Small Task-specific Models](2311.18237.md)
- [Scaling (Down) CLIP: A Comprehensive Analysis of Data, Architecture, and Training Strategies](2404.08197.md)
- [Knowledge Composition using Task Vectors with Learned Anisotropic Scaling](2407.02880.md)
- [MUSCLE: A Model Update Strategy for Compatible LLM Evolution](2407.09435.md)
- [Graph-Based Captioning: Enhancing Visual Descriptions by Interconnecting Region Captions](2407.06723.md)
- [MoDE: CLIP Data Experts via Clustering](2404.16030.md)
- [CLIP meets Model Zoo Experts: Pseudo-Supervision for Visual Enhancement](2310.14108.md)
- [EVLM: An Efficient Vision-Language Model for Visual Understanding](2407.14177.md)
