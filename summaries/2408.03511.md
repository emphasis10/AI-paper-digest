# MoExtend: Tuning New Experts for Modality and Task Extension
## TL;DR
## Summary
- [https://arxiv.org/pdf/2408.03511.pdf](https://arxiv.org/pdf/2408.03511.pdf)

### 1. 섹션별 요약

#### Introduction (소개)
- **주요 내용:**
이 논문은 대규모 언어 모델(LLM)의 멀티모달(시각-언어) 이해 능력을 확장하는 방법을 다룹니다. 기존의 방법론들(예: LLaVA)은 비전 인코더와 LLM을 연결하여 새로운 모달리티를 통합하지만, 이는 고비용과 이전 지식 상실 문제(카타스트로픽 포깅)로 어려움을 겪습니다. 이를 해결하기 위해 MoExtend라는 새로운 프레임워크를 제안하여 비용 효율적으로 모달리티를 확장합니다. 
  - **기여와 혁신:**
    - MoExtend는 사전 훈련된 모델을 튜닝하지 않고 새로운 전문가를 통합하여 카타스트로픽 포깅을 방지합니다.

#### Methodology (방법론)
- **주요 내용:**
  - **Alignment Stage (정렬 단계):**
    - CLIP 비전 인코더를 사용하여 이미지-텍스트 페어를 통해 모달 정보를 초기 정렬.
  - **Extension Stage (확장 단계):**
    - 어느 레이어에 새로운 전문가를 추가할지 결정.
  - **Fine-tuning Stage (세부 조정 단계):**
    - 새로운 전문가의 파라미터 튜닝.
  - **기여와 혁신:**
    - MoExtend는 적절한 위치에 새로운 전문가를 추가하여 학습 비용과 카타스트로픽 포깅 문제를 줄입니다.

#### Experiments (실험)
- **주요 내용:**
  - **설정:** LLaVA 1.5 설정을 따르며 CLIP 비전 인코더 사용.
  - **데이터셋:** LLaVA 1.5-558k 및 LLaVA 1.5-mix-665k 사용.
  - **평가:** MoExtend는 이미지 질문 응답 벤치마크에서 LLaVA-1.5 모델과 경쟁하며 뛰어난 성능을 보임.
  - **기여와 혁신:**
    - MoExtend는 전면적인 튜닝 없이도 상당한 성능을 유지하며, 200시간이 아닌 30시간의 튜닝 시간만 필요합니다.

#### Conclusion (결론)
- **주요 내용:**
논문은 MoExtend가 모달리티 적응과 확장 문제를 효율적으로 해결하는 방법을 증명하며, 기존 모델의 성능 저하 없이 새로운 모달리티를 통합할 수 있음을 보입니다. 
  - **기여와 혁신:**
    - MoExtend는 시간과 비용을 절감하면서도 멀티모달 이해 능력을 크게 향상시킵니다.

### 2. 전체 요약
이 논문은 대규모 언어 모델(LLM)에서 멀티모달 데이터를 처리할 수 있는 새로운 프레임워크, MoExtend를 제안합니다. MoExtend는 사전 훈련된 모델을 튜닝하지 않고도 새로운 모달리티를 추가할 수 있는 방법을 제공합니다. 이는 모델의 카타스트로픽 포깅 문제를 방지하고, 학습 비용을 현저히 줄여줍니다. 실험 결과 MoExtend는 이미지 질문 응답 등 다양한 벤치마크에서 뛰어난 성능을 보였으며, 기존의 전면적인 튜닝 방법보다 훨씬 효율적입니다. 이 논문은 멀티모달 AI 연구에 중요한 기여를 합니다.