# Step-DPO: Step-wise Preference Optimization for Long-chain Reasoning of LLMs
## TL;DR
## Summary
- [https://arxiv.org/pdf/2406.18629.pdf](https://arxiv.org/pdf/2406.18629.pdf)

### 요약

#### 1. 소개 (Introduction)
이 논문은 대형 언어 모델(LLMs)이 긴 논리 체인을 필요로 하는 수학적 추론 능력을 향상시키기 위한 새로운 접근법인 Step-DPO를 소개합니다. 기존의 Direct Preference Optimization(DPO)는 긴 수학적 논리 체인에서 제한된 효용성을 보이며, 이를 보완하기 위해 Step-DPO는 각 중간 추론 단계를 기본 단위로 처리하여 오류를 빠르게 식별하고 최적화합니다.

#### 2. 관련 연구 (Related Works)
LLMs의 수학적 추론 성능을 향상시키기 위해 여러 연구들이 진행되었습니다. 특히, Chain-of-Thought(CoT) 추론 프레임워크와 다양한 데이터 증강 기법이 시도되었지만, 이들 방법은 일반화에 제한이 있고 성능이 일정 수준 이상에서는 정체되는 경향이 있습니다.

#### 3. Step-DPO
Step-DPO는 기존의 DPO와 달리 전체 답변이 아닌 중간 추론 단계를 기본 단위로 최적화합니다. 이를 통해 모델은 오류를 더 명확히 식별하고 개선할 수 있습니다. Step-DPO는 고품질의 쌍별 선호 데이터셋 생성 파이프라인을 포함하며, 약 10,000개의 샘플로 구성된 데이터셋을 구축합니다.

#### 4. 실험 (Experiments)
Step-DPO의 효능을 검증하기 위한 여러 실험을 수행했습니다. 다양한 대형 모델들(Qwen2, Meta-Llama-3 등)과 데이터를 사용하여 수학적 추론 성능을 비교한 결과, Step-DPO를 적용한 모델들이 기존 모델들보다 뛰어난 성능을 보였습니다. 특히 MATH와 GSM8K 데이터셋에서 높은 정확도를 기록했습니다.

#### 5. 결론 (Conclusion)
Step-DPO는 긴 논리 체인의 문제를 해결하기 위해 설계된 효율적이고 효과적인 방법으로, 수학적 추론 성능을 크게 향상시킵니다. 이 접근법은 고품질의 쌍별 선호 데이터를 통해 오류를 명확히 식별하고 최적화함으로써 모델의 예측 정확도를 높입니다.

---

### 전체 요약
이 논문은 대형 언어 모델(LLMs)의 수학적 추론 능력을 향상시키기 위한 새로운 접근법, Step-DPO를 제안합니다. 기존의 Direct Preference Optimization(DPO)의 한계를 보완하기 위해 개발된 Step-DPO는 각 중간 추론 단계를 기본 단위로 최적화하여 오류를 빠르게 식별하고 개선합니다. Step-DPO는 고품질의 데이터셋을 구축하는 경제적이고 효과적인 파이프라인을 포함하며, 이를 통해 모델의 성능을 크게 향상시킵니다. 실험 결과, Step-DPO를 적용한 모델은 다양한 수학적 추론 과제에서 높은 정확도를 기록하였으며, 이는 LLMs의 잠재력을 더욱 극대화하는 데 기여할 수 있음을 보여줍니다.

## Similar Papers
- [LiteSearch: Efficacious Tree Search for LLM](2407.00320.md)
- [Step-Controlled DPO: Leveraging Stepwise Error for Enhanced Mathematical Reasoning](2407.00782.md)
- [Weak-to-Strong Reasoning](2407.13647.md)
- [Toward Self-Improvement of LLMs via Imagination, Searching, and Criticizing](2404.12253.md)
- [Self-play with Execution Feedback: Improving Instruction-following Capabilities of Large Language Models](2406.13542.md)
- [Improve Mathematical Reasoning in Language Models by Automated Process Supervision](2406.06592.md)
- [Iterative Reasoning Preference Optimization](2404.19733.md)
- [Synthesizing Text-to-SQL Data from Weak and Strong LLMs](2408.03256.md)
- [Self-Training with Direct Preference Optimization Improves Chain-of-Thought Reasoning](2407.18248.md)
