# d1: Scaling Reasoning in Diffusion Large Language Models via Reinforcement Learning
## TL;DR
## Summary
- [https://arxiv.org/pdf/2504.12216.pdf](https://arxiv.org/pdf/2504.12216.pdf)

### 1. 각 섹션의 주요 내용 요약

#### 서론
이 논문은 비오토레그로시브(비순차적) 고확률 문장 생성 모델의 한계를 극복하고, 이 모델의 학습 성과를 향상시키기 위한 방법으로 `masked diffusion large language models` (dLLMs)의 적용 가능성을 탐색합니다. 이를 통해 온라인 강화 학습(RL)을 통한 적용 가능성을 조사합니다.

#### 강화 학습을 통한 LLM의 이유 능력 향상
이 논문은 SFT(감독 학습 조정)와 RL(강화 학습)을 활용하여 dLLMs의 추론 능력을 개선하는 방법을 소개합니다. 특히 diffu-GRPO라는 새로운 critic-free 정책 그래디언트 기반 RL 알고리즘을 도입하여 SFT와 비교할 때 확장성이 뛰어난 것으로 나타났습니다.

#### 실험 및 결과
LLaDA라는 모델을 사용하여, 수학적 및 논리적 추론과 관련된 네 가지 과제에 대해 실험이 수행되었습니다. 수학적 추론에서는 다단계 초등수학 문제와 고등학교 수학 문제를 대상으로 하였고, 논리적 추론에서는 스도쿠 퍼즐과 몇 가지 조합적 산술 게임이 포함되었습니다. 결과적으로 diffu-GRPO는 SFT보다 우수한 성능을 나타냈으며, 두 방법을 결합한 d1은 두 방법 모두를 단독으로 사용할 때보다 더 큰 성능 향상을 보여주었습니다.

#### 결론
이 연구에서는 dLLMs의 추론 능력을 확장하기 위한 여러 가지 방법을 제안하였으며, 특히 diffu-GRPO가 여러 벤치마크에서 SFT를 능가하는 것으로 나타났습니다. 새로운 연구 방향으로는 효율적인 디코딩 전략의 개발을 통해 더 효율적인 RL 훈련을 구현하는 방안이 제안되었습니다.

### 2. 전체 요약
이 논문에서는 dLLMs를 이용한 문장 생성 모델의 추론 능력을 강화하기 위해 두 가지 주요 방법론인 SFT와 RL을 탐구하고 결합하였습니다. diffu-GRPO는 강화 학습의 일환으로 제안되어 있어, 기존의 SFT를 보완하면서 더 나은 성능을 제공합니다. 실험 결과는 이러한 결합 방법이 수학적 및 논리적 과제 모두에서 상당한 성과 향상을 가져왔음을 보여주어, 향후 AI 모델 개발의 중요한 기초가 될 수 있음을 시사합니다.