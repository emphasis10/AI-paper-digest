# AceReason-Nemotron 1.1: Advancing Math and Code Reasoning through SFT and RL Synergy
## TL;DR
## Summary
- [https://arxiv.org/pdf/2506.13284.pdf](https://arxiv.org/pdf/2506.13284.pdf)

### 1. 주요 내용 요약

#### 서론
이 논문은 대형 언어 모델(LLM)을 활용한 수학 및 코드 추론을 다룹니다. 연구는 강화학습(RL)과 함께 감독된 미세 조정(SFT) 기법을 통합하여 최첨단 추론 모델을 구축하는 방법을 제시합니다.

#### 연구 기여
주요 기여는 두 가지로 나뉩니다:
1. 수학 및 코드 벤치마크에서 모델의 추론 성능을 개선하기 위해, 다양한 설정에서 SFT를 확장하고 이에 대한 성과를 분석합니다.  
2. RL과 SFT의 시너지를 규명함으로써 이를 활용해 추론 능력이 뛰어난 모델을 구축합니다.

#### 방법론
논문에서는 SFT와 RL을 이용한 학습 방법을 제시합니다. 특히, AceReason-Nemotron-1.1 모델이 Qwen2.5 기반 7B 모델 중에서 최고의 성능을 보여주는 것을 강조합니다.

#### 결과
AceReason-Nemotron-1.1 모델은 수학과 코드 및 다학문 분야에서 뛰어난 결과를 보여줍니다. 특히, 수학 전용 RL 트레이닝은 코딩 성능도 크게 향상시킵니다.

#### 결론
논문은 SFT와 RL의 시너지를 통해 향상된 학습 및 일반화 능력을 가진 모델을 어떻게 구축할 수 있는지 보여줍니다.

### 2. 전반적인 요약

이 논문은 수학과 코드 추론을 개선하기 위해 SFT와 RL의 시너지를 연구합니다. 확장된 SFT 전략과 단계적으로 적용된 RL 기법을 통해, 이상적인 상태의 문제 해결 능력을 가지는 모델을 개발할 수 있음을 보여줍니다. 특히 AceReason-Nemotron-1.1 모델은 최상의 성능을 발휘하며 수학 및 코드 영역 모두에서 강력한 일반화 능력을 입증합니다.