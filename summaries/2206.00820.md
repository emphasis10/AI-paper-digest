# NIPQ: Noise proxy-based Integrated Pseudo-Quantization
## TL;DR
## Summary
- [https://arxiv.org/pdf/2206.00820.pdf](https://arxiv.org/pdf/2206.00820.pdf)

### 1. 섹션별 주요 내용 요약

#### 1. 소개 (Introduction)
종합적인 최적화 기법으로, 신경망 양자화는 메모리 사용량 줄이기 및 계산량 감소를 목표로 한다. 주로 네트워크의 활성화와 가중치를 저정밀도로 저장하여 성능을 향상시키며, 대규모 서버와 내장 장치에서도 적용 가능하다. 그러나 기존 양자화 방법은 정확도 저하 문제가 있다.

#### 2. 관련 연구 (Related Work)
STE (Straight-Through Estimator)를 통한 양자화 인식 훈련(QAT)은 비결정적 함수 상에서 그래디언트 흐름을 허용하는 방식으로 경험적인 정확도 개선을 보였으나, 불안정성과 편향 문제로 인해 최적화 네트워크에서 정확도 손실을 초래할 수 있다. 최근에는 PQT (Pseudo-Quantization Training)가 제안되었으며, 이는 양자화 파라미터를 더 안정적으로 업데이트하는 방식을 채택하고 있으나 여전히 개선 여지가 남아 있다.

#### 3. NIPQ 제안 (Noise Proxy-based Integrated Pseudo-Quantization)
NIPQ는 양자화 인식 훈련의 불안정성을 해소하고, 트렁케이션을 통합하여 활성화와 가중치 양자화에서 정확도 저하를 최소화하는 새로운 방법을 제안한다. 주요 공헌은 다음과 같다:
- 가중치뿐만 아니라 활성화 양자화도 지원하는 최초의 PQT 통합.
- 주어진 자원 제한 내에서 임의의 네트워크를 혼합 정밀도로 최적화 가능.
- 양자화 하이퍼파라미터가 양자화 오류를 최소화하도록 업데이트됨.
- 광범위한 실험 결과에서 기존 방법들보다 성능이 크게 향상됨.

#### 4. 실험 결과 (Experimental Results)
여러 비전 및 언어 응용 프로그램들에서 NIPQ는 기존의 혼합 정밀도 양자화 방법들보다 높은 정확도를 보인다. MobilNet-v2/v3와 같은 최적화된 네트워크에서 NIPQ는 특히 뛰어난 성능을 발휘하며, 객체 탐지 및 초해상도 작업에서도 높은 안정성을 검증함으로써 실용성을 입증했다.

#### 5. 결론 (Conclusion)
NIPQ는 기존 양자화 기법들의 단점을 극복하고, 안정적인 양자화 및 최적화를 가능한 새로운 프레임워크를 제안한다. 이 방법은 이론적인 분석을 통해 양자화 하이퍼파라미터의 최적화가 양자화 오류를 최소화한다는 점을 입증했으며, 양자화 트렁케이션을 포함하여 활성화 및 가중치 양자화 모두에서 상당한 성능 향상을 보인다. 다양한 비전 및 언어 응용 프로그램에 대해 폭넓은 실험 결과가 이를 뒷받침한다.

### 2. 전체 요약
논문은 신경망 양자화 기법의 불안정성과 정확도 저하 문제를 해결하기 위해 NIPQ(Noise Proxy-based Integrated Pseudo-Quantization)라는 새로운 방법을 제안한다. 기존의 양자화 인식 훈련(QAT)은 비결정적 함수 상의 그래디언트 흐름 문제를 가진 반면, NIPQ는 이 문제를 극복하여 안정적인 양자화 및 최적화를 가능하게 한다. 특히 트렁케이션을 통합하여 가중치뿐만 아니라 활성화 양자화에서도 높은 정확도를 유지한다. 다양한 실험에서 NIPQ는 기존 방법들보다 월등한 성능을 보였다.