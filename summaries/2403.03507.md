# GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection
## TL;DR
## Summary
- [https://arxiv.org/pdf/2403.03507.pdf](https://arxiv.org/pdf/2403.03507.pdf)

### 섹션별 요약

#### 1. 소개
**내용 요약:**
대규모 언어 모델(LLM)들은 여러 분야에서 뛰어난 성과를 보이고 있으나, 이들의 학습에는 막대한 메모리 자원이 필요합니다. 기존의 저메모리 적응 기법인 LoRA는 학습 시 메모리 사용을 줄일 수 있으나, 성능 면에서 한계가 있습니다. 이를 해결하기 위해 본 논문에서는 GaLore(Gradient Low-Rank Projection)라는 새로운 학습 전략을 제안합니다. GaLore는 LLM의 전수 모수를 학습하면서도 메모리 효율성을 높일 수 있는 방법입니다.

#### 2. 관련 연구
**내용 요약:**
기존 연구들은 저메모리 학습과 파라미터 효율 적응 기법에 집중해 왔습니다. 대표적으로 LoRA는 저랭크 행렬을 도입하여 메모리 사용을 줄이는 방식입니다. 그러나 LoRA는 학습 초기 단계에서 고랭크 모델로의 초기화가 필요하고, 최적의 성능을 내기 어려운 문제가 있습니다. 이에 반해 GaLore는 이러한 문제를 해결하기 위해 설계되었습니다.

#### 3. 제안 방법
**내용 요약:**
GaLore는 학습 중 기울기 행렬이 저랭크 구조를 갖는다는 이론적 근거를 바탕으로, 기울기 행렬을 두 개의 프로젝션 행렬을 사용해 저랭크 형태로 변환합니다. 이를 통해 옵티마이저 상태의 메모리 사용을 크게 줄일 수 있습니다. GaLore는 저랭크 기울기 업데이트를 통해 학습 역동성을 변경하지 않고 메모리 효율성을 달성합니다.

#### 4. 실험 결과
**내용 요약:**
실험 결과, GaLore는 LLaMA 7B 모델을 C4 데이터셋으로 학습할 때, 기존의 방법들보다 최대 65.5% 메모리 절감을 이루었습니다. 또한, RoBERTa를 GLUE 벤치마크에서 파인튜닝할 때도 기존 방법보다 뛰어난 성능을 보였습니다. GaLore는 다양한 옵티마이저와 쉽게 결합할 수 있으며, NVIDIA RTX 4090과 같은 소비자용 GPU에서 24GB 메모리로도 대규모 모델 학습이 가능함을 보여주었습니다.

#### 5. 결론
**내용 요약:**
GaLore는 LLM 학습 시 메모리 효율성을 크게 향상시킬 수 있는 기법으로, 저메모리 환경에서도 대규모 모델을 학습할 수 있게 합니다. 이를 통해 연구자들은 더 낮은 비용으로 더 큰 모델을 훈련할 수 있으며, 이는 AI 연구와 개발에 큰 기여를 할 것입니다.

### 전체 요약
**주요 기여 및 혁신:**
GaLore는 기울기 저랭크 프로젝션을 이용해 메모리 효율성을 극대화하면서도 전체 파라미터 학습을 가능하게 하는 새로운 학습 전략입니다. 기존의 저메모리 적응 기법들이 가진 한계를 극복하고, 대규모 언어 모델의 학습 시 필요한 메모리 자원을 크게 절감할 수 있습니다. 이를 통해 소비자용 GPU로도 대규모 모델 학습이 가능해졌으며, 이는 AI 연구 및 응용의 범위를 넓히는 데 기여할 것입니다.

## Similar Papers
- [From GaLore to WeLore: How Low-Rank Weights Non-uniformly Emerge from Low-Rank Gradients](2407.11239.md)
- [Grass: Compute Efficient Low-Memory LLM Training with Structured Sparse Gradients](2406.17660.md)
- [QLoRA: Efficient Finetuning of Quantized LLMs](2305.14314.md)
- [Q-GaLore: Quantized GaLore with INT4 Projection and Layer-Adaptive Low-Rank Gradients](2407.08296.md)
- [LoRA Learns Less and Forgets Less](2405.09673.md)
- [TriForce: Lossless Acceleration of Long Sequence Generation with Hierarchical Speculative Decoding](2404.11912.md)
- [Memory-Efficient Fine-Tuning of Compressed Large Language Models via sub-4-bit Integer Quantization](2305.14152.md)
- [Medusa: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads](2401.10774.md)
- [VeRA: Vector-based Random Matrix Adaptation](2310.11454.md)
