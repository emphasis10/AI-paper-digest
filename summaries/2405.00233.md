# SemantiCodec: An Ultra Low Bitrate Semantic Audio Codec for General Sound
## TL;DR
## Summary
- [https://arxiv.org/pdf/2405.00233.pdf](https://arxiv.org/pdf/2405.00233.pdf)

이 연구 논문에서는 "SemantiCodec"라는 새로운 초저비트율 음성 코덱을 소개하고 있습니다. 이 코덱은 오디오를 초당 100개 미만의 토큰으로 압축하여 다양한 오디오 유형(대화, 일반 오디오, 음악)에 걸쳐 고품질을 유지합니다. SemantiCodec은 이중 인코더 구조를 특징으로 하며, 의미론적 인코더와 음향 인코더의 출력을 사용하여 확산 모델 기반 디코더를 통해 오디오를 재구성합니다. 또한, SemantiCodec은 0.31kbps에서 1.43kbps 사이의 초저비트율을 지원하며, 실험 결과에 따르면 이는 기존의 최신 코덱보다 재구성 품질에서 뛰어난 성능을 보여줍니다.

### 주요 내용 요약

1. **서론 및 배경**:
   - 오디오 코덱은 디지털 오디오의 효율적인 통신과 방송을 위해 사용됩니다. 최근 깊은 학습의 도입으로 오디오 코덱의 오디오 품질과 비트율 효율이 크게 향상되었습니다.

2. **SemantiCodec의 구조 및 기능**:
   - SemantiCodec은 의미론적 인코더와 음향 인코더를 사용하여 오디오를 토큰화하고 이를 확산 모델을 사용하여 재구성합니다. 이 코덱은 자기 지도 학습을 통해 풍부한 오디오 표현을 학습하고, 이를 벡터 양자화(VQ) 레이어를 통해 효율적으로 처리합니다.

3. **성능 평가 및 응용**:
   - SemantiCodec은 다양한 비트율에서 우수한 재구성 품질을 제공하며, 특히 낮은 비트율에서도 비교적 높은 오디오 품질을 유지합니다. 이는 통신 및 저장 공간의 효율성을 크게 향상시킬 수 있습니다.

### 혁신적인 부분
SemantiCodec의 혁신성은 높은 의미 정보와 유사한 재구성 품질을 낮은 비트율에서 달성할 수 있다는 점에 있습니다. 이 코덱은 음성 언어 모델링의 효율성을 크게 향상시킬 수 있으며, 이는 향후 오디오 기반 언어 모델에 큰 영향을 미칠 수 있습니다.

이 논문은 오디오 데이터 압축과 오디오 언어 모델링 분야에서 새로운 접근 방식을 제시하며, 오디오 처리 기술의 발전에 기여할 것입니다.

## Similar Papers
- [E2 TTS: Embarrassingly Easy Fully Non-Autoregressive Zero-Shot TTS](2406.18009.md)
- [Efficient Audio Captioning with Encoder-Level Knowledge Distillation](2407.14329.md)
- [PicoAudio: Enabling Precise Timestamp and Frequency Controllability of Audio Events in Text-to-audio Generation](2407.02869.md)
- [Naturalistic Music Decoding from EEG Data via Latent Diffusion Models](2405.09062.md)
- [Applying Guidance in a Limited Interval Improves Sample and Distribution Quality in Diffusion Models](2404.07724.md)
- [Codecfake: An Initial Dataset for Detecting LLM-based Deepfake Audio](2406.08112.md)
- [Separating the "Chirp" from the "Chat": Self-supervised Visual Grounding of Sound and Language](2406.05629.md)
- [Towards Robust Speech Representation Learning for Thousands of Languages](2407.00837.md)
- [WavCraft: Audio Editing and Generation with Large Language Models](2403.09527.md)
