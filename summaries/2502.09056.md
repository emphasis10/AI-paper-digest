# An Open Recipe: Adapting Language-Specific LLMs to a Reasoning Model in One Day via Model Merging
## TL;DR
## Summary
- [https://arxiv.org/pdf/2502.09056.pdf](https://arxiv.org/pdf/2502.09056.pdf)

### 1. 섹션별 요약

**초록**: 이 논문은 데이터 선택과 모델 결합 방법론을 연구하여, 고급 추론 능력을 언어 특화 대규모 언어 모델(LLMs)에 도입하는 방법을 제안합니다. 특히, 태국어 LLM에 집중하며, DeepSeek R1의 추론 능력을 언어 특화 LLMs에 구현하려고 합니다.

**서론**: 최근 LLMs는 복잡한 추론 작업에서 그 능력을 입증해 왔지만, 대부분 영어와 중국어와 같은 고자원 언어에 초점이 맞춰졌습니다. 이러한 언어들은 양적, 질적으로 뛰어난 학습 데이터를 가지고 있으며, 이로 인해 저자원 언어의 성능이 제한됩니다.

**방법론**:
- **모델 선택**: 태국어 특화 모델과 장기 추론 능력을 가진 모델을 선택하여, 두 모델의 학습 파라미터를 효율적으로 결합했습니다.
- **표현 정렬(SFT)**: 두 모델의 내부 표현을 정렬하여, 원활한 모델 병합이 가능하게 했습니다. 이를 통해 언어 및 추론 능력의 조화를 도모했습니다.
- **능력 중심 모델 병합**: 서로 다른 능력의 모델 간 가중치를 최적화하여, 초반부는 추론 모델의 영향을, 후반부는 언어 모델의 영향을 받도록 하여 각각의 장점을 결합했습니다.

**실험 설정 및 평가**: 실험에서는 추론 능력과 언어 작업 성능을 평가하였으며, 깊은 추론 모델을 활용한 태국어 특화 모델의 성능 향상을 달성했습니다.

**결론 및 한계**: 제안한 방법론은 다국어 추론과 모델 병합에서의 여러 과제들을 제기하며, 모델 내부의 학습 표현을 보다 깊이 이해하고자 하는 목표를 내세웠습니다.

### 2. 전체 요약

이 논문은 저자원 언어 환경에서 고도의 추론 능력을 구현하는 방법론을 탐구합니다. 태국어 특화 모델과 선진 추론 모델을 결합하여, 언어와 추론 능력을 모두 향상시켰으며, 저비용으로도 고성능을 유지할 수 있는 가능성을 증명했습니다. 이러한 연구는 다국어 지역 모델 이니셔티브의 발전을 장려하고, 저자원 언어에서도 AI의 경쟁력을 강화하고자 하는 중요한 기여점을 가지고 있습니다.