# Deliberation in Latent Space via Differentiable Cache Augmentation
## TL;DR
## Summary
- [https://arxiv.org/pdf/2412.17747.pdf](https://arxiv.org/pdf/2412.17747.pdf)

**1. 각 섹션의 요약**

- **소개 (Introduction):**
  이 논문은 대형 언어 모델(LLM)이 중간 과정에서 메모리를 활용하여 보다 효과적으로 사고할 수 있도록 하는 방법을 제안합니다. 기존의 '더 많이 생각하기' 방법론이 수행한 결과를 개선하기 위해 캐시 보완을 통해 모델의 성능을 높이는 방식입니다.

- **방법론 (Methodology):**
  키-값 캐시(kv-cache)를 입력받아 이를 소프트 토큰을 통해 보완합니다. 이 작업은 기존 LLM을 변경하지 않고 코프로세서라는 별도 모듈을 학습시켜 수행됩니다. 학습은 기존 LLM의 파라미터를 고정한 채로 이루어져, 효율적인 최적화를 가능하게 합니다.

- **실험 (Experiments):**
  이 방법론은 다양한 논리적 과제를 다루는데 있어 일관된 성능 향상을 보여줍니다. Gemma-2 모델을 사용해 증명된 실험에서는, 라틴 임베딩의 수를 늘릴수록 성능 향상이 두드러졌습니다. 특히 어려운 문제들을 풀 때 더 많은 '생각'을 수행할 수 있도록 개선되었습니다.

- **결론 (Conclusion):**
  제안된 방법은 LLM 성능을 향상시키는 혁신적인 접근 방식입니다. 이는 기존 아키텍처 수정 없이 보다 풍부한 문맥 정보를 제공하여 예측 성능을 높입니다. 향후 연구에서는 코프로세서를 보다 큰 모델에 확장하거나, 여러 개의 모듈러 코프로세서를 사용하는 방향으로 나아갈 예정입니다.

**2. 전체 요약**

이 논문은 대형 언어 모델(LLM)의 성능을 높이기 위해 차분적 캐시 보완(differentiable cache augmentation) 방법을 제시합니다. 제안된 방법은 LLM의 기존 구조를 변경하지 않고, 코프로세서라 불리는 모듈을 통해 중간 임베딩을 생성하여 모델의 예측 능력을 향상시킵니다. 다양한 문제 해결에서 일관되게 성능이 향상되었으며, 특히 복잡한 논리 과제를 해결하는데 있어 유의미한 성과를 보였습니다.