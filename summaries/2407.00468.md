# MMEvalPro: Calibrating Multimodal Benchmarks Towards Trustworthy and Efficient Evaluation
## TL;DR
## Summary
- [https://arxiv.org/pdf/2407.00468.pdf](https://arxiv.org/pdf/2407.00468.pdf)

### 주요 내용 요약 (한국어)

#### 1. 서론
대규모 멀티모달 모델(LMMs)의 신뢰성 있는 평가를 위해 MMEVALPRO 벤치마크를 제안합니다. 기존의 평가 방법론인 다중선택 질문(MCQs)은 모델이 실제로 시각적 데이터를 처리하지 않고도 높은 점수를 얻을 수 있게 합니다. 이를 방지하기 위해 MMEVALPRO는 각 질문에 대해 지각과 지식 질문을 추가함으로써 진정한 정확도를 평가합니다.

#### 2. 방법론
MMEVALPRO는 MMMU, ScienceQA, MathVista benchmarks에서 각기 다른 질문 트리플렛을 구성하여 평가합니다. 인간 전문 검토자가 각 질문 트리플렛을 수동으로 레이블링하여 품질을 보장합니다. 진정한 정확도(Genuine Accuracy)는 모델이 지각 및 지식 질문을 모두 제대로 답할 수 있는지 여부를 평가하는 주요 지표입니다.

#### 3. 실험 및 결과
다양한 모델, 특히 최신의 LLM과 LMM을 대상으로 실험을 수행했습니다. LMM이 LLM보다 일관된 답변을 도출할 수 있는데, 이는 시각 처리 능력의 차이에서 기인합니다. GPT-4o 모델은 MCQ 기반 평가에서 높은 점수를 얻었지만 MMEVALPRO 평가에서는 큰 폭으로 점수가 하락했습니다.

#### 4. 세부 분석
LLM과 LMM 간의 성능 차이를 세부적으로 분석하였습니다. 일관성 격차(Consistency Gap)는 원래 질문에 대한 답변 정확도와 지각 및 지식 질문에 대한 정확도 간의 차이를 의미합니다. LLM의 일관성 격차가 LMM보다 더 큰 것으로 나타났습니다. 이는 LLM이 시각 정보를 처리하지 못하기 때문입니다.

#### 5. 관련 연구
여러 기존 벤치마크와 MMEVALPRO 간의 차이점을 비교하고, 기존 모델들이 가진 평가의 한계를 살펴보았습니다. PCA-Bench, MMStar, MathVerse 등의 벤치마크가 LLM의 우회적 답변을 허용할 가능성을 지적했습니다.

#### 6. 결론
MMEVALPRO는 기존의 평가 방식과 달리, 모델의 진정한 능력을 반영하는 더 신뢰할 수 있는 벤치마크입니다. 모델들이 지식과 지각 질문을 정확히 답할 수 있는지를 평가하여 신뢰성 있는 결과를 제공합니다. GPT-4o와 Qwen-VL-Max와 같은 최신 모델조차 인간 성과에 비해 큰 차이를 보였습니다.

### 주 내용 및 혁신적인 부분 요약
MMEVALPRO는 기존 다중선택 질문(MCQ)의 한계를 극복하고 LLM과 LMM 간의 진정한 능력을 평가하기 위해 지각 및 지식 앵커 질문을 도입하였습니다. 이 방법은 모델이 실제로 문제를 이해하고 있는지 평가하는데 더 신뢰성 있는 지표를 제공합니다.

### 전체 요약
MMEVALPRO는 대규모 멀티모달 모델(LMMs)을 평가하기 위해 설계된 벤치마크로, 기존의 MCQ 방식의 평가 한계를 극복하고자 합니다. 각 질문에 지각 및 지식 앵커 질문을 추가하여 모델이 실제로 지식을 이해하고 있는지 평가합니다. 이를 통해, LLM과 LMM 간의 진정한 능력을 평가하며, 인간 성과와의 비교를 통해 모델의 한계를 명확하게 나타냅니다. 실험 결과, 최신 모델조차도 인간 성과에 크게 미치지 못하는 것으로 나타났습니다.

---

이 요약을 바탕으로 프레젠테이션 자료를 만들면 좋습니다. 추가적인 도움이 필요하시면 말씀해 주세요!