# Don't Do RAG: When Cache-Augmented Generation is All You Need for Knowledge Tasks
## TL;DR
## Summary
- [https://arxiv.org/pdf/2412.15605.pdf](https://arxiv.org/pdf/2412.15605.pdf)

## 1. 섹션별 요약

### 서론
이 논문은 "Cache-Augmented Generation(CAG)"라는 새로운 접근 방식을 제안합니다. 이는 전통적인 "Retrieval-Augmented Generation(RAG)" 시스템의 단점을 해결하기 위해, 관련 문서를 미리 로드하여 실시간 검색 없이 질의에 답변할 수 있도록 합니다. 저자들은 CAG가 효율성과 정확성을 높일 수 있는 기반을 제공한다고 주장하며, 기존 RAG 시스템의 의존도를 줄일 수 있는 가능성을 제시합니다.

### 방법론
CAG 프레임워크는 다음의 세 가지 단계로 구성됩니다:
1. **외부 지식 미리 로드**: 관련 문서를 모델의 확장된 컨텍스트 창에 맞추어 처리하고, precomputed key-value(KV) 캐시를 생성합니다.
2. **추론**: 미리 로드된 KV 캐시를 사용하여 사용자의 질의에 대한 답변을 생성합니다. 이 시점에서 실시간 검색 과정이 제거되어 빠른 응답이 가능합니다.
3. **캐시 리셋**: 여러 추론 세션을 통해 시스템 성능을 유지하기 위해 KV 캐시를 효율적으로 재설정합니다.

### 실험
저자들은 SQuAD와 HotPotQA 데이터셋을 이용해 CAG의 성능을 평가합니다. 결과적으로 CAG는 전통적인 RAG 시스템보다 더 빠른 응답 시간과 높은 정확성을 기록했습니다. 특히, RAG가 실제로 검색 오류를 발생시킬 때 CAG는 모든 관련 정보를 통합하여 보다 정확한 답변을 산출합니다.

### 결론
CAG는 전통적인 RAG 시스템의 효율성을 개선할 뿐만 아니라 복잡도를 줄이고 응답 시간을 단축시키는 데 기여합니다. 향후 LLM들이 더욱 발전함에 따라 CAG의 활용 가능성 또한 확대될 것으로 보입니다. 저자들은 CAG 접근 방식이 지식 통합 작업에 있어 신뢰할 수 있는 대안이 될 수 있음을 강조합니다.

## 주요 기여 및 혁신 부분
- **검색 없는 긴 컨텍스트 패러다임**: 질의에 대한 응답 시 검색을 없애고, 미리 로드한 문서와 precomputed KV 캐시를 활용하여 전체적인 문맥의 이해를 높입니다.
- **성능 비교**: CAG가 전통 RAG 시스템보다 효율성과 정확성에서 우위를 점하는 여러 시나리오를 실험을 통해 입증했습니다.
- **실용적인 통찰**: 특정 애플리케이션을 위한 지식 집약적인 워크플로우 설정을 최적화하는 방법을 제시하며, 검색 없는 방법의 활용 가능성을 보여줍니다.

## 2. 전체 요약
이 논문은 Cache-Augmented Generation(CAG)이라는 새로운 패러다임을 소개하여, Retrieval-Augmented Generation(RAG) 시스템의 한계를 극복하고자 합니다. CAG는 전통적인 검색 과정 없이 미리 로드된 문서와 KV 캐시를 활용하여 보다 효율적이고 직관적인 질의 응답 시스템을 구현합니다. 아울러, SQuAD와 HotPotQA 데이터셋을 통한 실험 결과, CAG가 전통 RAG 시스템보다 더 높은 성능을 보여주며 응답 시간을 단축시키는 데 성공했음을 나타냅니다. CAG 접근 방식의 발전에 따라 지식 통합 작업에서의 적용 가능성이 높아질 것으로 기대됩니다.