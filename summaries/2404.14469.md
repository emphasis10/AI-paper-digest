# SnapKV : LLM Knows What You are Looking for Before Generation
## TL;DR
## Summary
- [https://arxiv.org/pdf/2404.14469.pdf](https://arxiv.org/pdf/2404.14469.pdf)

이 논문은 SnapKV, 새로운 LLM의 키-값(KV) 캐시 압축 기술을 소개합니다. 주요 내용은 다음과 같습니다:

1. **서론**: 현재 LLM은 큰 입력 길이를 다루는 데 효율성 문제가 있으며, KV 캐시의 성장은 메모리와 시간 효율성에 도전을 제기합니다. SnapKV는 이 문제를 해결하기 위해 고안되었습니다.

2. **SnapKV 기법**: 이는 모델 내 각 주의 헤드가 생성 중 특정 프롬프트 주의 특성에 일관되게 집중함을 발견하고, 이를 통해 중요한 KV 위치를 클러스터링하여 선택합니다. 이 접근법은 긴 입력 시퀀스를 처리할 때 계산 오버헤드와 메모리 사용량을 크게 줄입니다.

3. **성능 평가**: SnapKV는 입력이 16K 토큰일 때 기준 모델과 비교하여 생성 속도를 3.6배, 메모리 효율을 8.2배 향상시키며, 긴 시퀀스 데이터셋에서 기준 모델과 비슷한 성능을 유지합니다.

4. **실용적 적용**: SnapKV는 실제 애플리케이션에 큰 잠재력을 가지고 있으며, 다양한 LLM 및 긴 시퀀스 데이터셋에서의 효율성과 정확성을 입증하였습니다.

5. **결론**: 이 연구는 KV 캐시의 크기를 효율적으로 줄이면서도 모델의 정확성을 유지하는 방법을 제시하며, 특히 긴 입력을 처리할 때 LLM의 성능을 개선할 수 있는 중요한 기술적 진전을 나타냅니다.

SnapKV는 키-값 캐시를 효과적으로 관리하며, 이를 통해 대용량 언어 모델의 처리 효율성과 실용성을 크게 향상시킬 수 있습니다.

## Similar Papers
- [Retrieval Augmented Generation or Long-Context LLMs? A Comprehensive Study and Hybrid Approach](2407.16833.md)
- [Medusa: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads](2401.10774.md)
- [Evaluating Open Language Models Across Task Types, Application Domains, and Reasoning Types: An In-Depth Experimental Analysis](2406.11402.md)
- [ThinK: Thinner Key Cache by Query-Driven Pruning](2407.21018.md)
- [Does Fine-Tuning LLMs on New Knowledge Encourage Hallucinations?](2405.05904.md)
- [MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression](2406.14909.md)
- [Efficient Streaming Language Models with Attention Sinks](2309.17453.md)
- [DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](2405.04434.md)
- [Knowledge Card: Filling LLMs' Knowledge Gaps with Plug-in Specialized Language Models](2305.09955.md)
