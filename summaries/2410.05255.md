# SePPO: Semi-Policy Preference Optimization for Diffusion Alignment
## TL;DR
## Summary
- [https://arxiv.org/pdf/2410.05255.pdf](https://arxiv.org/pdf/2410.05255.pdf)

각 섹션별 요약을 다음과 같이 제공합니다:

### 1. 서론
현재 AI 생성 콘텐츠(AIGC) 산업에서 텍스트-비주얼 모델이 중요한 역할을 하고 있습니다. 이 논문에서는 텍스트 - 비주얼 생성 작업에서 강화 학습 기법들이 기존의 한계를 어떻게 극복하는지를 소개합니다. 특히, DDPO와 같은 방법들은 보상 모델을 활용하여 이미지와 텍스트의 일치를 개선합니다.

### 2. 관련 연구
기존의 연구들은 인공지능 모델에 대한 사람의 선호를 반영하기 위한 방법들을 제안해 왔습니다. 이 논문은 특히 SPIN-Diffusion, DITTO와 같은 이전의 연구들이 가지는 한계점에 대해 설명하며 새로운 접근 방식을 제시합니다.

### 3. 배경
모델의 기본 개념으로 Denoising Diffusion Probabilistic Model(DDPM)을 소개합니다. 이 모델은 샘플에 점진적으로 노이즈를 추가하고 이를 제거하여 최종 이미지를 생성합니다.

### 4. 직접 선호 최적화
선호 데이터 수집과 이에 기반한 보상 함수의 학습 과정을 단순화하고, 선호도를 직접 최적화하는 방법론을 설명합니다.

### 5. 세미-폴리시 선호 최적화
SePPO라는 기법을 통해 보상 모델 없이도 인간의 선호도에 맞는 모델을 생성하고자 합니다. 기존 체크포인트를 참고 모델로 활용함으로써 학습의 방향성을 개선합니다.

### 6. Anchor-based Adaptive Flipper
참고 모델이 생성한 샘플의 품질을 평가하고, 이를 통해 모델이 올바르게 학습할 수 있도록 돕는 AAF라는 평가 기준을 개발합니다.

### 7. 실험 결과
SePPO는 텍스트-이미지 그리고 텍스트-비디오 생성 벤치마크에서 다른 방법들을 능가하는 성능을 보여주었습니다. 특히, 다양한 데이터셋에서 그 성능이 검증되었습니다.

### 8. 결론
보상 모델 없이도 인간의 선호도를 반영할 수 있는 새로운 기법을 제안함으로써 기존의 한계를 극복하였습니다. 보다 경제적이고 효율적인 AI 모델 학습이 가능해졌습니다.

---

### 전체 요약
이 논문은 AI 생성 모델이 인간의 선호도를 효과적으로 반영할 수 있도록 하는 새로운 기법, 세미-폴리시 선호 최적화(SePPO)를 제안합니다. SePPO는 보상 모델이나 인간이 주석을 달아야 하는 데이터를 필요로 하지 않으며, 기존 체크포인트를 활용하여 최적화 과정을 개선합니다. 실험을 통해 이 방법이 여러 벤치마크에서 높은 성능을 나타냈으며, 특히 데이터 수집과 인프라 비용을 절감하면서도 큰 성과를 낼 수 있음을 보여주었습니다. 이러한 혁신적인 접근 방식은 AI와 머신러닝의 발전에 기여할 가능성이 높습니다.