# Semantic Entropy Probes: Robust and Cheap Hallucination Detection in LLMs
## TL;DR
## Summary
- [https://arxiv.org/pdf/2406.15927.pdf](https://arxiv.org/pdf/2406.15927.pdf)

### 1. 섹션별 중요한 내용 요약

#### 1. 서론
이 연구는 대규모 언어 모델(LLM)에서의 허상(hallucination) 검출 문제를 다룹니다. 허상은 사실 아닐지라도 그럴듯하게 들리는 결과물을 의미합니다. 최신 연구에서는 모델이 여러 세대의 결과물을 샘플링하여 의미적 불확실성을 추정함으로써 허상을 감지할 수 있다고 제안했습니다. 그러나 이 방법은 계산 비용이 많이 듭니다. 이에 대한 해결책으로, 우리는 단일 세대의 숨겨진 상태에서 의미적 불확실성을 직접 추정하는 '의미적 엔트로피 프로브(SEP)'를 제안합니다.

#### 2. 관련 연구
여기서는 기존 연구들을 검토합니다. 언어 모델의 허상을 탐지하는 방법은 크게 샘플링 기반 접근법과 검색 기반 접근법으로 나뉩니다. 샘플링 기반 방법은 여러 결과물을 샘플링하여 의미적 차이를 분석하는 반면, 검색 기반 방법은 외부 지식 기반을 활용합니다. SEP는 샘플링 기반 접근법의 높은 비용 문제를 해결하는 데 중점을 둡니다.

#### 3. 의미적 엔트로피
의미적 엔트로피는 모델이 생성한 여러 결과물의 의미적 동등성을 클러스터링하여 불확실성을 추정합니다. 이를 통해 텍스트 생성의 의미적 불확실성을 계산하며, 이는 허상 감지에 유용합니다. 이 섹션에서는 이러한 방법의 구현 및 계산 절차를 설명합니다.

#### 4. 의미적 엔트로피 프로브
SEP는 LLM의 숨겨진 상태에서 의미적 엔트로피를 직접 추정하며, 이는 다중 샘플링 없이도 값비싼 불확실성 추정을 가능하게 합니다. 이 섹션에서는 SEP의 학습과정과 사용법을 논의합니다.

#### 5. 실험 설정
SEP의 성능을 평가하기 위해 여러 LLM과 데이터셋을 바탕으로 실험을 진행합니다. 실험은 짧은 답변 생성 및 긴 답변 생성 설정으로 나누어 평가됩니다. 이 섹션은 평가에 사용된 모델, 데이터셋, 기본선(baseline) 및 프로브의 설정을 설명합니다.

#### 6. LLM 숨겨진 상태의 의미적 엔트로피 포착
LLM의 숨겨진 상태가 의미적 엔트로피를 잘 나타낸다는 것을 확인합니다. 실험 결과 SEP는 다양한 모델, 작업, 레이어에서 우수한 성능을 보여주었습니다.

#### 7. 성능 평가
SEP는 허상 감지에서 이전의 정확도 예측 프로브보다 일반화된 성능이 뛰어납니다. 비용 대비 효율성 면에서도 뛰어나며, 특히 새로운 작업에 대한 일반화 성능이 우수합니다.

#### 8. 논의 및 결론
SEP는 LLM의 의미적 불확실성을 성공적으로 추정할 수 있으며, 샘플링 기반 방법보다 계산 비용이 훨씬 낮습니다. 이는 LLM의 행위를 이해하고 제어하는 데 중요한 내부 상태를 잘 반영한다고 결론지었습니다.

### 2. 전체 요약
이 논문은 비용 효율적이고 신뢰성 있는 허상 감지 방법인 '의미적 엔트로피 프로브(SEP)'를 제안합니다. SEP는 LLM의 숨겨진 상태를 활용하여 의미적 불확실성을 직접 추정하며, 이는 기존의 비용이 많이 드는 샘플링 기반 방법의 대안이 됩니다. 실험 결과 SEP는 새로운 작업에 대한 일반화 성능이 뛰어나며, 다양한 상황에서 모델의 내부 상태가 의미적 엔트로피를 잘 나타낸다는 것을 확인했습니다. 이로써 SEP는 대규모 언어 모델의 신뢰성을 높이는 데 중요한 기여를 합니다.