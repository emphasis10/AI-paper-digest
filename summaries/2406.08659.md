# Vivid-ZOO: Multi-View Video Generation with Diffusion Model
## TL;DR
## Summary
- [https://arxiv.org/pdf/2406.08659.pdf](https://arxiv.org/pdf/2406.08659.pdf)

### 1. 개별 섹션 요약
---

#### Introduction

도입부에서는 텍스트로부터 다중 뷰 비디오(T2MVid) 생성을 위한 새로운 확산 모델인 Vivid-ZOO를 소개하고 있습니다. T2MVid 생성은 기하학적 일관성과 시계적 일관성을 동시에 유지해야 하는 어려운 문제입니다. 이 논문은 다중 뷰 이미지 확산 모델과 2D 비디오 확산 모델을 결합한 새로운 접근 방식을 통해 효과적으로 해결합니다.

#### Related Work

이 섹션은 기존의 2D 비디오 확산 모델, 다중 뷰 이미지 확산 모델, 그리고 4D 생성 모델에 대해 다루고 있습니다. 기존의 방법들은 주로 대규모의 교육 데이터를 필요로 하거나, 제한된 데이터를 사용해 결과의 질을 떨어뜨릴 수 있습니다. Vivid-ZOO는 이에 비해 더 적은 데이터로도 높은 품질의 다중 뷰 비디오를 생성할 수 있습니다.

#### Multi-view video diffusion model

이 섹션에서는 T2MVid 생성을 위한 문제 정의와 해결책을 다룹니다. T2MVid 생성을 뷰포인트 공간과 시간 구성 요소로 나누어 다루며, 이를 통해 다중 뷰 공간 모듈과 다중 뷰 시간 모듈을 제안합니다. 또한, 다중 뷰 이미지 확산 모델과 2D 비디오 확산 모델의 사전 학습된 계층을 결합하는 방법을 설명합니다.

#### Multi-view spatial module

다중 뷰 공간 모듈은 사전 학습된 다중 뷰 이미지 확산 모델을 이용해 각 뷰가 텍스트와 기하학적으로 일치하게 만듭니다. 이 모듈은 기하학적 일관성을 보장합니다.

#### Multi-view temporal module

시계적 일관성을 유지하기 위해 사전 학습된 2D 비디오 확산 모델의 시간 계층을 효과적으로 사용합니다. 3D-2D 정렬 계층과 2D-3D 정렬 계층을 도입하여 두 모델 간의 도메인 격차를 극복합니다.

#### Training objectives

훈련 목표 섹션에서는 데이터셋 구성과 훈련 전략을 설명합니다. 짧은 시간 안에 훈련 데이터를 처리하고 모델을 최적화하는 방법을 다룹니다.

#### Multi-view video dataset Construction

논문에서 제안된 모델을 훈련시키기 위한 다중 뷰 비디오 데이터셋의 구축 방법을 설명합니다. 이 데이터셋은 다양한 텍스트 프롬프트를 기반으로 비디오를 생성하는 데 사용됩니다.

#### Experiments

실험 섹션에서는 제안된 모델의 정량적 및 정성적 결과를 평가하고, 기존 모델 관련 성능 비교와 함께 제안된 접근 방식의 효율성을 입증합니다.

#### Conclusions

결론에서는 제안된 모델의 주요 기여와 향후 연구 방향을 요약합니다. Vivid-ZOO는 높은 품질의 다중 뷰 비디오 생성을 가능하게 하며, 이는 텍스트 기반 다중 뷰 비디오 생성 모델로는 최초입니다.

---

### 2. 전체 요약

이 논문은 텍스트로부터 다중 뷰 비디오를 생성하는 새로운 확산 모델 Vivid-ZOO를 소개합니다. 기존 방법들과 달리, 이 모델은 다중 뷰 이미지 확산 모델과 2D 비디오 확산 모델을 결합하여 효과적으로 높은 품질의 비디오를 생성합니다. 제안된 3D-2D 정렬과 2D-3D 정렬 계층을 통해 도메인 간 격차를 극복하며, 시계적 일관성과 기하학적 일관성을 동시에 유지합니다. 또한, 비교적 작은 데이터셋을 사용하여 훈련 비용을 절감하면서도 높은 성능을 발휘하는 것이 특징입니다. 이 논문의 기여는 첫 번째 T2MVid 확산 모델을 제안하였으며, 이는 향후 연구와 응용에 있어 중요한 토대를 제공합니다.