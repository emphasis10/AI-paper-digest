# Scalify: scale propagation for efficient low-precision LLM training
## TL;DR
## Summary
- [https://arxiv.org/pdf/2407.17353.pdf](https://arxiv.org/pdf/2407.17353.pdf)

### SCALIFY: Low-Precision LLM Training과 관련된 논문 요약

#### 섹션별 중요 내용 요약
---

1. **초록 (Abstract)**:
   - **내용 요약**: 이 논문은 대규모 언어 모델(LLM)의 학습과 추론을 위한 저정밀도 형식(FP8 등)의 사용을 더욱 효율적으로 하기 위해 SCALIFY라는 엔드 투 엔드 스케일 전파 패러다임을 제안합니다. 이 방법은 기존의 텐서 스케일링 방법을 일반화하고 공식화합니다. SCALIFY는 FP8 행렬 곱셈과 그래디언트 표현을 원활하게 지원하며, FP16 최적화 상태 저장을 가능하게 합니다.
   - **핵심 기여**: SCALIFY를 통해 더욱 연산 효율적이고 메모리 사용량이 적은 LLM 학습 및 추론 환경 구성.

2. **도입 (Introduction)**:
   - **내용 요약**: LLM의 파라미터 수가 증가함에 따라, 연구자들은 학습 및 추론 효율성을 높이기 위해 저정밀도 형식을 실험해왔습니다. 기존에는 FP16 형식이 주로 사용되었으며, 머신러닝 커뮤니티 내에서 저정밀도 형식 adopt에는 몇 가지 기술적 문제가 있었습니다. SCALIFY는 이런 문제를 해결하기 위해 등장했습니다.
   - **핵심 기여**: 기존의 복잡하고 불안정한 기술들을 간소화하여 FP8 등의 저정밀도 형식의 실용성을 높였습니다.

3. **배경 및 관련 연구 (Background and Related Work)**:
   - **내용 요약**: 저정밀도 형식(FP16, FP8) 관련 많은 연구가 진행되었습니다. 특히, FPGA 및 유사 하드웨어에서의 계산 효율성과 메모리 사용량 감소가 주요 초점입니다.
   - **핵심 기여**: SCALIFY는 저정밀도 형식의 기존 한계를 보완하고, 보다 정교하고 일관된 스케일 전파를 지원합니다.

4. **방법론 (Methodology)**:
   - **내용 요약**: SCALIFY는 기본적인 JAX 연산 프리미티브를 사용하여 텐서의 단위 스케일 속성을 유지합니다. 또한, 기본 연산자에 대한 스케일 전파 규칙을 제시하여 연산 정확성을 높이고 계산 과부하를 줄입니다.
   - **핵심 기여**: 유닛 스케일링 전략을 도입하여 기본적인 신경망 연산자에 대한 스케일 전파를 자동화하고 최적화했습니다.

5. **실험 결과 (Experiment Results)**:
   - **내용 요약**: 다양한 설정에서 SCALIFY의 성능을 테스트하였으며, FP8 및 FP16 형식에서도 높은 정확도와 안정성을 입증했습니다. SCALIFY는 섬세한 동적 재스케일링을 통해 최적화된 메모리 사용과 학습 효율성을 제공합니다.
   - **핵심 기여**: SCALIFY의 전파 기법을 통한 저정밀도 형식의 실용성과 효용성을 실험적 데이터로 입증했습니다.

6. **결론 (Conclusion)**:
   - **내용 요약**: SCALIFY는 저정밀도 학습 형식을 위한 매우 실용적이고 효율적인 도구입니다. 앞으로의 과제로는 더 큰 규모의 LLM 훈련과 최신 아키텍처로의 확장을 목표로 하고 있습니다.
   - **핵심 기여**: 저정밀도 형식을 가진 LLM 학습의 새로운 패러다임을 제시하며, SCALIFY의 유용성과 확장 가능성을 보여줍니다.

---

### 종합 요약
---
이 논문은 대규모 언어 모델의 학습과 추론을 효율적으로 하기 위해 SCALIFY라는 새로운 엔드 투 엔드 스케일 전파 패러다임을 제안합니다. SCALIFY는 기존의 복잡한 저정밀도 학습 기법들을 간소화하고, 다양한 실험을 통해 높은 효율성과 정확성을 입증했습니다. 이 방법론은 FP8 및 FP16 형식에서 주목할 만한 성능을 보여주며, 테크니컬한 면에서의 주요 기여는 저정밀도 형식의 한계를 극복하고, 신경망 학습의 안정성과 효율성을 높인 것입니다. SCALIFY는 머신러닝 실무자들이 저정밀도 학습 설정을 쉽게 사용자 정의할 수 있게 하여 LLM 훈련을 더욱 효율적으로 만듭니다.

앞으로의 과제는 SCALIFY를 더 큰 모델 및 최신 아키텍처로 확장하여, 저정밀도 형식의 실질적인 활용을 더욱 활성화하는 것입니다.

## Similar Papers
- [BiQGEMM: Matrix Multiplication with Lookup Table For Binary-Coding-based Quantized DNNs](2005.09904.md)
- [FP8 Formats for Deep Learning](2209.05433.md)
- [Q-GaLore: Quantized GaLore with INT4 Projection and Layer-Adaptive Low-Rank Gradients](2407.08296.md)
- [2BP: 2-Stage Backpropagation](2405.18047.md)
- [Conformer-Based Speech Recognition On Extreme Edge-Computing Devices](2312.10359.md)
- [Symbolic Learning Enables Self-Evolving Agents](2406.18532.md)
- [LLM-FP4: 4-Bit Floating-Point Quantized Transformers](2310.16836.md)
- [Masked Attention is All You Need for Graphs](2402.10793.md)
- [NIPQ: Noise proxy-based Integrated Pseudo-Quantization](2206.00820.md)
