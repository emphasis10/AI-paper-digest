# CLEAR: Character Unlearning in Textual and Visual Modalities
## TL;DR
## Summary
- [https://arxiv.org/pdf/2410.18057.pdf](https://arxiv.org/pdf/2410.18057.pdf)

1. 각 섹션의 요약

- **서론 및 개요**:
  이 논문은 대형 다중모달 언어 모델(MLLMs)에서 특정한 개인 정보 또는 유해 정보를 제거하는 머신 신경망(MU)이 중요한 역할을 한다고 강조합니다. 이를 위해 CLEAR라는 새로운 벤치마크가 제안되었으며, 텍스트와 이미지의 다중모달 세팅에서 MU 방법론의 평가를 목적으로 합니다.

- **머신 신경망 방법론(A Unlearning Methods)**:
  여러 언러닝 방법이 제안되었는데, 이는 주로 잊어야 할 데이터(Forget set)와 유지해야 할 데이터(Retain set)를 어떻게 다룰지에 중점을 두고 있습니다. 대표적으로, 잊어야 할 데이터의 손실을 최대화하거나 불확실성을 증가시키는 방식의 방법들을 설명합니다.

- **결론 및 한계 (Conclusion & Limitations)**:
  결론에서는 연구가 다중 모달의 언러닝이 텍스트나 이미지 각각의 언러닝보다 어려운 과제임을 보여주었음을 강조합니다. 더불어 연구는 사전정규화를 통한 언러닝의 품질 개선 가능성을 제안합니다. 한편, 연구의 한계로는 합성 데이터를 사용하여 실제 시나리오를 완벽하게 반영하지 못할 수 있다는 점을 언급합니다.

- **관련 연구 및 기존 방법들(Related Works)**:
  기존의 MU 방법들과 관련 텍스트 벤치마크에 대해 논의한 후, MMU 연구가 초기 단계이며 여러 새로운 방법론들이 필요하다고 결론지었습니다. 특히 다중 모달 언러닝(MMLMs)에 새로운 벤치마크가 필요함을 강조합니다.

2. 전체 요약

이 논문은 CLEAR라는 벤치마크를 사용하여 다중모달 머신 언러닝(MMU)을 평가하고, 이 과정에서 여러 기존 MU 방법론들의 성과를 종합적으로 분석합니다. 연구는 텍스트와 이미지 모두를 포함한 다중 모달 환경에서 언러닝이 단일 모달에서보다 더 복잡한 도전 과제임을 발견했습니다. 핵심은 모델의 연산을 간단하게 하기 위한 LoRA 가중치 정규화를 도입하여 언러닝 과정을 개선할 수 있음을 설명하고 있지만, 여전히 가장 효과적인 방법을 찾는 데 있어 많은 도전과 어려움을 가지고 있음을 지적합니다.