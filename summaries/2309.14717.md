# QA-LoRA: Quantization-Aware Low-Rank Adaptation of Large Language Models
## TL;DR
## Summary
- [https://arxiv.org/abs/2309.14717](https://arxiv.org/abs/2309.14717)

이 논문은 대규모 언어 모델(LLMs)의 양자화 인식 저랭크 적응(QA-LoRA) 알고리즘을 제안합니다. 주요 내용은 다음과 같습니다.

1. **서론**: 대규모 언어 모델은 많은 언어 이해 작업에서 높은 성능을 보이지만, 많은 파라미터로 인한 높은 계산 부담이 문제입니다. 이를 해결하기 위해 QA-LoRA는 양자화와 적응의 자유도 불균형 문제를 해결하는 것을 목표로 합니다.

2. **관련 연구**: 저랭크 적응(LoRA)은 파라미터 효율적인 미세 조정(PEFT) 방법으로, 큰 모델의 기존 가중치를 보존하면서 작은 수의 학습 가능한 파라미터를 도입합니다. 이 연구는 PEFT와 양자화를 통합하는 방법을 탐구합니다.

3. **QA-LoRA 방법론**: QA-LoRA는 그룹별 연산자를 도입하여 양자화의 자유도를 증가시키고 적응의 자유도를 감소시키는 방식으로 작동합니다. 이는 효율적인 미세 조정 단계와 경량화된 모델을 가능하게 합니다.

4. **실험 결과**: LLaMA 및 LLaMA2 모델 패밀리에 QA-LoRA를 적용하고, 다양한 언어 이해 벤치마크에서 그 효과를 검증합니다. QA-LoRA는 특히 낮은 비트 폭에서도 성능을 유지하며, QLoRA와 비교했을 때 더 우수한 성능을 보입니다.

5. **결론**: QA-LoRA는 LLMs의 양자화와 저랭크 적응을 통합함으로써, 계산 효율성을 개선하고 모델의 확장성을 증가시키는 방법을 제공합니다. 이는 특히 에지 디바이스와 같은 자원 제한 환경에서 LLM의 배치에 유용할 것입니다.

이 논문은 대규모 언어 모델의 양자화와 저랭크 적응을 효과적으로 통합하여 계산 부담을 줄이고, 성능을 유지하는 새로운 방법을 제안합니다. 이로 인해 자원이 제한된 환경에서도 LLMs의 활용 가능성이 향상될 것으로 보입니다.