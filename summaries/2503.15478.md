# SWEET-RL: Training Multi-Turn LLM Agents on Collaborative Reasoning Tasks
## TL;DR
## Summary
- [https://arxiv.org/pdf/2503.15478.pdf](https://arxiv.org/pdf/2503.15478.pdf)

### 1. 섹션별 요약 및 기여

#### 소개(Introduction)
논문은 현실 세계에서 다중 턴 상호작용을 필요로 하는 대형 언어 모델(LLM) 에이전트를 다룬다. 기존의 강화 학습(RL) 알고리즘들은 효과적으로 크레딧 할당을 수행하지 못한다. 이를 해결하기 위해, 새로운 벤치마크인 ColBench를 도입하여 에이전트가 사람과 상호 작용하여 프로그램이나 웹 디자인 같은 실제 작업을 수행하게 한다. SWEET-RL이라는 새로운 RL 알고리즘이 제안되었는데, 이는 훈련 시점 정보를 활용하여 크리틱 모델을 평가하며, 에이전트의 성공률과 승률을 크게 향상시킨다.

#### 관련 연구(Related Work)
현존하는 벤치마크는 다양한 LLM 에이전트를 다루지만, 주로 단일 턴 상호작용에 집중되어 있고 다중 턴 상호작용에서의 RL 알고리즘 개발에는 적합하지 않다. ColBench는 이러한 격차를 해결하고자 설계되었으며, 다중 턴 상호작용에 기반한 새로운 알고리즘 연구에 초점을 맞춘다.

#### SWEET-RL 알고리즘
이 알고리즘은 훈련 시점을 활용한 평가 방법으로, 특히 크리틱과 액터 사이의 비대칭 관찰 공간을 활용한다. 이를 통해 전통적인 가치 함수 학습 없이도 개선된 성능을 제공하며, GPT-4와 같은 선도적 모델들과 경쟁할 수 있을 만큼 성능을 향상시킨다.

#### 문제 설정 및 전환 크레딧 학습
컴퓨터와 인간 사이의 협업을 부분적으로 관찰 가능한 마르코프 결정 과정으로 설정한다. 여기서 에이전트는 제한된 정보만을 사용해 최적의 행동을 선택하며, SWEET-RL은 이 문제를 해결하기 위해 학습 과정에서 크리틱이 제공하는 추가 정보를 효과적으로 사용한다.

#### 실험
SWEET-RL이 ColBench에서 다중 턴 상호작용을 통한 다양한 협업 작업에서 성공률과 승률을 더 높게 유지하는지를 검증하기 위해 다양한 설정에서 실험이 진행되었다. 결과적으로, SWEET-RL은 기존의 최첨단 알고리즘보다 높은 성능을 나타냈다.

#### 결론
고성능 다중 턴 RL 알고리즘 개발을 위한 기초가 되는 ColBench 및 SWEET-RL의 소개가 이루어졌다. 미래 연구에서는 보다 나은 다중 턴 RL 알고리즘 개발이 필요하다.

### 2. 전체 요약
이 논문은 다중 턴 상호작용이 필요한 대형 언어 모델 에이전트를 위한 새로운 강화 학습 알고리즘, SWEET-RL을 제안한다. ColBench 벤치마크를 통해 에이전트가 보다 실제적인 작업을 인간과 협업하여 수행하는 환경을 제공한다. SWEET-RL은 전통적인 방법보다 더 나은 성과를 보여주며, LLM의 일반화와 추론 능력을 최대한 활용하는 방향으로 설계되었다. 이로 인해 훈련 및 평가가 쉬워지며, 실제 협업 성과에서도 향상된 결과를 가져온다. 본 논문은 향후 이러한 알고리즘의 발전을 위한 기초로 작용할 수 있다.