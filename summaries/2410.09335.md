# Rethinking Data Selection at Scale: Random Selection is Almost All You Need
## TL;DR
## Summary
- [https://arxiv.org/pdf/2410.09335.pdf](https://arxiv.org/pdf/2410.09335.pdf)

**1. 중요 섹션 요약과 논문의 주요 기여 및 혁신적 부분**

**서론**
논문은 최근의 대규모 언어 모델(LLMs)의 발전을 고찰하며, 이 모델들이 주로 사람의 명령어와 정렬하여 자연어 처리(NLP) 과제를 확대하려는 목적을 강조합니다. 주로 감독된 미세 튜닝(SFT)을 통해 LLM을 특정 도메인이나 과제에 맞게 조정함으로써 성능을 향상시킬 수 있음을 설명합니다.

**데이터 선택에 관한 재고**
LLM의 명령어 조정에 있어 데이터 선택의 중요성을 논하며, 대부분의 기존 데이터 선택 기술은 소규모 데이터 풀에서 작동하도록 설계되었다고 설명합니다. 대규모 데이터에 대해서 랜덤 선택이 거의 최상의 결과를 제공함을 밝혀냈습니다.

**실험과 결과**
실험을 통해 두 개 큰 규모의 데이터셋(OpenHermes2.5와 WildChat-1M)을 사용하여 데이터 선택 기법의 효율성 및 성능을 평가했습니다. 결과적으로, 대규모 데이터셋에서 데이터 다양성이 데이터 품질보다 더 중요한 요소라는 결론이 도출되었습니다. 특히, 데이터 선택에 있어 토큰 길이를 활용하는 방법이 유용하다는 것을 제안합니다.

**결론**
논문은 많은 SFT 데이터 선택 방법들이 소규모 데이터 세트에 의존하며 실제 시나리오의 요구를 충족시키지 못한다고 결론지으며, 다양한 대규모 IT 데이터에서 더 나은 성능을 위해 데이터 다양성이 데이터 품질보다 더욱 중요함을 강조합니다.

**2. 전체 요약**

이 연구는 최근 대규모 언어 모델이 발전함에 따라 감독된 미세 튜닝(SFT)에서 데이터 선택의 중요성을 재고합니다. 대규모 데이터셋에서는 랜덤 선택이 기존의 데이터 선택 방법보다 우수할 수 있으며, 데이터의 다양성이 품질보다 중요하게 작용합니다. 특히, 토큰 길이를 기준으로 하는 데이터 필터링이 효율적이며, 대규모 IT 데이터 다루기에 적합하다는 결론을 내립니다. 이러한 분석은 미래 AI 모델의 효율성을 높이기 위한 방향성을 제시합니다.