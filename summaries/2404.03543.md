# CodeEditorBench: Evaluating Code Editing Capability of Large Language Models
## TL;DR
## Summary
- [https://arxiv.org/pdf/2404.03543.pdf](https://arxiv.org/pdf/2404.03543.pdf)

본 논문은 대규모 언어 모델(LLMs)이 코드를 편집하는 능력을 평가하는 새로운 벤치마크인 CodeEditorBench를 소개합니다. 이는 코드 생성에만 초점을 맞춘 기존의 벤치마크와 달리 현실 세계의 시나리오와 소프트웨어 개발의 실용적 측면을 강조합니다. 다양한 프로그래밍 언어, 복잡성 수준, 편집 과제를 포괄하는 여러 소스에서 다양한 코딩 도전과 시나리오를 큐레이션합니다. 평가 결과, 특히 Gemini-Ultra와 GPT-4와 같은 닫힌 소스 모델이 CodeEditorBench에서 오픈 소스 모델을 능가하는 것으로 나타났으며, 문제 유형과 프롬프트에 대한 감수성에 따라 모델 성능에 차이가 있음을 밝혔습니다.

이 논문을 한국어로 요약해 드리겠습니다:

### 1. 논문 요약:
#### 도입부
최근 대규모 언어 모델(LLMs)의 발전은 프로그래밍 지원을 넘어 다양한 도구 사용 응용 프로그램에 이들의 코딩 능력을 확장시켜야 한다는 점을 강조합니다. 기존의 평가 방법들은 소프트웨어 개발에서 중요한 코드 편집 측면을 무시한 채 코드 생성에 주로 집중하고 있습니다. 이러한 평가 방법의 중요한 격차를 메우기 위해, 저자들은 LLMs의 코드 편집 능력을 종합적으로 평가하기 위한 새로운 벤치마크인 CodeEditorBench를 제안합니다.

#### 관련 작업
코드 LLMs 분야는 코드 이해와 생성의 도전 과제를 해결하기 위해 급격히 성장하고 있습니다. 이 추세는 OpenAI에 의해 Codex의 도입으로 시작되었으며 CodeGen, CodeT5, InCoder와 같은 여러 영향력 있는 모델의 출현을 목격했습니다. 최근 인기 있는 오픈 소스 모델로는 CodeLLaMa, DeepSeek Coder 등이 있습니다.

### 2. 전반적인 요약:
CodeEditorBench는 코드 편집 작업에 대한 LLMs의 성능을 엄격하게 평가하기 위한 평가 프레임워크입니다. 코드 생성에만 초점을 맞췄던 기존 벤치마크와 대비하여, 실제 소프트웨어 개발의 맥락을 반영하여 설계되었습니다. 이 벤치마크는 코드 디버깅, 번역, 폴리싱 및 요구 사항 변경 등 다양한 편집 작업 유형을 다루며, 닫힌 소스 모델이 일반적으로 오픈 소스 모델보다 뛰어난 성능을 보이는 것으로 나타났습니다. CodeEditorBench는 LLMs의 코드 편집 기능을 평가하는 데 있어 중요한 기여를 하며, 연구자와 실무자 모두에게 유용한 자원을 제공합니다.