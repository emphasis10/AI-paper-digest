# DeepSpeak Dataset v1.0
## TL;DR
## Summary
- [https://arxiv.org/pdf/2408.05366.pdf](https://arxiv.org/pdf/2408.05366.pdf)

### 섹션별 요약 및 설명

#### 1. 소개 (Introduction)
이 논문은 DeepSpeak v1.0이라는 데이터셋을 소개합니다. 이 데이터셋은 사람들의 실제 및 딥페이크 동영상을 포함하고 있으며, 이를 디지털 포렌식 연구를 위해 제공됩니다. 주요 기여로는 다양한 페이스-스왑(face-swap)과 립-싱크(lip-sync) 딥페이크 영상이 포함된 34시간 분량의 데이터셋이 있습니다.

#### 2. 데이터 수집 (Data Collection)
220명의 참가자로부터 동영상을 수집했습니다. 참가자는 UCB 웹 비디오 녹화 앱을 사용해 32개의 프롬프트에 응답했으며, 이 중 10개의 문장은 표준 스크립트로, 나머지는 랜덤 스크립트와 자유 응답 프롬프트로 구성되었습니다. 모든 녹화는.webm 포맷으로 캡쳐되었고, 이후.mp4로 변환되었습니다.

#### 3. 오디오 생성 (Audio Generation)
참가자들의 음성은 ElevenLabs API를 사용해 복제되었으며, 이 복제된 음성으로 참가자의 원본 음성을 대체한 합성 음성을 생성했습니다. 이러한 음성 합성 과정은 실제 오디오와 비디오가 일치하도록 보장합니다.

#### 4. 비디오 생성 (Video Generation)
총 5,958개의 딥페이크 동영상이 세 가지 페이스-스왑 및 두 가지 립-싱크 모델을 사용해 생성되었습니다. 여기에는 총 25.78시간 분량의 영상이 포함되어 있으며, 데이터는 학습용(80%)과 테스트용(20%)으로 나뉩니다.

##### 4.1 페이스-스왑 (Face-Swap)
페이스-스왑 딥페이크는 원본 영상의 얼굴을 다른 얼굴로 대체하는 방식으로 생성됩니다. 이를 위해 CLIP 임베딩과 집합적 군집화 알고리즘을 사용해 유사한 얼굴을 매칭하고, FaceFusion과 FaceFusion+GAN 모델을 통해 품질을 향상시켰습니다. 라이브 스트리밍 환경에서는 FaceFusion Live 모델이 사용되었습니다.

##### 4.2 립-싱크 (Lip-Sync)
립-싱크 딥페이크는 원본 영상의 입 모양을 다른 오디오 트랙과 일치시키는 방식으로 생성됩니다. 여기에는 Wav2Lip과 VideoRetalking 모델이 사용되어 높은 품질의 립-싱크 동영상을 생성합니다.

#### 5. 토론 (Discussion)
DeepSpeak 데이터셋은 미디어 포렌식 연구 공동체를 지원하고, 딥페이크 탐지 기술을 개발 및 개선하기 위한 목적으로 만들어졌습니다. 이 데이터셋은 학술 및 상업적 요청에 따라 다양한 응용 프로그램에서도 사용할 수 있습니다. 향후 1년에 한두 번 주기로 데이터셋을 업데이트할 계획입니다.

### 전반적인 요약
이 논문에서는 DeepSpeak v1.0 데이터셋의 개발 및 구축 과정을 설명합니다. 이 데이터셋은 34시간 분량의 실제 및 딥페이크 영상으로 구성되어 있으며, 디지털 포렌식 연구와 딥페이크 탐지 기술의 발전을 목표로 합니다. 데이터 수집 과정에서는 220명의 참가자로부터 다양한 프롬프트에 대한 응답을 수집했으며, 이를 페이스-스왑 및 립-싱크 딥페이크 영상으로 변환했습니다. 주요 기여로는 다양한 최첨단 모델을 사용한 고품질의 딥페이크 영상을 포함하고 있으며, 이를 통해 미디어 포렌식 연구 공동체에 큰 도움을 줄 것입니다.