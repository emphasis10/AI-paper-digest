# Spurious Rewards: Rethinking Training Signals in RLVR
## TL;DR
## Summary
- [https://arxiv.org/pdf/2506.10947.pdf](https://arxiv.org/pdf/2506.10947.pdf)

1. **섹션 요약 및 주요 기여**

   - **소개 (Introduction)**
     강화 학습을 활용한 검증 가능한 보상(RLVR)이 어떠한 모델에서도 자극 신호의 일관성 여부와 상관없이 수학적 추론을 강화할 수 있음을 보여줍니다. 특히 Qwen2.5-Math-7B 모델에서는 잘못된 레이블조차도 성능을 개선할 수 있다는 점에서 주목할 만합니다.
   
   - **배경 및 실험 설계 (Background and Experimental Setup)**
     다양한 모델에서 보상을 통해 강화 학습의 한계와 성능 향상을 알아보기 위해 실험이 진행되었습니다. 이 중에서 특히 Qwen2.5-Math 모델이 두드러진 성능 향상을 보였습니다.

   - **일반화의 부재 (Lack of Generalization to Other Models)**
     Qwen2.5-Math 모델을 제외한 Llama3 및 OLMo2 모델은 약한 보상이나 스푸리어스 (형식 및 무작위 보상 포함)로는 성능이 개선되지 않았습니다. 이는 각 모델의 사전 훈련 시기 이미 습득된 유지된 패턴이 행위에 따라 결정된다는 점을 시사합니다.

   - **결론 및 토론 (Conclusion and Discussion)**
     RLVR은 사전에 학습된 기존의 사고 패턴을 증폭하여 Qwen2.5-Math 모델의 논리적 사고를 향상시킬 수 있습니다. 실험 결과에 따르면, 코드 추론을 통해 해결하려는 경향이 있는 모델에서는 성능이 높아졌습니다. 반면, 이 방법은 다른 모델에는 적용될 수 없기에 다양한 모델을 사용해 검증해야 한다는 점도 중요합니다.

2. **전체 요약**

   이 논문은 강화 학습의 새로운 접근법으로서 RLVR(강화 학습 검증 가능한 보상)을 제안하고, 이 방법이 Qwen2.5-Math 모델에서 수학적 추론 능력을 향상시킬 수 있음을 보여줍니다. 특히 기존의 잘못된 정보로 인한 보상조차도 모델의 성능을 높일 수 있다는 점에서 놀라운 결과를 보여줍니다. 그러나 이 성과는 Qwen2.5-Math 모델에 국한되며, 다른 모델에서는 일반화되지 않아 단일 모델에 국한하지 않는 폭넓은 검증이 필요하다는 점을 강조하고 있습니다.