# Layer-Condensed KV Cache for Efficient Inference of Large Language Models
## TL;DR
## Summary
- [https://arxiv.org/pdf/2405.10637.pdf](https://arxiv.org/pdf/2405.10637.pdf)

### 요약

#### 1. Introduction
**소개**
- 대형 언어 모델(LLM)을 실제 응용 프로그램에 배포하기 위해서는 높은 처리량과 낮은 지연 시간이 필수적입니다. 하지만 LLM의 높은 메모리 소비는 큰 배치 크기와 높은 처리량 생성을 방해하는 주요 병목 현상입니다. 
- 특히, Key-Value(KV) 캐시는 변환기 구조에서 많은 메모리를 소비하며, 이는 시퀀스 길이와 레이어 수에 비례합니다.

#### 2. Layer-Condensed KV Cache
**레이어 압축 KV 캐시**
- 새로운 변형된 변환기 디코더를 제안하여, 최상위 레이어의 키와 값(KV)을 모든 레이어의 쿼리와 쌍으로 매칭하여 KV 캐시를 획기적으로 줄입니다.
- 이를 통해 메모리 소비를 줄이고, 계산 및 모델 파라미터를 절약합니다.
- 특정 레이어에서 토큰이 자기 자신을 참조하지 않도록 주의하고, 대각선 마스크를 사용하여 성능에 영향을 주지 않습니다.

#### 3. Training
**훈련**
- 표준 변환기 디코더와 달리, 각 토큰의 계산이 이전 토큰의 최상위 레이어에 의존하므로 순차적 의존성이 생겨 병렬 훈련이 어려워집니다.
- 이를 해결하기 위해 병렬 훈련을 지원하는 새로운 근사 훈련 방법을 도입합니다.

#### 4. Experiments
**실험**
- Llama 모델에서 우리의 방법의 효과를 실증적으로 검증합니다.
- 1.1B, 7B, 30B 파라미터 모델을 사용하여 NVIDIA RTX 3090 및 A100 GPU에서 테스트한 결과, 표준 변환기 대비 최대 32배 큰 배치 크기와 최대 26배 높은 처리량을 달성했습니다.
- 메모리 절약 기술인 StreamingLLM과의 통합도 간단히 할 수 있어 추가적인 효율성 향상을 이룰 수 있습니다.

#### 5. Conclusion
**결론**
- KV 캐시 메모리 소비를 줄이고 처리량을 개선하는 새로운 방법을 제안합니다.
- 우리의 방법은 성능 저하 없이 메모리 절약 및 처리량 개선을 실현하며, 다른 메모리 절약 기술과의 통합이 용이합니다.
- 더 효율적인 훈련 접근법과 대규모 LLM에 대한 검증이 향후 과제로 남아 있습니다.

### 논문의 주요 기여 및 혁신 부분
1. **메모리 절약**: 최상위 레이어만 KV를 저장함으로써 메모리 소비를 획기적으로 줄였습니다.
2. **병렬 훈련 가능성**: 새로운 근사 훈련 방법을 설계하여 병렬 훈련을 가능하게 했습니다.
3. **효율성 검증**: 다양한 실험을 통해 우리의 방법이 표준 변환기 대비 성능 저하 없이 메모리와 처리량에서 큰 개선을 보임을 검증했습니다.

### 전체 요약
이 논문은 대형 언어 모델(LLM)의 효율적인 추론을 위해 메모리 소비를 줄이고 처리량을 향상시키는 새로운 방법을 제안합니다. 제안된 방법은 변환기 디코더에서 최상위 레이어의 KV만을 저장하고 계산하여 메모리 사용을 크게 줄이며, 새로운 근사 훈련 방법을 통해 병렬 훈련을 가능하게 합니다. 다양한 실험 결과, 이 방법은 표준 변환기 대비 메모리와 처리량에서 큰 개선을 이루었으며, 다른 메모리 절약 기술과의 통합도 용이합니다. 이를 통해 대형 언어 모델의 실제 응용 프로그램 배포가 더욱 효율적으로 이루어질 수 있습니다.

## Similar Papers
- [Sparser is Faster and Less is More: Efficient Sparse Attention for Long-Range Transformers](2406.16747.md)
- [Never Miss A Beat: An Efficient Recipe for Context Window Extension of Large Language Models with Consistent "Middle" Enhancement](2406.07138.md)
- [You Only Cache Once: Decoder-Decoder Architectures for Language Models](2405.05254.md)
- [MLKV: Multi-Layer Key-Value Heads for Memory Efficient Transformer Decoding](2406.09297.md)
- [FlexGen: High-Throughput Generative Inference of Large Language Models with a Single GPU](2303.06865.md)
- [Reducing Transformer Key-Value Cache Size with Cross-Layer Attention](2405.12981.md)
- [Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache Consumption](2407.18003.md)
- [CLLMs: Consistency Large Language Models](2403.00835.md)
- [Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention](2404.07143.md)
