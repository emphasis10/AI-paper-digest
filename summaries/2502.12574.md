# HeadInfer: Memory-Efficient LLM Inference by Head-wise Offloading
## TL;DR
## Summary
- [https://arxiv.org/pdf/2502.12574.pdf](https://arxiv.org/pdf/2502.12574.pdf)

1. 각 섹션 요약:

- **서론 (Introduction):** 이 논문은 HEADINFER라는 새로운 개체 캐시 관리 프레임워크를 소개합니다. 대규모 언어 모델의 중요한 성능 요소 중 하나인 메모리 문제를 해결하고자 하며, 일반적인 GPU에서는 처리하기 어려운 100만 개 이상의 토큰을 소비자 GPU로도 처리할 수 있도록 합니다.

- **HEADINFER 설명 (HEADINFER Explanation):** HEADINFER의 주된 목적은 CPU 메모리로 개체 캐시를 분산시키는 데 있습니다. 이때, 정교한 '헤드 단위 오프로드' 기법을 사용하여 특정 주의 헤드만 GPU에 유지하며 나머지는 CPU로 보냅니다. 이는 GPU 메모리 사용량을 크게 줄여주며, 특히 소비자 수준의 하드웨어에서 긴 문맥 처리에 적합합니다.

- **결론 (Conclusion):** HEADINFER의 적용은 AI 접근의 민주화를 의미합니다. 고급 대규모 언어 모델 기능을 저렴한 소비자 하드웨어에서도 사용할 수 있게 하여, 기술 발전이 모두에게 혜택이 돌아갈 수 있도록 합니다.

2. 전체 요약:

HEADINFER는 대규모 언어 모델의 장기 문맥 추론을 소비자 GPU에서도 메모리 효율적으로 수행할 수 있게 하는 구체적인 해결책을 제공합니다. 이 시스템의 주된 혁신점은 CPU 메모리를 활용하여 개체 캐시를 효율적으로 관리하고, 메모리 사용량을 줄이면서도 성능을 유지할 수 있다는 점입니다. 이러한 기술은 대규모 모델에 대한 접근성을 넓히며, AI가 산업 전반에서 더 폭넓게 활용될 수 있도록 합니다.