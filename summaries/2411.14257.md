# Do I Know This Entity? Knowledge Awareness and Hallucinations in Language Models
## TL;DR
## Summary
- [https://arxiv.org/pdf/2411.14257.pdf](https://arxiv.org/pdf/2411.14257.pdf)

1. 섹션 요약 및 주요 기여 요약

   - **초록 (Abstract):**
     이 논문은 대형 언어 모델(LLM)에서의 환각(hallucination) 및 지식 인식 메커니즘을 조사합니다. Sparse autoencoders (SAE)를 사용하여 모델이 엔티티를 인식하고 그에 대한 사실을 회상할 수 있는지를 감지하는 방향을 찾았습니다. 이러한 방향은 알려진 엔티티에 대해 답변을 거부하거나 모르는 엔티티에 대해 환각을 유발할 수 있습니다.

   - **서론 (Introduction):**
     대형 언어 모델은 사실적으로 정확하지 않거나 근거 없는 정보를 생성할 가능성이 있습니다. 이 논문은 언어 모델이 주어진 질문에 대해 환각할지 여부를 예측하는 메커니즘을 설명하고자 합니다. Sparse autoencoders를 사용하여 엔티티 인식 메커니즘을 이해하고 이를 통해 모델의 지식 거부 행동을 조정할 수 있음을 보여줍니다.

   - **기술적 접근 (Methodology):**
     엔티티 인식 방향은 다양한 엔티티 타입에 대해 일반화될 수 있으며, Sparse autoencoders로 발견된 방향은 모델의 대화 거부 행동을 조절할 수 있습니다. 이는 엔티티의 사실적 회상을 중단시키고, 이에 따라 알려지지 않은 엔티티에 대한 환각을 증가시킬 수 있습니다.

   - **주요 결과 (Key Results):**
     이 논문은 Sparse autoencoders를 이용하여 모델 내부에서 자가-지식을 반영하는 방향을 밝혀내었습니다. 이 방향들은 모델이 엔티티에 대한 지식 거부를 하거나 정보 환각을 일으킬지 여부에 영향을 미칩니다.

   - **결론 (Conclusion):**
     연구 결과는 언어 모델의 엔티티 인식 방향이 모델의 주의 메커니즘과 속성을 추출하는 능력을 조절하는 데 중요한 역할을 한다고 결론짓습니다. 이는 모델 행위를 개선하고 환각을 줄이는 방안이 될 수 있습니다.

2. 전체 요약:
   이 논문은 대형 언어 모델에서의 지식 인식 및 환각 문제를 다룹니다. Sparse autoencoders를 사용하여 모델이 자체의 지식을 평가할 수 있는 방향을 발견하고, 이러한 방향이 모델의 대화에서 환각을 방지하고 지식 거부를 유도하는 방법을 논의합니다. 이는 언어 모델의 안정성 및 정확성을 향상시키기 위한 중요한 발견입니다.