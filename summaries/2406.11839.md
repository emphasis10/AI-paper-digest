# mDPO: Conditional Preference Optimization for Multimodal Large Language Models
## TL;DR
## Summary
- [https://arxiv.org/pdf/2406.11839.pdf](https://arxiv.org/pdf/2406.11839.pdf)

#### 1. 서론
이 논문에서는 인간의 선호도에 맞춰 대형 언어 모델(LLM)을 정렬하는 데 있어 직접 선호 최적화(DPO)가 널리 사용되어 왔음을 설명하고 있습니다. 최근에는 이러한 DPO를 다중모드(언어 외에도 이미지 등 다른 형태의 데이터 포함) 시나리오로 확장하는 시도가 있었습니다. **하지만** 결과가 일관되게 긍정적이지 않고, 환각(Hallucination) 문제를 악화시킬 수 있음을 언급하고 있습니다.

#### 2. 선호 최적화의 문제점
DPO가 다중모드 LLM에서 효과를 발휘하지 못하는 이유를 탐구합니다. 제어된 실험을 통해 DPO가 시각적 모달리티를 효과적으로 활용하지 못하며, 이로 인해 모델이 시각적 정보를 무시하고 언어 데이터만 우선시하는 문제를 지적합니다.

#### 3. MDPO 제안
MDPO (Multimodal Direct Preference Optimization)는 기존 DPO의 문제점을 해결하기 위해 제안되었습니다. MDPO는 **조건부 선호 최적화**와 **고정된 보상 최적화**를 도입하여 모델이 시각적 정보도 충분히 반영하도록 합니다. 이를 통해 환각 빈도를 줄이고 모델 성능을 개선합니다.

#### 4. 실험 설정 및 결과
본 논문은 MDPO의 효과를 검증하기 위해 다양한 규모의 모델과 데이터를 사용해 실험을 수행했습니다. Bunny-v1.0-3B와 LLaVA-v1.5-7B 모델을 사용했으며, 실험 결과 MDPO는 표준 DPO보다 모든 평가 지표에서 우수한 성능을 보였습니다. 특히 환각 감소에 탁월한 성과를 보여줍니다.

#### 5. 논의 및 분석
MDPO의 결과를 더 깊이 분석하여 조건부 선호 최적화가 MDPO에서 중요한 역할을 한다는 것을 설명합니다. 데이터 규모에 따른 평가에서도 MDPO는 DPO보다 일관되게 뛰어난 성과를 보였으며, 이는 데이터의 양이나 다양성보다 적절한 최적화 목표가 중요하다는 것을 시사합니다.

#### 6. 결론
MDPO는 다중모드 시나리오에 최적화된 선호 최적화 방법으로, 시각적 정보와 언어 정보를 동시에 반영하여 환각 빈도를 줄이고 모델 성능을 향상시킵니다. 실험 결과는 MDPO의 효과를 확인하였으며, 다양한 모델과 데이터 규모에서도 일관된 성능을 보여줍니다.

### 종합 요약

이 논문은 다중모드 대형 언어 모델의 선호 최적화 문제를 해결하기 위해 MDPO(Multimodal Direct Preference Optimization)를 제안합니다. 기존의 DPO 방식이 시각적 정보를 무시하는 문제를 지적하고, 이를 해결하기 위해 조건부 선호 최적화와 고정된 보상 최적화를 도입합니다. 이 새로운 방법은 모델이 시각적 정보도 충분히 반영하도록 하여 환각 문제를 크게 줄이고 모델 성능을 향상시킵니다. 다양한 실험 결과, MDPO가 기존 DPO보다 뛰어난 성능을 보이며, 데이터의 양이나 다양성보다 적절한 최적화 목표가 중요함을 시사합니다.

## Similar Papers
- [Hallucination of Multimodal Large Language Models: A Survey](2404.18930.md)
- [MuirBench: A Comprehensive Benchmark for Robust Multi-image Understanding](2406.09411.md)
- [Mitigating Hallucinations in Large Language Models via Self-Refinement-Enhanced Knowledge Retrieval](2405.06545.md)
- [Direct Preference Knowledge Distillation for Large Language Models](2406.19774.md)
- [From $r$ to $Q^*$: Your Language Model is Secretly a Q-Function](2404.12358.md)
- [On the Transformations across Reward Model, Parameter Update, and In-Context Prompt](2406.16377.md)
- [Improved Baselines with Visual Instruction Tuning](2310.03744.md)
- [Iterative Length-Regularized Direct Preference Optimization: A Case Study on Improving 7B Language Models to GPT-4 Level](2406.11817.md)
- [Learn Beyond The Answer: Training Language Models with Reflection for Mathematical Reasoning](2406.12050.md)
