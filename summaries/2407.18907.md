# SHIC: Shape-Image Correspondences with no Keypoint Supervision
## TL;DR
## Summary
- [https://arxiv.org/pdf/2407.18907.pdf](https://arxiv.org/pdf/2407.18907.pdf)

### 논문 요약

#### 1. 각 섹션 요약

**서론**
이 논문은 3D 템플릿과 이미지를 연결하는 방법인 SHIC(Shape-Image Correspondences)라는 새로운 비지도 학습 방법을 제안합니다. 기존의 방법들은 수작업으로 키포인트를 할당해야 하는 문제로 성능이 제한되었지만, SHIC는 DINO 및 Stable Diffusion과 같은 기초 모델을 사용하여 수작업 없이 뛰어난 성능의 키포인트 매핑을 학습할 수 있습니다.

**관련 연구**
많은 연구자들이 수작업 레이블 없이 이미지를 일치시키려 노력해 왔습니다. 기존 방법들은 주로 합성 왜곡을 사용하거나 순환 일관성 손실을 사용했습니다. 최근에는 DINO와 Stable Diffusion 같은 기초 모델을 사용한 비지도 학습이 증가하고 있습니다.

**방법론**
SHIC는 다음 단계로 구성됩니다:
1. 기초 모델(features from DINO and Stable Diffusion)을 사용하여 이미지 간 대응 관계 설정.
2. 초기 대응 관계를 사용하여 전통적인 키포인트 탐지기 학습.
3. Stable Diffusion을 사용한 합성 이미지 생성으로 템플릿의 현실감을 높임으로써 추가적인 슈퍼비젼 제공.

**실험**
DensePose-LVIS와 PF-PASCAL 데이터셋을 사용하여 SHIC의 성능을 평가한 결과, SHIC는 인간의 개입 없이도 뛰어난 성능을 보였습니다. 특히 DensePose-LVIS에서 SHIC는 기존의 감독 학습을 통한 방법들보다도 더 좋은 결과를 보였습니다.

**결론**
SHIC는 3D 템플릿과 이미지 간의 대응 관계를 학습하는 비지도 학습 모델로, 수작업 없이도 매우 적은 데이터를 사용하여 다양한 객체에 적용할 수 있습니다. 이는 기존의 많은 수의 수작업 레이블이 필요한 방법과 비교하여 큰 진보를 이뤄냈습니다.

### 2. 전체 요약

이 논문은 3D 템플릿과 이미지 간의 대응 관계를 수작업 없이 학습하는 방법인 SHIC를 제안합니다. SHIC는 DINO와 Stable Diffusion 같은 기초 모델을 이용하여 수작업 없이도 고품질의 키포인트 매핑을 학습할 수 있습니다. 이 방법은 DensePose-LVIS와 PF-PASCAL 데이터셋에서 기존의 방법들보다 뛰어난 성능을 보였으며, 다양한 객체에 적은 데이터로도 적용 가능하여 많은 잠재적인 활용 가능성을 지니고 있습니다. 이 논문의 주된 혁신은 매우 적은 데이터와 수작업을 사용하지 않고도 높은 성능을 달성할 수 있는 점입니다.

## Similar Papers
- [Puppet-Master: Scaling Interactive Video Generation as a Motion Prior for Part-Level Dynamics](2408.04631.md)
- [HumanSplat: Generalizable Single-Image Human Gaussian Splatting with Structure Priors](2406.12459.md)
- [Shap-E: Generating Conditional 3D Implicit Functions](2305.02463.md)
- [Invisible Stitch: Generating Smooth 3D Scenes with Depth Inpainting](2404.19758.md)
- [Shape of Motion: 4D Reconstruction from a Single Video](2407.13764.md)
- [IllumiNeRF: 3D Relighting without Inverse Rendering](2406.06527.md)
- [MeshLRM: Large Reconstruction Model for High-Quality Mesh](2404.12385.md)
- [Vision Transformers Need Registers](2309.16588.md)
- [4M-21: An Any-to-Any Vision Model for Tens of Tasks and Modalities](2406.09406.md)
