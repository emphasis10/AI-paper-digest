# Improve Mathematical Reasoning in Language Models by Automated Process Supervision
## TL;DR
## Summary
- [https://arxiv.org/pdf/2406.06592.pdf](https://arxiv.org/pdf/2406.06592.pdf)

### 1. 각 섹션의 핵심 내용 요약

#### 1. 소개 (Introduction)
- **내용 요약:** 현재 대형 언어 모델(LLM)의 복잡한 추론 능력 부족 문제를 다룹니다. 이 연구는 특히 수학 문제 해결 및 코드 생성에 중점을 둡니다. 체인-오브-생각(Chain-of-Thought, CoT) 프롬프트를 통해 복잡한 문제를 단계별로 나누어 해결하고, 자기 일관성(self-consistency) 디코딩 전략을 제안합니다.

#### 2. 관련 연구 (Related Work)
- **내용 요약:** LLM의 수학적 추론 능력을 향상시키기 위한 여러 연구와 기법들을 다룹니다. 특히 체인-오브-생각(CoT) 프롬프팅, 감독 학습(SFT) 및 결과 보상 모델(ORM)과 과정 보상 모델(PRM)을 비교합니다. PRM이 더 나은 성능을 보임을 확인합니다.

#### 3. 방법론 (Methods)
- **내용 요약:** 과정 보상 모델(PRM) 훈련을 위한 새로운 알고리즘인 OmegaPRM을 소개합니다. 이 알고리즘은 몬테카를로 트리 탐색(Monte Carlo Tree Search, MCTS) 방식을 사용하여 고품질의 과정 감독 데이터를 자동으로 수집합니다. 이 데이터는 모델 훈련과 성능 향상에 사용됩니다.

#### 4. 실험 (Experiments)
- **내용 요약:** 여러 실험을 통해 PRM 모델의 성능을 평가합니다. OmegaPRM이 다른 데이터셋을 사용한 모델보다 우수한 성능을 보입니다. 특히, 수학 문제 해결을 위한 Hendrycks MATH 벤치마크에서 69.4%의 성공률을 기록합니다.

#### 5. 제한사항 (Limitations)
- **내용 요약:** 자동화된 과정 감독 주석이 노이즈를 포함할 수 있으며, 이는 훈련된 PRM의 성능에 영향을 미칠 수 있습니다. 또한, 인간 감독이 여전히 필요하며, 이는 확장성에 한계를 둡니다.

#### 6. 결론 (Conclusion)
- **내용 요약:** OmegaPRM 알고리즘의 효율성을 강조하며, 이를 통해 LLM의 수학적 추론 능력이 크게 향상됨을 확인합니다. 인간 주석에 비해 비용 효과적이라는 장점도 언급합니다.

### 2. 전체 요약

이 논문은 대형 언어 모델(LLM)의 복잡한 수학적 추론 능력을 향상시키기 위해 새로운 과정 보상 모델(PRM) 알고리즘인 OmegaPRM을 제안합니다. 이 알고리즘은 몬테카를로 트리 탐색(MCTS) 방식을 활용하여 고품질의 과정 주석 데이터를 자동으로 수집합니다. OmegaPRM을 통해 수집된 데이터로 훈련된 PRM은 Hendrycks MATH 벤치마크에서 69.4%의 성공률을 기록하며, 이는 기존 방법들에 비해 월등히 높은 성과입니다. 

이 연구의 주요 혁신점은 인간의 주석 없이 자동화된 데이터 수집이 가능하다는 점입니다. 이는 비용과 시간 측면에서 효율성을 크게 향상시킵니다. 마지막으로, 이 논문은 현재 방법의 한계를 인식하고 있으며, 향후 연구 방향으로 인간과 자동화된 주석을 결합하는 방안을 제안하고 있습니다.

이 요약과 분석을 통해 대형 언어 모델의 수학적 문제 해결 능력을 한 단계 끌어올릴 수 있는 가능성을 확인할 수 있습니다.

## Similar Papers
- [Toward Self-Improvement of LLMs via Imagination, Searching, and Criticizing](2404.12253.md)
- [THOUGHTSCULPT: Reasoning with Intermediate Revision and Search](2404.05966.md)
- [LiteSearch: Efficacious Tree Search for LLM](2407.00320.md)
- [AlphaMath Almost Zero: process Supervision without process](2405.03553.md)
- [Step-DPO: Step-wise Preference Optimization for Long-chain Reasoning of LLMs](2406.18629.md)
- [Q*: Improving Multi-step Reasoning for LLMs with Deliberative Planning](2406.14283.md)
- [Accessing GPT-4 level Mathematical Olympiad Solutions via Monte Carlo Tree Self-refine with LLaMa-3 8B](2406.07394.md)
- [Synthesizing Text-to-SQL Data from Weak and Strong LLMs](2408.03256.md)
- [Distilling System 2 into System 1](2407.06023.md)
