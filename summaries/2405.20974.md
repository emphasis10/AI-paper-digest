# SaySelf: Teaching LLMs to Express Confidence with Self-Reflective Rationales
## TL;DR
## Summary
- [https://arxiv.org/pdf/2405.20974.pdf](https://arxiv.org/pdf/2405.20974.pdf)

### 1. 주요 내용 요약 (각 섹션별)

#### 1. Introduction
- **핵심 기여**: SaySelf는 기존의 대형 언어 모델(LLM)들이 자신감 점수를 추출하는 데 있어 겪는 문제들을 해결합니다. 특히 LLM들이 자신의 불확실성을 정교하게 설명하는 '자기 성찰적 근거(self-reflective rationales)'를 생성하는 것을 목표로 합니다.

#### 2. Related Work
- **관련 연구**:
  - **환각(Hallucination)과 불확실성 표현**: 환각은 LLM이 실제로 제공되지 않은 정보를 생성하는 것을 의미합니다. 이는 모델의 신뢰성에 부정적 영향을 미칩니다.
  - **자신감 유도**: 기존 접근법은 프롬프트 기반과 훈련 데이터 기반으로 나뉘며, 이들 모두가 결정적 자신감 수치를 제공하는 데 한계가 있습니다.
  - **설명가능성**: LLM의 결정 과정을 설명하는 자기 성찰적 근거는 모델의 내부 불확실성을 더 잘 드러내도록 합니다.

#### 3. SaySelf
- **SaySelf의 기법**:
  - **프롬프트와 훈련 데이터**: SaySelf는 다수의 추론 체인을 샘플링하고, 클러스터링을 통해 선택된 예시를 GPT-4로 분석하여 언어로 불확실성을 요약합니다.
  - **강화 학습**: 정확한 자신감 추정을 위해 보상 함수를 설계하여 모델이 정확한 높은 자신감 예측을 하도록 유도합니다.

#### 4. Experiments
- **실험 결과**:
  - **자신감 신뢰도**: SaySelf는 기존 접근법보다 낮은 신뢰도 오차(ECE)와 높은 AUROC 점수를 기록하며 더 정교한 자신감 점수를 제공했습니다.
  - **과제 성능**: 다른 접근법과 비교하여 SaySelf는 과제 성능을 유지하면서도 더 정확한 자신감 추정을 가능하게 했습니다.
  - **자기 성찰적 근거의 신뢰성 평가**: GPT-4로 평가된 결과, SaySelf의 자기 성찰적 근거는 높은 신뢰도를 보였습니다.

#### 결론
- **결론**: SaySelf는 대형 언어 모델이 더 정교하고 신뢰할 수 있는 자신감 점수를 제공하도록 함으로써 환각 문제를 개선하고, 자기 성찰적 근거를 통해 모델이 내부 불확실성을 더 잘 표현할 수 있도록 합니다. 이로 인해 LLM의 실제 응용 가능성이 향상됩니다.

---

### 2. 전체 요약

SaySelf는 대형 언어 모델이 자신의 불확실성을 더 잘 표현할 수 있도록 훈련시켜 그 신뢰성을 향상시키는 프레임워크입니다. SaySelf는 이전 접근법들이 가지는 한계를 극복하고, 정확한 자신감 점수를 생성하는 것을 목표로 합니다. 이를 위해 모델의 다수 추론을 기반으로 한 요약 데이터로 훈련하며, 강화 학습을 통해 더욱 세밀한 자신감 추정을 가능하게 합니다. 실험 결과, SaySelf는 자신감 신뢰도와 과제 성능에서 모두 우수한 성과를 보였으며, 특히 자기 성찰적 근거를 통해 모델의 내부 불확실성을 잘 포착했습니다. 이는 AI의 실제 응용을 보다 안전하고 효과적으로 만들 수 있는 중요한 기여입니다.

## Similar Papers
- [Executable Code Actions Elicit Better LLM Agents](2402.01030.md)
- [Investigating How Large Language Models Leverage Internal Knowledge to Perform Complex Reasoning](2406.19502.md)
- [Active Prompting with Chain-of-Thought for Large Language Models](2302.12246.md)
- [AutoDetect: Towards a Unified Framework for Automated Weakness Detection in Large Language Models](2406.16714.md)
- [Sparks of Artificial General Intelligence: Early experiments with GPT-4](2303.12712.md)
- [Advancing LLM Reasoning Generalists with Preference Trees](2404.02078.md)
- [LongIns: A Challenging Long-context Instruction-based Exam for LLMs](2406.17588.md)
- [Step-Controlled DPO: Leveraging Stepwise Error for Enhanced Mathematical Reasoning](2407.00782.md)
- [Lookback Lens: Detecting and Mitigating Contextual Hallucinations in Large Language Models Using Only Attention Maps](2407.07071.md)
