# WebRL: Training LLM Web Agents via Self-Evolving Online Curriculum Reinforcement Learning
## TL;DR
## Summary
- [https://arxiv.org/pdf/2411.02337.pdf](https://arxiv.org/pdf/2411.02337.pdf)

1. 각 섹션의 주요 내용 요약 및 기여와 혁신적인 부분 설명:
   - **서론**:
     이 논문에서는 대형 언어 모델(LLM)이 웹 기반 작업에서 자율 에이전트로서의 잠재력을 보여주고 있음을 설명합니다. 기존의 웹 에이전트는 고가의 독점 LLM API에 의존하고 있었으나, 이는 비용 문제를 야기하였습니다. 본 논문에서는 공개된 LLM을 사용하여 웹 에이전트를 효과적으로 훈련할 수 있는 WEBRL이라는 강화 학습 프레임워크를 소개합니다.

   - **WEBRL: 자기 진화 온라인 커리큘럼 RL**:
     WEBRL은 강화 학습을 통해 웹 에이전트를 훈련하기 위한 자기 진화형 커리큘럼 교육 체계를 제공합니다. 웹 환경 내 에이전트가 실시간으로 상호작용하며 데이터를 수집하고, 이 데이터를 바탕으로 점차 복잡한 작업을 수행 가능하도록 자기 진화를 이어갑니다.

   - **문제 법칙화**:
     웹 작업 완료 과정을 마르코프 결정 프로세스(MDP)로 모델링하여, 상태와 행동에 기반한 보상을 통해 웹 에이전트의 정책을 최적화하는 방법을 설명합니다. 이러한 MDP 기반 접근은 웹 에이전트가 주어진 작업을 효과적으로 수행할 수 있도록 합니다.

   - **ORM 훈련**:
     결과 감독 기반 보상 모델(ORM)을 활용하여 에이전트의 작업 성공 여부를 평가합니다. 불확실한 피드백 환경에서 ORM은 에이전트의 성과를 성공 또는 실패로 자동 평가하여 피드백을 제공합니다.

   - **관련 연구**:
     본 논문이 제안하는 WEBRL은 다른 기존 방법론에 비해 상당히 향상된 성능을 제공하며, 특히 WebArena-Lite 벤치마크에서 최고 성능을 발휘합니다.

2. 전체 요약:
   이 논문은 공개된 LLM을 활용하여 웹 에이전트를 훈련시키는 효과적인 방법을 제시합니다. WEBRL은 자기 진화형 커리큘럼과 적응형 강화 학습 전략을 도입하여, 계속적인 성능 개선이 가능하게 합니다. 그 결과, Llama-3.1 및 GLM-4 모델은 성공률이 큰 폭으로 향상되었으며, 이는 기존의 독점 LLM API보다 월등히 높은 성능을 보여줍니다. 이러한 성과는 LLM의 잠재력을 적극 활용하여 더욱 접근 가능하고 강력한 자율 웹 상호작용 시스템의 발전을 의미합니다.