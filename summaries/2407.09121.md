# Refuse Whenever You Feel Unsafe: Improving Safety in LLMs via Decoupled Refusal Training
## TL;DR
## Summary
- [https://arxiv.org/pdf/2407.09121.pdf](https://arxiv.org/pdf/2407.09121.pdf)

### 중요 섹션 요약

#### 1. 서론 (Introduction)
- **요약**: LLMs (대규모 언어 모델)의 지능은 매우 인상적이며, 연구자들은 이를 인간 윤리와 맞추기 위해 다양한 전략을 도입해 왔습니다. 하지만 새로운 안전 위험이 지속적으로 등장하고 있으며, 이 논문에서는 LLM의 안전 튜닝 데이터에서 거부 위치 편향을 식별하고 이에 대응하는 새로운 안전 튜닝 방법인 Decoupled Refusal Training (DeRTa)를 제안합니다. DeRTa는 LLM이 응답의 어느 위치에서든 해로운 프롬프트에 대한 준수를 거부할 수 있도록 설계되었습니다.

#### 2. 방법론 (Methodology)
- **요약**: DeRTa는 두 가지 핵심 구성 요소를 포함합니다:
  1. **MLE with Harmful Response Prefix**: 안전한 응답의 시작 부분에 해로운 응답의 일부를 추가하여 모델이 어느 위치에서든 준수를 거부하도록 학습시킵니다.
  2. **Reinforced Transition Optimization (RTO)**: 단일 전환 학습이 아닌 해로운 응답 시퀀스의 모든 위치에서 안전 거부로 전환할 수 있도록 모델을 최적화합니다.

#### 3. 실험 (Experiment)
- **요약**: DeRTa는 LLaMA3 (8B 및 70B)와 Mistral (7B 및 8×7B) 모델에서 6가지 공격 시나리오를 테스트하여 모델 안전성을 향상시키면서 성능 저해 없이 GPT-4 및 LLaMA3-70B과 같은 잘 알려진 모델들을 넘어서는 성과를 보였습니다. DeRTa는 모델이 잠재적인 위험을 인식하고 안전하지 않은 콘텐츠 생성을 중지할 수 있는 능력을 효과적으로 향상시킵니다.

### 전체 요약

이 논문은 LLM이 해로운 프롬프트에 대해 응답하는 것을 거부할 수 있도록 하는 새로운 방식인 Decoupled Refusal Training (DeRTa)을 제안했습니다. 이 방법은 두 가지 주요 구성 요소로 구성되어 있습니다:
1. **MLE with Harmful Response Prefix**: 응답의 시작 부분에 해로운 응답의 일부를 추가하여 어느 위치에서든 거부할 수 있도록 모델을 학습시킵니다.
2. **Reinforced Transition Optimization (RTO)**: 해로운 응답 시퀀스의 모든 위치에서 안전 거부로 전환하는 능력을 강화합니다.

이 접근 방법은 LLaMA3 및 Mistral 모델들을 통해 검증되었으며, 새로운 공격 방법들에 대해 매우 높은 수준의 안전성을 제공하는 것으로 나타났습니다. DeRTa는 LLM이 잠재적인 위험을 인식하고 안전하지 않은 콘텐츠 생성을 중단할 수 있는 능력을 효율적으로 강화하여 더 안전하고 신뢰할 수 있는 언어 모델의 개발에 기여할 것으로 기대됩니다.