# Reward Steering with Evolutionary Heuristics for Decoding-time Alignment
## TL;DR
## Summary
- [https://arxiv.org/pdf/2406.15193.pdf](https://arxiv.org/pdf/2406.15193.pdf)

### 요약

#### 1. 요약 및 세부 분석

##### Abstract (초록)
이 논문은 대규모 언어 모델(LLM)의 응답을 사용자나 이해 관계자의 선호도에 맞추는 방법을 다룹니다. 기존의 방법은 모델 성능을 저하시킬 수 있고, 사용자의 변하는 선호도에 빠르게 대응하지 못하는 문제점이 있었습니다. 이 문제를 해결하기 위해, 보상 전개의 탐색과 활용 전략을 분리하여 모델의 응답을 진화적인 방식으로 개선하는 방법을 제안합니다. DARWIN이라는 새로운 방법은 명령어 변형을 통한 탐색과 주기적인 보상 기반의 대체 과정을 결합하여 더 나은 성능을 보입니다.

##### Introduction (서론)
LLM은 다양한 복잡한 문제를 해결하고, 일상 생활과 업무에서 큰 도움이 될 수 있지만, 사용자의 의도와 맞지 않거나 유해한 방식으로 응답할 수 있습니다. 기존의 강화학습(RLHF)과 직접 선호 최적화(DPO) 방법은 효과적이지만, 모델의 기존 학습을 방해하거나 변하는 사용자 선호도를 실시간으로 반영하기 어렵습니다. DARWIN은 명령어 변형과 보상 기반 대체 과정을 통해 이런 문제를 해결하고자 합니다.

##### Methodology (방법론)
DARWIN은 탐색과 활용의 두 측면을 분리하여 실행합니다. 새로운 명령어로 변형하여 다양한 응답을 생성하고, 보상 모델을 통해 더 나은 응답을 선택하여 반복적으로 개선합니다. 이것은 진화 알고리즘과 유사한 접근 방식으로, 실험 결과 ARGS와 같은 기존 방법보다 뛰어난 성과를 보였습니다.

##### Experiments (실험)
DARWIN은 두 가지 명령어 튜닝된 LLM(Meta-Llama-3-8B-Instruct와 Mistral-7B-Instruct)에서 평가되었으며, AlpacaEval 2와 MT-Bench 두 가지 벤치마크에서 더 나은 성능을 보였습니다. DARWIN의 주요 기여는 탐색과 활용의 균형을 맞추고, 명령어 변형과 주기적인 보상 기반 대체 방법을 제안하여 모델 정렬을 개선한 점입니다.

##### Related Work (관련 작업)
기존의 보상 기반 접근 방식과 비아이라인먼트 접근 방식에 대해 설명하며, DARWIN의 차별화된 점을 강조합니다. 현재의 보상 모델의 한계와 그로 인해 발생하는 단점을 보완하는 다양한 접근 방식이 제안되었지만, DARWIN은 이보다 더 나은 성과를 보입니다.

##### Conclusion (결론)
DARWIN은 진화적인 탐색과 활용 전략을 통해 일관된 보상 최적화를 달성했으며, 기존의 디코딩-타임 정렬 방법과 선호 최적화 방법보다 더 나은 성과를 보여줍니다. 실험 결과, DARWIN은 다양한 설정에서 강력한 성능을 나타내었습니다.

#### 2. 전체 요약
이 논문은 대규모 언어 모델(LLM)의 응답을 사용자 선호도에 맞추기 위해 탐색과 활용의 두 측면을 분리하여 실행하는 새로운 방법인 DARWIN을 제안합니다. DARWIN은 명령어 변형과 보상 기반 대체 과정을 결합하여 모델의 출력 응답을 개선합니다. 실험 결과, 기존의 방법들보다 더 나은 성능을 보였으며, 특히 AlpacaEval 2와 MT-Bench 벤치마크에서 유의미한 성과를 보였습니다. 이 접근법은 LLM의 성능 저하 없이 사용자의 변하는 선호도에 맞춰 빠르게 적응할 수 있는 가능성을 제시합니다.

## Similar Papers
- [Safety Arithmetic: A Framework for Test-time Safety Alignment of Language Models by Steering Parameters and Activations](2406.11801.md)
- [THOUGHTSCULPT: Reasoning with Intermediate Revision and Search](2404.05966.md)
- [Tango 2: Aligning Diffusion-based Text-to-Audio Generations through Direct Preference Optimization](2404.09956.md)
- [Self-Evaluation as a Defense Against Adversarial Attacks on LLMs](2407.03234.md)
- [Is Programming by Example solved by LLMs?](2406.08316.md)
- [SimPO: Simple Preference Optimization with a Reference-Free Reward](2405.14734.md)
- [Diffusion Augmented Agents: A Framework for Efficient Exploration and Transfer Learning](2407.20798.md)
- [Direct Preference Knowledge Distillation for Large Language Models](2406.19774.md)
- [InverseCoder: Unleashing the Power of Instruction-Tuned Code LLMs with Inverse-Instruct](2407.05700.md)
