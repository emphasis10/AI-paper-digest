# Mixture-of-Subspaces in Low-Rank Adaptation
## TL;DR
## Summary
- [https://arxiv.org/pdf/2406.11909.pdf](https://arxiv.org/pdf/2406.11909.pdf)

### 요약

#### Abstract
이 논문에서는 두 개의 작은 차원의 하위 공간을 결합하여 성능을 향상시키는 LoRA(낮은 순위 적응) 방법을 소개합니다. 이 방법은 대규모 언어 모델, 다중 모드 모델 및 확산 모델에 적용 가능합니다. 우리는 이러한 현상을 세분화된 하위 공간 렌즈를 통해 분석하고, 이러한 하위 공간들을 더 유연하게 결합하는 Mixture-of-Subspaces LoRA (MoSLoRA)를 제안합니다.

#### Introduction
대규모 언어 모델(LLM)은 대규모 데이터와 모델 파라미터를 통해 다양한 분야에서 뛰어난 성능을 보여주지만, 이러한 모델의 모든 파라미터를 미세 조정하는 데에는 어려움이 있습니다. 이를 해결하기 위해 LoRA 같은 파라미터 효율적 미세 조정 방법이 개발되었습니다. 기존 LoRA는 하나의 하위 공간을 업데이트하지만 MoSLoRA는 여러 하위 공간을 결합해 성능을 높입니다.

#### Methodology
MoSLoRA는 기존의 LoRA 방식을 하위 공간으로 분해하고, 학습 가능한 결합기를 사용하여 다양한 하위 공간을 더 유연하게 결합합니다. 이는 LoRA와 비교해 약간의 추가 파라미터 및 연산 비용만 필요로 하며, 하위 공간을 결합함으로써 정보를 더 잘 처리합니다.

#### Experiments and Analysis
MoSLoRA는 다양한 다운스트림 작업에서 일관되게 LoRA보다 뛰어난 성능을 보여줍니다. 이는 상식 추론, 시각적 지시 조정, 주제 주도 텍스트-이미지 생성 작업에서 확인되었습니다. 이러한 결과는 MoSLoRA가 효과적이고 강력하다는 것을 입증합니다.

#### Contributions
- LoRA를 하위 공간으로 분해하여 새로운 연구 경로 제시
- 학습 가능한 결합기를 사용한 MoSLoRA 제안
- 다양한 다운스트림 작업에서 실험을 통해 MoSLoRA의 효과와 강력함 입증

#### Conclusion
MoSLoRA는 파라미터 효율적인 미세 조정 방법으로서, 낮은 순위 하위 공간들을 결합해 성능을 향상시킵니다. 이는 추가적인 하위 공간의 정보를 더 유연하게 결합하여 LoRA보다 뛰어난 성능을 보여줍니다. MoSLoRA는 다양한 작업에서 강력한 성능을 입증하였으며, 향후 특정 작업에 대한 초기화 방법을 연구하는 것도 흥미로운 주제가 될 것입니다.

### 전체 요약

이 논문은 대규모 언어 모델의 파라미터 효율적인 미세 조정 방법인 LoRA를 개선한 MoSLoRA를 제안합니다. MoSLoRA는 여러 하위 공간을 결합하여 성능을 향상시키며, 실험을 통해 상식 추론, 시각적 지시 조정, 주제 주도 텍스트-이미지 생성 작업에서 뛰어난 성능을 보여줍니다. 이는 하위 공간을 유연하게 결합함으로써 더 많은 정보를 처리할 수 있게 한 것으로, LoRA 대비 약간의 추가 파라미터와 연산 비용만 필요로 합니다. 이러한 결과는 MoSLoRA가 다양한 작업에서 효과적이고 강력하다는 것을 입증하며, 향후 연구에서는 특정 작업에 대한 초기화 방법을 추가적으로 탐구할 필요가 있습니다.

## Similar Papers
- [DoRA: Weight-Decomposed Low-Rank Adaptation](2402.09353.md)
- [MLCM: Multistep Consistency Distillation of Latent Diffusion Model](2406.05768.md)
- [LLaMA Pro: Progressive LLaMA with Block Expansion](2401.02415.md)
- [Multi-Head Mixture-of-Experts](2404.15045.md)
- [Knowledge Composition using Task Vectors with Learned Anisotropic Scaling](2407.02880.md)
- [Plot2Code: A Comprehensive Benchmark for Evaluating Multi-modal Large Language Models in Code Generation from Scientific Plots](2405.07990.md)
- [E5-V: Universal Embeddings with Multimodal Large Language Models](2407.12580.md)
- [Adapting LLaMA Decoder to Vision Transformer](2404.06773.md)
- [EVLM: An Efficient Vision-Language Model for Visual Understanding](2407.14177.md)
