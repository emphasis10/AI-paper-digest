# Understanding Alignment in Multimodal LLMs: A Comprehensive Study
## TL;DR
## Summary
- [https://arxiv.org/pdf/2407.02477.pdf](https://arxiv.org/pdf/2407.02477.pdf)

### 1. 섹션 요약

#### 1.1 서론
현대의 멀티모달 대형 언어 모델(MLLMs)은 이미지와 텍스트의 통합을 통해 비전-언어 태스크에 대한 이해도와 상호작용 능력을 크게 향상시켰습니다. 그러나 이러한 모델은 이미지 컨텐츠를 제대로 반영하지 못하고, 비전 입력과 일치하지 않는 응답을 생성하는 '환상' 문제를 겪습니다. 환상 방지를 위해 다양한 선호도 정렬 방법이 개발되고 있습니다.

#### 1.2 관련 연구
현재까지도 많은 연구들이 MLLMs의 성능을 향상시키기 위해 선호도 데이터셋과 정렬 방법을 검토했습니다. Direct Preference Optimization (DPO)와 Proximal Policy Optimization (PPO) 같은 방법이 주로 사용되었으며, 각 방법의 효과는 데이터셋 구성, 기본 모델 유형, 정렬 방법에 따라 다릅니다. 또한, BDHS라는 새로운 데이터 생성 전략을 통해 추가적인 주석이나 외부 모델 없이도 환상을 줄이는 데 성공했습니다.

#### 1.3 주요 기여점
1. **선호도 정렬 방법 분류**: 오프라인(DPO)과 온라인(Online-DPO) 방법으로 정렬 알고리즘을 분류하고, 온라인과 오프라인 방법을 혼합하여 성능을 향상시킬 수 있는 시나리오를 제시했습니다.
2. **데이터셋 분석**: 다양한 멀티모달 선호도 데이터셋을 분석하여 각 데이터셋의 강점과 약점을 식별하고, 특정 데이터 유형이 모델 성능을 향상시키는 방법을 탐구했습니다.
3. **새로운 데이터 샘플링 전략 개발**: BDHS라는 새로운 데이터 샘플링 전략을 도입하여, LLaVA 1.6 모델에 적용했을 때 다양한 벤치마크에서 두드러진 성능 향상을 이끌어냈습니다.

#### 1.4 방법론
RLHF (Reinforcement Learning from Human Feedback) 및 DPO 등 다양한 정렬 방법을 통해 얻은 선호도 데이터를 사용하는 방법을 탐구했습니다. 특히, BDHS 샘플링 방법을 통해 선호도 데이터를 효율적으로 생성하고, 이를 통해 기존 방법 대비 경쟁력 있는 성능을 기록했습니다.

#### 1.5 결과
BDHS를 통해 생성된 선호도 데이터를 사용한 결과, 다양한 멀티모달 벤치마크에서 더 나은 성능을 기록했습니다. 특히, 환상 문제를 효과적으로 줄일 수 있었습니다. 이러한 결과는 다른 복잡한 방법을 사용하지 않고도 SFT 데이터만으로도 성능을 향상시킬 수 있는 가능성을 보여줍니다.

#### 1.6 결론 및 미래 연구
본 연구는 선호도 정렬 방법이 MLLMs의 성능을 어떻게 향상시킬 수 있는지 상세히 분석하고, 새로운 데이터 샘플링 방법을 통해 그 가능성을 입증했습니다. 다음 단계로는 더 나은 환상 벤치마크 개발 및 RL 기반 정렬 방법의 추가적인 연구가 필요합니다.

### 2. 전체 요약
이 논문은 멀티모달 대형 언어 모델(MLLMs)의 성능 향상에 중대한 기여를 했습니다. 주요 초점은 모델이 시각 정보를 반영하지 못하고 환상을 일으키는 문제를 해결하는 것입니다. 이를 위해 다양한 선호도 정렬 방법을 조사하고, 특히 BDHS라는 새로운 데이터 샘플링 전략을 제안하여 모델의 성능을 효과적으로 향상시켰습니다. 이러한 방법들은 LLaVA 1.6 모델을 비롯한 여러 벤치마크에서 탁월한 성능을 보여줬으며, 추가적인 데이터 주석이나 외부 모델 없이도 성능을 높일 수 있는 잠재력을 입증했습니다. 이 연구는 향후 MLLMs 분야에서 새로운 정렬 방법과 데이터셋 개발을 위한 중요한 토대를 마련했습니다.

## Similar Papers
- [MIA-Bench: Towards Better Instruction Following Evaluation of Multimodal LLMs](2407.01509.md)
- [MoMA: Multimodal LLM Adapter for Fast Personalized Image Generation](2404.05674.md)
- [Improving GFlowNets for Text-to-Image Diffusion Alignment](2406.00633.md)
- [BPO: Supercharging Online Preference Learning by Adhering to the Proximity of Behavior LLM](2406.12168.md)
- [Dallah: A Dialect-Aware Multimodal Large Language Model for Arabic](2407.18129.md)
- [Openstory++: A Large-scale Dataset and Benchmark for Instance-aware Open-domain Visual Storytelling](2408.03695.md)
- [Is DPO Superior to PPO for LLM Alignment? A Comprehensive Study](2404.10719.md)
- [EdgeFusion: On-Device Text-to-Image Generation](2404.11925.md)
- [MOFI: Learning Image Representations from Noisy Entity Annotated Images](2306.07952.md)
