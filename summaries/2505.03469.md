# Long-Short Chain-of-Thought Mixture Supervised Fine-Tuning Eliciting Efficient Reasoning in Large Language Models
## TL;DR
## Summary
- [https://arxiv.org/pdf/2505.03469.pdf](https://arxiv.org/pdf/2505.03469.pdf)

1. 각 섹션 요약:
   
  - **서론**: 대규모 언어 모델(LM)의 발전은 복잡한 작업에서 강력한 추론 능력을 보여주었습니다. 그러나 이 모델들은 종종 과도한 추론 경향으로 인해 비효율적입니다. 논문에서는 이러한 문제를 해결하기 위해 LS-Mixture SFT라는 새로운 방법론을 제시하여, 비추론 모델에도 효과적인 추론 능력을 부여하고자 합니다.

  - **방법론**: LS-Mixture SFT 방법은 다음 단계로 구성됩니다:
    1. **구조 보존 CoT 재작성**: 장황한 추론 체인을 더 간결하게 재작성하면서 필수적인 논리 구조와 주요 단계를 유지합니다. 
    2. **혼합 감독 학습**: 길고 짧은 추론 데이터를 섞어 비추론 LM을 교육합니다. 
    3. **추론 시 균형 있는 사고**: 주어진 문제의 복잡성에 따라 모델이 적절한 수준의 추론 깊이를 조절합니다.

  - **실험 및 결과**: LS-Mixture SFT를 통해 훈련된 s1-mix-32B 모델은 기존 모델(s1.1-32B) 대비 평균 응답 길이를 47.61% 줄이면서도 다양한 벤치마크 성능을 개선하였습니다.

  - **결론**: LS-Mixture SFT 방법론은 추론 능력을 강화하면서도 응답 길이를 효과적으로 줄여 모델의 효율성을 높이고자 하였습니다. 이것은 교사 모델로부터 과도한 생각의 문제를 피하고 학생 모델로서 효과적인 추론을 가능하게 합니다.

2. 전체 요약:
   
   논문은 대규모 언어 모델에서 효율적인 추론 능력을 발현시키기 위해 LS-Mixture SFT라는 혁신적인 방법을 소개합니다. 이는 주어진 문제의 복잡성에 따라 길고 짧은 사고 체인을 결합하여 균형 잡힌 추론 능력을 갖춘 모델을 생성합니다. 이를 통해 응답 길이를 줄이면서도 성능을 개선할 수 있음을 실험적으로 검증하였습니다. 이 연구는 비추론 모델에 추론 능력을 부여하는 데 있어서 기존의 과도한 생각의 문제를 해결하고 향상된 효율성을 제시합니다.