# Reinforcement Learning for Reasoning in Large Language Models with One Training Example
## TL;DR
## Summary
- [https://arxiv.org/pdf/2504.20571.pdf](https://arxiv.org/pdf/2504.20571.pdf)

1. 주요 섹션 요약:

- **서론**: 이 논문은 대규모 언어 모델(LLMs)의 수리적 추론 능력을 강화하기 위해 단 한 개의 훈련 예제만 사용하는 1-shot RLVR(강화 학습 검증 보상)를 소개합니다. 이 방법은 사전 훈련된 모델 Qwen2.5-Math-1.5B의 성능을 큰 폭으로 향상시킵니다.

- **기초 지식**: RLVR는 수학적 문제 해결 능력을 개선하기 위해 검증된 보상을 사용하는 강화 학습 방법입니다. 보상은 비교적 간단하며, 모델의 최종 답변이 정답과 일치하는지 여부에 따라 결정됩니다.

- **실험**:
  - **설정**: 실험은 각 질문의 문항에 대해 구 출력을 샘플링하여 최적화합니다. 여기에는 정책 그레이디언트 손실, KL 다이버전스, 엔트로피 손실이 포함됩니다.
  - **1/Few-Shot RLVR 관찰**: 하나의 예제가 있을 때 빠른 학습 정확도 포화 현상이 발생하지만, 테스트 성능은 개선을 지속합니다. 이 현상을 "포화 후 일반화"라고 부릅니다.
  - **다른 모델/알고리즘에 대한 1/소수 샷 RLVR**: 다양한 모델과 강화 학습 알고리즘 간의 1-shot RLVR의 효과가 검증되었습니다.

- **분석**:
  - **정책 그레이디언트 손실**: 1-shot RLVR의 주된 기여 요소로 확인되었으며, 탐색 촉진의 중요성을 강조합니다.
  - **진로 손실의 추가 효과**: 이 방법은 모델 성능을 개선하며, 대규모 실험에서 더 나은 탐색 방법의 필요성을 나타냅니다.

- **결론**: 이 연구는 1-shot RLVR로 인해 모델의 수리적 추론 가능성을 자극하는 데 성공했음을 보여줍니다. 이는 데이터를 선택하고 수집하는 방법에 대한 중요성을 강조하며 향후 연구의 방향성을 제시합니다.

2. 전체 요약:

이 논문에서는 1-shot RLVR이 대규모 언어 모델의 수리적 추론 능력을 강화할 수 있음을 보여줍니다. 단 한 개의 훈련 예제로도 수천 개의 예제를 사용하는 것과 유사한 성능을 이끌어내며, 다양한 모델과 알고리즘에서도 그 효과가 증명되었습니다. 1-shot RLVR은 빠른 학습 정확도 포화 후에도 테스트 성능이 지속적으로 향상되는 현상이 특징이며, 데이터 선택과 수집의 중요성도 재조명됩니다.