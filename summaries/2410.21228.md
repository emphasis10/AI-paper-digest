# LoRA vs Full Fine-tuning: An Illusion of Equivalence
## TL;DR
## Summary
- [https://arxiv.org/pdf/2410.21228.pdf](https://arxiv.org/pdf/2410.21228.pdf)

### 1. 섹션별 요약

**도입부**
이 논문은 AI와 머신러닝에서 "LoRA"와 "완전 미세 조정"이라는 두 가지 미세 조정 방법을 비교합니다. 각각의 기법이 어떻게 모델의 스펙트럼 속성에 영향을 미치는지, 그리고 이로 인해 발생하는 일반화 성능 차이를 설명합니다.

**배경 및 관련 연구**
이 섹션에서는 LoRA와 전통적인 방법론의 차이점에 대해 설명합니다. 기존 연구는 모델 미세 조정의 본질적 차원을 설명하며, 각 방법이 적용된 모델에서의 스펙트럼 변화의 중요성을 언급합니다.

**LoRA와 완전 미세 조정의 구조적 차이**
LoRA는 업데이트 과정에서 "침입자 차원"이라는 새로운 벡터를 도입하며, 이러한 벡터는 미세 조정의 구조적 변화를 야기합니다. 반면, 완전 미세 조정된 모델은 초기 모델과 스펙트럼적 유사함을 지속적으로 유지합니다.

**행동적 차이**
LoRA로 미세 조정된 모델은 특정 과제를 잊는 경향이 높습니다. 이는 특히 연속 학습 환경에서 드러나며, LoRA의 낮은 순위가 이러한 잊혀짐을 심화시킬 수 있습니다.

**침입자 차원의 존재 이유**
새로운 벡터의 도입은 LoRA가 낮은 순위 매개변수 공간에서 작업하면서 발생하는 특징입니다. 이러한 특징은 매개변수 행렬의 지속성과 안정성에 악영향을 미칩니다.

**결론**
LoRA와 완전 미세 조정의 근본적인 차이는 벡터 차원과 모델 구조에 대한 영향을 미칩니다. LoRA 기법은 과잉 적합 문제를 해결하는 데 일부 한계를 가질 수 있으며, 이는 모델의 일반화 능력에 영향을 줄 수 있습니다.

---

### 2. 전체 요약

이 논문은 AI 및 머신러닝에서 LoRA(저순위 적응)와 전통적인 완전 미세 조정 방법의 차이점을 분석합니다. LoRA는 학습 과정에서 새로운 "침입자 차원"을 도입하여 스펙트럼적 변화와 함께 모델의 행동을 변화시킵니다. 이러한 차원은 모델이 초기 학습 데이터를 잊게 하거나 학습 세션 간 전이 학습의 문제를 야기할 수 있습니다. 반면, 완전 미세 조정은 모델의 초기 스펙트럼 속성을 유지하면서 보다 안정적인 학습을 지원합니다. LoRA의 주요 혁신점은 매개변수 절감과 낮은 계산 비용을 가능하게 하지만, 이로 인한 잦은 잊혀짐과 안정성 문제로 인해 특정 응용환경에서는 사용이 제한될 수 있습니다.