# Large Concept Models: Language Modeling in a Sentence Representation Space
## TL;DR
## Summary
- [https://arxiv.org/pdf/2412.08821.pdf](https://arxiv.org/pdf/2412.08821.pdf)

### 1. 요약 

#### 서론
자연어처리 분야에서 큰 언어 모델(LLM)은 중요한 연구 대상으로 자리 잡았습니다. 이 논문에서는 LLM의 새로운 아키텍처로 "큰 개념 모델(LCM)"을 소개합니다. 기존 모델은 주로 단어 수준에서 작동하지만, LCM은 더 높은 수준의 추상적 의미 표현에 초점을 맞춥니다.

#### 주요 설계 원칙
LCM은 여러 언어와 모달리티를 통합하여 추론을 수행하며, 기본적으로 언어 및 모달리티 비의존적입니다. 이 모델은 긴 문맥과 장문의 출력을 처리하는 능력을 가집니다. 또한, LCM은 고유한 계층 구조를 명시적으로 가지고 있어 사람이 읽기 쉽게 하며, 사용자 상호작용을 용이하게 합니다.

#### 모델 구조
이 논문에서는 One-Tower와 Two-Tower라는 두 가지 변형의 확산 LCM을 제안합니다. 이 두 가지 접근법 모두 문장 예측을 수행합니다. 그러나 두 가지 모델은 특히 이전 문맥을 인코딩하고 다음 문장을 확산하는 방식에서 차이를 보입니다.

#### 실험적 평가
LCM은 여러 생성 작업에서 기존 LLM보다 뛰어난 성능을 보였으며 많은 언어에 대한 뛰어난 제로샷 일반화 능력을 입증했습니다.  특히 요약과 요약 확장에서 인상적인 성과를 보였으며, 이는 온라인에서 자유롭게 사용 가능한 훈련 코드로 이용할 수 있습니다.

#### 결론
기존의 대규모 언어 모델과는 달리, LCM은 고차원 임베딩 공간에서 작동하며, 특정 언어 또는 모달리티에 얽매이지 않습니다. 이러한 혁신은 자연어처리의 새로운 방향성을 제시하며, 더 많은 연구와 개선 여지를 남깁니다.

### 2. 전체 요약
이 논문은 기존 언어 모델이 주로 단어 수준에서 작동했다면, 개념 수준에서 작동하는 새로운 언어 모델 아키텍처인 큰 개념 모델(LCM)을 소개합니다. 이 모델은 여러 언어 및 모달리티를 끌어들여 더 높은 수준의 추론을 가능하게 하며, 제로샷 학습을 통해 문장의 예측 정확도를 크게 향상시킵니다. 이러한 혁신은 자연어처리 분야에서의 모델 설계에 대한 새로운 통찰을 제공합니다.