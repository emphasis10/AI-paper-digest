# MOFA-Video: Controllable Image Animation via Generative Motion Field Adaptions in Frozen Image-to-Video Diffusion Model
## TL;DR
## Summary
- [https://arxiv.org/pdf/2405.20222.pdf](https://arxiv.org/pdf/2405.20222.pdf)

### 섹션 요약

#### 1. 소개

이 논문에서는 **Stable Video Diffusion (SVD)**라는 새로운 통합 프레임워크를 제안합니다. 이 프레임워크는 여러 모션 도메인에서 컨트롤 가능한 이미지 애니메이션을 가능하게 합니다. 여기에는 **MOFA-Adapter**라는 새로운 네트워크 구조를 도입하여 명시적인 희소 모션 힌트를 사용하여 영상 생성을 제어하는 방법이 포함됩니다.

#### 2. 관련 연구

**2.1 컨트롤 가능한 이미지 애니메이션**:
- 기존의 이미지 애니메이션 연구들은 특정 도메인에 집중하며, 제너레이티브 모델(GANs 등)을 사용하여 비제어 방식으로 동영상을 생성합니다.
- 그러나 이러한 방법들은 제어 핸들을 제공하지 않아 실제 응용에서 제한됩니다.
- 본 연구는 다양한 도메인에서 모션을 모델링하고, 이를 사전 훈련된 비디오 디퓨전 모델에서 동작하도록 하는 새로운 통합 프레임워크를 제안합니다.

**2.2 이미지-to-비디오 디퓨전 모델(I2V)**:
- I2V 모델들은 레퍼런스 이미지로부터 의미적 특징을 추출하여 비디오 생성에 사용하며, 텍스트 기반 모션 생성을 통한 제어는 어렵다고 주장합니다.
- 본 연구는 반대로, 다양한 도메인에서의 모션 힌트를 사용하여 사전 훈련된 SVD 모델에서 제어가 가능한 방법을 제안합니다.

#### 3. 방법

**3.1 MOFA-Adapter**:
- 희소 모션 힌트를 활용하여 다양한 대상의 생성을 개별적으로 또는 공동으로 제어합니다.
- MOFA-Adapter는 참조 인코더, 희소-to-밀집(S2D) 모션 생성 네트워크, 특징 융합 인코더로 구성됩니다.

**3.2 Stable Video Diffusion에서 MOFA-Adapters 훈련**:
- Stable Video Diffusion를 기반 이미지-to-비디오 디퓨전 모델로 사용하여 비디오를 생성합니다.
- 희소 모션 힌트를 사용하여 밀집 모션 필드를 생성하고, 이를 통해 첫 프레임의 다중 스케일 특징을 왜곡하여 생성하는 방식을 채택합니다.

#### 4. 실험 및 결과

- **4.1 기타 최첨단 방법과의 비교**:
  - 드래그뉴와(DragNUWA), 모션컨트롤(MotionCtrl), 스타일히트(StyleHEAT), 새드토커(SadTalker) 등과 비교했을 때 더 나은 성능을 보임.
  - 사용자 선호도 평가에서도 높은 점수를 받아 우수함을 입증.

**4.2 제한점**:
- 새로운 콘텐츠를 제어하거나 생성하는 데는 어려움이 있으며, 큰 모션 가이드에서는 시각적 아티팩트가 발생할 수 있습니다.

#### 5. 결론

본 논문에서는 여러 모션 도메인에서 일반적이고 컨트롤 가능한 이미지-to-비디오 애니메이션을 달성하기 위한 새로운 파이프라인을 제안하였습니다. 제안된 프레임워크를 통해 다양한 응용 분야에서 높은 품질의 비디오 생성이 가능합니다.

---

### 전체 요약

이 논문은 **Stable Video Diffusion (SVD)** 및 **MOFA-Adapter**를 사용하여 다양한 모션 도메인에서 컨트롤 가능한 이미지 애니메이션을 제안하고 테스트했습니다. 주요 기여는 다음과 같습니다:
- 하나의 통일된 프레임워크를 사용해 여러 도메인의 모션을 개별적으로 또는 공동으로 제어할 수 있는 새로운 방법을 제안함.
- 기술된 방법을 통해 사용자 선호도 및 성능 면에서 현존하는 대부분의 방법들을 능가함.
- 영상 생성 과정에서 발생할 수 있는 시각적 아티팩트와 같은 제한점도 명시됨.

이를 통해 AI 및 기계 학습 분야에서 고품질의 컨트롤 가능한 비디오 생성의 가능성을 열었으며, 관련 연구에 많은 기여를 할 수 있을 것으로 보입니다.