# AgentBench: Evaluating LLMs as Agents
## TL;DR
## Summary
- [https://arxiv.org/pdf/2308.03688.pdf](https://arxiv.org/pdf/2308.03688.pdf)

### 1. 섹션별 요약

#### 도입부 (Introduction)
이 논문은 AGENTBENCH라는 새로운 벤치마크를 제안합니다. AGENTBENCH는 대규모 언어 모델(LLM)을 에이전트로서 다양한 환경에서 평가하기 위해 설계되었습니다. 8개의 독립된 환경에서 AGENTBENCH는 LLM의 추론 능력과 의사 결정 능력을 평가합니다. 주요 목표는 상업용 API 모델과 오픈 소스 모델 간의 성능 격차를 밝히고, 보다 향상된 에이전트로서의 LLM 개발을 위한 방향을 제시하는 것입니다.

#### 요약 (Abstract)
이 논문은 LLM이 전통적인 자연어 처리(NLP) 작업을 넘어 현실 세계의 실제 과제를 수행하는 데 있어서 얼마나 잘 작동하는지를 평가하기 위해 AGENTBENCH를 소개합니다. 27개의 상업용 및 오픈 소스 LLM을 테스트한 결과, 상업 모델은 복잡한 환경에서 에이전트로서 뛰어난 능력을 보여주었지만, 오픈 소스 모델은 성능 면에서 큰 차이가 있음을 확인하였습니다.

#### 관련 연구 (Related Work)
기존의 LLM 평가 기준은 주로 한정된 작업에 중점을 두고 있으며, 열린 환경에서 다양한 상호작용을 하는 에이전트로서의 LLM 평가는 부족했습니다. AGENTBENCH는 이러한 한계를 극복하기 위해 설계되었습니다.

#### 방법론 (Methodology)
AGENTBENCH는 8개의 환경으로 구성되어 있으며, 다음 세 가지 유형으로 분류됩니다:
- 코드: 운영 체제, 데이터베이스, 지식 그래프
- 게임: 디지털 카드 게임, 횡단 퍼즐, 가사
- 웹: 웹 쇼핑, 웹 브라우징.

이 벤치마크는 LLM의 명령 따르기, 코딩, 지식 획득, 논리적 추론 및 상식 기반 능력을 체계적으로 평가합니다. 통합 평가 도구를 도입하여 다양한 커스터마이즈된 에이전트 작업에서 27개의 다른 LLM을 벤치마킹할 수 있습니다.

#### 결과 (Results)
실험 결과, 코딩과 높은 품질의 정렬 데이터 훈련이 LLM 에이전트의 성능을 향상시킬 수 있음을 발견했습니다. 또한, 상업용 LLM이 오픈 소스 모델보다 월등한 능력을 보여주었으며, 다양한 환경에서 에이전트로서 기능하는 데 상당한 잠재력을 보였습니다.

#### 논의 (Discussion)
상업용 LLM이 계획 수립, 계획 실행, 도구 호출 및 자기 반성 등에서 에이전트로서의 우수한 능력을 보여주었지만, 오픈 소스 모델은 이러한 능력이 부족하거나 일부분만 갖추고 있었습니다. 코드 훈련과 고품질의 다중 턴 정렬 데이터는 에이전트 성능을 향상시킬 수 있는 중요한 요소로 밝혀졌습니다.

#### 결론 (Conclusion)
AGENTBENCH는 LLM의 에이전트 평가를 위한 다차원적이고 진화하는 벤치마크로, 현실 세계의 도전에 대한 LLM의 대응 능력을 평가합니다. 오픈 소스 커뮤니티를 돕기 위해 관련 데이터셋과 환경이 포함된 통합 평가 패키지를 공개합니다.

### 2. 전체 요약
이 논문은 AI와 머신러닝 분야에서 AGENTBENCH라는 새로운 벤치마크를 제안합니다. 이는 대규모 언어 모델(LLM)을 다양한 실제 환경에서 평가하기 위해 설계된 최초의 종합적이고 다차원적인 벤치마크입니다. AGENTBENCH는 운영 체제, 데이터베이스, 지식 그래프 같은 코드 환경과 디지털 카드 게임, 횡단 퍼즐 등의 게임 환경, 그리고 웹 쇼핑, 웹 브라우징 같은 웹 환경 등 총 8개의 독립된 환경에서 LLM을 평가합니다.

논문은 AGENTBENCH를 통해 27개의 상업용 및 오픈 소스 LLM을 테스트하고, 상업용 모델이 오픈 소스 모델에 비해 우수한 성능을 보임을 입증합니다. 또한 장기간의 추론, 의사 결정 및 명령 따르기 능력이 현재의 LLM 에이전트 개발에서 주요 장애물임을 밝히고, 고품질의 다중 턴 정렬 데이터와 코드 훈련이 에이전트 성능 향상에 중요한 역할을 할 수 있음을 강조합니다. 

AGENTBENCH는 LLM을 에이전트로서 평가하는 데 필요한 통합 툴킷과 데이터셋을 제공하여 연구 커뮤니티가 보다 나은 LLM 에이전트를 개발할 수 있도록 지원합니다.