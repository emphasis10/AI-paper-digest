# V-Express: Conditional Dropout for Progressive Training of Portrait Video Generation
## TL;DR
## Summary
- [https://arxiv.org/pdf/2406.02511.pdf](https://arxiv.org/pdf/2406.02511.pdf)

### 각 섹션 요약

#### 1. 서론
포트레이트 비디오 생성 기술은 단일 이미지를 이용해 비디오를 생성하는 방법으로 발전해왔습니다. 이 과정에서 텍스트, 오디오, 참조 이미지, 포즈, 깊이 지도 등 다양한 제어 신호를 활용합니다. 그러나 이 신호들 중 약한 신호는 강한 신호에 의해 효과가 감소되는 문제가 있습니다. V-Express는 이러한 문제를 해결하기 위해 점진적 학습과 조건부 드롭아웃 기법을 사용하여 약한 신호도 효과적으로 활용할 수 있게 합니다.

#### 2. 방법
V-Express는 참조 이미지, 오디오, V-Kps 이미지 시퀀스를 통해 톡킹 헤드 비디오를 생성합니다. 참조 이미지는 배경과 얼굴 정체성을, 오디오는 입술 움직임을, V-Kps 이미지는 얼굴 위치와 포즈를 제어합니다. Latent Diffusion Model (LDM)을 사용하여 비디오 프레임을 생성하며, 점진적 학습과 조건부 드롭아웃을 통해 약한 신호의 영향을 강화합니다.

#### 2.1. 기초
V-Express는 LDM을 사용하여 비디오 프레임을 생성합니다. VAE 인코더로 입력 이미지를 잠재 공간으로 인코딩하고, 가우시안 노이즈를 점진적으로 추가하여 복원합니다. 학습 시 노이즈 제거 손실을 통해 모델을 최적화합니다.

#### 2.2. 모델 구조
V-Express의 백본은 SDv1.5의 구조를 따르며, 네 가지 주의 층을 포함합니다. 참조 이미지와 오디오, 비디오 프레임 간의 관계를 인코딩하는 주의 층과, 시간적 관계를 캡처하는 모션 주의 층이 포함됩니다. ReferenceNet, V-Kps Guider, Audio Projection 모듈을 통해 각각의 제어 입력을 인코딩합니다.

#### 2.3. 점진적 학습 전략
V-Express의 학습은 세 단계로 이루어집니다. 첫 번째 단계에서는 단일 프레임을, 두 번째와 세 번째 단계에서는 다중 프레임을 대상으로 합니다. 두 번째 단계에서는 오디오 투영과 관련 주의 층을 학습하고, 세 번째 단계에서는 모든 파라미터를 미세 조정합니다.

#### 2.4. 학습 요령
입 모양 동기화를 가속화하기 위해 입 부위의 노이즈 제거 손실 가중치를 증가시키고, 조건부 드롭아웃을 통해 강한 신호의 영향을 줄입니다.

#### 2.5. 추론
추론 시 주어진 오디오와 프레임 속도에 맞춰 비디오 프레임을 생성합니다. V-Kps 시퀀스는 주어진 프레임 길이에 맞게 선형 보간되며, 다중 세그먼트로 비디오 프레임이 생성됩니다. 프레임 간의 일관성을 유지하기 위해 모션 주의 층을 사용합니다.

#### 3. 실험
V-Express는 다양한 데이터셋으로 학습되며, PyTorch를 사용해 구현되었습니다. 실험 결과, V-Express는 품질과 제어 신호 정렬에서 우수한 성능을 보입니다.

#### 3.2. 정량적 비교
V-Express는 Wav2Lip 및 DiffusedHeads와 비교했을 때, 전반적인 비디오 품질과 제어 신호의 정렬에서 뛰어난 성능을 나타냅니다.

#### 3.3. 결과
V-Express는 오디오와 V-Kps를 효과적으로 제어하며 포트레이트 비디오를 생성합니다. 오디오 주의 층의 가중치를 조절하여 입 모양 움직임을 더욱 명확하게 할 수 있습니다.

#### 4. 결론
V-Express는 약한 신호와 강한 신호를 균형 있게 조절하여 고품질의 포트레이트 비디오를 생성합니다. 점진적 학습과 조건부 드롭아웃을 통해 약한 신호의 효과를 극대화하며, 다양한 제어 신호를 동시에 효과적으로 사용할 수 있습니다. 미래 연구 방향으로는 다국어 지원, 계산 부담 감소, 명시적 얼굴 속성 제어 등이 있습니다.

### 전체 요약
V-Express는 단일 이미지에서 포트레이트 비디오를 생성하는 데 있어 약한 제어 신호를 효과적으로 활용할 수 있도록 점진적 학습과 조건부 드롭아웃을 사용합니다. Latent Diffusion Model을 기반으로 한 이 방법은 참조 이미지, 오디오, V-Kps 이미지를 통해 얼굴 정체성, 입술 움직임, 얼굴 포즈를 제어하며, 실험 결과 높은 품질의 비디오를 생성하는 데 성공하였습니다. 이 모델은 다양한 강도의 제어 신호를 균형 있게 통합하는 데 뛰어나며, 다국어 지원 및 계산 효율성 향상 등 미래 연구 방향이 제시됩니다.