# Merging Improves Self-Critique Against Jailbreak Attacks
## TL;DR
## Summary
- [https://arxiv.org/pdf/2406.07188.pdf](https://arxiv.org/pdf/2406.07188.pdf)

### 종합 요약

**논문의 주요 기여 및 혁신 요소**
이 논문은 대형 언어 모델(LLM)의 보안을 향상시키기 위해 자체 비판 기능을 강화하고 합성 데이터를 통해 모델을 추가로 미세 조정하는 새로운 접근 방법을 제안합니다. 주된 혁신 요소는 외부 비평 모델을 도입하여 본래 모델과 병합해 자신의 응답을 재작성하여 유해하거나 불법적인 응답을 피할 수 있도록 한 것입니다. 이 방법은 인간의 라벨이 필요 없다는 점에서 실질적으로 다양한 상황에 배포할 수 있는 잠재력을 지니고 있습니다.

---

### 섹션별 요약

**1. 서론**
최근 대형 언어 모델(LLM)들은 다양한 텍스트 생성 작업에서 뛰어난 성능을 보여주었으나, 공격적으로 설계된 프롬프트나 'jailbreak' 공격에 취약한 문제를 가지고 있습니다. 이러한 공격은 LLM을 조작해 유해한 또는 부적절한 출력을 생성하도록 유도합니다. 본 논문은 LLM의 이러한 취약성을 보완하기 위해 단계적인 방어 프레임워크를 제안합니다.

**2. 배경 및 관련 연구**
최근까지도 LLM의 안전을 보장하기 위해 많은 연구가 이루어졌습니다. 그러나, 안전 훈련이 잘 된 모델도 'jailbreak' 공격에는 여전히 취약합니다. 기존 연구들은 대체로 대화 중 특정 인물 역할을 연기하게 하거나, 자동으로 해커 프롬프트를 추가하는 등 다양한 방식으로 안전 메커니즘을 우회해왔습니다. 본 연구는 이러한 문제를 해결하기 위해 자체 비판과 외부 비평 모델을 병합하여 보다 강력한 방어 메커니즘을 개발하고자 합니다.

**3. 방법론**
본 연구에서는 먼저 대화 응답의 안전성을 높이기 위해 자체 비판 템플릿을 도입하고, 다음으로 합성 데이터를 사용해 모델을 더욱 강화하는 병합 방식을 제안합니다. 구체적으로, 외부 비평 모델을 통한 비판과 재작성 단계를 거쳐 보다 안전한 응답을 생성하고, 이를 다시 모델에 학습시켜 공격에 대한 저항력을 높이는 방법을 사용합니다.

**4. 실험**
실험 결과, 제안한 방법은 다양한 'jailbreak' 프롬프트에 대한 공격 성공률을 크게 낮췄으며, 일반적인 작업에서의 성능 저하 없이 모델의 방어능력을 강화했습니다. 특히, 외부 비평 모델을 병합하여 자체 비판 기능을 개선한 모델은 기존의 단순한 대응 방법보다 훨씬 뛰어난 성과를 보였습니다.

**5. 결론**
이 논문은 모델 병합과 자체 비판을 활용하여 대형 언어 모델의 'jailbreak' 공격 저항력을 크게 향상시키는 새로운 접근방법을 제시합니다. 실험 결과, 이 방법은 모델의 일반 능력에 크게 영향을 미치지 않으면서도 방어능력을 향상시키는 것으로 나타났습니다. 향후 연구 방향으로는 미세 조정과 자동 공격 생성 방법의 결합 등이 제시되었습니다.