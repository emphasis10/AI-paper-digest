# MemServe: Context Caching for Disaggregated LLM Serving with Elastic Memory Pool
## TL;DR
## Summary
- [https://arxiv.org/pdf/2406.17565.pdf](https://arxiv.org/pdf/2406.17565.pdf)

### 1. Section Summaries

#### Abstract
- **요약**: 이 논문은 대규모 언어 모델(LLM) 서빙 시스템을 위해 MemServe라는 시스템을 제안했습니다. MemServe는 MemPool이라는 엘라스틱 메모리 풀이 있어, 인퍼런스 요청 간 최적화를 함께 운용할 수 있습니다. 실험 결과, MemServe가 작업 완료 시간(JCT)과 첫 토큰 생성 시간(TTFT)을 크게 단축시켰음을 보여줍니다.

#### Introduction
- **요약**: LLM 서빙 시스템의 주요 과제는 대규모 모델을 효율적으로, 저비용으로 서빙하는 것입니다. 이를 위해, 문맥 캐싱 및 분산 인퍼런스와 같은 다양한 최적화 기술이 제안되었습니다. 이 논문은 이러한 기술을 하나의 통합 시스템으로 결합한 MemServe를 소개합니다.

#### Background
- **요약**: LLM 인퍼런스는 입력 프롬프트를 처리하는 prefill 단계와 생성된 키-값(KV) 캐시를 사용하여 토큰을 생성하는 decode 단계로 나뉩니다. 두 단계 모두에서 의존성을 최적화하는 기법들이 제안되었으며, 각 기법은 LLM의 KV 캐시를 관리하고 전송하는 새로운 논리를 필요로 합니다.

#### MemServe Overview
- **요약**: MemServe는 글로벌 스케줄러, 다양한 유형의 인퍼런스 인스턴스, 그리고 MemPool이라는 엘라스틱 메모리 풀로 구성됩니다. MemPool은 메모리 할당, 인덱스 관리 및 분산 전송을 위한 API를 제공합니다. 이를 통해, 정규 및 분산 인퍼런스 아키텍처 위에 문맥 캐싱을 구축할 수 있습니다.

#### Elastic Memory Pool
- **요약**: MemPool은 CPU 드램 및 GPU HBM을 포함한 모든 클러스터 메모리를 관리합니다. MemPool은 로컬 인덱스 레이어를 통해 프롬프트 토큰을 KV 캐시에 매핑하여 캐시된 데이터의 효율적인 검색을 보장합니다. 다양한 메모리 조각화를 줄이며 데이터를 교환하는 효율적인 메커니즘을 제공합니다.

#### Caching for Disaggregated Inference
- **요약**: 기본 PD 설계부터 풀 플레지드 디자인까지의 네 가지 주요 설계 단계를 통해 문맥 캐싱을 포함한 분산 인퍼런스를 구축하는 방법을 설명합니다. 이 설계 단계는 prefill 및 decode 인스턴스에서 캐싱을 활성화하며, decode에서 prefill로 데이터 전송을 가능한 메커니즘을 제공합니다.

#### Evaluation
- **요약**: MemServe는 ShareGPT와 같은 실제 워크로드를 사용하여 평가되었습니다. 그 결과, PD-colocated 인스턴스에 비해 최대 42%의 작업 완료 시간(JCT) 개선을 보였습니다. 특히, 문맥 캐싱을 포함한 분산 인퍼런스는 최대 26.9%의 추가적인 성능 향상을 제공했습니다.

#### Conclusion
- **요약**: 이 논문은 LLM 서빙의 효율성을 높이기 위해 MemServe를 제안했습니다. MemPool을 통해 문맥 캐싱 및 분산 인퍼런스의 조합을 실현하였으며, 실험 결과에서 큰 성능 향상을 달성했습니다.

### 2. Overall Summary

논문의 주요 기여 및 혁신적인 부분은 MemServe라는 통합 시스템을 도입하여 LLM 서빙을 최적화한 것입니다. MemServe는 MemPool이라는 엘라스틱 메모리 풀이 있어, 다양한 최적화 기법을 하나의 시스템에서 관리할 수 있습니다. 이를 통해 문맥 캐싱과 분산 인퍼런스를 동시에 활용하는 첫 사례를 제공하며, 글로벌 스케줄러와 로컬리티 인지 정책을 사용해 KV 캐시 재사용을 극대화합니다. 실험 결과에서 작업 완료 시간(JCT)과 첫 토큰 생성 시간(TTFT)를 크게 향상시켜, LLM 서빙의 효율성을 크게 증대시켰습니다.

이 논문을 통해 AI 및 기계 학습 분야에서 대규모 언어 모델의 효율적인 서빙 방법을 이해하고 적용할 수 있는 기회가 주어지며, 향후 연구와 실무 적용에서 중요한 참고 자료가 될 것입니다.