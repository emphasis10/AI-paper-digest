# Linear Correlation in LM's Compositional Generalization and Hallucination
## TL;DR
## Summary
- [https://arxiv.org/pdf/2502.04520.pdf](https://arxiv.org/pdf/2502.04520.pdf)

1. **각 섹션의 중요 내용 요약 (한국어)**

   - **서론**  
     이 논문은 언어 모델(LM)의 지식 구성에서 선형 상관 관계를 밝혀내는 데 주안점을 두고 있습니다. 선형 변환이 특정 지식 쌍 간의 다음 토큰 예측 로그잇을 서로 연결할 수 있음을 보여줍니다. 이 연구는 LM의 일반화 능력이 실제 관계와 일치할 때 지식을 업데이트하고, 반대로 이러한 관계가 deviating할 때는 환각(hallucination)을 유발함을 지적합니다.

   - **관련 연구**  
     LM의 동작 메커니즘과 관련된 다양한 연구들이 언급되며, LM의 블랙박스 특성이 인간 이해를 어떻게 저해하는지를 다룹니다. 또한, LM이 지식을 암호화하는 방식에 대해 설명하며 선형성의 역할을 강조합니다.

   - **선형 상관 관계 발견**  
     LM에서 특정 관련 NTP(Next Token Prediction) 간의 선형 관계를 발견하는 방법이 세부적으로 설명됩니다. 이 연구는 다양한 입력에 대해 NM의 로그잇 쌍을 통해 선형 변환을 학습하고, 이를 평가하는 실험 결과를 제시합니다.

   - **훈련에 대한 강건한 상관 관계**  
     선형 상관 관계가 훈련 중에도 유지됨을 보여주고, 이를 통해 LM의 일반화 능력이 어떻게 작용하는지를 설명합니다. W(변환 행렬)가 지식 관계를 얼마나 잘 반영하는지를 조사하고, 이러한 정보가 LM에 의해 어떻게 해석되는지를 제시합니다.

   - **결론**  
     논문은 LM의 지식 조합 메커니즘에 대한 새로운 통찰력을 제공하며, LM의 일반화 능력을 개선하기 위한 방향성을 제시합니다. 이 연구는 LM이 지식을 구성하는 데 선형성을 활용할 수 있는 가능성을 강조합니다.

   - **주요 기여 및 혁신**  
     1) LM 출력 로그잇 간의 선형 상관 관계를 발견함.
     2) 이 선형 상관 관계가 훈련에 강건하게 유지됨을 밝혀내어 조성 일반화(compositional generalization) 및 환각과 연결됨을 제시.
     3) 선형성을 구성하는 데는 어휘 표현(vocabulary representation)이 주요 역할을 한다는 것을 입증.

2. **전체 요약 (한국어)**  
이 논문은 언어 모델의 지식 구성 동작을 선형 상관 관계에 기반하여 분석합니다. 특정 지식 쌍 간의 로그잇 변환을 통해 LM이 어떻게 지식을 조합하는지를 설명하며, 이러한 조합이 일반화와 환각에 미치는 영향을 밝힙니다. 연구는 LM의 작동 원리를 이해하고, 지식의 일관성을 개선하는 방법을 제시합니다. LM이 현실의 관계를 잘 반영할 때는 정상적으로 작동하지만, 관계가 어긋날 경우에는 환각이 발생함을 강조합니다. 논문의 기여는 선형 상관 관계가 LM의 일반화 특성을 이해하는 데 중요한 질문을 제기한다는 점에서 의미가 있습니다.