# CRAG -- Comprehensive RAG Benchmark
## TL;DR
## Summary
- [https://arxiv.org/pdf/2406.04744.pdf](https://arxiv.org/pdf/2406.04744.pdf)

### 요약 (Summary)

#### 1. 각 섹션 요약
- **Introduction (소개)**
  - 대형 언어 모델(LLM)이 QA 작업에서의 문제점인 허상(hallucination)을 해결하기 위해 검색 기반 생성(RAG)이 제안됨. --> 현실적인 벤치마크인 CRAG를 소개함으로써 이 문제를 해결하고자 함.  .

- **Problem Description (문제 설명)**
  - 문제에 대한 명확한 정의를 제공하고, QA 시스템이 외부 소스로부터 정보를 검색하여 답변을 생성하는 과정을 설명함.  .

- **Dataset Description (데이터셋 설명)**
  - 다양한 도메인과 질문 유형을 포함한 4409개의 QA 쌍을 수집하고, 웹 및 지식 그래프(KG) 검색을 모의한 API를 제공함.  .

- **Question Answering Pairs (질문 응답 쌍)**
  - QA 쌍의 생성 과정을 설명하며, 다양한 엔티티의 인기도와 시계열성을 반영하여 현실적인 질문을 구성함.  .

- **Contents for Retrieval (검색용 콘텐츠)**
  - 검색용 콘텐츠로 웹페이지와 모의 지식 그래프(KG) 데이터를 포함하여 검색 시뮬레이션을 현실감 있게 구성함.  .

- **Metrics and Evaluation (메트릭 및 평가)**
  - 정확도(Accuracy), 허상율(Hallucination rate), 누락율(Missing rate), 점수(Score) 각 측정 지표의 정의와 평가 방법을 설명함.  .

- **Benchmarking (벤치마킹)**
  - 최첨단(SOTA) RAG 솔루션과 기존 LLM 솔루션의 성능을 비교하여 문제점과 개선 방향을 제시함.  .

- **Conclusion (결론)**
  - CRAG 벤치마크의 기여와 앞으로의 연구 방향에 대해 논의함. 다국어 질문, 다중 모드 질문, 다중 턴 대화 등 확장 계획을 제시함.  .

#### 2. 전체 요약
이 논문은 대형 언어 모델(LLM)의 대표적인 문제인 '허상'을 해결하기 위해 검색 기반 생성(RAG)을 제안하고, 이를 평가하기 위한 현실적이고 포괄적인 벤치마크인 CRAG를 소개합니다. CRAG 벤치마크는 4409개의 질문-응답 쌍을 포함하고 있으며, 다양한 도메인과 질문 유형을 반영합니다. 벤치마크 평가 결과, 기존의 단순한 RAG 솔루션이 정확도에서 일부 개선을 보였지만, 여전히 허상의 문제를 완전히 해결하지 못함을 보여줍니다. 이 논문은 CRAG를 통해 현재 RAG 연구의 문제점과 향후 개선 방향을 제시함으로써, 신뢰성 있는 QA 시스템 개발에 기여하고자 합니다.