# Depth-Adaptive Transformer
## TL;DR
## Summary
- [https://arxiv.org/pdf/1910.10073.pdf](https://arxiv.org/pdf/1910.10073.pdf)

### 1. 섹션별 요약

#### 1.1. 소개 (Introduction)
현대의 시퀀스-투-시퀀스 모델들은 입력 시퀀스의 난이도에 관계없이 고정된 양의 연산을 수행합니다. 본 논문에서는 단일 입력 시퀀스에 대해 가변적인 레이어 수를 적용하여 처리 속도와 정확도 간의 균형을 맞추는 Tranformers를 제안합니다. 기존의 유니버설 트랜스포머는 동일한 레이어를 반복적으로 적용하지만, 우리는 다양한 레이어를 단계별로 적용하여 더 효과적인 계층 깊이 예측을 진행합니다.

#### 1.2. 항시 구조적 예측 (Anytime Structured Prediction)
컴퓨터 비전 모델에서 언제든지 예측할 수 있는 모델을 구조적 예측에도 확장하였습니다. 트랜스포머 모델을 기반으로 하여 각 단계에서 서로 다른 출력 분류기를 활용해 예측을 수행하고 블록이 언제 멈출지 결정하는 메커니즘을 추가했습니다. 이 접근법을 통해 다양한 탈출 지점에서 예측을 수행할 수 있게 됩니다.

#### 1.3. 적응적 깊이 추정 (Adaptive Depth Estimation)
시퀀스 특정 깊이와 토큰 특정 깊이 두 가지 접근법을 사용하여 디코더 블록의 탈출을 예측합니다. 시퀀스 특정 깊이에서는 전체 출력 시퀀스에 동일한 블록을 사용하고, 토큰 특정 깊이에서는 매 토큰마다 다르게 탈출 지점을 설정합니다. 두 접근법 모두 정규화 항을 사용하여 빠른 탈출을 유도합니다.

#### 1.4. 실험 (Experiments)
IWSLT14 독일어-영어와 WMT14 영어-프랑스어 번역 작업에서 기존의 잘 튜닝된 트랜스포머 모델 대비 최대 76% 적은 디코더 레이어로 동일한 성능을 달성할 수 있음을 실험을 통해 입증했습니다. 다양한 하이퍼파라미터 실험을 통해 최고 성능을 달성하는 방법을 탐구했습니다.

#### 1.5. 결론 (Conclusion)
본 연구에서는 언제든지 예측할 수 있는 방식을 구조적 예측에 적용하여, 간단하면서도 효과적인 방법을 통해 시퀀스-투-시퀀스 모델이 네트워크의 다양한 지점에서 예측할 수 있도록 했습니다. 정확도와 속도 간의 무역 오프에서 가장 좋은 성능을 보이는 간단한 정규화 기반 기하학적 분류기를 찾았습니다.

### 2. 전체 요약
이 논문은 입력 시퀀스의 난이도에 따라 변하는 적응적 연산을 통해 효율적인 트랜스포머 모델을 제안합니다. 기존의 고정된 레이어 수를 사용하는 접근법에서 벗어나 각 단계에서 다른 레이어를 적용함으로써, 처리 속도를 희생하지 않고도 높은 정확도를 유지할 수 있습니다. 또한 언제든지 예측 가능한 모델을 구조적 예측을 위해 확장하였으며, 다양한 메커니즘을 통해 계층 깊이를 예측하고 실험을 통해 그 효율성을 입증하였습니다. 이는 최적의 성능을 유지하면서 연산 비용을 획기적으로 줄여주는 혁신적인 접근법입니다.

## Similar Papers
- [Mixture-of-Depths: Dynamically allocating compute in transformer-based language models](2404.02258.md)
- [Attention Is All You Need](1706.03762.md)
- [Griffin: Mixing Gated Linear Recurrences with Local Attention for Efficient Language Models](2402.19427.md)
- [Accurate Knowledge Distillation with n-best Reranking](2305.12057.md)
- [DenseFormer: Enhancing Information Flow in Transformers via Depth Weighted Averaging](2402.02622.md)
- [SPEED: Speculative Pipelined Execution for Efficient Decoding](2310.12072.md)
- [ALBERT: A Lite BERT for Self-supervised Learning of Language Representations](1909.11942.md)
- [Fast Inference from Transformers via Speculative Decoding](2211.17192.md)
- [Full Stack Optimization of Transformer Inference: a Survey](2302.14017.md)
