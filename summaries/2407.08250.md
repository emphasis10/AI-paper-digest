# Gradient Boosting Reinforcement Learning
## TL;DR
## Summary
- [https://arxiv.org/pdf/2407.08250.pdf](https://arxiv.org/pdf/2407.08250.pdf)

### 1. 섹션별 요약

#### 1.1. 서론 (Introduction)
이 논문에서는 Gradient-Boosting Reinforcement Learning (GBRL) 프레임워크에 대해 소개합니다. 이 프레임워크는 Gradient Boosting Trees (GBT)의 장점을 강화학습 (Reinforcement Learning, RL) 도메인으로 확장합니다. GBT는 해석 용이성, 범주형 데이터 처리, 경량 구현 등 슈퍼바이즈드 러닝에서 강력한 성능을 보이는 반면, 강화학습에서는 제한적으로 사용되어 왔습니다. 본 연구에서는 GBT를 RL 도메인에 적용하여 다양한 액터-크리틱 (actor-critic) 알고리즘을 구현하고, GBT의 경쟁력을 입증합니다.

#### 1.2. 관련 연구 (Related Work)
GBT는 슈퍼바이즈드 러닝에서 광범위하게 사용되어 왔으며, 최근에는 순위 문제, 불확실성 정량화, 그래프 신경망과의 조합 등 다양한 분야로 확장되었습니다. 하지만 RL 도메인에서는 아직 상대적으로 적게 활용되고 있습니다. 이전 연구에서는 GBT를 Q-learning 등의 오프-폴리시(on-policy) RL 방법론에 적용한 바 있습니다.

#### 1.3. Gradient Boosting Trees의 작동 원리
GBT는 결정 트리 앙상블과 함수적 경사 하강법을 결합한 비모수적(non-parametric) 머신러닝 기법입니다. GBT는 반복적으로 손실을 최소화하며 학습하며, 각 반복마다 이전 모델에 새 트리를 추가하여 성능을 향상시킵니다.

#### 1.4. Gradient Boosting Reinforcement Learning (GBRL)
본 연구는 GBT 프레임워크를 RL의 액터-크리틱 알고리즘에 확장 적용합니다. 이를 통해 AC 파라미터, 정책 및 가치 함수 등을 최적화하며, 특히 구조적 혹은 범주형 특성이 있는 도메인에서 높은 경쟁력을 보입니다.

#### 1.5. 실험 (Experiments)
GBT 기반의 AC 알고리즘 (A2C, PPO, AWR)을 구현하고, 이를 NN 기반과 비교 실험합니다. 실험은 클래식 제어 작업, 고차원 벡터화 문제, 범주형 작업 등 다양한 RL 영역에서 진행되었습니다. GBT는 특히 범주형 작업에서 우수한 성능을 보였으며, NN과 비교해 여러 도메인에서 경쟁력을 입증했습니다.

#### 1.6. 한계 및 미래 방향 (Limitations and Future Directions)
GBRL은 초기 단계이므로 한계가 존재합니다. 정책 업데이트를 통해 트리 앙상블의 크기가 비약적으로 증가해 메모리 사용과 계산 효율성에 문제를 유발할 수 있습니다. 이를 해결하기 위해 트리 가지치기, 앙상블 압축 등의 최적화 전략이 필요합니다. 또한, GBT의 비미분 특성으로 인해 최신 RL 알고리즘들과의 통합에 어려움이 있습니다.

### 2. 전체 요약
이 논문에서는 Gradient Boosting Trees (GBT)의 장점을 강화학습(RL) 도메인으로 확장한 Gradient-Boosting Reinforcement Learning (GBRL) 프레임워크를 제안합니다. 기존의 신경망 기반 RL 알고리즘이 해석 용이성, 범주형 데이터 처리, 경량 구현 등의 문제를 겪는 반면, GBT는 이러한 문제를 자연스럽게 해결합니다. GBRL을 적용해 여러 액터-크리틱 알고리즘(A2C, PPO, AWR)을 구현하고, 다양한 RL 작업에서 신경망 기반 알고리즘과 성능을 비교 실험하였습니다. 특히 구조적 데이터나 범주형 데이터를 포함한 도메인에서 GBT의 장점을 입증하였으며, GPU 가속을 통해 대규모 RL 문제에서도 효율적인 성능을 보였습니다. 이러한 연구는 RL에서 더 해석 가능하고 경량화된 솔루션을 제공하여, 실세계 애플리케이션에 크게 기여할 수 있을 것입니다.

