# Inference Performance Optimization for Large Language Models on CPUs
## TL;DR
## Summary
- [https://arxiv.org/pdf/2407.07304.pdf](https://arxiv.org/pdf/2407.07304.pdf)

### 섹션별 요약 및 주요 기여사항

#### 1. 서론
이 논문은 대형 언어 모델(LLM)의 성능을 CPU 기반으로 최적화하는 방법을 제안합니다. GPU 자원이 제한된 상황에서 CPU를 활용하는 방안을 논의하고, 이를 통해 실용적인 애플리케이션에서 LLM을 보다 효율적으로 배포할 수 있도록 설계되었습니다.

#### 2. 접근법
논문에서는 세 가지 주요 최적화 접근법을 소개합니다:
1. **LLM 최적화**: 각 LLM 연산 및 레이어에 대해 개별 최적화 솔루션을 제안합니다. 특히, SlimAttention이라는 새로운 주의 메커니즘을 도입하여 메모리 사용량을 줄이고 계산 효율성을 높였습니다.
2. **효율적인 KV 캐시 최적화**: INT8 KV 캐시 접근 방식을 통해 메모리 사용 효율성을 개선하고, 이를 통해 모델 출력 품질을 크게 저하시키지 않으면서 메모리 사용을 최적화했습니다.
3. **분산 추론 최적화**: oneAPI Collective Communications Library(oneCCL)을 활용해 분산 추론 최적화 솔루션을 구현했습니다. 이를 통해 데이터 복사를 최소화하고 고속으로 추론을 수행할 수 있도록 설계되었습니다.

#### 3. 실험 결과
- **Llama2-70B 모델의 경우**: 제안한 분산 솔루션을 통해 여러 개의 소켓에서 Latency가 크게 개선되었습니다.
- **SlimAttention vs FlashAttention**: SlimAttention이 CPU에서 더 나은 성능을 보여줍니다. 다양한 토큰 크기에 대한 성능 비교 결과 SlimAttention이 더 효율적임을 확인할 수 있었습니다.

#### 4. 결론
이 연구는 CPU에서 LLM의 효율적인 추론 성능을 최적화하는 솔루션을 제안합니다. 앞으로 더 다양한 CPU 환경에서 추가적인 연구를 통해 솔루션을 확장하고, 최신 모델에 대한 최적화 방안을 모색할 계획입니다. 기존의 GPU 솔루션에 대한 실질적인 대안을 제공하기 위한 목표를 가지고 있습니다.

### 전반적인 요약
이 논문에서는 대형 언어 모델(LLM)을 CPU에서 효율적으로 배포하고 최적화하는 방법을 제안합니다. 주요 기여는 SlimAttention과 같은 새로운 주의 메커니즘 도입, INT8 KV 캐시 최적화, 그리고 분산 추론 솔루션 구현입니다. 이를 통해 메모리 사용과 계산 효율을 크게 개선할 수 있습니다. 실험 결과는 제안된 방법이 다양한 모델에서 우수한 성능을 발휘함을 보여줍니다. 이 연구는 GPU 자원이 제한된 환경에서 CPU를 활용한 대안 솔루션을 제공하는 데 중요한 기여를 합니다.

## Similar Papers
- [Efficient LLM Inference on CPUs](2311.00502.md)
- [LLM in a flash: Efficient Large Language Model Inference with Limited Memory](2312.11514.md)
- [The Impact of Hyperparameters on Large Language Model Inference Performance: An Evaluation of vLLM and HuggingFace Pipelines](2408.01050.md)
- [FlexGen: High-Throughput Generative Inference of Large Language Models with a Single GPU](2303.06865.md)
- [LLoCO: Learning Long Contexts Offline](2404.07979.md)
- [AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration](2306.00978.md)
- [Boosting Large-scale Parallel Training Efficiency with C4: A Communication-Driven Approach](2406.04594.md)
- [TriForce: Lossless Acceleration of Long Sequence Generation with Hierarchical Speculative Decoding](2404.11912.md)
- [Searching for Best Practices in Retrieval-Augmented Generation](2407.01219.md)
