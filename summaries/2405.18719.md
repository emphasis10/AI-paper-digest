# Contextual Position Encoding: Learning to Count What's Important
## TL;DR
## Summary
- [https://arxiv.org/pdf/2405.18719.pdf](https://arxiv.org/pdf/2405.18719.pdf)

### 논문 요약

#### 1. 각 섹션 요약

**Introduction**:
이 논문은 대규모 언어 모델(LLM)의 중요한 구성 요소인 어텐션 메커니즘이 순서에 무관한 특성을 가지므로, 위치 정보 인코딩(Position Encoding, PE)이 필요하다고 설명합니다. 현재의 PE 방법들은 토큰 개수를 이용해 위치를 결정하며, 이는 문장이나 단락과 같은 더 높은 수준의 추상화를 일반화할 수 없음을 언급합니다. 새로운 위치 인코딩 방법인 **Contextual Position Encoding(CoPE)**를 제안하여, 모델이 특정 토큰에 위치를 조건화할 수 있도록 합니다. CoPE는 기존 PE 방법들이 실패하는 선택적 복사, 카운팅 및 Flip-Flop 과제를 해결할 수 있으며, 언어 모델링 및 코딩 작업에서 개선된 성능을 보여줍니다.

**Background on Position Encoding**:
PE는 절대 위치 인코딩과 상대 위치 인코딩으로 나눌 수 있습니다. 절대 PE는 각 토큰에 고정된 위치 벡터를 할당하며, 상대 PE는 현재 토큰에서 이전 토큰까지의 거리를 계산하여 상대적인 위치 정보를 가지고 있습니다. 이러한 PE 방법들은 일반적으로 토큰의 위치를 기반으로 하지만, 토큰의 위치는 문장이나 단어와 같은 의미적으로 중요한 단위와 일치하지 않을 수 있음을 설명합니다.

**Motivation for Contextual Position Encoding**:
표준 PE 방법은 단순 토이(task)에서 실패할 수 있습니다. 예를 들어 긴 문장에서 마지막 'x' 토큰을 찾는 것은 여러 'y' 토큰 사이의 위치 정보를 제대로 반영하지 못해 어려울 수 있습니다. CoPE는 토큰의 컨텍스트에 따라 위치를 측정하여 이러한 문제를 해결합니다.

**Contextual Position Encoding**:
CoPE는 주어진 토큰의 컨텍스트 벡터를 사용해 각 이전 토큰에 대해 게이트 값을 계산합니다. 이 게이트 값을 통해 상대 위치를 계산하고, 정수 값을 할당하여 위치 벡터를 생성합니다. CoPE는 여러 유닛 단위로 거리를 동시에 측정할 수 있어 기존 PE 방법보다 더 유연합니다.

**Experiments**:
- **Flip-Flop Task**: 다양한 PE 방법과 비교하여 CoPE는 인-도메인(In-Domain) 및 아웃-도메인(Out-of-Domain) 테스트에서 더 나은 성능을 보여줍니다.
- **Selective Copy Task**: CoPE는 선택적 복사 작업에서도 기존 PE 방법보다 뛰어난 성능을 보이며, 특히 OOD 상황에서 큰 차이를 보입니다.
- **Counting Task**: 여러 변수를 가진 상황에서도 CoPE는 상대 PE보다 우수한 성능을 발휘합니다.
- **Language Modeling**: Wikipedia 텍스트를 사용한 언어 모델링 작업에서 CoPE는 다른 PE 방법보다 낮은 비용으로 더 나은 퍼포먼스를 보여줍니다.
- **Code Modeling**: 코드 데이터를 사용한 실험에서도 CoPE는 더 낮은 혼란도(perplexity)를 보이며, RoPE와 결합 시 더 좋은 성능을 보였으나 CoPE 단독 사용만큼은 아니었습니다.

**Related Work**:
PE는 처음에는 RNN 기반의 모델에서 필요하지 않았으나, Transformer와 같은 모델이 인기를 얻으면서 중요한 연구 주제가 되었습니다. 기존 연구들은 주로 상대 PE와 절대 PE의 변형에 초점을 맞추고 있으며, CoPE는 컨텍스트 의존적 위치를 사용하여 기존 연구와 차별화됩니다.

**Conclusion**:
논문에서는 CoPE가 텍스트 및 코드 도메인에서 더 나은 성능을 발휘하며, 비디오 및 음성과 같이 위치 정보가 덜 직관적인 도메인에서도 잠재적으로 성능을 개선할 수 있는 가능성을 제안합니다. CoPE는 더 큰 모델에서도 좋은 성능을 보일 수 있으며, 추가적인 연구가 필요합니다.

#### 2. 전체 요약

이 논문은 새로운 위치 인코딩 방법인 **Contextual Position Encoding(CoPE)**를 제안합니다. CoPE는 기존의 위치 인코딩 방법들이 실패하는 여러 과제를 성공적으로 해결할 수 있으며, 텍스트, 코드 등 다양한 도메인에서 우수한 성능을 보입니다. 주요 기여는 컨텍스트 의존적 위치를 사용함으로써 기존의 토큰 기반 위치 인코딩 방법들보다 더 유연하고 효과적인 위치 계산을 가능하게 만든 것입니다. CoPE는 학습이 쉽고 성능이 뛰어나며, 대규모 모델에서도 좋은 성능을 발휘할 잠재력을 가지고 있습니다.