# Long Context Transfer from Language to Vision
## TL;DR
## Summary
- [https://arxiv.org/pdf/2406.16852.pdf](https://arxiv.org/pdf/2406.16852.pdf)

### 1. 문서 요약 - 한국어

#### Abstract
이 논문은 장기간의 비디오를 이해하는 대형 멀티모달 모델(LMM)의 문제를 해결하려고 합니다. 우리는 언어 모델의 컨텍스트 길이를 확장하여 이를 시각 입력과 정렬함으로써 LMM의 비디오 처리 능력을 크게 향상시켰습니다. 이를 통해 LongVA라는 모델을 개발하였으며, 이 모델은 2000프레임 이상의 정보를 처리할 수 있으며, Video-MME 데이터셋에서 최첨단 성능을 달성했습니다. 또한, 우리는 V-NIAH라는 벤치마크를 도입하여 매우 긴 컨텍스트의 시각 정보를 효과적으로 측정할 수 있게 했습니다.

#### Introduction
이 논문은 멀티모달 모델(Multi-modal Models, LMMs)의 최적화와 관련된 내용입니다. 특히, 기존의 LMM이 장기간의 비디오 데이터를 이해하지 못하는 문제를 해결하기 위해, 우리는 LMM의 언어 모델의 컨텍스트 길이를 확장하는 방식을 채택했습니다. 이를 통해 시각적 컨텍스트 길이가 확장된 모델인 LongVA를 개발하였으며, Video-MME 데이터셋에서 뛰어난 성능을 보였습니다.

#### Related Work
기존 연구들은 주로 시각적 피처를 추출하고 언어 모델에 주입하는 다양한 아키텍처를 탐구하고 있습니다. Flamingo를 포함한 여러 연구들이 비주얼 피처를 압축하는 리샘플러를 채택하며, 이와 다르게 LLaVA 시리즈는 간단하고 확장 가능한 디자인을 사용하여 이미지 피처를 직접 언어 모델에 투영합니다. 우리 연구는 이러한 접근법들과는 다른 컨텍스트 전송 개념을 제안하여, 장기간의 비디오 데이터를 더 효과적으로 처리할 수 있게 하였습니다.

#### Long Context Transfer
우리는 언어 모델의 긴 컨텍스트가 멀티모달 모델에 직접 전송될 수 있는 '긴 컨텍스트 전송' 현상을 발견했습니다. 이를 통해 모델은 더 많은 프레임을 처리할 수 있게 되었으며, V-NIAH 벤치마크를 통해 모델의 성능을 측정했습니다. 이 접근법은 짧은 시각 데이터(이미지)로 훈련을 진행하면서도 긴 비디오를 제로샷으로 이해할 수 있도록 합니다.

#### Long Video Assistant (LongVA)
우리는 LongVA라는 모델을 개발했습니다. 이 모델은 언어 모델의 긴 컨텍스트와 UniRes라는 통합 인코딩 방식을 결합하여 200K 이상의 비주얼 토큰을 인식할 수 있습니다. Video-MME 데이터셋에서 LongVA는 최첨단 성능을 발휘하였습니다. 특히, 더 많은 프레임을 샘플링하였을 때 성능이 크게 향상되는 것을 확인할 수 있었습니다.

#### Conclusion
이 연구는 LMM이 장기간의 비디오를 이해하는 능력을 크게 향상시킬 수 있는 새로운 접근법을 제안합니다. 우리는 언어 모델의 컨텍스트 길이를 확장하고 이를 시각적 피처와 정렬하는 방식으로, LMM의 장기간의 비디오 처리 능력을 크게 향상시켰습니다. LongVA는 Video-MME 데이터셋에서 최첨단 성능을 달성하였으며, V-NIAH 벤치마크를 통해 시각적 컨텍스트 길이를 측정할 수 있게 했습니다. 이 연구는 향후 멀티모달 에이전트 개발에 큰 영향을 미칠 것으로 기대됩니다.

### 2. 문서 전체 요약
이 논문은 기존의 대형 멀티모달 모델이 장기간의 비디오를 효과적으로 이해하지 못하는 문제를 해결하기 위해, 언어 모델의 컨텍스트 길이를 확장하여 시각적 입력과 정렬하는 방식을 제안하고 있습니다. 이를 통해 LongVA라는 모델을 개발하였으며, 이 모델은 2000프레임 이상의 비주얼 데이터를 처리할 수 있습니다. Video-MME 데이터셋에서 최첨단 성능을 달성하였고, V-NIAH라는 새로운 벤치마크를 도입하여 모델의 성능을 측정하였습니다. 이 연구는 멀티모달 에이전트가 장기간의 비디오 데이터를 더 효과적으로 이해할 수 있게 하는 중요한 기여를 하고 있습니다.