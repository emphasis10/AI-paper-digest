# MobileLLM: Optimizing Sub-billion Parameter Language Models for On-Device Use Cases
## TL;DR
## Summary
- [https://arxiv.org/pdf/2402.14905.pdf](https://arxiv.org/pdf/2402.14905.pdf)

### 각 섹션 요약

#### 1. Introduction
이 논문은 모바일 장치에서 사용 가능한 서브-빌리언 파라미터 언어 모델(LLM)의 최적화를 다룹니다. 현재의 모델들은 클라우드 비용 및 지연 문제 때문에 모바일 환경에서 비효율적입니다. 이 논문은 데이터 및 파라미터의 양이 모델 품질을 결정짓는다는 기존 믿음에 반하여, 서브-빌리언 규모의 LLM에서는 모델 아키텍처가 더 중요함을 강조합니다. MobileLLM은 얇고 깊은 아키텍처를 채택하고, 임베딩 공유 및 그룹화된 쿼리 주의 메커니즘을 통해 기존 125M/350M 모델에 비해 2.7%/4.3% 정확도 향상을 이루었습니다.

#### 2. Improving Sub-billion Scale LLM Design
이 섹션에서는 서브-빌리언 파라미터 모델의 설계 개선 과정을 다룹니다. MobileLLM은 다양한 모델 설계 기법을 통해 높은 성능을 달성했으며, 여기에는 SwiGLU FFN 채택, 얇고 깊은 아키텍처 강제, 임베딩 공유, 그룹화된 쿼리 주의가 포함됩니다. 또한 블록 단위 가중치 공유 방법을 제안하여 추가적인 메모리 오버헤드 없이 성능을 향상시켰습니다.

#### 3. Training Setup
실험은 32개의 A100 GPU에서 진행되었으며, 각 GPU당 배치 크기는 32입니다. 120k 반복을 통해 탐색적 실험을 수행한 후, 상위 모델들은 1T 토큰으로 480k 반복을 통해 훈련되었습니다.

#### 4. Related Work
LLM의 우수한 성능은 널리 응용되고 있으며, 컴퓨팅 비용 및 에너지 소비를 고려한 모델 축소 연구가 증가하고 있습니다. 모델 압축, 작은 모델 디자인, 효율적인 주의 계산, 하드웨어 스케줄링 및 가중치 이동 최적화 등 다양한 접근법이 연구되고 있습니다.

#### 5. Conclusion
이 연구는 모바일 장치에서의 적용을 위해 서브-빌리언 규모 모델을 최적화하는 것을 목표로 합니다. 작은 모델의 경우 깊이를 우선시하는 것이 성능을 향상시킴을 발견했으며, 고급 가중치 공유 기술을 통해 메모리 제한 상황에서 성능을 극대화했습니다. MobileLLM 모델은 기존 최고 성능 방법보다 뛰어난 성능을 보였으며, 특히 채팅 및 API 호출 같은 일반적인 온-디바이스 사용 사례에서 우수한 성능을 입증했습니다.

### 주요 기여 및 혁신 부분 요약

이 논문의 주요 기여는 다음과 같습니다:
1. 깊이 우선 접근법을 통해 작은 LLM에서 성능을 극대화.
2. 임베딩 공유 및 그룹화된 쿼리 주의 메커니즘을 적용하여 가중치 활용 극대화.
3. 블록 단위 가중치 공유 방법을 통해 추가적인 메모리 오버헤드 없이 성능 향상.
4. MobileLLM 모델이 채팅 및 API 호출 등 다양한 온-디바이스 사용 사례에서 높은 정확도를 보임.

### 전체 요약

이 논문은 모바일 환경에서 사용 가능한 서브-빌리언 파라미터 언어 모델의 최적화를 목표로 합니다. 얇고 깊은 아키텍처와 임베딩 공유, 그룹화된 쿼리 주의 메커니즘을 통해 MobileLLM 모델은 기존 모델보다 뛰어난 성능을 보였습니다. 또한 블록 단위 가중치 공유를 통해 메모리 오버헤드를 최소화하면서 성능을 향상시켰습니다. 이 논문의 결과는 모바일 장치에서의 LLM 사용을 촉진하고, 클라우드 비용 및 지연 문제를 해결하는 데 기여할 것입니다.