# Iterative Reasoning Preference Optimization
## TL;DR
## Summary
- [https://arxiv.org/pdf/2404.19733.pdf](https://arxiv.org/pdf/2404.19733.pdf)

이 연구 논문에서는 언어 모델을 활용하여 반복적인 추론 선호 최적화 방법을 개발하고 있습니다. 이 방식은 생성된 생각의 사슬(Chain-of-Thought, CoT) 후보들 사이의 선호를 최적화하여, 올바른 답변으로 이어지는 추론 단계를 선택하는 방법에 초점을 맞춥니다.

### 주요 내용 요약

1. **서론 및 배경**:
   - 추론 작업에 적합한 언어 모델 훈련에 기존의 선호 최적화 방법들이 적용되어 왔으나, 이러한 방법들은 일반적인 지시 사항 조정 작업에는 효과적이지만 표준 추론 작업에서는 큰 개선을 이루지 못했습니다. 본 연구는 반복적인 추론 선호 최적화 방법을 통해 이러한 문제를 해결하고자 합니다.

2. **반복적 추론 선호 최적화 방법**:
   - 연구팀은 DPO(Direct Preference Optimization) 손실 함수에 부정 로그 가능도(Negative Log-Likelihood, NLL) 항을 추가한 새로운 훈련 변형을 사용합니다. 이 방법은 추론 과정에서 올바른 답변을 선택하도록 모델을 훈련시키며, 반복적인 훈련을 통해 성능이 점차 개선됩니다.

3. **성능 평가 및 실험 결과**:
   - GSM8K, ARC-Challenge, MATH 데이터셋에서의 실험을 통해, 제안된 방법은 기존의 기준 모델과 훈련 데이터를 사용하는 다른 기법들을 능가하는 성능을 보여줍니다. 특히 GSM8K에서는 초기 55.6%의 정확도가 81.6%로 크게 향상되었습니다.

### 혁신적인 부분
이 연구의 혁신적인 점은 추론 작업에 특화된 반복적인 선호 최적화 접근법을 통해 언어 모델의 추론 능력을 향상시키는 데 중점을 두고 있다는 것입니다. NLL 손실 항의 추가는 모델이 올바른 추론 단계를 학습하도록 도와줌으로써, 전체적인 성능 개선을 이끌어냈습니다.

이 논문은 추론 기반 작업에 대한 언어 모델의 성능을 개선하는 새로운 방법을 제시하며, 이는 추론 작업의 어려움을 극복하고, 언어 모델의 활용 범위를 확장하는 데 기여할 것입니다.