# Qwen2 Technical Report
## TL;DR
## Summary
- [https://arxiv.org/pdf/2407.10671.pdf](https://arxiv.org/pdf/2407.10671.pdf)

### 주요 내용 요약

#### 1. 도입부 (Introduction)
이 섹션에서는 대형 언어 모델(LLM)의 발전과 오픈소스 커뮤니티의 참여에 대한 배경을 설명합니다. 최근 Llama 시리즈와 ChatGPT의 등장으로 LLM에 대한 관심이 급증하고 있습니다. Qwen2 모델은 이러한 맥락에서 개발되었으며, 언어 이해, 생성, 다국어 능력, 코딩, 수학 및 추론에서 뛰어난 성능을 보입니다.

#### 2. 토크나이저와 모델 (Tokenizer & Model)
- **토크나이저**: Byte-level Byte-Pair Encoding(BPE)를 사용하여 고효율의 다국어 처리가 가능합니다.
- **모델 아키텍처**: Transformer 기반으로 자기 주의 기법을 사용하며, Dense 모델과 Mixture-of-Experts(MoE) 모델로 구성됩니다. GQA(Grouped Query Attention)를 도입하여 키-값 캐시 사용을 최적화하고, DCA(Dual Chunk Attention) 및 YARN 메커니즘을 통해 긴 문맥 처리 성능을 향상시켰습니다.

#### 3. 사전 학습 (Pre-training)
Qwen2 모델의 사전 학습 데이터는 7조 개 토큰으로 구성된 대규모 다국어 데이터셋을 사용하여 품질과 다양성을 높였으며, 코딩, 수학 및 다국어 데이터의 볼륨을 크게 증가시켰습니다. 또한, RoPE(Rotary Positional Embeddings)와 YARN 메커니즘을 이용해 긴 문맥에서의 성능을 최적화했습니다.

#### 4. 사후 학습 (Post-training)
- **협업 데이터 주석**: 여러 전문가와 협력하여 데이터의 질을 높이고 정확한 주석처리를 합니다.
- **자동화된 데이터 합성**: 모델 기반 기법을 활용해 높은 품질의 데이터를 자동으로 합성합니다.
- **지도 학습**: Qwen2 모델은 다양한 작업에서 성능을 향상시키기 위해 지도학습을 통해 미세 조정됩니다.
- **인간 피드백을 통한 강화 학습**: 인간의 피드백을 통해 모델이 실제 환경에서 더 나은 성능을 발휘할 수 있도록 강화를 진행합니다.

#### 5. 평가 (Evaluation)
Qwen2 모델은 다양한 공개 벤치마크와 자체 제작된 벤치마크를 사용하여 평가되었습니다. 언어 이해, 생성 및 다국어 능력에서 뛰어난 성능을 발휘하며, GPT-4 및 Mixtral 모델과도 경쟁력 있는 성적을 보여줍니다. 특히, 긴 문맥 처리와 다국어 평가에서 우수한 성능을 보였습니다.

#### 6. 결론 (Conclusion)
Qwen2 모델은 Qwen1.5를 능가하는 성능을 보이며, 언어 이해와 생성, 다국어 능력, 코딩, 수학 및 추론에서 경쟁력 있는 성능을 발휘합니다. 또한, 오픈소스 커뮤니티의 혁신과 접근성을 촉진하기 위해 모델 가중치를 공개하였습니다. 이를 통해 AI 기술의 발전과 사회에 긍정적인 영향을 미치는 데 기여하고자 합니다.

### 전체 요약
Qwen2 모델은 Transformer 기반 대형 언어 모델로, 언어 이해와 생성, 다국어 능력, 코딩, 수학 및 추론에서 뛰어난 성능을 발휘합니다. 이 모델은 고효율의 다국어 처리를 위한 고품질의 대규모 데이터를 이용하여 사전 학습되었으며, 긴 문맥 처리 능력을 강화하기 위한 다양한 기법을 도입하였습니다. 평가 결과, Qwen2 모델은 기존의 Qwen1.5 모델을 능가하며, 다양한 벤치마크에서 GPT-4 및 Mixtral 모델과 경쟁력 있는 성능을 보여줍니다. 오픈소스 커뮤니티와의 협력을 통해 AI 기술의 혁신과 접근성을 촉진하고, 보다 안전하고 책임감 있는 AI 모델을 만들기 위해 노력하고 있습니다.

## Similar Papers
- [Qwen2-Audio Technical Report](2407.10759.md)
- [SeaLLMs 3: Open Foundation and Chat Multilingual Large Language Models for Southeast Asian Languages](2407.19672.md)
- [DotaMath: Decomposition of Thought with Code Assistance and Self-correction for Mathematical Reasoning](2407.04078.md)
- [ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools](2406.12793.md)
- [The Good, The Bad, and The Greedy: Evaluation of LLMs Should Not Ignore Non-Determinism](2407.10457.md)
- [CS-Bench: A Comprehensive Benchmark for Large Language Models towards Computer Science Mastery](2406.08587.md)
- [The Llama 3 Herd of Models](2407.21783.md)
- [Self-play with Execution Feedback: Improving Instruction-following Capabilities of Large Language Models](2406.13542.md)
- [JetMoE: Reaching Llama2 Performance with 0.1M Dollars](2404.07413.md)
