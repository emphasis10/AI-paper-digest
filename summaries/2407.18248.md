# Self-Training with Direct Preference Optimization Improves Chain-of-Thought Reasoning
## TL;DR
## Summary
- [https://arxiv.org/pdf/2407.18248.pdf](https://arxiv.org/pdf/2407.18248.pdf)

### 섹션 요약

#### 1. 소개 (Introduction)
AI와 기계 학습 모델의 수학적 추론 수행 능력을 향상시키는 연구는 매우 중요하지만 도전적인 과제입니다. 특히 소형 언어 모델(LM)의 추론 능력을 향상시키는 방법이 상대적으로 덜 연구되었습니다. 기존 연구는 주로 체인 오브 사고(chain-of-thought) 프롬프팅, 지속적 사전 훈련, 외부 검증자 추가 등의 방법을 사용해 대형 LMs의 추론 능력을 향상시키는데 초점을 맞추고 있습니다.

#### 2. 배경 (Background)
수학 단어 문제 해결은 입력 질문과 출력 추론의 시퀀스-투-시퀀스(seq2seq) 작업으로 공식화할 수 있습니다. 이 작업을 위해 모델을 감독 학습 방식으로 미세 조정하는 것이 일반적입니다. 또한, 자기 훈련(self-training)은 고전적인 반지도 학습(semi-supervised learning) 접근 방식 중 하나로, 라벨된 데이터셋을 사용하는 선생님 모델이 라벨 추정 데이터셋을 만들고, 이를 학생 모델이 학습해 성능을 향상시키는 방법입니다.

#### 3. 방법론 (Methodology)
이 논문에서는 기존의 자기 훈련 프레임워크를 직접 선호 최적화(DPO) 알고리즘과 통합해 수학 추론 능력을 향상시키는 새로운 방법을 제안합니다. 이 방법은 기계 학습 모델의 추론 성능을 크게 향상시키면서도 필요한 계산 자원을 최소화합니다. 또한, 외부 계산기와의 효율적인 통합 방법을 제시해 모델 성능을 향상시키면서 추론 속도를 거의 감소시키지 않습니다.

#### 4. 실험 결과 (Experimental Results)
제안된 방법 'DPO-강화 자기 훈련(DPO-augmented Self-Training)'은 기존의 감독 미세 조정(SFT) 및 자기 훈련(ST) 방법에 비해 성능이 크게 향상되었음을 보여줍니다. 다수의 실험을 통해 모든 데이터셋에서 제안된 방법이 더 좋은 성능을 발휘했으며, 특히 큰 모델에서는 훈련 단계별로 지속적인 성능 향상이 확인되었습니다.

#### 5. 결론 (Conclusion)
비용 효율적이고 자원 효율적인 'DPO-강화 자기 훈련(DPO-augmented Self-Training)' 방법은 소형 언어 모델의 수학적 추론 능력을 크게 향상시키며, 계산 자원을 효율적으로 사용합니다. 이는 기존의 대형 언어 모델을 사용하는 방법에 비해 경제적이고 확장 가능성이 높은 방법입니다.

### 전체 요약 및 기여도

이 논문은 소형 언어 모델의 수학적 추론 능력을 향상시키기 위해 'DPO-강화 자기 훈련(DPO-augmented Self-Training)' 방법을 제안합니다. 기존 연구가 주로 대형 언어 모델의 추론 능력을 향상시키는데 초점을 맞춘 반면, 이 연구는 소형 모델의 성능을 경제적이고 자원 효율적으로 향상시키는 데 성공했습니다. 이를 통해 AI와 기계 학습 분야에서 보다 실용적이고 확장 가능한 모델 개발이 가능해졌습니다.

## Similar Papers
- [LiteSearch: Efficacious Tree Search for LLM](2407.00320.md)
- [Step-Controlled DPO: Leveraging Stepwise Error for Enhanced Mathematical Reasoning](2407.00782.md)
- [Improve Mathematical Reasoning in Language Models by Automated Process Supervision](2406.06592.md)
- [Step-DPO: Step-wise Preference Optimization for Long-chain Reasoning of LLMs](2406.18629.md)
- [Applying RLAIF for Code Generation with API-usage in Lightweight LLMs](2406.20060.md)
- [Synthesizing Text-to-SQL Data from Weak and Strong LLMs](2408.03256.md)
- [Toward Self-Improvement of LLMs via Imagination, Searching, and Criticizing](2404.12253.md)
- [Large Language Models as Analogical Reasoners](2310.01714.md)
- [Q*: Improving Multi-step Reasoning for LLMs with Deliberative Planning](2406.14283.md)
