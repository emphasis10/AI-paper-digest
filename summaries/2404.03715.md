# Direct Nash Optimization: Teaching Language Models to Self-Improve with General Preferences
## TL;DR
## Summary
- [https://arxiv.org/pdf/2404.03715.pdf](https://arxiv.org/pdf/2404.03715.pdf)

이 논문은 "직접 나쉬 최적화(Direct Nash Optimization, DNO)"라는 새로운 알고리즘을 제시하며, 이를 통해 언어 모델이 일반적인 선호도(feedback)에 기반하여 스스로를 지속적으로 개선할 수 있음을 탐구합니다. 전통적인 강화 학습에서의 보상 최대화 접근법과 달리, DNO는 직접적인 선호도 최적화를 목표로 합니다. 이는 복잡한 선호 관계를 더 잘 표현할 수 있는 새로운 방법을 제공합니다.

### 소개 및 관련 연구
AI 분야는 인간의 가치와 선호에 부합하는 고도의 모델을 개발하려는 방향으로 발전하고 있습니다. 대규모 언어 모델(LLMs)은 다양한 작업에서 인간과 유사한 텍스트를 생성할 수 있는 능력을 보여주었지만, 신뢰성, 안전성, 윤리적 정렬과 같이 높은 정확도가 요구되는 작업에서는 여전히 도전 과제가 남아 있습니다. 이를 해결하기 위해, 인간의 피드백에서 학습하는 강화 학습(RLHF)이 효과적인 접근법으로 제시되었습니다. 그러나 기존의 보상 최대화 프레임워크는 복잡한 선호 관계를 표현하는 데 한계가 있습니다.

### 직접 나쉬 최적화(DNO)
DNO는 이러한 한계를 극복하고 일반적인 선호도 최적화를 가능하게 하는 새로운 알고리즘입니다. DNO는 대규모 언어 모델의 후속 학습에 적용되며, 강력한 오라클(예: 인간 평가자 또는 고성능 모델)로부터의 선호도 피드백을 바탕으로 모델이 자기 자신을 반복적으로 개선하도록 돕습니다. DNO의 핵심은 콘트라스트 학습의 단순성과 일반적인 선호도 최적화의 이론적 일반성을 결합하는 것입니다.

### 실험 결과 및 결론
DNO를 통해 학습된 7B 파라미터의 Orca-2.5 모델은 GPT-4-Turbo와의 비교에서 상태-아트(state-of-the-art) 성능을 달성했습니다. 특히, AlpacaEval 2.0에서 GPT-4-Turbo에 대한 승률 33%를 달성하며, 기존 모델보다 훨씬 뛰어난 결과를 보여주었습니다. 또한, 이 연구는 DNO가 LLMs의 후속 학습에 있어서 큰 가능성을 가지고 있음을 시사하며, AI 연구 커뮤니티에 유용한 통찰을 제공합니다.

이 요약을 통해, 이 논문이 언어 모델의 자가 개선과 일반적인 선호도에 기반한 학습 방법론에 있어서 중요한 기여를 한 것을 볼 수 있습니다. DNO 알고리즘은 복잡한 선호 관계를 더 정확하게 반영할 수 있는 새로운 경로를 제시하며, 언어 모델의 개선을 위한 효과적인 접근법으로 기대됩니다.

## Similar Papers
- [Self-Play Preference Optimization for Language Model Alignment](2405.00675.md)
- [Active Prompting with Chain-of-Thought for Large Language Models](2302.12246.md)
- [SimPO: Simple Preference Optimization with a Reference-Free Reward](2405.14734.md)
- [SeaKR: Self-aware Knowledge Retrieval for Adaptive Retrieval Augmented Generation](2406.19215.md)
- [New Desiderata for Direct Preference Optimization](2407.09072.md)
- [Large Language Models Are Reasoning Teachers](2212.10071.md)
- [Language Models as Compilers: Simulating Pseudocode Execution Improves Algorithmic Reasoning in Language Models](2404.02575.md)
- [AgentInstruct: Toward Generative Teaching with Agentic Flows](2407.03502.md)
- [Self-Exploring Language Models: Active Preference Elicitation for Online Alignment](2405.19332.md)
