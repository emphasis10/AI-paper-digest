# OmniBench: Towards The Future of Universal Omni-Language Models
## TL;DR
## Summary
- [https://arxiv.org/pdf/2409.15272.pdf](https://arxiv.org/pdf/2409.15272.pdf)

### 1. 각 섹션 요약

#### **1. 서론**
이 논문은 최근 멀티모달 대형 언어 모델(MLLMs)의 발전에 대해 논의하며, 이 모델들이 텍스트, 이미지, 오디오 등 다양한 데이터를 통합하여 처리하는 능력을 갖추어야 한다고 주장합니다. 그러나 여러 모드의 데이터를 동시에 처리하고 추론하는 능력은 아직 충분히 탐구되지 않았습니다. 이를 해결하기 위해 OmniBench라는 새로운 벤치마크를 소개하며, 이 벤치마크는 모델이 시각적, 청각적, 텍스트 입력을 동시에 인식하고 해석하며 추론하는 능력을 평가하도록 설계되었습니다.

#### **2. 관련 연구**
기존의 MLLMs는 주로 텍스트와 이미지를 통합하여 처리하는 모델 및 텍스트와 오디오를 처리하는 모델로 한정되어 왔습니다. Whisper, BEATs 및 MERT 등의 모델이 대표적입니다. 그러나 이러한 모델들은 대부분 특정한 모달리티에만 집중하고 있으며, 실제로 다양한 모달리티를 동시에 처리하는 능력은 부족합니다. OmniBench는 이를 해결하고자 설계되었습니다.

#### **3. OmniBench**
OmniBench는 기존의 멀티모달 벤치마크를 바탕으로 새로운 과제 유형 분류를 제안하며, 인식, 해석, 설명, 추론 능력을 포괄적으로 평가합니다. 1142개의 질문-답변 쌍을 포함하고 있으며, 이는 모델의 삼중 모달리티 설정에서의 성능을 평가할 수 있게 합니다. 또한, 엄격한 주석 프로토콜을 통해 높은 데이터 품질을 유지합니다.

#### **4. 실험 설정 및 주요 발견**
현재의 공개 소스 OLM들은 삼중 모달리티 설정에서 무작위 추측보다 높은 정확도를 보였으나, 성능은 여전히 제한적입니다. 예를 들어, AnyGPT와 video-SALMONN 모델은 각각 18.04%와 35.64%의 정확도를 보였으며, 전반적으로는 Gemini-1.5-Pro 모델이 더 나은 성능을 보였습니다. 또한, 텍스트로 대체된 이미지 및 오디오 입력을 사용하는 경우 성능이 상대적으로 개선된 것을 발견했습니다.

#### **5. 결론 및 미래 연구**
OmniBench는 현재의 MLLMs가 시각적, 청각적, 텍스트 입력을 동시에 처리하는 데 어려움을 겪고 있음을 드러냈습니다. 특히, 음성 오디오에 대한 일반적인 편향과 텍스트로 대체된 입력을 사용할 때의 성능 차이를 확인했습니다. 앞으로는 멀티모달 통합 기술과 다양한 훈련 데이터셋, 모달리티 편향 감소 기술에 대한 연구가 필요함을 강조합니다.

---

### 2. 전체 요약
이 논문은 MLLMs의 성능 평가를 위해 OmniBench라는 새로운 벤치마크를 제안했습니다. 이는 텍스트, 이미지, 오디오 데이터를 동시에 처리하고 해석하며 추론하는 능력을 평가하는 도구입니다. OmniBench는 1142개의 질문-답변 쌍을 통해 모델의 인식, 해석, 설명, 추론 능력을 포괄적으로 평가하며, 삼중 모달리티 설정에서의 성능을 검증합니다. 현재 공개된 MLLMs는 삼중 모달리티 설정에서 여전히 성능이 부족하지만, 텍스트로 대체된 입력을 사용할 때는 성능이 일부 개선됩니다. 이 논문은 앞으로 멀티모달 통합 기술과 다양한 훈련 데이터셋, 모달리티 편향 감소 기술에 대한 연구가 필요함을 강조합니다.

이 정보를 바탕으로 프레젠테이션을 작성할 수 있을 것입니다.