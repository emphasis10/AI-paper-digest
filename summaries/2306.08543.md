# MiniLLM: Knowledge Distillation of Large Language Models
## TL;DR
## Summary
- [https://arxiv.org/pdf/2306.08543.pdf](https://arxiv.org/pdf/2306.08543.pdf)

### 요약: MiniLLM: Knowledge Distillation of Large Language Models

#### 1. 논문 주요 내용 요약

##### 1.1 개요
이 논문은 대규모 언어 모델(LLM)의 컴퓨팅 자원 요구를 줄이기 위한 지식 증류(Knowledge Distillation, KD)의 새로운 접근 방식을 제안합니다. 기존 KD 방법은 주로 흰색 상자 분류 모델(white-box classification models) 또는 ChatGPT와 같은 검은 상자 모델 API를 모방해 소형 모델을 훈련하는 데 사용되었습니다. 그러나 이 논문에서는 white-box LLM의 지식을 소형 모델로 효과적으로 증류하는 방법을 탐구합니다.

##### 1.2 방법론
- 표준 KD 방법에서 전방 Kullback-Leibler divergence (KLD) 목적을 역방향 KLD로 대체하여, 학생 모델이 교사 분포의 저확률 영역을 과대 평가하지 않게 합니다.
- 이를 최적화하기 위한 효과적인 최적화 방법을 도출하고, 제안된 학생 모델을 "MINILLM"이라고 명명합니다.

##### 1.3 실험 및 결과
- 명령어 따르기 설정에서 광범위한 실험을 통해 MINILLM이 기존 KD 방법보다 정교한 응답을 생성하고 전반적인 품질이 높음을 확인하였습니다.
- 다양한 모델 패밀리(120M에서 13B 파라미터)에 대해 확장 가능성 또한 입증되었습니다.

#### 2. 각 섹션 요약

##### 2.1 서론
대규모 언어 모델의 컴퓨팅 요구를 줄이기 위한 지식 증류(KD)의 필요성을 서술합니다. 주요 목표는 흰색 상자 모델의 지식을 소형 모델로 효과적으로 증류하는 것입니다.

##### 2.2 관련 연구
- 대규모 언어 모델 및 지식 증류 분야의 최근 연구 동향을 정리합니다.
- 기존 연구는 주로 전방 KLD를 최소화하는 데 집중했으나, 이는 여러 모드(mode)를 포함하는 언어 생성 작업에 적합하지 않음을 지적합니다.

##### 2.3 방법론
이 논문은 역방향 KLD를 최소화하는 것을 목표로 하며, 이를 위해 최적화 방법을 상세히 설명합니다. 역방향 KLD는 모델이 주요 모드를 집중하도록 하여, 학생 모델이 교사 모델의 분포에서 저확률 영역을 과대 평가하지 않게 합니다.

##### 2.4 실험
다양한 데이터셋과 모델로 실험한 결과, MINILLM이 전방 KLD를 최소화하는 기존 방법보다 더 나은 성능을 보였음을 확인하였습니다.

##### 2.5 결론
MINILLM이 기존 KD 모델보다 더 정교한 응답을 생성하며, 낮은 편향, 더 나은 보정(calibration), 높은 긴 텍스트 생성 성능을 가지고 있다고 결론 내립니다.

#### 3. 연구의 주요 기여 및 혁신적인 부분 요약
- **주요 기여**: 전방 KLD 대신 역방향 KLD를 사용한 지식 증류 방법을 제안함으로써, 여러 모드를 포함하는 언어 생성 작업에 적합한 새로운 KD 방법론을 제공합니다.
- **혁신적인 부분**: 전방 KLD의 한계를 극복하고자, 역방향 KLD를 도입하여 학생 모델이 주요 모드에 집중하도록 유도하고, 최적화를 위한 새로운 방식을 도출하였습니다.

#### 종합 요약
이 논문은 대규모 언어 모델의 효율적 증류를 위해 역방향 Kullback-Leibler Divergence를 활용하는 새로운 방법론을 제안합니다. MINILLM은 전방 KLD의 한계를 극복하고, 더 정교하고 신뢰할 수 있는 결과를 제공합니다. 논문의 실험 결과 및 새로운 기여는 향후 다양한 언어 모델 연구에 중요한 시사점을 제공할 것입니다.

--- 

이 요약이 도움이 되었길 바랍니다! 더 궁금한 점이나 추가적인 도움이 필요하면 언제든지 말씀해 주세요.