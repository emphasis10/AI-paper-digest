# Mixture of A Million Experts
## TL;DR
## Summary
- [https://arxiv.org/pdf/2407.04153.pdf](https://arxiv.org/pdf/2407.04153.pdf)

1. 각 섹션의 요약:

### Introduction
이 논문은 'Parameter Efficient Expert Retrieval (PEER)'이라는 새로운 레이어 디자인을 소개합니다. 기존의 Mixture of Experts (MoE) 모델은 규모가 크면 최적화와 계산 효율성에서 어려움을 겪습니다. PEER는 제품 키 기법을 활용하여 수백만 개의 소형 전문가를 효율적으로 사용하는 방법을 제안합니다. 실험을 통해 PEER는 기존의 밀집 FFW 레이어와 기존의 MoE 모델보다 더 나은 성능-계산 효율성을 보여줍니다.

### Method
PEER 레이어는 제품 키를 사용하는 라우터와 단일 뉴런 MLP 전문가를 사용합니다. 입력 벡터를 쿼리 벡터로 변환하고, 제품 키와 비교하여 최상위 k개의 전문가를 선택합니다. 선택된 전문가의 출력을 합산하여 최종 출력을 생성합니다. 이 과정에서 제품 키 기법을 사용하여 많은 수의 전문가를 효율적으로 활용합니다.

### Experiments
PEER 레이어는 다양한 언어 모델링 과제에서 실험됩니다. PEER는 밀집 FFW 레이어와 기존 MoE 모델보다 적은 FLOP 예산에서도 더 나은 성능을 보입니다. 또한 PEER는 전문가 수, 활성 매개변수, 헤드 수 등의 변수에 대한 세밀한 실험 결과를 제공합니다.

### Pretraining isoFLOP Analysis 및 Evaluation on Language Modeling Datasets
PEER 레이어는 FLOP 예산이 동일한 조건에서 다양한 테스트 데이터셋에서 밀집 FFW와 MoE 모델을 초과하는 성능을 보입니다. 특히, PEER는 적은 계산 자원을 사용하면서도 높은 품질의 모델링을 할 수 있음을 증명합니다.

### Related Works
기존의 MoE 모델들, 제품 키 메모리 레이어 등에 대한 논의를 통해 PEER의 혁신성을 설명합니다. 이전 연구들과의 비교를 통해 PEER가 어떻게 더 효율적이고 확장 가능한지를 강조합니다.

### Conclusion
PEER 레이어는 수백만 개의 소형 전문가를 사용할 수 있는 새로운 방법을 제공하여 MoE 모델을 효과적으로 확장할 수 있는 가능성을 보여줍니다. 이는 밀집 FFW 레이어와 기존 MoE 모델에 비해 뛰어난 성능-계산 효율성을 제시합니다. PEER의 설계가 더 많은 전문가를 포함하여 향후 연구 및 응용에 중요한 기회를 제공한다고 결론지었습니다.

2. 전체 요약:
이 논문은 PEER 레이어라는 새로운 개념을 도입하여, 많은 수의 소형 전문가를 효율적으로 활용할 수 있도록 설계된 Mixture of Experts (MoE) 아키텍처를 소개합니다. PEER는 제품 키를 사용하여 전문가를 선택하고 이들의 출력을 조합하여 최종 결과를 산출합니다. 실험 결과, PEER는 기존의 밀집 FFW 레이어와 MoE 모델에 비해 더 나은 성능-계산 효율성을 보여주는 것으로 나타났습니다. 특히, 더 큰 규모의 모델에서도 계산 자원을 효율적으로 사용하면서 성능을 높일 수 있습니다. PEER는 향후 MoE 모델의 확장성과 효율성을 크게 향상시킬 수 있는 혁신적인 접근법이 될 것입니다.