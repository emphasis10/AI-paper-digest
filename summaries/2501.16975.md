# Over-Tokenized Transformer: Vocabulary is Generally Worth Scaling
## TL;DR
## Summary
- [https://arxiv.org/pdf/2501.16975.pdf](https://arxiv.org/pdf/2501.16975.pdf)

**1. 각 섹션 요약:**

- **소개 (Introduction):** 소개 섹션은 큰 언어 모델(LLM)에서 토큰화의 중요성을 강조합니다. 모델 크기가 커짐에 따라 임베딩 단계를 확장하는 것이 모델 성능에 미치는 긍정적인 영향을 설명합니다. 본 연구는 입력과 출력 어휘를 분리하여 더 큰 입력 어휘를 활용함으로써 모델 성능을 향상시키는 새로운 방식, 'Over-Tokenized Transformer'를 제안합니다.

- **관련 연구 (Related Work):** 이 섹션에서는 기존의 토큰화 방식과 규모 확장 관련 연구를 검토합니다. 기존의 Byte-Pair Encoding(BPE) 등과 같은 방법론에 대해 설명하고 있으며, 새로운 확장 방식의 필요성을 강조합니다.

- **방법론 (Methodology):** 연구진은 Over-Encoding(OE)와 Over-Decoding(OD)라는 기법을 도입해 모델의 입력과 출력 어휘를 각각 확장하여 모델 성능을 향상시키는 방법을 제안합니다. 이 방식을 통해 입력 어휘를 크게 확장하면서도 추가적인 계산 비용 없이 모델의 학습 손실을 줄이는 것을 목표로 하고 있습니다.

- **결과 및 발견 (Results/Findings):** 실험 결과는 입력 어휘를 확장함으로써 모델 성능이 꾸준히 향상된다는 것을 보여줍니다. 특히 큰 입력 어휘가 작은 모델보다 큰 모델에 더 긍정적인 효과를 미친다는 것을 강조합니다. 또한, OE와 OD 기법을 통합하여 보다 큰 개선 효과를 보였습니다.

- **결론 (Conclusion):** 연구는 토큰화 설계와 모델 확장의 연결을 통해 차세대 언어 모델을 발전시키는 데 토큰화가 핵심적 역할을 한다고 결론지었습니다. 제안된 'Over-Tokenized Transformer' 프레임워크가 차후 연구에 영감을 줄 것이라고 기대하고 있습니다.

**2. 전체 요약:**

이 논문은 토큰화 및 어휘 확장의 새로운 프레임워크인 'Over-Tokenized Transformer'를 소개하며, 이는 입력과 출력 어휘를 각각 확장하여 모델의 성능을 높이는 방법론을 제시합니다. 입력 어휘를 크게 늘리면 추가적인 계산 비용 없이 성능이 향상된다는 것을 실험적으로 검증하였으며, 이러한 접근이 대규모 언어 모델의 설계와 확장에 있어서 중요하다는 것을 강조합니다. 이 연구는 향후 토큰화 전략 연구와 효율적인 LLM 확장의 새로운 방향성을 제시합니다.