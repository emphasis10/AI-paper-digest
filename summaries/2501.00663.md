# Titans: Learning to Memorize at Test Time
## TL;DR
## Summary
- [https://arxiv.org/pdf/2501.00663.pdf](https://arxiv.org/pdf/2501.00663.pdf)

### 1. 논문의 주요 기여 및 배경

이 논문은 Titans라는 새로운 신경망 아키텍처를 소개합니다. 주요 기여는 다음과 같습니다:
	•	긴 문맥 처리 문제 해결: Transformers가 긴 문맥을 처리할 때 발생하는 메모리 및 계산 비용 문제를 해결하기 위해, **단기 메모리(Transformers의 Attention)**와 **장기 메모리(Neural Memory)**를 통합한 새로운 아키텍처를 제안.
	•	학습 중 기억 능력 강화: Titans는 학습뿐 아니라 테스트 단계에서도 정보를 학습하고 기억할 수 있도록 설계된 신경망 메모리 모듈을 포함.
	•	효율적이고 확장 가능한 모델: Titans는 200만 토큰 이상 문맥을 처리할 수 있으며, 다양한 벤치마크(언어 모델링, 상식 추론, 시간 연속 데이터)에서 뛰어난 성능을 보임.

### 2. 섹션별 요약

#### 1. 서론
	•	Transformers는 짧은 문맥에 대한 정확한 의존성 모델링에는 강점을 가지지만, 계산 비용이 **문맥 길이에 따라 기하급수적으로 증가(Quadratic Complexity)**하는 한계를 지님.
	•	Linear Transformers 등 여러 대안 모델이 제안되었으나, 성능 저하 문제 및 장기 정보를 효과적으로 압축하지 못하는 단점이 있음.
	•	이 연구는 인간의 단기 메모리와 장기 메모리 개념을 신경망에 적용해 효율적이면서도 장기 기억을 저장하고 활용할 수 있는 아키텍처를 제안함.

#### 2. Titans 아키텍처 설계

Titans는 세 가지 주요 구성 요소를 포함:
	1.	Core (단기 메모리): Attention 메커니즘으로 현재 문맥을 처리.
	2.	Long-term Memory (장기 메모리): 과거 정보를 저장하고, 필요시 불러오는 학습 가능한 메모리.
	3.	Persistent Memory (고정 메모리): 입력과 무관하게 작업 지식을 저장하는 파라미터.

세 가지 설계 변형:
	•	Memory as Context (MAC): Attention에 장기 메모리를 문맥으로 통합.
	•	Memory as Gating (MAG): 단기 메모리와 장기 메모리를 게이팅 메커니즘으로 결합.
	•	Memory as a Layer (MAL): 장기 메모리를 독립적인 네트워크 레이어로 활용.

#### 3. 신경 메모리 모듈 설계
	•	Surprise Metric: 입력 데이터가 기존 기억과 얼마나 다른지를 측정해, 놀라움(surprise) 정도에 따라 기억을 업데이트.
	•	망각 메커니즘: 적응형 가중치 감쇠를 통해 불필요한 기억을 제거해 메모리 용량을 효율적으로 관리.
	•	병렬 처리 최적화: GPU/TPU에서의 효율적인 병렬 학습을 위한 텐서 연산 기반 구현.

#### 4. 실험 및 결과
	•	언어 모델링 및 상식 추론: Titans는 GPT-4 및 Llama와 같은 모델보다 뛰어난 성능을 기록.
	•	Needle-in-a-Haystack(NIAH): 긴 문맥에서 특정 정보를 검색하는 실험에서 기존 모델 대비 월등한 정확도.
	•	BABI Benchmark: Titans는 긴 문맥 내 추론 작업에서 GPT-4보다 우수.
	•	시간 연속 데이터 예측: Titans는 메모리 깊이를 조정하여 더 긴 문맥에서도 성능 저하 없이 안정적으로 예측.

### 3. 논문의 주요 목적 및 혁신

Titans는 기존 모델의 한계를 해결하기 위해 설계된 장기 메모리 기반 아키텍처입니다. 특히:
	1.	장기 및 단기 메모리 통합: 기존 Transformers가 짧은 문맥에 초점이 맞춰진 반면, Titans는 장기 문맥에서 더 효과적으로 작동.
	2.	테스트 단계 학습: 테스트 중에도 기억을 업데이트하고 관리하는 메커니즘을 통해 더욱 적응적인 모델링 가능.
	3.	효율적 확장성: 200만 토큰 이상 문맥 길이에서도 계산 비용이 선형적으로 증가하여 실용성이 높음.

### 4. 전체 요약

이 연구 논문은 AI 및 기계 학습 분야에 대한 혁신적인 접근 방식을 제안합니다. 연구의 핵심은 장기 기억을 학습하는 신경 모듈을 활용하여 기존의 메모리와 주의(attention) 기반의 모델들 갖고 있는 한계를 극복하는 것입니다. 이 모듈은 인플레이션 단계에서 데이터를 기억하고 이를 테스트 시 활용하는 방법을 학습하도록 설계되었습니다. 주요 기여는 메모리의 용량을 늘리고 일반화 및 길이 확장 능력을 개선하며, 효율적인 메모리 관리 기술을 통해 성능을 향상시키는 것입니다. 

이 논문에서 소개된 'Titans' 아키텍처는 메모리를 효과적으로 통합하여 시퀀스 모델링의 성능을 향상시키며, 긴 문맥 창을 처리하는 데 있어서도 더 정확하고 효율적인 성능을 제공합니다. 이 아키텍처는 메모리의 깊이를 고려한 실험을 통해 기존의 Transformer 및 현대적인 선형 반복 모델들보다 더 높은 성능을 입증하였습니다. 

장기 메모리를 활용하여 효과적인 데이터 학습과 저장을 가능하게 함으로써, Titans는 실세계의 복잡한 문제를 해결하는 데 있어 많은 잠재력을 보여줍니다. 이러한 새로운 접근 방식은 더 나은 인간-기계 상호 작용을 가능하게 하고, 궁극적으로 AI의 발전에 기여할 것으로 기대됩니다.