# BM25S: Orders of magnitude faster lexical search via eager sparse scoring
## TL;DR
## Summary
- [https://arxiv.org/pdf/2407.03618.pdf](https://arxiv.org/pdf/2407.03618.pdf)

### 요약 및 주요 기여, 혁신적 부분

#### 1. 섹션별 요약

#### 서론 (Introduction)
이 논문에서는 BM25S라는 새로운 BM25 구현 방식을 소개합니다. 이는 Numpy와 Scipy에만 의존하며, 기존 인기 있는 파이썬 기반 프레임워크보다 최대 500배 빠른 속도를 자랑합니다. 또한 상업적으로 인기 있는 Java 기반 구현체와 비슷하거나 그보다 더 빠릅니다. BM25S는 또한 PyTorch에 의존하지 않고 Scipy의 희소 행렬을 사용하는 방식으로 기존의 BM25-PT 프로젝트의 아이디어를 확장했습니다.

#### 관련 연구 (Related Work)
BM25 계열의 희소 텍스트 검색 알고리즘은 학습이 필요 없고 여러 언어에 적용 가능하며 보통 빠른 속도로 인기가 있습니다. Pyserini와 같은 기존의 파이썬 기반 BM25 구현체와 달리, BM25S는 인덱싱할 때 모든 미래 쿼리에 대해 BM25 스코어를 미리 계산하고 그 결과를 희소 행렬로 저장하여 더 빠른 검색이 가능합니다.

#### 구현 (Implementation)
BM25S는 인덱싱 시점에서 쿼리 토큰의 모든 잠재적 스코어를 미리 계산하여 저장합니다. 이 구현 방식은 Scipy의 희소 행렬을 활용하여 수행되며, 이는 BM25-PT와는 다른 접근 방식을 취합니다. BM25S는 단순하지만 빠른 파이썬 기반 토크나이저를 도입했으며, Scikit-Learn의 텍스트 분할 방식을 사용합니다.

#### 실험 (Experiments)
BM25S는 대부분의 실험에서 현저히 더 높은 처리량을 자랑한다는 점을 보였습니다. 예를 들어, Rank-BM25에 비해 최대 500배 빠른 속도를 기록했습니다. 또한 다양한 토크나이제이션 방식이 BM25S의 성능에 미치는 영향을 분석하였습니다. 스테밍을 추가하면 성능이 평균적으로 개선되며, 스톱워드는 상황에 따라 성능에 영향을 미칩니다.

#### 결과 (Results)
실험 결과, BM25S는 기존의 BM25 구현체들보다 많은 장점을 가지고 있음을 보였습니다. BM25S는 다양한 BM25 변형 모델에서 높은 성능을 보였으며, 특히 피어슨 상관계수가 높은 데이터를 대상으로 하는 경우 더욱 그 성능이 돋보였습니다.

#### 결론 (Conclusion)
BM25S는 인덱싱 시점에서 BM25 스코어를 미리 계산함으로써 뛰어난 속도를 자랑하며, 저장 공간이 제한된 환경에서도 사용할 수 있는 경량 솔루션을 제공합니다. 이를 통해 브라우저 내에서 실행되거나 엣지 디바이스에서도 사용할 수 있습니다. 최소한의 의존성으로 구현되어 AI와 머신 러닝 연구에 확장 가능성을 제시합니다.

### 2. 전체 요약

논문은 BM25S라는 빠르고 효율적인 BM25 구현체를 소개합니다. 이 새로운 방법은 인덱싱 시점에서 쿼리 토큰의 모든 잠재적 스코어를 미리 계산하여 저장함으로써 기존의 파이썬 기반 BM25 구현체보다 최대 500배 빠른 성능을 보여줍니다. 또한, Scipy의 희소 행렬을 활용하여 메모리 사용을 최적화하고, 간단한 파이썬 기반 토크나이저 및 멀티스레딩 기능을 도입하여 처리 속도를 더욱 향상시킵니다. 결과적으로 BM25S는 다양한 BM25 변형 모델에서도 높은 성능을 보이며, 엣지 디바이스나 브라우저 내에서도 사용할 수 있는 확장 가능성을 갖추었습니다.

## Similar Papers
- [Sparser is Faster and Less is More: Efficient Sparse Attention for Long-Range Transformers](2406.16747.md)
- [MS MARCO Web Search: a Large-scale Information-rich Web Dataset with Millions of Real Click Labels](2405.07526.md)
- [AGRaME: Any-Granularity Ranking with Multi-Vector Embeddings](2405.15028.md)
- [RE-AdaptIR: Improving Information Retrieval through Reverse Engineered Adaptation](2406.14764.md)
- [EAGLE-2: Faster Inference of Language Models with Dynamic Draft Trees](2406.16858.md)
- [INDUS: Effective and Efficient Language Models for Scientific Applications](2405.10725.md)
- [D2LLM: Decomposed and Distilled Large Language Models for Semantic Search](2406.17262.md)
- [Fast Inference from Transformers via Speculative Decoding](2211.17192.md)
- [RoFormer: Enhanced Transformer with Rotary Position Embedding](2104.09864.md)
