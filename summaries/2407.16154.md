# DDK: Distilling Domain Knowledge for Efficient Large Language Models
## TL;DR
## Summary
- [https://arxiv.org/pdf/2407.16154.pdf](https://arxiv.org/pdf/2407.16154.pdf)

### 주요 내용 요약

1. **서론 (Introduction)**:
   최근 많은 주목을 받고 있는 대형 언어 모델(LLMs)들은 높은 지능을 보여주지만, 막대한 계산 및 저장 공간을 필요로 합니다. 이를 개선하기 위해 소형화된 LLM이 연구되고 있으며, 대표적인 방법 중 하나가 지식 증류(Knowledge Distillation, KD)입니다. KD는 성능이 좋은 교사 모델로부터 지식을 추출하여 학생 모델에 전이하는 방식입니다.

2. **관련 연구 (Related Works)**:
   최근의 연구들은 대형 언어 모델(LLM)의 지식 증류 방법에 큰 관심을 가지고 있으며, 이를 통해 작은 모델들이 더 효율적으로 학습될 수 있도록 돕고 있습니다. 기존의 백박스나 블랙박스 지식 증류 방법들과 달리, 본 연구는 도메인 특이적 데이터를 활용하여 각 도메인 간의 성능 차이를 최소화하는 새로운 접근 방식을 제안합니다.

3. **연구 방법론 (Methodology)**:
   DDK는 도메인 간 성능 차이를 기반으로 학습 데이터를 동적으로 조정하는 방법입니다. 교사 모델과 학생 모델 사이의 도메인 성능 격차를 지속적으로 계산하고 이를 바탕으로 데이터 샘플링 확률을 조정합니다. 또한, 안정성을 높이기 위해 'factor smooth updating'이라는 기법을 사용합니다.

4. **실험 결과 (Experiments)**:
   여러 벤치마크 데이터셋을 사용한 실험을 통해 제안한 DDK 방법이 기존 방법들보다 뛰어난 성능을 보임을 확인했습니다. 특히, 복잡한 문제 도메인에서 더 나은 성능 향상을 보였습니다.

5. **결론 (Conclusion)**:
   본 연구에서는 도메인 지식을 활용한 새로운 지식 증류 프레임워크인 DDK를 제안하였습니다. DDK는 도메인 간 데이터 혼합 비율을 최적화하여 더 안정적이고 효율적인 지식 증류 과정을 가능하게 합니다.

### 전체 요약

이 논문은 도메인 특이적 데이터를 활용해 교사 모델과 학생 모델 간 성능 격차를 줄이는 새로운 지식 증류 방법론인 DDK를 제안합니다. DDK는 도메인 성능 차이를 기반으로 학습 데이터의 샘플링 확률을 조정하며, 안정성을 높이기 위해 'factor smooth updating' 기법을 도입합니다. 여러 벤치마크 데이터셋으로 실험한 결과, DDK는 기존 방법들보다 뛰어난 성능을 보여주었습니다. 이 연구는 AI 및 머신러닝 모델의 성능 향상과 효율적인 소형화에 중요한 기여를 하고 있습니다.

## Similar Papers
- [MiniCPM: Unveiling the Potential of Small Language Models with Scalable Training Strategies](2404.06395.md)
- [GraphReader: Building Graph-based Agent to Enhance Long-Context Abilities of Large Language Models](2406.14550.md)
- [Direct Preference Knowledge Distillation for Large Language Models](2406.19774.md)
- [DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](2405.04434.md)
- [Spectra: A Comprehensive Study of Ternary, Quantized, and FP16 Language Models](2407.12327.md)
- [From GaLore to WeLore: How Low-Rank Weights Non-uniformly Emerge from Low-Rank Gradients](2407.11239.md)
- [PLaD: Preference-based Large Language Model Distillation with Pseudo-Preference Pairs](2406.02886.md)
- [SpeechGuard: Exploring the Adversarial Robustness of Multimodal Large Language Models](2405.08317.md)
- [Tuning Language Models by Proxy](2401.08565.md)
