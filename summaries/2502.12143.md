# Small Models Struggle to Learn from Strong Reasoners
## TL;DR
## Summary
- [https://arxiv.org/pdf/2502.12143.pdf](https://arxiv.org/pdf/2502.12143.pdf)

1. 섹션 요약:

- **서론**: 본 연구는 소형 모델의 학습 능력 격차를 논의하며, 이들은 교사 모델의 긴 사고 체인(CoT) 추론이나 증류에서 일관되게 혜택을 보지 못한다고 지적합니다. 소형 학습자는 간결한 reasoning망을 통해 더 나은 성능을 발휘하며, 이에 따라 소형 모델의 학습 능력을 증강시키기 위한 Mix Distillation 방법을 제안합니다.

- **이론적 배경**: 사전 조사에 따르면, LLMs의 CoT 추론 이런 전략은 문제를 중간 reasoning 단계로 분해하여 모델의 성능과 해석 가능성을 증진시키는 중요한 기술로 밝혀졌습니다. 이는 능력이 한정된 장치에서도 소형 모델을 개발할 필요성을 강조합니다.

- **Mix Distillation 방법론**: 긴 CoT 및 짧은 CoT 데이터를 결합하여 소형 모델에 보다 적합한 혼합 방식을 통해 reasoning 복잡성을 균형 잡습니다. 제안된 Mix Distillation 방법은 다양한 평가 지표에서 효과적인 성능 향상을 보였습니다.

- **실험 결과**: Mix-Long과 Mix-Large 모두 대조군보다 뛰어났으며, 이들은 서로 다른 데이터 세트에서 소형 학습자가 더 나은 성능을 발휘할 수 있도록 도와줍니다. 실험적으로 소형 모델은 각각의 데이터만 사용한 경우보다 혼합 증류에서 더 높은 성능 향상을 보였습니다.

- **결론 및 미래 작업**: 이 연구는 직관적으로 적용할 수 있는 실질적인 통찰을 제공하며, 소형 모델의 reasoning 성능을 향상시키는 새로운 연구 방향을 제안합니다. 향후 연구는 혼합 증류의 정교화를 통해 추가 조사와 학습 능력 격차를 더욱 줄이기 위한 전략을 제안할 예정입니다.

2. 전체 요약:
이 논문은 소형 인공 지능 모델의 학습 능력 격차(Small Model Learnability Gap)를 다루며, 소형 모델이 긴 사고 체인(CoT) 추론이나 큰 교사 모델에서 일관되게 이익을 얻지 못한다고 지적합니다. 이에 대응하여, 긴 CoT와 짧은 CoT 데이터를 혼합하여 reasoning 복잡성을 효과적으로 조절할 수 있는 혼합 증류(Mix Distillation) 방법을 제안합니다. 실험 결과, 이 방법은 소형 모델의 성능을 향상시키고 reasoning 능력을 증강시키는 데 도움을 준다는 것이 밝혀졌습니다.