# Scaling Embedding Layers in Language Models
## TL;DR
## Summary
- [https://arxiv.org/pdf/2502.01637.pdf](https://arxiv.org/pdf/2502.01637.pdf)

1. **각 섹션 요약**

   - **서론**: 본 논문은 SCONE이라는 새로운 접근법을 제안하여 언어 모델의 임베딩 레이어를 확장하는 방법에 대해 설명한다. 기존의 어휘 크기를 증가시키는 방식 한계로 인해 n-그램을 통해 문맥화된 표현을 제공함으로써 더 나은 성능을 이룰 수 있다. SCONE은 고정된 추론 시간의 FLOPS를 유지하면서 새로운 임베딩 전략을 제안한다.

   - **SCONE 아키텍처**: SCONE은 자주 발생하는 n-그램의 임베딩 레이어를 사용하는 새로운 구조를 도입한다. 이 레이어는 주어진 토큰에 대한 풍부한 문맥을 제공하며, 이를 통해 수십억 개의 항목을 포함하는 임베딩 테이블을 구축할 수 있다.

   - **실험 평가**: 본 연구의 실험 섹션에서 SCONE의 두 가지 주요 확장 전략을 평가한다. 첫 번째는 캐시된 f-그램 임베딩의 수를 증가시키는 것이고, 두 번째는 이 임베딩을 학습하는 모델의 규모를 확장하는 것이다. 실험 결과, SCONE이 기존 모델에 비해 성능이 개선된다는 것을 보여준다.

   - **결론**: SCONE은 언어 모델에서 n-그램 문맥화된 임베딩을 효과적으로 생성할 수 있는 확장 가능한 방법을 제안하며, 이는 특히 지연에 민감한 응용 프로그램에서 유용하다.

   - **주요 기여 및 혁신 포인트**: SCONE의 주요 기여는 임베딩 레이어를 효과적으로 확장할 수 있는 방법을 제공하여, 기존 모델에 비해 높은 성능을 유지하면서도 추론 시의 계산 비용을 거의 늘리지 않는 점이다. 이 혁신적인 접근법은 더 큰 임베딩을 학습할 수 있는 기반을 마련한다.

2. **전체 요약**

   본 논문은 SCONE이라는 혁신적인 방법론을 제안하며, 언어 모델에서 임베딩 레이어의 확장을 가능하게 한다. 이 접근법은 단순한 어휘 크기 증가 대신에 n-그램 임베딩을 도입하여 문맥적 표현력을 극대화하며, 실험을 통해 SCONE이 기존 모델들에 비해 성능이 향상됨을 보인다. SCONE은 특히 지연이 중요한 환경에서 뛰어난 성능을 발휘하며, 향후 다양한 모달리티에 대한 임베딩 레이어의 확장 가능성에 대해 더 많은 연구를 이끌어낼 수 있는 방향을 제시한다.