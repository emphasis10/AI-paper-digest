# MooER: LLM-based Speech Recognition and Translation Models from Moore Threads
## TL;DR
## Summary
- [https://arxiv.org/pdf/2408.05101.pdf](https://arxiv.org/pdf/2408.05101.pdf)

### 논문의 중요한 콘텐츠 요약

#### 1. Introduction (소개)
본 논문에서는 Moore Threads의 대규모 자동 음성 인식(ASR) 및 자동 음성 번역(AST) 모델인 MooER을 소개합니다. 이 모델은 5000시간의 가짜 라벨 데이터셋을 이용해 훈련되었으며, 다른 오픈 소스 모델과 경쟁할 수 있는 성능을 보여줍니다. 현대적인 LLM 기반 음성 모델을 활용하여 효율적이고 성능이 우수한 음성 인식 및 번역을 가능하게 했습니다. 주요 기여는 소량의 가짜 라벨 데이터를 사용하여 고성능 모델을 구현한 것과 ASR 및 AST 모델을 공개하고자 한다는 점입니다.

#### 2. Method (방법론)
MooER 모델은 Qwen2-7B-instruct 모델과 Paraformer 모델의 인코더를 사용합니다. 여기서는 압축과 샘플링 기법을 통해 음성 데이터를 처리하고, LLM을 사용해 음성 인식 및 번역 기능을 수행합니다. 모델 구조는 인코더, 어댑터, 디코더(LLM)로 구성되며, DeeSpeed와 기타 최적화 기법을 사용하여 학습 속도를 높였습니다.

#### 3. Dataset (데이터셋)
MooER 모델은 5000시간 규모의 MT5K 데이터셋을 사용하여 훈련되었습니다. 이 데이터셋은 Aishell2, Librispeech, multi_cn 등 다양한 출처의 데이터로 구성되어 있으며, 내부적으로 수집된 데이터도 포함됩니다. 전체 데이터는 가짜 라벨로 지정되어 있으며, 외부 클라우드 서비스를 통해 ASR 및 AST 라벨을 얻었습니다.

#### 4. Experiments (실험)
MooER 모델은 6개의 중국어 테스트 세트와 6개의 영어 테스트 세트에서 ASR 성능을 비교한 결과, 다른 최신 모델들에 비해 우수한 성능을 보여주었습니다. 80000시간의 내부 데이터로 학습된 모델도 우수한 성능을 나타냈습니다. AST 성능도 다양한 테스트 세트에서 높은 BLEU 점수를 기록하였습니다.

#### 5. Discussion (토론)
##### 5.1 Selection of Encoder (인코더 선택)
여러 인코더 중 Paraformer가 성능, 파라미터 크기 및 효율성 면에서 가장 우수하여 선택되었습니다.

##### 5.2 Granularity of audio modeling (오디오 모델링의 세분화 수준)
모델링의 세분화 수준으로 240ms, 180ms, 120ms를 실험한 결과, 120ms 단위가 가장 이상적이었습니다.

##### 5.3 Quickly adapt to the vertical domain (수직 도메인에 빠르게 적응)
MooER 모델은 영어 데이터 140시간을 사용해 더 나은 성능을 증명하며, 다른 저자원 오디오 이해 도메인에도 적용할 수 있는 가능성을 제시했습니다.

##### 5.4 Fully utilize the capabilities of large models (대규모 모델의 능력 최대한 활용)
LLM을 오디오 이해 학습에 통합해 더 빠르고 안정적인 수렴을 이루며 향상된 최종 결과를 얻을 수 있음을 확인했습니다.

##### 5.5 Acceleration method (가속 방법)
Deepspeed의 최적화 전략과 데이터 로더 최적화 기술을 사용해 학습 속도를 4-5배 향상시켰습니다.

#### 6. Demo
데모는 Moore Threads의 S4000 GPU를 사용하여 구현되었습니다. 모델 코드와 5000시간 데이터로 학습된 모델도 공개될 예정입니다.

#### 7. Models (모델)
MooER 모델은 GitHub 및 ModelScope에서 확인할 수 있습니다.

### Overall Summary (전반적인 요약)
이 논문은 제한된 데이터로도 높은 성능을 발휘할 수 있는 MooER 모델을 소개합니다. MooER은 Qwen2-7B 및 Paraformer 인코더를 이용해 고성능 ASR 및 AST 기능을 제공합니다. 5000시간의 가짜 라벨 데이터셋을 사용했으며, 다양한 최적화 기법을 통해 학습 속도와 성능을 높였습니다. 실험 결과, 중국어 및 영어 테스트 세트에서 우수한 성능을 보여주며, 다른 저자원 도메인에서도 효과적으로 적용될 수 있음을 증명했습니다. 향후 더 많은 데이터와 코드가 공개될 예정입니다.