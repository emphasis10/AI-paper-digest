# LLMs achieve adult human performance on higher-order theory of mind tasks
## TL;DR
## Summary
- [https://arxiv.org/pdf/2405.18870.pdf](https://arxiv.org/pdf/2405.18870.pdf)

#### 1. 서론
이 연구는 대형 언어 모델(LLMs)이 얼마나 높은 차원의 마음 이론(ToM) 능력을 발전시켰는지를 조사합니다. ToM은 자신과 다른 사람의 정신 상태를 추론하고 이해하는 능력으로, 인간의 사회적 지능의 핵심 요소입니다. 연구에서는 GPT-4와 Flan-PaLM이 ToM 작업에서 성인 수준의 성과를 보이며, 특히 GPT-4는 6차 주문 추론에서 성인 성과를 초과했습니다.

#### 2. 관련 연구
기존 연구들은 주로 2차 주문 ToM에 집중했으나, 본 연구는 2-6차 주문 ToM을 다룹니다. 인간의 ToM 능력은 일반적으로 5차 주문까지 가능하며, 이는 사회적 복잡성과 긍정적인 상관관계가 있습니다. LLM의 ToM 연구는 아직 초기 단계이며, 본 연구는 이를 확장하여 높은 차원까지 평가합니다.

#### 3. 자료 및 방법
MoToMQA(Multi-Order Theory of Mind Question & Answer)라는 새로운 벤치마크를 도입하여 인간과 LLM의 ToM 능력을 평가합니다. MoToMQA는 약 200단어의 짧은 이야기 7개와 20개의 진위문으로 구성됩니다. 이 중 10개는 2-6차 주문의 ToM 문장을 목표로 하고, 나머지 10개는 동등한 구문 복잡성의 사실 문장을 포함합니다. 이를 통해 ToM 명제와 사실 명제를 비교 평가합니다.

#### 4. 결과
GPT-4와 Flan-PaLM은 대부분의 ToM 작업에서 인간과 비슷한 성과를 보였고, 특히 GPT-4는 6차 주문 ToM에서 인간 성과를 초과했습니다. 반면, LaMDA와 GPT-3.5는 낮은 성과를 보였으며, 이는 모델 크기와 미세 조정의 영향을 받을 수 있습니다.

#### 5. 논의
모델 크기와 미세 조정이 LLM의 ToM 능력에 중요한 역할을 한다는 결론을 내렸습니다. GPT-4와 Flan-PaLM의 높은 성과는 이들이 더 큰 언어 모델로서 더 나은 언어 이해 능력을 갖추고 있음을 시사합니다. 또한, 이러한 모델의 성능은 단순한 통계적 관계 조작을 넘어서는 ToM 추론 능력을 발전시켰음을 나타냅니다.

### 전체 요약
이 논문은 대형 언어 모델이 높은 차원의 마음 이론(ToM) 능력을 어떻게 발전시켰는지에 대해 연구하고 있습니다. 새로운 벤치마크인 MoToMQA를 도입하여 GPT-4와 Flan-PaLM이 ToM 작업에서 인간과 비슷하거나 더 나은 성과를 보임을 입증했습니다. 이는 모델 크기와 미세 조정이 LLM의 ToM 능력에 중요한 영향을 미친다는 것을 시사합니다. 결과적으로, GPT-4와 Flan-PaLM은 더 큰 언어 이해 능력과 함께 복잡한 사회적 상호작용 맥락에서 뛰어난 성과를 보였습니다.

## Similar Papers
- [Measuring Psychological Depth in Language Models](2406.12680.md)
- [Open-Endedness is Essential for Artificial Superhuman Intelligence](2406.04268.md)
- [ClashEval: Quantifying the tug-of-war between an LLM's internal prior and external evidence](2404.10198.md)
- [Social Skill Training with Large Language Models](2404.04204.md)
- [Sparks of Artificial General Intelligence: Early experiments with GPT-4](2303.12712.md)
- [HelpSteer2: Open-source dataset for training top-performing reward models](2406.08673.md)
- [QLoRA: Efficient Finetuning of Quantized LLMs](2305.14314.md)
- [Tx-LLM: A Large Language Model for Therapeutics](2406.06316.md)
- [Observational Scaling Laws and the Predictability of Language Model Performance](2405.10938.md)
