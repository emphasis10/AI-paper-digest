# What matters when building vision-language models?
## TL;DR
## Summary
- [https://arxiv.org/pdf/2405.02246.pdf](https://arxiv.org/pdf/2405.02246.pdf)

### 주요 내용 요약

1. **서론 및 배경**:
   - 이 논문은 **Idefics2**라는 비전-언어 모델(VLM)을 소개합니다. Idefics2는 8억 개의 파라미터를 가지고 있으며, 다양한 멀티모달 벤치마크에서 뛰어난 성능을 보입니다. 이 모델은 텍스트와 이미지를 입력으로 받아 텍스트를 출력할 수 있으며, 여러 실험을 통해 모델 아키텍처, 데이터, 훈련 방법 등에 대한 중요한 결정을 뒷받침하는 실험적 증거를 제공합니다.

2. **방법론**:
   - Idefics2는 시각적 입력과 텍스트 입력을 결합하는 방법으로 완전 자회귀 아키텍처를 사용합니다. 이 접근 방식은 크로스 어텐션 아키텍처보다 효율적이며, 모델의 훈련 안정성을 보장합니다. 
   - 멀티모달 훈련 절차를 통해 모델의 훈련 안정성을 높이고, 시각적 입력을 텍스트 입력 공간으로 매핑하는 모듈을 도입하여 효율적인 추론을 가능하게 합니다. 
   - 다양한 데이터셋을 사용하여 모델을 훈련하며, 대규모 이미지-텍스트 쌍과 PDF 문서 등을 포함하여 훈련 데이터의 다양성을 확보합니다.

3. **실험**:
   - Idefics2는 VQAv2, TextVQA, OKVQA, COCO 등의 벤치마크에서 기존의 최첨단 모델들과 비교하여 우수한 성능을 보였습니다. 특히, VQAv2에서는 70.3%, TextVQA에서는 57.9%, OKVQA에서는 54.6%, COCO에서는 116.0의 점수를 기록했습니다.
   - 실험 결과, 시각적 입력과 텍스트 입력을 결합하는 방식에서 완전 자회귀 아키텍처가 크로스 어텐션 아키텍처보다 더 나은 성능을 보였으며, 모델의 훈련 안정성을 위해 Low-Rank Adaptation (LoRA)을 활용하는 것이 효과적임을 확인했습니다.

### 혁신적인 부분
Idefics2의 혁신성은 완전 자회귀 아키텍처를 사용하여 시각적 입력과 텍스트 입력을 결합하는 방식에서 효율성과 성능을 동시에 확보한 데 있습니다. 이 접근 방식은 크로스 어텐션 아키텍처보다 더 나은 성능을 제공하며, 모델의 훈련 안정성을 보장합니다. 또한, 다양한 데이터셋을 사용하여 훈련 데이터의 다양성을 확보하고, 이를 통해 다양한 멀티모달 작업에서 뛰어난 성능을 보입니다. Idefics2는 특히 대규모 비전-언어 모델의 성능을 극대화하면서도 효율성을 유지하는 데 중점을 둡니다.

## Similar Papers
- [Beyond the Speculative Game: A Survey of Speculative Execution in Large Language Models](2404.14897.md)
- [VoCo-LLaMA: Towards Vision Compression with Large Language Models](2406.12275.md)
- [Mamba: Linear-Time Sequence Modeling with Selective State Spaces](2312.00752.md)
- [Whiteboard-of-Thought: Thinking Step-by-Step Across Modalities](2406.14562.md)
- [Improving Visual Commonsense in Language Models via Multiple Image Generation](2406.13621.md)
- [The Instruction Hierarchy: Training LLMs to Prioritize Privileged Instructions](2404.13208.md)
- [ChartGemma: Visual Instruction-tuning for Chart Reasoning in the Wild](2407.04172.md)
- [WalledEval: A Comprehensive Safety Evaluation Toolkit for Large Language Models](2408.03837.md)
- [Transformer Layers as Painters](2407.09298.md)
