# The Unreasonable Ineffectiveness of the Deeper Layers
## TL;DR
## Summary
- [https://arxiv.org/pdf/2403.17887.pdf](https://arxiv.org/pdf/2403.17887.pdf)

이 논문은 인기 있는 오픈소스 대규모 언어 모델(LLM)의 성능을 크게 저하시키지 않으면서도 모델의 상당 부분(최대 절반까지)을 제거할 수 있는 간단한 층 제거(Pruning) 전략을 실증적으로 연구합니다. 이를 위해 층 간의 유사성을 고려하여 최적의 층 블록을 식별하고, 제거로 인한 손상을 "치유(Healing)"하기 위해 소량의 미세조정(Finetuning)을 수행합니다. 특히, Parameter-Efficient Finetuning(PEFT) 방법을 사용하여 각 실험을 단일 A100 GPU에서 수행할 수 있습니다. 이러한 결과는 층 제거 방법이 미세조정 시 계산 자원을 더욱 줄일 수 있음을 제안하며, 이는 기억 용량과 추론 시간을 개선할 수 있음을 의미합니다. 과학적 관점에서는 이러한 LLM의 층 제거에 대한 강인함이 현재의 사전 훈련 방법이 네트워크의 더 깊은 층에 있는 매개변수를 적절히 활용하지 못하거나, 얕은 층이 지식을 저장하는 데 중요한 역할을 할 수 있음을 시사합니다.

1. **서론(Introduction)**: 최근 몇 년간 대규모 언어 모델(LLM)이 연구 도구에서 유용한 제품으로 발전했습니다. 이러한 모델의 훈련에는 많은 자원이 소모되며, 훈련 후에는 모델 크기를 줄이고 추론 시간을 단축하는 다양한 기술이 연구되고 있습니다. 이 논문에서는 층 제거 방법을 사용하여 LLM의 계산 효율성을 높이는 새로운 접근 방식을 제시합니다.
2. **문헌 고찰(Literature Review)**: 기존의 층 제거 방법과 이를 대체할 수 있는 모델 압축 방법(예: 모델 축소, 효율적인 미세조정 및 추론 가속화 방법)에 대해 논의합니다. 또한 LLM의 층별 통계적 특성에 대한 과학적 연구를 소개합니다.
3. **방법(Method)**: 층 제거 전략의 직관과 구체적인 알고리즘을 설명합니다. 이는 층 간 유사도를 기반으로 최적의 층 블록을 식별하고, 제거 후 미세조정을 통해 모델을 치유하는 과정을 포함합니다.
4. **결과(Results)**: 다양한 질의응답(QA) 벤치마크와 토큰 예측 손실에 대한 층 제거 전략의 효과를 분석합니다. 이는 층 제거가 모델 성능에 미치는 영향과 층 간 유사도가 모델 크기와 가족에 따라 어떻게 달라지는지를 보여줍니다.
5. **토론 및 향후 방향(Discussion and Future Directions)**: 이 연구가 제시하는 층 제거 방법의 잠재적 응용과 함께, 모델의 더 깊은 층을 더 효과적으로 활용하는 방법에 대한 질문을 제기합니다. 또한, 이 연구가 어떻게 개방 소스 및 학술 커뮤니티에 기여할 수 있는지에 대해 논의합니다.

이 연구는 대규모 언어 모델의 계산 효율성을 높이기 위한 새로운 전략을 제시하며, 이는 모델의 크기와 추론 시간을 줄이는 동시에 성능을 유지할 수 있는 가능성을 열어줍니다.