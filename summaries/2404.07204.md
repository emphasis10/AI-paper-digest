# BRAVE: Broadening the visual encoding of vision-language models
## TL;DR
## Summary
- [https://arxiv.org/pdf/2404.07204.pdf](https://arxiv.org/pdf/2404.07204.pdf)

이 문서에서는 **BRAVE**라는 새로운 접근 방식을 통해 시각 언어 모델(VLMs: Vision-Language Models)의 한계를 넘어서는 방법을 제안하고 있습니다. **BRAVE**는 다양한 시각 인코더들을 결합하여 훨씬 더 강력하고 다양한 표현력을 가진 시각적 표현을 만들어냄으로써, 기존의 방식들이 가지고 있던 여러 문제점들을 해결하고, 특히 캡셔닝 및 시각적 질문 답변(VQA: Visual Question Answering) 작업에서 최고의 성능을 달성하는 방법에 대해 설명합니다.

### 1. 각 절의 중요 내용 요약

- **서론 및 문제 제기**: 이 연구는 VLMs가 특정한 이미지의 특징을 "인식하지 못하는 문제"와 "시각적 환상" 같은 여러 한계에 직면하여 있다고 시작합니다. 이러한 문제들을 해결하기 위해 다양한 시각적 인코더들의 특징을 결합하여 보다 포괄적인 시각적 표현을 만드는 것을 목표로 합니다.

- **BRAVE 방법론**: **BRAVE**는 여러 시각 인코더들로부터의 특징들을 결합하여, 언어 모델(LM)에 직접 투입할 수 있는 더 다양하고 압축된 시각적 표현을 만듭니다. 이 방법은 캡셔닝과 VQA 작업에 있어서 최고의 성능을 내며, VLMs가 가지던 문제들을 크게 줄입니다.

- **실험 결과 및 분석**: 연구진은 **BRAVE**의 다양한 캡셔닝 및 VQA 작업에서의 성능을 평가하고, 이전 방식들에 비해 그 성능이 얼마나 향상됐는지를 보여줍니다. 또한, 다양한 시각적 인코더들을 결합하는 것의 중요성을 강조하며, 시각 축(scaling along the vision axis)을 확장하는 것이 VLMs의 가능성을 더욱 키울 수 있다고 주장합니다.

### 2. 전체 요약

이 논문은 **BRAVE**를 통해 시각 언어 모델의 시각적 인코딩 능력을 넓혀, 기존에 한계로 지적되었던 여러 문제점들을 개선하고 캡셔닝 및 시각적 질문 답변 작업에서 최고의 성능을 달성했습니다. **BRAVE**의 핵심은 다양한 시각 인코더들의 특징을 효율적으로 결합하여 보다 포괄적이고 컨텍스트에 맞는 시각적 표현을 만드는 것에 있으며, 이는 기존 방식들이 가진 한계를 극복하는 데 큰 기여를 하였습니다. 이 연구는 VLMs의 더 큰 발전을 위한 새로운 방향을 제시하며, 특히 시각적 인코더들을 결합하는 방식의 중요성을 강조합니다.

## Similar Papers
- [An Introduction to Vision-Language Modeling](2405.17247.md)
- [Improved Baselines with Visual Instruction Tuning](2310.03744.md)
- [Diffusion Feedback Helps CLIP See Better](2407.20171.md)
- [TextHawk: Exploring Efficient Fine-Grained Perception of Multimodal Large Language Models](2404.09204.md)
- [AlignGPT: Multi-modal Large Language Models with Adaptive Alignment Capability](2405.14129.md)
- [4M-21: An Any-to-Any Vision Model for Tens of Tasks and Modalities](2406.09406.md)
- [DOCCI: Descriptions of Connected and Contrasting Images](2404.19753.md)
- [OmniCorpus: A Unified Multimodal Corpus of 10 Billion-Level Images Interleaved with Text](2406.08418.md)
- [Prism: A Framework for Decoupling and Assessing the Capabilities of VLMs](2406.14544.md)
