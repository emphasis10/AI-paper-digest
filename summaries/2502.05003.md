# QuEST: Stable Training of LLMs with 1-Bit Weights and Activations
## TL;DR
## Summary
- [https://arxiv.org/pdf/2502.05003.pdf](https://arxiv.org/pdf/2502.05003.pdf)

### 1. 각 섹션의 주요 내용 요약 (한국어)

#### 1. 서론
이 연구는 대형 언어 모델(LLMs)의 높은 컴퓨팅 비용을 줄이기 위해 양적화된 모델을 사용하는 방법을 제안합니다. 기존의 연구들은 8비트 정밀도가 최적이라고 주장하였으나, 본 연구에서는 QuEST라는 새로운 방법을 통해 4비트와 1비트 모델에서 더 나은 정확도를 보여줄 수 있음을 입증합니다.

#### 2. 배경 및 관련 연구
초기 연구는 고압축 신경망을 다루었으며, 이후 양적화 인식 훈련(QAT)으로 발전되었습니다. 이 연구는 QAT의 효율성을 개선하고 정확도를 높이기 위한 새로운 방법론을 제시합니다. 특히, QAT 방법의 두 가지 핵심 요소인 가중치/활성화의 양적화 및 그래디언트 추정에 중점을 두고 있습니다.

#### 3. QuEST 방법론
QuEST는 데이터 및 훈련 예산에 따라 가중치와 활성화의 양적화를 통해 높은 정확도를 달성하는 방법입니다. 본 논문에서 제안된 접지 정규화 및 새로운 신뢰 그래디언트 추정기를 통해 양적화 오류를 최소화하는 데 중점을 두고 있습니다. 이 방법은 모든 하드웨어 지원 정밀도에서 안정성을 갖추고 있습니다.

#### 4. 실험 결과
QuEST 모델은 다양한 크기와 정밀도의 모델에서 안정적으로 수렴하며, 1비트와 4비트 작동 시에도 기존의 BF16보다 낮은 손실률을 기록했습니다. 이는 새로운 스케일링 법칙을 유도하며, 4비트 모델이 정확성 면에서 기존 모델보다 우수하다는 결론에 도달했습니다.

#### 5. GPU 실행 지원
QuEST 모델은 효율적으로 실행될 수 있도록 GPU 커널 지원을 제공합니다. 이를 통해 양적화된 연산이 기존의 연산보다 훨씬 빠르게 수행될 수 있습니다. 모델의 특수한 구조로 인해 훈련 및 추론 성능이 향상되었습니다.

#### 6. 토론 및 향후 작업
QuEST는 저정밀 모델 훈련에 안정성을 제공하며, 4비트 모델이 표준 정밀도 모델보다 더 나은 성능을 발휘할 수 있다는 신뢰를 줍니다. 앞으로는 QuEST의 더 큰 모델에 대한 적용과 다른 아키텍처로의 확장이 필요한 과제로 남아있습니다.

### 2. 전체 요약 (한국어)
이 연구에서는 대형 언어 모델의 훈련 시 컴퓨팅 비용을 줄이기 위한 양적화 방법인 QuEST를 제안합니다. QuEST는 가중치와 활성화를 1비트 및 4비트 정밀도로 안정적으로 훈련할 수 있으며, 기존의 모델보다 더 높은 정확도를 자랑합니다. 이 연구는 다양한 모델 크기에서 실험하여 QuEST의 효과성을 입증하였고, 훈련 및 추론의 효율성을 높이는 여러 방법론을 제공하고 있습니다. 향후 QuEST의 적용 범위를 넓히고, 더 큰 모델에서의 성능을 평가하는 작업이 필요합니다.