# Maximizing Alignment with Minimal Feedback: Efficiently Learning Rewards for Visuomotor Robot Policy Alignment
## TL;DR
## Summary
- [https://arxiv.org/pdf/2412.04835.pdf](https://arxiv.org/pdf/2412.04835.pdf)

**요약 - 섹션별**

1. **서론**
   - 시각 운동 로봇 정책은 대규모 데이터셋으로 예비 훈련이 진행되고 있으나, 사용자 선호도와의 정렬 문제를 겪는다. RLHF(인간 피드백을 통한 강화 학습)는 비체화 도메인에서 성공적인 방법이지만, 시각 운동 정책에서는 높은 인간 피드백 요구량으로 인해 효과적이지 않다. RAPL(Representation-Aligned Preference-based Learning)는 적은 인간 피드백으로 시각적 보상을 학습하는 방법을 제안한다.

2. **관련 연구**
   - 시각적 보상 모델은 이미지 관찰을 통해 작업 선호도를 학습하려는 접근법으로, 자가 지도 학습 및 최적 수송을 통한 피처 매칭 등의 기술이 있다. 이러한 방법들은 사용자 선호도를 반영한 경우에 효과적이다.

3. **문제 설정**
   - 로봇이 사용자의 선호도에 맞춘 작업을 수행할 수 있도록 시각적 표현을 정렬해야 한다. 사용자와 로봇 간의 표현 정렬은 중요하며, 이는 주어진 시각적 관찰 내에서 적절한 피처를 선택하는 것을 포함한다.

4. **RAPL: 표현 정렬 선호 기반 학습**
   - RAPL은 사용자의 시각적 표현과 학습하기 위해 시각적 보상을 구성하며, 이를 위해 선호 기반 학습을 사용한다. 이 방법은 사용자의 선호도에 맞게 사전 훈련된 비전 인코더를 미세 조정하여 정책 정렬을 달성한다.

5. **모의 실험 및 실험 결과**
   - RAPL은 시뮬레이션에서 기존의 RLHF보다 적은 데이터로 높은 품질의 보상을 학습할 수 있으며, 새로운 로봇 구현에서도 일반화할 수 있다. 실제 하드웨어 실험을 통해 세 가지 물체 조작 작업에서 RAPL이 효과적으로 정책을 정렬할 수 있음을 증명한다.

**전체 요약**

이 논문에서는 시각 운동 로봇 정책과 사용자의 선호를 일치시키는 효율적인 방법을 제안하였다. RAPL은 사전 훈련된 비전 인코더의 미세 조정을 통해 시각적 표현을 사용자와 정렬하고, 이를 통해 더욱 적은 피드백 데이터로도 정밀한 정책 정렬을 실현한다. 이는 특히 시뮬레이션과 실제 작업 모두에서 실현 가능성이 높고, 차세대 시각 운동 정책의 효율적 정렬을 위한 첫걸음을 내딛었다고 볼 수 있다.