# JointNet: Extending Text-to-Image Diffusion for Dense Distribution Modeling
## TL;DR
## Summary
- [https://arxiv.org/pdf/2310.06347.pdf](https://arxiv.org/pdf/2310.06347.pdf)

이 연구 논문은 텍스트-이미지 확산 모델을 기반으로 하는 새로운 신경망 구조인 JointNet을 소개합니다. JointNet은 이미지와 밀접한 추가 데이터 모달리티(예: 깊이 맵)의 결합 분포를 효율적으로 모델링하기 위해 기존 텍스트-이미지 확산 모델을 확장한 것입니다. 이를 통해 다양한 컴퓨터 비전 응용 프로그램에서 사용될 수 있습니다.

### 주요 내용 요약

1. **서론 및 배경**:
   - 최근 텍스트-이미지 확산 모델이 놀라운 시각적 이미지 생성 능력을 보여주었습니다. 이 연구는 RGB 이미지 뿐만 아니라 픽셀별 밀도 라벨과의 결합 분포를 학습하는 것의 중요성을 강조하며, 이를 위한 JointNet을 제안합니다.

2. **JointNet의 구조 및 기능**:
   - JointNet은 기존의 확산 네트워크의 복사본을 만들어 추가 모달리티 분기로 사용하며, 이는 RGB 분기와 밀접하게 연결됩니다. 이 구조는 새로운 모달리티 분포를 효과적으로 학습하면서도 사전 훈련된 모델의 강력한 일반화 능력을 유지합니다.

3. **성능 평가 및 응용**:
   - JointNet은 RGBD 확산을 예로 들어 다양한 응용 프로그램에서의 유효성을 입증합니다. 이는 공동 RGBD 생성, 밀도 깊이 예측, 깊이 조건부 이미지 생성 및 일관된 타일 기반 3D 파노라마 생성을 포함합니다.

### 혁신적인 부분
JointNet의 혁신성은 텍스트-이미지 확산 모델을 확장하여 추가 밀도 모달리티와의 결합 분포를 모델링하는 새로운 네트워크 구조를 도입한 것입니다. 이는 다양한 다운스트림 작업에 적용 가능하며, 특히 대규모 이미지와 밀도 라벨의 결합 생성에 효과적입니다.

이 논문은 기존의 텍스트-이미지 확산 모델을 확장하여 추가 데이터 모달리티와의 결합 분포를 모델링하는 방법을 제시함으로써, 다양한 컴퓨터 비전 응용 프로그램에 큰 가능성을 제시합니다.

## Similar Papers
- [Direct2.5: Diverse Text-to-3D Generation via Multi-view 2.5D Diffusion](2311.15980.md)
- [Controlling Space and Time with Diffusion Models](2407.07860.md)
- [Affine-based Deformable Attention and Selective Fusion for Semi-dense Matching](2405.13874.md)
- [RealmDreamer: Text-Driven 3D Scene Generation with Inpainting and Depth Diffusion](2404.07199.md)
- [Lumina-mGPT: Illuminate Flexible Photorealistic Text-to-Image Generation with Multimodal Generative Pretraining](2408.02657.md)
- [GTR: Improving Large 3D Reconstruction Models through Geometry and Texture Refinement](2406.05649.md)
- [StoryDiffusion: Consistent Self-Attention for Long-Range Image and Video Generation](2405.01434.md)
- [DiffIR2VR-Zero: Zero-Shot Video Restoration with Diffusion-based Image Restoration Models](2407.01519.md)
- [Collaborative Video Diffusion: Consistent Multi-video Generation with Camera Control](2405.17414.md)
