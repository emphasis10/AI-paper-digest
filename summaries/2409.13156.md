# RRM: Robust Reward Model Training Mitigates Reward Hacking
## TL;DR
## Summary
- [https://arxiv.org/pdf/2409.13156.pdf](https://arxiv.org/pdf/2409.13156.pdf)

### 1. 각 섹션 요약 및 분석

#### 서론

이 논문은 인간의 피드백을 통한 강화 학습(RLHF)을 이용하여 대형 언어 모델(LLMs)을 인간의 선호도와 맞추는 방법을 다룹니다. 기존의 보상 모델(RM)은 특정 프롬프트에 연결된 응답 쌍에 의존하기 때문에 프롬프트로 인한 선호도와 프롬프트와 무관한 길이, 형식 등의 아티팩트를 구분하기 어렵습니다. 이를 해결하기 위해, 저자들은 아티팩트 없이 선호도를 학습할 수 있는 인과 프레임워크와 데이터 증강 기술을 도입했습니다. 실험 결과, 이 방법은 원하지 않는 아티팩트를 필터링하는 데 성공하여 더 견고한 보상 모델(RRM)을 만들었습니다.

#### 보상 모델 및 보상 해킹

전통적인 보상 모델 훈련은 문맥 신호와 문맥과 무관한 아티팩트를 차별화하는 데 실패하는 주요 문제를 해결하려고 합니다. 이를 위해, 저자들은 인간 선호도 모델링을 위한 인과 그래프를 제안하고 보상 모델이 학습한 아티팩트를 완화하기 위해 데이터 증강을 도입했습니다. 그 결과, 견고한 보상 모델로 훈련된 정책은 일관되게 기존의 보상 모델 기반 정책보다 우수한 성능을 보였습니다.

#### 방법론

RLHF는 LLM을 훈련하여 인간의 선호도와 맞도록 응답을 생성하는데, 이는 보상 모델을 사용하여 강화 학습을 통해 이루어집니다. 더 나은 보상 모델을 만들기 위해, 저자들은 인과 프레임워크를 사용하여 데이터 증강 접근법을 도입했습니다. 이 방법을 통해, 선택된 응답과 거부된 응답 간의 아티팩트를 균형있게 조정하여 더 견고한 보상 모델을 구축할 수 있었습니다.

#### 실험과 결과

주요 실험 결과는 다음과 같습니다:

- **보상 모델 정확도**: RRM은 "Chat Hard"와 "Safety" 항목에서 상당한 향상을 보였으나, "Reasoning"에서는 약간의 성능 저하가 있었습니다.
- **정책 품질**: RRM을 기반으로 한 정책이 기존 보상 모델을 기반으로 한 정책보다 짧은 응답을 생성하며, 이는 길이를 아티팩트로 효과적으로 제어한 결과라고 할 수 있습니다.

#### 결론

이 논문은 전통적인 보상 모델 훈련의 주요 문제인 문맥 신호와 문맥과 무관한 아티팩트를 차별화하지 못하는 문제를 지적합니다. 이를 해결하기 위해 인과 그래프와 데이터 증강을 도입했으며, 실험 결과 이 방법이 더 견고한 보상 모델을 만들어내는 데 효과적임을 입증했습니다. 향후 연구에서는 증강된 쌍 필터링과 응답 쌍 구성 시 아티팩트 매칭을 추가적으로 탐구함으로써 훈련 과정을 더욱 개선할 예정입니다.

### 2. 전체 요약

이 논문은 대형 언어 모델을 인간의 선호도에 맞추기 위한 강화 학습 방법론을 제안합니다. 특히, 보상 모델 훈련 시 문맥 신호와 문맥과 무관한 아티팩트를 차별화하는 어려움을 해결하기 위해, 저자들은 인과 프레임워크와 데이터 증강 방법을 도입했습니다. 실험 결과, 이러한 방법이 기존의 보상 모델보다 더 높은 정확성과 더 나은 정책 성능을 제공함을 입증했습니다. 이 연구는 향후 더욱 정교한 보상 모델 훈련 방식을 탐구하는 데 중요한 기초를 제공합니다.