# Less is More: Task-aware Layer-wise Distillation for Language Model Compression
## TL;DR
## Summary
- [https://arxiv.org/pdf/2210.01351.pdf](https://arxiv.org/pdf/2210.01351.pdf)

### 1. 요약 (각 섹션별 요약)

#### Abstract & Introduction
**요약**: 
- 대형 언어 모델은 많은 자연어 처리(NLP) 작업에서 최첨단 성능을 달성했으나, 큰 파라미터 수로 인해 자원 제한이 있는 환경에서 배포가 어려움.
- 지식 증류(KD)는 대형 모델(교사 모델)의 지식을 작은 모델(학생 모델)에 압축하는데 사용됨.
- 그러나, 레이어 단위 증류는 교사의 중간 표현을 활용하지 못하고, 학생이 교사보다 작은 용량을 가지므로 어렵다.
- 이를 해결하기 위해 Task-aware layEr-wise Distillation (TED) 방법을 제안.
- TED는 각 레이어에서 학생과 교사의 히든 표현을 정렬하기 위해 작업 인식 필터를 설계하며 목표 작업에 유용한 지식만을 추출함.

#### Task-aware layEr-wise Distillation (TED)
**요약**:
- **Stage I**: 교사와 학생 모델의 파라미터를 고정하고 작업 인식 필터만 훈련함. 각 필터는 모델의 히든 표현을 입력으로 받아 작업 손실을 출력.
- **Stage II**: 필터의 작업별 헤드를 제거하고 학생과 필터를 공동 훈련해 히든 표현의 차이를 최소화함.
- 필터는 교사와 학생 간에 전달되는 불필요한 정보를 줄이고 작업별 지식만 추출해 증류를 용이하게 함.

#### Experiments
**요약**:
- Continual pre-training과 task-specific fine-tuning 두 가지 설정에서 TED를 평가함.
- TD는 Continual pre-training 설정에서 GPT-2 모델을 사용해, task-specific fine-tuning 설정에서 DeBERTaV3 모델을 사용해 평가됨.
- TED는 GLUE 벤치마크와 SQuAD 데이터셋을 포함한 다양한 다운스트림 작업에서 기존 방법보다 성능 향상을 보여줌.

#### Analysis
**요약**:
- TED는 학생이 목표 작업에서 학습할 때 교사 모델에 비해 빠르게 수렴하고 낮은 학습 손실을 달성.
- 필터는 작업별 지식을 효과적으로 캡처해 증류를 용이하게 함.
- TED는 필터의 다양성을 통해 학생과 교사의 히든 표현과의 격차를 줄일 수 있음.

### 2. 전체 요약

이번 논문에서는 대형 언어 모델의 압축과 자원 제한 환경에서의 배포 문제를 해결하기 위해 Task-aware layEr-wise Distillation (TED) 방법을 제안합니다. TED는 작업별 필터를 사용하여 각 레이어에서 교사와 학생 모델 간의 히든 표현을 정렬하고, 이를 통해 작업별로 유용한 정보만을 학생 모델에 전달합니다. 두 가지 평가 시나리오에서 TED는 기존 방법들에 비해 일관되고 큰 성능 향상을 보였습니다. 핵심은 작업 인식 필터를 사용해 불필요한 정보를 줄이고, 목표 작업에서의 학습 성능을 향상시키는 데 있습니다.

이 설명을 바탕으로 프레젠테이션을 준비하면 논문의 주요한 기여와 혁신적인 부분을 잘 전달할 수 있을 것입니다.