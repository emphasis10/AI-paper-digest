# LLM in a flash: Efficient Large Language Model Inference with Limited Memory
## TL;DR
## Summary
- [https://arxiv.org/pdf/2312.11514.pdf](https://arxiv.org/pdf/2312.11514.pdf)

### 섹션별 요약

#### 1. Introduction (소개)
최근 몇 년 동안 GPT-3, OPT, PaLM과 같은 대규모 언어 모델(LLM)은 다양한 자연어 작업에서 뛰어난 성능을 보였습니다. 그러나 이러한 모델은 추론을 위해 상당한 계산 및 메모리 자원이 필요하며, 특히 메모리가 제한된 장치에서는 효율적으로 실행하기 어렵습니다. 본 논문은 모델 매개변수를 플래시 메모리에 저장하고 필요에 따라 DRAM으로 불러오는 방법을 제안합니다. 이를 통해 DRAM 용량을 초과하는 모델을 효율적으로 실행할 수 있습니다.

#### 2. Flash Memory & LLM Inference (플래시 메모리 및 LLM 추론)
플래시 메모리는 높은 대역폭과 낮은 지연 시간을 제공하지만, DRAM에 비해 성능이 떨어집니다. 본 논문에서는 플래시 메모리의 특성을 고려하여 추론 시 데이터 전송량을 줄이고 더 큰 연속 청크로 데이터를 읽는 방법을 제안합니다.

#### 3. Load From Flash (플래시에서 로드)
DRAM 용량이 모델 크기보다 작은 경우 추론을 수행하는 방법을 설명합니다. 데이터 전송을 최적화하기 위해 활성화 희소성을 활용하고, 윈도잉 및 행-열 번들링 기법을 사용하여 데이터 전송량을 줄이고 읽기 처리량을 증가시킵니다.

#### 4. Results (결과)
- **OPT 6.7B 모델**: 제안된 기법을 사용하여 DRAM 용량의 절반으로 OPT 6.7B 모델을 실행할 수 있으며, 추론 속도는 CPU에서 4-5배, GPU에서 20-25배 증가했습니다.
- **Falcon 7B 모델**: 유사한 기법을 사용하여 Falcon 7B 모델에서도 성능이 향상되었습니다.

#### 5. Related Works (관련 연구)
모델 압축 및 선택적 실행과 같은 기존의 효율적인 추론 방법들을 소개하며, 본 논문의 접근법이 이들과 어떻게 상호 보완적인지 설명합니다.

#### 6. Conclusion and Discussion (결론 및 논의)
본 연구는 메모리 용량이 제한된 장치에서 대규모 언어 모델을 실행하는 데 있어 중요한 문제를 해결했습니다. 플래시 메모리와 DRAM의 특성을 고려한 하드웨어 인식 전략을 통해 데이터 로드량을 줄이고 메모리 사용 효율을 높이는 두 가지 혁신적인 기법을 도입했습니다. 이러한 방법은 특히 자원이 제한된 환경에서 고급 LLM을 배포하는 데 필수적입니다.

### 전체 요약
본 논문은 메모리 용량이 제한된 장치에서 대규모 언어 모델(LLM)을 효율적으로 실행하는 방법을 제안합니다. 주요 기여는 다음과 같습니다:
1. **윈도잉(Windowing)**: 과거에 활성화된 뉴런을 재사용하여 데이터 전송을 줄이는 기법.
2. **행-열 번들링(Row-Column Bundling)**: 플래시 메모리의 연속적 데이터 접근 강점을 활용하여 더 큰 데이터 청크를 읽는 기법.

이러한 방법을 통해 모델 크기의 두 배에 해당하는 용량의 모델을 DRAM에서 실행할 수 있으며, 추론 속도를 CPU에서는 4-5배, GPU에서는 20-25배 향상시킬 수 있음을 보여주었습니다. 이 연구는 제한된 메모리 환경에서 대규모 언어 모델의 실용성을 크게 확장할 수 있습니다.

## Similar Papers
- [PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU](2312.12456.md)
- [LazyLLM: Dynamic Token Pruning for Efficient Long Context LLM Inference](2407.14057.md)
- [ReLU Strikes Back: Exploiting Activation Sparsity in Large Language Models](2310.04564.md)
- [MemServe: Context Caching for Disaggregated LLM Serving with Elastic Memory Pool](2406.17565.md)
- [KV-Runahead: Scalable Causal LLM Inference by Parallel Key-Value Cache Generation](2405.05329.md)
- [Accelerating LLM Inference with Staged Speculative Decoding](2308.04623.md)
- [SARATHI: Efficient LLM Inference by Piggybacking Decodes with Chunked Prefills](2308.16369.md)
- [Hierarchical Context Merging: Better Long Context Understanding for Pre-trained LLMs](2404.10308.md)
- [TriForce: Lossless Acceleration of Long Sequence Generation with Hierarchical Speculative Decoding](2404.11912.md)
