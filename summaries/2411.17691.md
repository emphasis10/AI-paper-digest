# Low-Bit Quantization Favors Undertrained LLMs: Scaling Laws for Quantized LLMs with 100T Training Tokens
## TL;DR
## Summary
- [https://arxiv.org/pdf/2411.17691.pdf](https://arxiv.org/pdf/2411.17691.pdf)

### 1. 섹션별 요약

- **소개**  
  논문은 양자화라는 기술이 대형 언어 모델(LLM)의 효율적인 배치를 위해 사용되는 방법임을 설명합니다. 양자화는 모델의 디스크 크기를 줄이고 메모리 사용량을 줄이며, 낮은 정밀도의 가중치와 활성화를 통해 추론 효율성을 높이는 것으로 설명됩니다.

- **거대 언어 모델을 위한 스케일링 법칙**  
  규모 증가에 따른 모델 성능 향상에 대한 스케일링 법칙이 설명됩니다. 더 큰 파라미터 수와 더 많은 훈련 토큰을 가진 모델들이 나아지는 경향이 있으며, 모델이 조금씩 복잡한 언어 패턴을 포착하고 일반화 능력을 높이는 것과 연관이 있습니다.

- **저비트 양자화를 위한 스케일링 법칙**  
  저비트 양자화를 적용했을 때의 모델 성과 변화 분석을 위한 스케일링 법칙을 제안합니다. 양자화 유도 열화(QiD)의 변화를 조사하고, 이를 통해 모델의 훈련 수준을 측정할 수 있는 신호로 사용될 수 있음을 설명합니다.

- **실험 설정**  
  Pythia 모델 세트를 이용한 실험환경과 방법론이 설명됩니다. 다양한 크기의 유의미한 데이터 포인트를 모으기 위해 모델의 주요 체크포인트를 저비트로 양자화하는 실험이 진행되었습니다.

- **결론**  
  연구는 저비트 양자화가 충분히 훈련되지 않은 LLM에 유리하다는 것을 밝히고, QiD가 모델의 훈련 수준을 결정짓는 신호로 활용될 수 있음을 제안합니다. 앞으로의 연구에서 모델의 훈련 수준이 저비트 양자화 연구의 평가 시 고려해야 한다고 지적합니다.

### 2. 전체 요약

이 논문은 저비트 양자화 기술이 대형 언어 모델에 미치는 영향에 대해 연구합니다. 주요 기여는 저비트 양자화가 아직 충분히 학습되지 않은 모델에서는 잘 작동하지만, 완전히 학습된 모델에서는 성능 저하가 발생한다는 점을 밝힌 것입니다. 이는 이전 연구에서는 주로 간과되었던 사실입니다. 논문은 또한 여러 훈련 토큰 수, 모델 크기, 비트 너비에 걸친 스케일링 법칙을 도출하여, 모델의 학습 수준을 평가하는 데 사용할 수 있는 새로운 관점을 제안합니다. 이를 통해 저비트 양자화 성능을 예측하고, 다가오는 100조개의 훈련 토큰에서 저비트 양자화의 실용성을 재고할 필요가 있음을 시사합니다.