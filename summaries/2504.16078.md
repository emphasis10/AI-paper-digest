# LLMs are Greedy Agents: Effects of RL Fine-tuning on Decision-Making Abilities
## TL;DR
## Summary
- [https://arxiv.org/pdf/2504.16078.pdf](https://arxiv.org/pdf/2504.16078.pdf)

1. 각 섹션 요약

   - **소개 (Introduction)**: 대형 언어 모델(LLM)은 다양한 분야에서 성공을 거두었으나, 의사결정 시나리오에서는 기대에 못 미치는 성능을 보입니다. 본 연구는 LLM이 실제로 왜 최적이 아닌 성능을 보이는지에 중점을 두고, 탐험(exploration), 알고-행동 격차(knowing-doing gap), 빈도 편향(frequency bias)을 세 가지 주요 실패 모드로 정의하였습니다.
   
   - **관련 작업 (Related Work)**: 강화학습(리인포스먼트 러닝)과 LLM의 탐험 기법을 비교하고, 기존의 해결책인 탐험 및 착취의 균형 통제 기법들을 검토합니다.
   
   - **실험 방법론 (Methodology)**: 연구는 Multi-armed bandits(MAB)와 텍스트 기반 틱택토를 사용하여 실험을 진행합니다. 대상은 LLM의 결정능력을 향상시키기 위한 강화학습 파인튜닝 전략 (RLFT)을 탐구합니다.
   
   - **결과 (Results)**: 연구는 RLFT가 LLM의 탐험 기능과 성능 향상에 기여한다는 것을 보여줍니다. RLFT는 의사결정 시나리오에서 greedy behavior(탐욕적 행동)를 감소시키고 빈도 편향을 상쇄하지만, 여전히 최적의 탐험 전략에는 미치지 못합니다.
   
   - **결론 (Conclusion)**: 연구는 LLM이 의사결정 시나리오에서 가지고 있는 주된 문제점과 이를 개선하기 위한 RLFT 접근법의 효과성을 제시합니다. LLM의 성능을 최적화하기 위해서는 보상의 형성과 탐험 전략의 통합이 중요하다고 강조합니다.

2. 전체 요약

   이 논문은 LLM이 의사결정 시나리오에서 왜 최적의 성능을 발휘하지 못하는지를 탐구합니다. 연구는 주된 문제를 탐험 부족, 알고-행동 격차, 빈도 편향으로 정의하고, 이를 극복하기 위한 RLFT의 효과를 실험적으로 입증합니다. 결론적으로 RLFT는 LLM의 탐험을 개선하고, 결정 능력을 향상시킬 수 있지만, 여전히 최적의 탐험 공식을 찾기 위한 연구가 필요합니다.