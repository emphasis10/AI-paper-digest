# Emu3: Next-Token Prediction is All You Need
## TL;DR
## Summary
- [https://arxiv.org/pdf/2409.18869.pdf](https://arxiv.org/pdf/2409.18869.pdf)

### 1. 섹션별 요약 및 주요 공헌
#### 1. 서론 (Introduction)
- **요약**: Emu3는 텍스트, 이미지, 비디오 모두를 다루는 차세대 멀티모달 모델을 소개합니다. 기존의 확산 모델(Stable Diffusion)과 조합적 접근법(CLIP와 LLMs 결합) 대신 다음 토큰 예측(next-token prediction)을 사용하여 멀티모달 태스크에서 탁월한 성능을 발휘합니다. 모델은 이미지, 텍스트, 비디오를 이산 공간으로 변환하고, 단일 트랜스포머(transformer)를 사용해 다양한 멀티모달 데이터를 학습합니다.
- **주요 공헌**: 확산 및 조합적 방법에 의존하지 않고 멀티모달 태스크에서 최고 성능을 달성했습니다. SDXL, LLaVA-1.6을 능가하며, Emu3은 접근 방식을 간소화하고 확장성을 높였습니다.

#### 2. 접근 방법 (Approach)
- **요약**: Emu3는 텍스트, 이미지, 비디오 데이터를 혼합하여 학습했습니다. 언어 데이터는 Aquila에서 사용된 중국어 및 영어 데이터입니다. 이미지 및 비디오 데이터는 이산 토큰으로 변환되어, 단일 트랜스포머를 통해 예측됩니다.
- **주요 공헌**: Emu3는 영상 및 비디오를 이산 토큰으로 변환하는 강력한 비전 토크나이저를 제공하며, 인간 선호도 최적화(DPO)를 통해 모델을 인간의 선호도에 맞추는 방식을 도입했습니다.

#### 3. 주요 결과 (Main Results)
- **요약**: Emu3는 이미지 생성, 시각-언어 이해, 비디오 생성에서 기존의 여러 모델을 능가합니다. 이미지 생성 측면에서, Emu3는 인간 평가와 공개 텍스트-이미지 벤치마크에서 SDXL을, 비디오 생성 측면에서 VBench 벤치마크에서 경쟁 모델을 능가합니다.
- **주요 공헌**: Emu3는 고해상도 비디오와 이미지를 예측하며, 비디오 내 상황을 예측하고 텍스트 설명에 따라 비디오를 생성하는 능력을 입증했습니다.

#### 4. 관련 작업 (Related Work)
- **요약**: Emu3는 초기의 통합 시도(Emu, Chameleon)와 달리, 확산 모델이나 조합적 접근을 사용하지 않고도 최고 성능을 달성했습니다. 다른 모델들이 투스테이지 프레임워크 등의 복잡한 구조를 사용하는 반면, Emu3는 단순화를 통해 성능을 극대화했습니다.
- **주요 공헌**: Emu3는 다른 멀티모달 모델이 직면한 문제를 해결하고, 다음 토큰 예측을 통해 멀티모달 지능을 실현할 수 있음을 증명했습니다.

#### 5. 결론 (Conclusion)
- **요약**: Emu3는 다음 토큰 예측을 통해 멀티모달 생성 및 인식에서 뛰어난 성능을 보이는 새로운 모델 시리즈로 소개되었습니다. 이 방법은 확산 및 조합적 방법을 제거하고, Emu3는 다양한 태스크에서 최고 성능을 발휘함으로써 일반 멀티모달 지능을 향한 유망한 경로를 제시합니다.
- **주요 공헌**: Emu3는 다음 토큰 예측의 유효성을 증명하며, 이를 통해 AI 연구를 발전시키고자 합니다.

### 2. 전체 요약
Emu3는 텍스트, 이미지, 비디오 데이터를 단일 트랜스포머를 사용해 학습하는 멀티모달 모델로, 기존의 복잡한 확산 모델이나 조합적 접근법을 사용하지 않고도 뛰어난 성능을 발휘합니다. 주요 결과로는 이미지 생성, 시각-언어 이해, 비디오 생성 부문에서 기존 모델들을 능가하는 성능을 입증하였으며, 인간 선호도 최적화를 통해 모델을 인간의 선호도에 맞추었습니다. Emu3의 핵심 기여는 모델 구조의 단순화와 확장성에 있으며, 이를 통해 일반 멀티모달 지능 실현에 한 걸음 다가섰습니다. Emu3는 AI 연구의 새로운 지평을 열고, 향후 연구를 위한 강력한 토대를 마련했습니다.