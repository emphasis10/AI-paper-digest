# ALBERT: A Lite BERT for Self-supervised Learning of Language Representations
## TL;DR
## Summary
- [https://arxiv.org/pdf/1909.11942.pdf](https://arxiv.org/pdf/1909.11942.pdf)

### 섹션별 요약 및 주요 기여

#### 1. 도입 (Introduction)
이 논문은 언어 표현 학습의 최첨단 성능을 달성하기 위해 모델 크기를 증가시키는 것이 중요한 역할을 한다고 언급합니다. 그러나 GPU/TPU 메모리 제한과 긴 훈련 시간 때문에 모델 크기를 계속해서 늘리는 것이 어려워지는 문제를 지적합니다. 이러한 문제를 해결하기 위해 BERT의 메모리 소비를 줄이고 훈련 속도를 높이는 두 가지 파라미터 감소 기술을 제안합니다. 

#### 2. 관련 연구 (Related work)
언어 표현 학습의 발전과 관련된 기존 연구들을 소개합니다. 특히 대형 모델의 중요성과 관련된 연구, 메모리 관리 기법, 모델 병렬화 등을 다룹니다. 또한 교차 레이어 파라미터 공유와 문장 순서 예측 목표 등 ALBERT가 도입한 주요 기술에 대해 설명합니다.

#### 3. ALBERT의 요소들 (The Elements of ALBERT)
ALBERT는 두 가지 주요 파라미터 감소 기술을 도입하여 모델 크기를 줄이면서도 성능을 유지합니다. 
1. **팩토라이즈드 임베딩 파라미터화**: 큰 어휘 임베딩 행렬을 두 개의 작은 행렬로 분할하여, 은닉 층의 크기를 어휘 임베딩 크기와 분리합니다. 이는 메모리 사용량을 크게 줄입니다.
2. **교차 레이어 파라미터 공유**: 모델의 깊이에 따라 파라미터가 증가하는 것을 방지합니다. 이는 훈련 속도를 향상시키고, 모델의 안정성을 높입니다.

#### 4. 실험 결과 (Experimental Results)
ALBERT와 BERT를 다양한 벤치마크에서 비교한 결과, ALBERT는 파라미터 크기가 훨씬 작음에도 불구하고 성능이 우수한 것으로 나타났습니다. 특히 GLUE, SQuAD, RACE 등에서 새로운 최첨단 결과를 달성하였습니다.

#### 5. 논의 (Discussion)
ALBERT의 성능 향상은 신경망 파라미터 감소 및 문장 순서 예측 손실 덕분입니다. 이 논문은 향후 연구 방향으로 희소 주의(attention) 기법과 블록 주의 기법을 통한 훈련 및 추론 속도의 향상을 제안합니다.

### 전체 요약
이 논문은 BERT 모델의 메모리 소비와 훈련 속도를 향상시키기 위한 ALBERT 모델을 제안합니다. 두 가지 주요 기법인 팩토라이즈드 임베딩 파라미터화와 교차 레이어 파라미터 공유를 통해 모델 크기를 크게 줄이면서도 성능을 최고 수준으로 유지합니다. ALBERT는 GLUE, SQuAD, RACE 등의 벤치마크에서 새로운 최첨단 성과를 달성하며, 후속 연구로 훈련 및 추론 속도를 향상시키기 위한 방법들을 제시합니다.

이러한 기법들을 통해 ALBERT는 BERT 모델 대비 18배 적은 파라미터로, 약 1.7배 더 빠른 훈련 속도를 달성하면서도 높은 성능을 유지합니다.