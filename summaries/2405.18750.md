# T2V-Turbo: Breaking the Quality Bottleneck of Video Consistency Model with Mixed Reward Feedback
## TL;DR
## Summary
- [https://arxiv.org/pdf/2405.18750.pdf](https://arxiv.org/pdf/2405.18750.pdf)

#### 1. 소개

- **문제**: 텍스트-비디오 변환(T2V) 모델은 높은 품질의 비디오를 생성할 수 있으나, 샘플링 속도가 느림.
- **목표**: 빠르고 고품질의 비디오 생성을 위해 비디오 일관성 모델(VCM)의 품질 병목 현상을 해결.
- **방법**: 혼합 보상 피드백을 활용하여 비디오 일관성 모델을 증류하는 과정에서 최적화.

#### 2. 관련 연구

- **Diffusion Models (DMs)**: 이미지와 비디오 합성을 위한 강력한 프레임워크.
- **Consistency Distillation (CD)**: DM의 샘플링 속도를 가속화하기 위한 방법.
- **보상 모델(RMs)**: 인간의 선호도를 반영하기 위해 이미지-텍스트, 비디오-텍스트 보상 모델을 활용.

#### 3. T2V-Turbo의 훈련 과정

- **훈련 파이프라인**: 혼합 보상 피드백을 통해 VCM을 증류.
  - **보상 최적화**: 개별 비디오 프레임과 비디오 전체의 인간 선호도를 반영하는 보상 모델을 사용.
  - **메모리 제약 회피**: 반복 샘플링 과정에서의 메모리 문제를 피하기 위해 단일 스텝 생성에 집중.

#### 4. 실험 결과

- **자동 평가 (VBench)**: T2V-Turbo가 다양한 기준에서 SOTA 모델들을 능가.
  - **평가 항목**: 비디오 품질, 일관성, 텍스트-비디오 정렬 등.
- **인간 평가**: 인간 평가에서 T2V-Turbo가 기존 모델들보다 선호됨.
  - **평가 항목**: 시각적 품질, 텍스트와 비디오의 일치성 등.

#### 5. 결론 및 한계

- **결론**: T2V-Turbo는 VCM의 품질 병목을 해결하고, 빠르고 고품질의 비디오 생성을 가능하게 함.
- **한계**: 더 고급의 비디오-텍스트 보상 모델의 필요성.

### 주요 기여 및 혁신 부분

- **주요 기여**:
  1. 혼합 보상 피드백을 사용하여 고품질 비디오 생성을 가능하게 함.
  2. VBench에서 4단계 샘플링만으로 SOTA 모델들을 능가.
  3. 인간 평가에서 50단계 샘플링 모델보다 선호됨.

- **혁신 부분**:
  1. 비디오-텍스트 보상 모델을 통합하여 비디오의 시간적 역학을 평가.
  2. 단일 스텝 생성의 보상을 최적화하여 메모리 제약을 우회.
  3. 혼합 보상 피드백을 통한 비디오 일관성 모델 증류의 새로운 방법 제시.

### 전체 요약

이 논문에서는 텍스트-비디오 변환 모델의 품질 병목을 해결하기 위해 T2V-Turbo라는 새로운 모델을 제안합니다. 이 모델은 혼합 보상 피드백을 통해 비디오 일관성 모델을 증류하여, 빠르고 고품질의 비디오 생성을 가능하게 합니다. 실험 결과, T2V-Turbo는 다양한 평가 기준에서 기존 모델들을 능가하며, 인간 평가에서도 더 높은 선호도를 보였습니다. 이를 통해 비디오 생성의 품질과 속도를 모두 향상시키는 새로운 방법론을 제시했습니다.

## Similar Papers
- [ZeroSmooth: Training-free Diffuser Adaptation for High Frame Rate Video Generation](2406.00908.md)
- [TC-Bench: Benchmarking Temporal Compositionality in Text-to-Video and Image-to-Video Generation](2406.08656.md)
- [BPO: Supercharging Online Preference Learning by Adhering to the Proximity of Behavior LLM](2406.12168.md)
- [Live2Diff: Live Stream Translation via Uni-directional Attention in Video Diffusion Models](2407.08701.md)
- [VIMI: Grounding Video Generation through Multi-modal Instruction](2407.06304.md)
- [Ferret-v2: An Improved Baseline for Referring and Grounding with Large Language Models](2404.07973.md)
- [Guiding Instruction-based Image Editing via Multimodal Large Language Models](2309.17102.md)
- [OpenVid-1M: A Large-Scale High-Quality Dataset for Text-to-video Generation](2407.02371.md)
- [CamCo: Camera-Controllable 3D-Consistent Image-to-Video Generation](2406.02509.md)
