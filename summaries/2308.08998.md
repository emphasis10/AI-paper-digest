# Reinforced Self-Training (ReST) for Language Modeling
## TL;DR
## Summary
- [https://arxiv.org/pdf/2308.08998.pdf](https://arxiv.org/pdf/2308.08998.pdf)

## 논문의 주요 섹션 요약

### 1. 서론 
이 논문은 인간의 선호도에 맞추어 대형 언어 모델(LLM)의 출력을 개선하는 강화 학습 알고리즘(ReST)을 제안합니다. ReST는 오프라인 강화 학습(RL) 알고리즘을 사용하여 초기 모델을 개선합니다. 이 방법은 기존의 온라인 RLHF보다 더 효율적이고, 데이터 재사용이 가능합니다. ReST는 기계 번역을 중심으로 연구하였고, 자동화된 평가와 인간 평가에서 번역 품질을 크게 향상시켰습니다.

### 2. 기초 지식
ReST 알고리즘은 대형 언어 모델을 강화 학습으로 조정하는 방법론을 제시합니다. 이 방법에서는 주어진 데이터셋에 기반을 두고 초기 모델을 훈련시킨 뒤, 모델의 출력을 개선하기 위한 새로운 데이터셋을 생성합니다. 이 두 단계가 반복됩니다. 이 과정에서 인간의 선호도를 학습한 보상 모델을 사용하여 모델을 미세 조정합니다.

### 3. 강화된 자기 학습 (ReST)
ReST는 언어 모델의 출력을 인간의 선호도에 맞추기 위해 강화 학습을 사용하는 방법으로, 기본 프로그램의 성능을 향상시킵니다. 이 알고리즘은 데이터셋 성장과 정책 개선 단계를 분리하여 각각의 작업을 오프라인에서 수행합니다. ReST는 점진적으로 증가하는 필터링 임계값을 사용해 데이터셋을 정제하고, 최종 정제된 데이터셋으로 모델을 여러 차례 미세 조정하는 방법을 사용합니다.

### 4. 실험
ReST의 성능을 평가하기 위해 다양한 기계 번역 벤치마크에서 실험을 수행하였습니다. 실험 결과, ReST는 기존의 지도학습 방식과 다른 오프라인 RL 알고리즘보다 뛰어난 성능을 보였습니다. 특히, 인간 평가에서는 ReST 방법이 더 높은 선호도를 받은 번역을 생성했습니다.

### 5. 토의
ReST는 간단하면서도 여러 가지 배치 설계와 함께 작동할 수 있는 유연한 알고리즘입니다. 기계 번역에서 우수한 성능을 보였고, 여러 오프라인 RL 손실 함수 중 BC (Behavior Cloning)가 가장 좋은 성과를 냈습니다. 그러나 보상 모델에 과적합될 수 있는 위험이 존재하며, 인간 선호도에 따라 보상 모델을 미세 조정하면 더 나은 성능을 얻을 수 있을 것입니다.

### 6. 관련 작업
ReST는 기존의 자기 학습, 전문가 반복 학습(EI), 자기 모방 학습(SIL) 등과 관련이 있습니다. 각 방법들과 비교했을 때, ReST는 데이터 탐색 및 보상 활용이 가능하고 계산 효율성이 뛰어납니다. 또한 여러 향후 연구 방향이 열려있습니다.

## 전체 요약
논문은 ReST라는 강화된 자기 학습 알고리즘을 제안하여 대형 언어 모델의 출력을 인간의 선호도와 일치시키는 방법을 제시합니다. ReST는 데이터셋을 확장하고 정책을 개선하는 두 단계를 오프라인에서 반복하여 구현됩니다. 특히, 기계 번역 실험 결과 ReST는 기존 방법들보다 더 높은 성능을 보였으며, 인간 평가에서도 더 우수한 번역을 생성했습니다. ReST는 다른 언어 생성 작업에도 적용 가능하며, 보상 모델을 개선하면 더 나은 결과를 기대할 수 있습니다. 이 알고리즘은 간단하고 유연하며 계산 효율성이 뛰어나 향후 연구가치가 높습니다.

 