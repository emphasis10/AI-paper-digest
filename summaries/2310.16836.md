# LLM-FP4: 4-Bit Floating-Point Quantized Transformers
## TL;DR
## Summary
- [https://arxiv.org/pdf/2310.16836.pdf](https://arxiv.org/pdf/2310.16836.pdf)

## 요약

### 1. 서론 (Introduction)

이 논문은 트랜스포머 기반 대규모 언어 모델(LLM)의 4비트 부동 소수점 양자화를 다룹니다. 저자들은 부동 소수점 양자화가 기존의 정수 기반 양자화보다 더 유연하며, 특히 비대칭적이거나 벨 모양의 분포를 처리하는 데 더 적합하다고 주장합니다. 본 연구는 다양한 트랜스포머 모델들(BERT, Vision Transformer 등)에 대한 실험을 통해 높은 채널 간 분산과 낮은 채널 내 분산 패턴을 발견하고 이를 해결하기 위한 방법을 제안합니다.

### 2. 관련 연구 (Related Works)

- **사후 양자화 (Post-Training Quantization, PTQ)**: 기존 연구들은 주로 CNN 모델에 중점을 두고 있으며, 트랜스포머 모델에 대한 연구는 제한적입니다.
- **부동 소수점 양자화 (Floating-Point Quantization)**: 부동 소수점 양자화는 다양한 활성화 및 가중치 분포를 더 잘 처리할 수 있는 유연성을 제공하며, 최신 GPU에서도 지원됩니다.

### 3. 방법론 (Methodology)

- **부동 소수점 변수의 공식화**: 표준 부동 소수점 수는 부호 비트, 지수 비트, 그리고 가수 비트로 구성됩니다.
- **최적의 지수 편향과 최대 양자화 값을 찾기 위한 검색 기반 프레임워크**: 레이어별 재구성을 통해 최적의 지수 비트와 최대 값을 공동으로 검색합니다.
- **사전 이동된 지수 편향 (Pre-Shifted Exponent Bias)**: 활성화 분포의 높은 채널 간 분산을 처리하기 위해 제안된 기법입니다.

### 4. 실험 결과 (Experiments)

- **LLaMA-13B 모델에 대한 실험**: 제안된 방법은 4비트 양자화된 LLaMA-13B 모델에서 63.1의 평균 점수를 기록하며, 이는 전체 정밀도 모델보다 5.8 낮고, 이전 최고 성능 대비 12.7 점 향상된 결과입니다.
- **다른 모델들에 대한 확장**: BERT 및 Vision Transformer 모델에서도 이전 연구들보다 우수한 성능을 기록하였습니다.

### 5. 논의 및 결론 (Discussion and Conclusion)

- **주요 기여**: 이 논문은 트랜스포머 모델의 4비트 부동 소수점 양자화를 위한 새로운 검색 기반 프레임워크와 사전 이동된 지수 편향 기법을 제안하고, 이를 통해 높은 성능을 달성하였습니다.
- **미래 연구 방향**: 제안된 방법을 더 다양한 모델과 작업에 적용해볼 필요가 있습니다.

---

### 전체 요약

이 논문은 대규모 언어 모델의 효율적 양자화를 위해 4비트 부동 소수점 양자화 기법을 제안하고, 이를 통해 높은 성능을 유지하면서도 모델의 크기를 줄이는 방법을 제시합니다. 기존 정수 기반 양자화의 한계를 극복하기 위해 부동 소수점 양자화를 사용하였으며, 특히 높은 채널 간 분산을 처리하기 위한 사전 이동된 지수 편향 기법을 도입하였습니다. 실험 결과, 제안된 방법은 LLaMA-13B 모델을 포함한 다양한 트랜스포머 모델들에서 우수한 성능을 보여주었으며, 이는 양자화 연구의 새로운 방향을 제시합니다.