# Sheared LLaMA: Accelerating Language Model Pre-training via Structured Pruning
## TL;DR
## Summary
- [https://arxiv.org/pdf/2310.06694.pdf](https://arxiv.org/pdf/2310.06694.pdf)

### 논문의 주요 내용 요약

#### 1. 서론
이 논문은 LLaMA와 같은 중간 크기의 대형 언어 모델(LLM)을 보다 효율적으로 만들기 위해 구조적 가지치기를 연구합니다. 이러한 모델들은 기존의 큰 모델을 기반으로 하여, 작은 모델로 축소함으로써, 학습에 필요한 계산 자원을 크게 줄이려는 목적을 가지고 있습니다.

#### 2. LLM-Shearing 기법
LLM-Shearing 기법은 두 가지 주요 구성 요소로 이루어집니다:
1. **타겟 구조적 가지치기 (Targeted Structured Pruning)**: 기존의 큰 모델에서 특정 구조를 타겟으로 하여 불필요한 부분을 가지치기합니다. 이를 통해 최적의 성능을 유지하면서도 모델의 크기를 줄입니다.
2. **동적 배치 로딩 (Dynamic Batch Loading)**: 학습 도중 각 도메인의 손실 감소율에 따라 데이터 로딩 비율을 동적으로 조정하여 데이터 사용의 효율성을 높이고 전반적인 성능 향상을 가속화합니다.

#### 3. 실험
- **설정**: 다양한 크기의 모델을 대상으로 실험을 진행하며, 주요한 LLM 벤치마크와 지시 조정(Instruction Tuning)에서 성능을 평가합니다.
- **성능 비교**: Sheared-LLaMA는 유사한 크기의 기존 모델들보다 적은 계산 자원으로도 더 나은 성능을 보입니다. 예를 들어, Sheared-LLaMA-1.3B는 50B 토큰으로 훈련되어, Pythia, INCITE, OpenLLaMA 모델들을 능가합니다.

#### 4. 분석
- **동적 배치 로딩의 효과**: 이 방법을 통해 도메인 간 손실 균형이 잘 맞춰져, 최종 손실이 균등하게 줄어들어 보다 나은 다운스트림 성능을 얻습니다.
- **다른 가지치기 기법과의 비교**: Sheared-LLaMA는 기존의 CoFiPruning이나 LLM-Pruner보다 더 높은 추론 속도와 낮은 손실을 기록합니다. 이는 가지치기 후 추가 학습 단계를 통해 성능을 회복했기 때문입니다.

#### 5. 관련 연구
이 논문은 기존의 LLM 가지치기 기술들을 비교하며, 기존의 작업들이 가지치기 후 손실을 최소화하기 위해 많은 계산 자원을 필요로 했음을 지적합니다. 이를 개선하기 위해 동적 배치 로딩과 같은 새로운 접근 방식을 제안합니다.

#### 6. 결론 및 토론
본 연구는 구조적 가지치기와 동적 배치 로딩을 결합하여, 기존 모델을 기반으로 효율적인 작은 모델을 만드는 방법을 제시합니다. 이를 통해 학습에 필요한 계산 자원을 크게 줄이면서도 높은 성능을 유지할 수 있음을 실험적으로 증명했습니다. 이는 특히 제한된 자원을 가진 기관들이 강력한 LLM을 사용할 수 있는 길을 열어줍니다.

### 전체 요약
이 논문은 LLaMA와 같은 대형 언어 모델을 효율적으로 축소하는 방법을 제시합니다. 두 가지 주요 기법인 타겟 구조적 가지치기와 동적 배치 로딩을 통해 모델 크기를 줄이면서도 성능을 유지하거나 향상시킵니다. 실험 결과, Sheared-LLaMA 모델은 기존의 동급 모델보다 적은 계산 자원으로 더 나은 성능을 보였습니다. 이러한 접근 방식은 제한된 자원으로도 강력한 언어 모델을 사용할 수 있게 하여, 널리 활용될 수 있는 가능성을 보여줍니다.

## Similar Papers
- [LoRAShear: Efficient Large Language Model Structured Pruning and Knowledge Recovery](2310.18356.md)
- [Lory: Fully Differentiable Mixture-of-Experts for Autoregressive Language Model Pre-training](2405.03133.md)
- [From GaLore to WeLore: How Low-Rank Weights Non-uniformly Emerge from Low-Rank Gradients](2407.11239.md)
- [Efficient Streaming Language Models with Attention Sinks](2309.17453.md)
- [Turbo Sparse: Achieving LLM SOTA Performance with Minimal Activated Parameters](2406.05955.md)
- [Knowledge Fusion of Chat LLMs: A Preliminary Technical Report](2402.16107.md)
- [OneBit: Towards Extremely Low-bit Large Language Models](2402.11295.md)
- [Your Transformer is Secretly Linear](2405.12250.md)
- [SimPO: Simple Preference Optimization with a Reference-Free Reward](2405.14734.md)
