# SFT Memorizes, RL Generalizes: A Comparative Study of Foundation Model Post-training
## TL;DR
## Summary
- [https://arxiv.org/pdf/2501.17161.pdf](https://arxiv.org/pdf/2501.17161.pdf)

### 1. 각 섹션의 주요 내용 요약 (한국어)

**1. 서론**
본 연구에서는 AI 모델의 훈련 후 처리 기법인 감독 세부 조정(Supervised Fine-Tuning, SFT)과 강화 학습(Reinforcement Learning, RL)의 차별적인 역할을 조사합니다. SFT는 훈련 데이터를 암기하는 경향이 있으며, RL은 일반화 성능을 높이는 데 더 효과적이라는 주장을 제기합니다. 연구의 목표는 테스크 규칙의 의미 범주와 시각적 일반화의 적용을 엄밀히 분석하는 것입니다.

**2. 관련 연구**
이 섹션에서는 모델의 성능 향상을 위한 다양한 사후 훈련 기법에 대한 이전 연구를 언급합니다. SFT는 주어진 데이터셋에 맞춰 모델을 조정하며, RL은 모델을 인간의 선호에 맞도록 조정하는 데 사용됩니다. 본 연구는 특히 두 가지 접근 방식의 비교 분석을 통해 일반화와 암기의 역할을 이야기합니다.

**3. 방법론**
SFT와 RL을 사용하여 모델을 훈련하고, GeneralPoints와 V-IRL이라는 두 가지 작업을 통해 성능을 평가합니다. GeneralPoints는 산술 추론 작업이며, V-IRL은 실제 내비게이션 환경을 반영한 작업입니다.

**4. 결과**
RL은 모든 작업에서 성공률을 증가시켰으며, 특히 비일체형(Out-of-Distribution, OOD) 상황에서 우수한 일반화를 보여줍니다. SFT는 OOD에서 성능이 저조한 반면, RL은 모든 환경에서 일관된 성능을 나타냈습니다.

**5. 논의**
RL 방식이 어떻게 시각적 인식 능력을 향상시키고, SFT가 RL의 효과적인 트레이닝을 보조하는지를 논의합니다. 오히려 SFT만으로는 일반화 능력이 떨어질 수 있다는 점을 강조합니다.

**6. 결론 및 한계**
RL이 SFT보다 전반적으로 더 나은 일반화 성능을 보이며, SFT는 오히려 훈련 데이터를 암기하는 경향이 있음을 보여줍니다. RL이 가진 잠재적인 강화 학습 미세 조정 방식은 중요하다는 사실을 강조합니다.

### 혁신적인 기여
1. RL이 다양한 작업에서 SFT보다 우수한 일반화 성능을 보임을 실험적으로 입증.
2. SFT는 RL 학습 전 안정성을 추가하는 역할을 할 수 있음.
3. 시각적 불명확성이 존재하는 다중 모드 작업에서 RL 접근법의 유용성 제시.

### 2. 전체 요약 (한국어)
본 연구는 AI 모델의 훈련 후 처리 기법인 SFT와 RL의 비교 분석을 통해 모델의 일반화 및 암기 성능에 대한 새로운 통찰을 제시하고 있습니다. RL은 다양한 변형 작업에서 우수한 일반화 성능을 보였으나, SFT는 훈련 데이터에 대한 과도한 암기의 경향을 지닙니다. 본 연구는 AI 시스템의 신뢰성과 강 robustness을 높이기 위한 방향으로 중요한 기여를 하며, 향후 연구에 있어 두 기법의 조화로운 통합이 필요함을 강조합니다.