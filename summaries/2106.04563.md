# XtremeDistilTransformers: Task Transfer for Task-agnostic Distillation
## TL;DR
## Summary
- [https://arxiv.org/pdf/2106.04563.pdf](https://arxiv.org/pdf/2106.04563.pdf)

## 요약

### 1. 도입 (Introduction)
AI와 머신 러닝 분야에서 대규모 사전 훈련된 모델들은 매우 유용하지만 자원 제약이 있는 환경에서는 그 크기와 비용이 큰 문제가 됩니다. 이를 해결하기 위해 이 논문은 XtremeDistilTransformers라는 새로운 모델 압축 프레임워크를 개발했습니다. 이 프레임워크는 작업별 지식 증류와 무작업별 지식 증류의 장점을 결합하여 다양한 작업과 언어에 적용할 수 있는 작은 일반 모델을 만듭니다.

### 2. 전이 작업 탐구 (Exploring Tasks for Transfer)
### 2.1 증류 작업의 역할 (Role of Tasks for Distillation)
작업별 지식 증류는 특정 작업에 대해 인간이 라벨을 붙힌 데이터를 활용하며 이는 높은 수준의 압축을 가능하게 하지만 리소스 소모가 크다. 반면에 무작업별 지식 증류는 라벨이 필요하지 않으며, 대규모 텍스트로부터 학습할 수 있지만 압축률이 낮다.

### 2.2 작업의 전이 가능성 (Transferability of Tasks)
특정 작업에서 학습된 모델이 다양한 다른 작업에서도 잘 작동하도록 하기 위해 최상의 소스 작업을 선택합니다. 이 논문에서는 대규모 언어 모델 BERT를 다양한 소스 작업으로 학습시킨 후 타겟 작업에 대한 성능을 평가합니다. 그 결과, 논리적 추론(NLI) 작업이 여러 타겟 작업에서 가장 우수한 성능을 보였습니다.

### 3. 증류 프레임워크 (Distillation Framework)
### 3.1 단어 임베딩 인수분해 (Word Embedding Factorization)
단어 임베딩의 차원을 맞추기 위해 인수분해 기법을 사용합니다. 이를 통해 큰 모델에서 작은 모델로 지식을 효율적으로 전이할 수 있습니다.

### 3.2 감추어진 층 표현 (Hidden Layer Representations)
교사 모델의 여러 층에서 학생 모델로 감추어진 표현을 전이합니다. 이는 모델의 깊이에 관련된 문제를 해결하는 데 도움을 줍니다.

### 3.3 다중 헤드 자기 주의 (Multi-head Self-attention)
다중 헤드 자기 주의 메커니즘을 사용해 교사 모델의 여러 층에서 학생 모델로 관심 상태를 전이합니다.

### 4. 실험 (Experiments)
### 4.1 증류 성능 GLUE 벤치마크 (Distillation Performance in GLUE Benchmark)
논리적 추론(NLI)을 소스 작업으로 사용하는 것이 여러 타겟 작업에서 우수한 성능을 보였습니다.

### 4.2 대규모 다중 언어 NER에 대한 전이 증류 (Transfer Distillation for Massive Multilingual Named Entity Recognition)
XtremeDistilTransformers를 사용해 41개 언어의 다중 언어 명명 엔티티 인식(NER) 성능을 평가했습니다. 결과적으로, 작은 모델임에도 불구하고 높은 성능을 유지했습니다.

### 5. 관련 연구 (Related Work)
다른 최신 모델 압축 기법들과 비교하여 XtremeDistilTransformers는 다중층 지식 전이 기술을 통해 뛰어난 성능을 보였습니다.

### 6. 결론 (Conclusions)
이 논문은 XtremeDistilTransformers라는 새로운 프레임워크를 제안하며, 이를 통해 대규모 모델의 크기와 계산 비용 문제를 해결하면서도 높은 성능을 유지할 수 있음을 입증했습니다.

## 전체 요약
이 논문은 대규모 사전 훈련된 언어 모델을 효과적으로 압축할 수 있는 새로운 프레임워크(XtremeDistilTransformers)를 제안합니다. 이 프레임워크는 작업별 지식 증류와 무작업별 지식 증류의 장점을 결합하여 다양한 작업과 언어에 적용할 수 있는 작은 일반 모델을 만듭니다. 여러 실험 결과, 이 모델은 기존 방법들보다 더 높은 성능을 보이며, 특히 논리적 추론(NLI) 작업이 다양한 타겟 작업에 대해 우수한 전이 성능을 발휘함을 보여줍니다. 이 연구는 AI와 머신 러닝 분야에서 모델 압축 및 성능 개선에 중요한 기여를 할 것입니다.