# GIFT-SW: Gaussian noise Injected Fine-Tuning of Salient Weights for LLMs
## TL;DR
## Summary
- [https://arxiv.org/pdf/2408.15300.pdf](https://arxiv.org/pdf/2408.15300.pdf)

### 1. 각 섹션 요약 및 주요 기여와 혁신적 부분 요약

#### Abstract
이 논문은 파라미터 효율적 미세 조정(Parameter Efficient Fine-Tuning, PEFT) 기법인 GIFT-SW를 제안합니다. 이 기법은 주목할 만한 가중치는 업데이트하고 나머지 가중치에는 가우시안 노이즈를 주입합니다. 이 방법은 최신 PEFT 방법과 풀 파인트에 비해 뛰어난 성능을 보이며, 적은 연산 예산으로 모델 성능을 회복할 수 있습니다.

#### Introduction
대형 언어 모델(LLM)은 널리 활용되고 있으며, 이를 효율적으로 미세 조정하는 것이 주요 과제입니다. PEFT 기법은 이러한 문제를 해결하기 위해 고안되었으나, 기존 방법들은 여전히 풀 파인튜닝에 비해 정확도가 떨어집니다. 이 논문에서는 이 문제를 해결하고, 주목할 만한 가중치만 업데이트하는 새로운 PEFT 방법인 GIFT-SW를 제안합니다.

#### Related Work
기존의 PEFT 방법들인 LoRA, SparseGPT, Wanda 등이 소개되었습니다. 이들 방법은 LLM 내 주목할 만한 가중치를 식별하는 데 중점을 둡니다.

#### Method
GIFT-SW 방식은 주목할 만한 컬럼의 가중치를 식별하고, 여기에만 업데이트를 수행하며 나머지 가중치에는 노이즈를 주입합니다. 이는 다양한 민감도 매트릭스를 일반화하여 주목할 만한 컬럼을 선택합니다.

#### Experiments
LLM 모델을 실험하여 GIFT-SW 방법이 얼마나 효율적인지 입증하였습니다. 다양한 데이터셋과 비교 실험을 통해 GIFT-SW의 우수성이 확인되었습니다.

#### Results
GIFT-SW는 풀 파인튜닝과 저전력 어댑터보다 우수한 성능을 보였으며, 적은 연산 자원으로도 높은 정확도를 유지할 수 있음을 실험적으로 입증하였습니다.

#### Conclusion
본 논문에서는 GIFT-SW 방법의 적은 연산 자원을 사용하면서도 높은 성능을 유지하는 장점을 강조하였습니다. 이는 다양한 LLM 모델을 통해 실험적으로 검증되었습니다.

### 2. 전체 요약

본 논문에서는 대형 언어 모델(LLM)의 파라미터 효율적 미세 조정 방법인 GIFT-SW를 제안하고 실험을 통해 그 효율성을 입증하였습니다. GIFT-SW는 적은 연산 자원으로도 주목할 만한 성능 향상을 가능하게 합니다. 이 방법은 기존의 PEFT 기법보다 우수한 성능을 보이며, 주목할 만한 가중치만 업데이트하고 나머지 가중치에는 노이즈를 주입하여 모델의 성능을 극대화합니다. 다양한 데이터셋과 LLM 모델에서 실험을 통해 GIFT-SW의 뛰어난 성능이 확인되었습니다. GIFT-SW는 풀 파인튜닝과 비교해 안정적이고 효율적이며, 적은 자원으로도 높은 성능을 유지할 수 있어 향후 AI 연구에 크게 기여할 것입니다.