# Elsevier Arena: Human Evaluation of Chemistry/Biology/Health Foundational Large Language Models
## TL;DR
## Summary
- [https://arxiv.org/pdf/2409.05486.pdf](https://arxiv.org/pdf/2409.05486.pdf)

### 1. 섹션별 요약

#### 서론
이 논문은 대형 언어 모델(LLMs)의 품질을 자동화된 벤치마크 평가만으로는 충분히 평가할 수 없다고 설명합니다. 특히 특정 도메인(예: 생물의학)에 맞춰 훈련된 커스텀 LLMs의 경우, 벤치마크 평가의 한계를 가진다고 주장합니다. 이를 보완하기 위해 인간 평가(human evaluation)가 필요하며, 이 과정에서는 A/B 테스트를 사용합니다.

#### 방법론
논문에서는 생명과학 도메인에 특화된 LLM을 훈련하기 위해 Elsevier의 데이터를 사용했습니다. 여기에는 저널 논문, 특허 등 고품질 데이터가 포함되어 있습니다. 평가 방법론으로는 7명의 평가자를 통해 47개의 입력 프롬프트를 비교하는 A/B 테스트가 사용되었습니다. 평가 기준은 사실성, 일관성, 관련성 및 종합 평가입니다.

#### 결과
실험 결과, 대형 모델인 GPT-3.5-turbo가 상대적으로 더 높은 평가를 받았지만, 작은 도메인 특정 모델도 경쟁력 있는 결과를 보였습니다. 평가자 간의 만족도는 고르지 않았으며, 평가자 간의 합의도를 나타내는 Krippendorff의 알파가 낮았습니다.

#### 결론
이 논문은 도메인 특화된 모델이 대형 일반 모델보다 상호 보완적인 성능을 보일 수 있음을 보여줍니다. 이 모델들은 훈련 데이터의 질과 양에 따라 다양한 평가 기준에서 차별화된 성능을 보였습니다. 인간 평가가 LLMs 품질을 평가하는 데 중요한 역할을 하며, 이는 특히 도메인 특정 모델에서 두드러집니다.

### 2. 전체 요약
이 논문은 대형 언어 모델의 성능 평가에서 인간 평가의 필요성을 강조합니다. Elsevier의 도메인 특화된 생명과학 LLM을 평가하기 위해 A/B 테스트를 실시했고, 이 결과 GPT-3.5-turbo와 같은 대형 모델이 우수한 성능을 보였지만, 작은 도메인 모델도 경쟁력 있는 결과를 나타냈습니다. 핵심은 인간 평가가 자동화된 벤치마크 도구의 한계를 보완하며, 도메인 특화된 데이터와 모델이 특정 상황에서 더 효과적일 수 있다는 점입니다.