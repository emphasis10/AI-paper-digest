# Training LLMs over Neurally Compressed Text
## TL;DR
## Summary
- [https://arxiv.org/pdf/2404.03626.pdf](https://arxiv.org/pdf/2404.03626.pdf)

이 문서는 신경 압축 텍스트를 통한 대규모 언어 모델(LLMs)의 훈련에 관한 연구를 다루고 있습니다. 전통적인 서브워드 토크나이저 대신 신경 압축기를 사용하여 높은 압축률을 달성하고, 이를 통해 훈련 및 서빙 효율성을 향상시키며, 긴 텍스트 범위를 더욱 쉽게 다룰 수 있는 장점을 탐구하고 있습니다. 연구는 기존의 Arithmetic Coding(AC)와 같은 압축 방식을 사용하여 텍스트를 압축하고, 이를 기반으로 두 번째 언어 모델(M2)을 훈련시키는 과정을 포함합니다. 특히, 모든 텍스트 블록이 같은 비트 길이로 압축되는 새로운 압축 기술인 Equal-Info Windows를 제안하고 있으며, 이 방법을 사용하여 신경 압축 텍스트를 효과적으로 학습할 수 있음을 보여줍니다. 

1. **서론 및 배경**(Introduction): 
이 연구는 서브워드 토크나이저 대신 더 높은 압축률을 달성할 수 있는 신경 텍스트 압축기를 사용하여 대규모 언어 모델을 훈련시키는 아이디어를 탐구합니다. 이를 통해 모델 훈련과 실행 시간의 효율성을 증가시키고, 긴 텍스트를 더 쉽게 처리할 수 있는 가능성을 제시합니다.

2. **Equal-Info Windows 방법론**(Methods): 
기존의 텍스트를 동등한 비트 길이의 블록으로 세분화하여 압축하는 새로운 기술인 Equal-Info Windows를 제안합니다. 이 방법은 모델(M2)이 신경 압축된 텍스트를 통해 효과적으로 학습할 수 있도록 하며, 압축률을 크게 향상시키는 동시에 짧은 시퀀스 길이의 이점을 제공합니다.

3. **결과 및 분석**(Results & Analysis): 
이 연구는 Equal-Info Windows를 사용하여 Arithmetic Coding으로 압축된 텍스트의 학습 가능성을 입증합니다. 단기 윈도우에서 더 좋은 성능을 보이며, 이는 장기 윈도우보다 모델이 압축된 토큰을 더 잘 학습하게 만듭니다.

**종합 요약**: 
이 문서는 신경 압축기를 사용하여 얻은 고압축률 텍스트를 기반으로 대규모 언어 모델을 훈련시키는 새로운 접근 방식을 탐구합니다. Arithmetic Coding과 같은 전통적인 압축 기법 대신 Equal-Info Windows라는 기법을 통해 텍스트를 압축하고, 이를 통해 학습 효율성을 향상시키며 짧은 시퀀스 길이의 이점을 제공합니다. 이 연구의 혁신적인 부분은 개선된 압축률과 더불어, 모델이 신경 압축 텍스트로부터 효과적으로 학습할 수 있는 능력을 증명한 점입니다.