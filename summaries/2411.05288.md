# Balancing Pipeline Parallelism with Vocabulary Parallelism
## TL;DR
## Summary
- [https://arxiv.org/pdf/2411.05288.pdf](https://arxiv.org/pdf/2411.05288.pdf)

**1. 논문의 각 섹션 요약 및 주요 기여와 혁신적인 부분 요약**

- **서론**
  이 논문은 대규모 변환기 모델의 학습 시 발생하는 불균형 문제에 초점을 맞추고 있습니다. 특히, 어휘 계층에서의 불균형이 처리량과 메모리 사용에 부정적인 영향을 미친다고 지적합니다. 이러한 문제를 해결하기 위해 어휘 병렬처리법을 통해 어휘 계층을 파이프라인 장치들 전체에 걸쳐 균등하게 분할하고, 이를 기존 파이프라인 스케줄에 통합하는 방법을 제안합니다.

- **연관 연구**
  다양한 모델 병렬화 기술이 대규모 언어 모델 학습의 어려움을 해결하기 위해 개발되었습니다. 특히 ZeRO, 텐서 병렬처리(TP), 파이프라인 병렬처리(PP)이 주목을 받았으며, 각각의 장점과 제한점을 갖고 있습니다.

- **어휘 병렬처리 접근법**
  이 논문에서는 PP의 불균형 문제를 완전히 해결하기 위해, 어휘 병렬처리를 제안합니다. 어휘 계층을 어휘 차원으로 분할하여 모든 파이프라인 장치에 고르게 배분하고, 기존의 파이프라인 스케줄에 통합하도록 설계되었습니다.

- **실험 및 결과**
  제안된 방법론은 성능 실험을 통해 검증되었으며, 기존 방법에 비해 최대 51%의 처리량 개선 효과를 나타냈습니다. 또한 메모리 사용을 최적화하여 대규모 어휘 처리 과정에서도 효율성을 보여주었습니다.

- **결론과 미래 작업**
  제안된 어휘 병렬처리 방법은 파이프라인 병렬처리에서 어휘 계층의 계산과 메모리 불균형 문제를 효과적으로 해결합니다. 향후 작업은 멀티모달(Multimodal) LLM에서의 임베딩 계층의 불균형 문제 해결을 탐색할 예정입니다.

**2. 논문의 전체적인 요약**

이 논문은 대규모 변환기 언어 모델의 학습에서 파이프라인 병렬처리(Pipeline Parallelism, PP)의 불균형 문제를 해결하기 위해 새로운 어휘 병렬처리 방법을 제안합니다. 이 방법은 어휘 계층을 파이프라인 장치에 균등하게 분할하고, 이를 통해 계산과 메모리 사용의 균형을 맞춥니다. 이를 통해 기존의 단순한 계층 재분배 방식보다 성능이 향상된 것을 실험적으로 입증했습니다. 특히, 어휘 병렬처리 방법은 최대 51%의 처리량 개선을 이루어 낸 점에서 혁신적입니다. 이 방법은 주로 텍스트 기반 대형 언어 모델에서의 어휘 계층 불균형 문제를 다루며, 미래에는 멀티모달 모델로의 확장을 고려합니다.