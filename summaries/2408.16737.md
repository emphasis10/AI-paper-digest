# Smaller, Weaker, Yet Better: Training LLM Reasoners via Compute-Optimal Sampling
## TL;DR
## Summary
- [https://arxiv.org/pdf/2408.16737.pdf](https://arxiv.org/pdf/2408.16737.pdf)

### 1. 논문의 각 섹션 요약

#### 1.1. 서론 (Introduction)
이 논문은 강력하면서도 비용이 많이 드는 언어 모델(Strong but Expensive, SE)과 약하지만 저렴한 언어 모델(Weak but Cheap, WC)을 비교합니다. 목표는 고비용 SE 모델 대신 저비용 WC 모델이 학습 데이터 생성에서 계산 효율성을 더 높일 수 있는지를 입증하는 것입니다. 이를 위해 커버리지, 다양성, 및 오탐률 등의 주요 지표를 사용해 생성된 데이터를 평가하고, 이 데이터를 통해 모델을 미세 조정하는 다양한 방법을 탐구합니다.

#### 1.2. 주요 기여 (Main Contributions)
연구의 주요 기여는 다음과 같습니다:
1. 고정된 계산 예산 하에서 WC 모델이 더 나은 커버리지와 다양성을 제공할 수 있음을 보여줍니다.
2. SE 모델 대신 WC 모델의 데이터를 사용하면 다양한 벤치마크에서 일관되게 더 나은 성능을 낼 수 있음을 입증합니다.
3. 새로운 weak-to-strong 개선 설정에서 더 약한 LM이 더 강한 LM에게 추론을 가르치는 방법을 소개합니다.

#### 1.3. 결과 요약 (Summary of Results)
WC 모델이 생성한 데이터로 미세 조정된 모델이 SE 모델의 데이터로 미세 조정된 모델을 여러 벤치마크와 설정에서 일관되게 능가한다는 결과를 얻었습니다. 이는 높은 품질의 데이터 생성에서 SE 모델에 의존하는 기존의 관행을 도전합니다. 이 결과는 WC 모델이 고급 언어 모델 추론자를 훈련하는 데 계산 효율성이 더 높은 접근법일 수 있음을 시사합니다.

#### 1.4. 결론 (Conclusion)
이 연구는 약하지만 저렴한 LM을 사용하여 계산 효율성을 극대화하는 방법을 제시합니다. 고정된 계산 예산 하에서 작은 LM이 더 큰 LM보다 빠르게 성능을 향상시키고 있음을 발견했습니다. 이는 작은 LM의 성능 차이가 시간이 지날수록 좁혀지고 있기 때문입니다. 이러한 발견은 향후 더 작고 약한 LM을 활용한 훈련 방법이 더욱 중요해질 것임을 의미합니다.

### 2. 전체 요약

이 논문은 강력하고 비용이 많이 드는 언어 모델(SE)과 약하고 저렴한 언어 모델(WC)을 비교하여, 고비용 SE 모델 대신 저비용 WC 모델을 사용할 때 계산 효율성이 더 높음을 입증합니다. 구체적으로 WC 모델이 더 높은 커버리지와 다양성을 제공하고, SE 모델보다 여러 벤치마크에서 일관되게 더 나은 성능을 낸다는 사실을 발견했습니다. 새로운 weak-to-strong 개선 설정을 도입하여, 더 약한 LM이 더 강한 LM에게 추론을 가르치는 방법도 탐구하였습니다.

이 연구는 기존의 SE 모델에 의존하는 관행을 도전하며, 향후 더 작고 약한 LM을 활용한 훈련 방법이 더욱 중요해질 것임을 시사합니다.