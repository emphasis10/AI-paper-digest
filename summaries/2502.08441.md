# Better Embeddings with Coupled Adam
## TL;DR
## Summary
- [https://arxiv.org/pdf/2502.08441.pdf](https://arxiv.org/pdf/2502.08441.pdf)

1. 섹션 요약:

   - **서론 (Introduction):** 이 논문은 대형 언어 모델(LLM)의 학습에서 발생하는 비등방성 임베딩 문제를 다룹니다. 비등방성은 모델의 표현력과 일반화 가능성을 제한합니다.
   
   - **비등방성 임베딩의 근본 원인 (On the Root Cause of Anisotropic Embeddings):** 비등방성의 원인은 Adam 최적화 알고리즘이 사용된 결과입니다. 이는 임베딩 벡터의 집단적인 이동을 야기하여 성능을 저하시키는 요인으로 작용합니다.

   - **결합 Adam (Coupled Adam):** 표준 Adam 최적화 알고리즘을 수정한 "결합 Adam"을 제안합니다. 이는 임베딩 벡터에 동일한 학습률을 적용하여 비등방성을 줄이는 데 효과적입니다.

   - **실험 (Experiments):** 결합 Adam의 성능을 평가하기 위해 다양한 크기의 모델과 데이터셋을 사용하여 소규모 및 대규모 실험을 수행했습니다. 그 결과, 대규모 데이터셋에서는 결합 Adam이 더 우수한 성능을 보였습니다.

   - **결론 (Conclusions):** 결합 Adam은 임베딩 특정 메트릭을 개선할 뿐만 아니라 대규모 데이터셋의 상하위 성능까지 향상시켰습니다. 이러한 결과는 LLM 학습에 있어 중요한 시사점을 제공합니다.

2. 전체 요약:

   이 논문은 대형 언어 모델의 임베딩 벡터가 집단적으로 이동하여 비등방성을 보이는 문제를 다루고 있습니다. 이는 Adam 최적화 알고리즘이 원인이라고 지적하며, 결합 Adam이라는 수정된 알고리즘을 제안합니다. 결합 Adam은 임베딩 벡터 간의 균일한 학습률을 적용하여 이러한 문제를 해결하고, 대규모 데이터셋에서 성능을 향상시킵니다. 연구 결과는 결합 Adam이 모델의 표현력과 일반화 가능성을 높이는 데 기여할 수 있음을 보여줍니다.