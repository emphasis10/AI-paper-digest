# Towards Modular LLMs by Building and Reusing a Library of LoRAs
## TL;DR
## Summary
- [https://arxiv.org/pdf/2405.11157.pdf](https://arxiv.org/pdf/2405.11157.pdf)

### 논문 요약

#### 1. 서론
대형 언어 모델(LLM)을 특정 작업, 도메인, 사용자 프로필에 맞게 조정하는 것은 매우 중요합니다. 이는 주로 파라미터 효율적인 미세 조정 방법(PEFT)인 LoRA를 사용합니다. LoRA는 사전 학습된 LLM의 선형 변환을 수정하여 파라미터 효율성을 달성합니다.

#### 2. 사전 지식
여러 작업이 주어지면, 각 작업에 대해 LoRA를 사용하여 작업 어댑터 라이브러리를 생성합니다. LoRA는 사전 학습된 모델의 선형 변환을 수정하여 성능과 파라미터 효율성 사이의 경쟁적 균형을 유지합니다.

#### 3. LoRA 라이브러리 구축
- **개별 어댑터:** 각 작업에 대해 별도의 어댑터를 훈련시킵니다. 이는 데이터가 공유될 수 없는 상황에서 유용합니다.
- **공유 어댑터:** 모든 멀티태스크 훈련 데이터를 사용하여 단일 어댑터를 훈련시킵니다. 이는 작업 간 간섭 문제를 야기할 수 있습니다.
- **모델 기반 클러스터링(MBC):** 유사한 작업을 클러스터링하여 각 클러스터에 대해 하나의 어댑터를 훈련시킵니다. 이는 성능을 향상시키고 연산 자원을 절약합니다.

#### 4. LoRA 라이브러리 재사용
- **제로샷 라우팅:** 새로운 입력에 대해 적절한 어댑터를 선택합니다. 이를 통해 공동 훈련 없이도 독립적으로 훈련된 전문가를 사용할 수 있습니다.
- **지도 적응:** 새로운 작업의 훈련 데이터를 사용하여 어댑터를 조정합니다. 이는 기존 연구에서 다루어진 바 있습니다.

#### 5. 실험 결과
- **제로샷 성능:** 다양한 작업에서 MBC 기반 어댑터가 강력한 성능을 보여주었으며, 특히 Phi-2 모델에서 뛰어난 결과를 나타냈습니다.
- **지도 적응:** Phi-2와 Mistral 모델 모두에서 MBC 기반 어댑터가 성능 향상을 보여주었으며, 이는 작업 간 유사성을 고려한 클러스터링의 효과를 입증합니다.

#### 6. 결론 및 향후 연구
이 연구는 LoRA 어댑터 라이브러리를 구축하고 재사용하는 방법을 조사하였습니다. 특히 제로샷 라우팅 전략의 잠재력을 보여주었습니다. 향후 연구에서는 다양한 어댑터 타입의 혼합과 지속적인 학습 전략을 탐구할 예정입니다.

### 전체 요약
이 논문은 대형 언어 모델(LLM)의 효율적인 적응을 위해 LoRA 어댑터 라이브러리를 구축하고 재사용하는 방법을 제안합니다. 모델 기반 클러스터링(MBC)을 통해 유사한 작업을 클러스터링하고, 이를 통해 각 클러스터에 대해 어댑터를 훈련시켜 성능을 향상시킵니다. 제로샷 라우팅 전략인 Arrow를 도입하여 새로운 입력에 대해 적절한 어댑터를 선택합니다. 실험 결과, MBC 기반 어댑터와 Arrow 라우팅이 다양한 작업에서 뛰어난 성능을 나타냈습니다. 이는 LLM의 모듈화와 적응성을 높이는 방향으로 나아가는 중요한 연구입니다. 향후 연구는 다양한 어댑터 타입의 혼합과 지속적인 학습을 탐구할 예정입니다.