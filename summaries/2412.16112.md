# CLEAR: Conv-Like Linearization Revs Pre-Trained Diffusion Transformers Up
## TL;DR
## Summary
- [https://arxiv.org/pdf/2412.16112.pdf](https://arxiv.org/pdf/2412.16112.pdf)

1. 논문의 각 섹션 요약:

- **서론**: 이 논문은 디퓨전 모델과 DiT(디퓨전 트랜스포머)에 대해 설명합니다. 전통적으로 UNet 구조가 이 분야를 지배했지만, DiT은 최근 주목받으며 각광받고 있습니다. 그러나 DiT의 주의 메커니즘은 높은 계산 복잡도로 고해상도 이미지 생성에서 지연을 초래합니다.

- **효율적인 주의 메커니즘 개요**: 일반적인 주의 메커니즘은 토큰 간 관계를 모델링하면서 시간과 메모리 복잡성을 증가시킵니다. 이를 해결하기 위한 다양한 접근법이 소개됩니다. 이 논문은 기존의 방법을 '형식 변형', '키 값 압축', '키 값 샘플링'으로 분류합니다.

- **메소드**: 기존의 DiT를 효율화하기 위해 주의 레이어를 대체하는 실험을 시도합니다. 실험 결과는 기존 DiT가 강조하는 지역성, 형식적 일관성, 고계급의 주의 맵, 피처의 무결성을 충족해야 한다고 결론 짓습니다.

- **결론**: CLEAR(Convolution-Like Attention for Efficient Rendering)는 고해상도 이미지 생성에 있어 DiT를 선형화하는 효율적인 방법입니다. 이 접근은 토큰이 지역적으로 제한된 윈도우 내에서만 상호작용함으로써 선형 복잡성을 달성합니다.

논문의 주요 기여는 CLEAR라는 새로운 주의 메커니즘을 제안하여 DiT의 복잡성을 줄이고, 고해상도 이미지 생성에서의 효율성을 높이는 것입니다. 이는 DiT의 계산 시간과 노력이 크게 개선되었습니다.

2. 전체 요약:

이 논문은 DiT의 비효율성을 극복하기 위해 CLEAR라는 새로운 주의 메커니즘을 제안합니다. CLEAR는 각 쿼리가 제한된 지역 창 내의 토큰과만 상호작용하여 DiT의 선형 복잡성을 달성합니다. 실험 결과, CLEAR는 기존의 복잡한 DiT보다 크게 향상된 계산 효율성을 보여주었으며, 8K 해상도 이미지 생성에서 6.3배 빠른 속도를 기록했습니다. 이 접근법은 특히 고해상도 이미지 생성에 유리하며, 다양한 모델 및 플러그인에서 일반화 가능합니다.