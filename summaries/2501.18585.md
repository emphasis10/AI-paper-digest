# Thoughts Are All Over the Place: On the Underthinking of o1-Like LLMs
## TL;DR
## Summary
- [https://arxiv.org/pdf/2501.18585.pdf](https://arxiv.org/pdf/2501.18585.pdf)

1. **각 섹션 요약 (한국어)**

   - **서론**: 대규모 언어 모델(LLM)은 복잡한 문제를 해결할 수 있는 능력을 보이며, 인간과 유사한 깊은 사고를 모방하는 기능이 주목받고 있다. 그러나 이 연구에서는 '과소 사고(underthinking)'라는 현상을 발견하고, 이는 LLM이 유망한 추론 경로를 조기에 포기하는 경향이 있어 성능에 제한을 준다고 설명한다.

   - **과소 사고 관찰**: 연구에서는 LLM이 어려운 문제에 대해 사고를 빈번히 전환하는 경향을 보인다는 점을 여러 테스트 세트를 통해 분석하였다. 이러한 사고 전환은 정확한 해결책에 도달하기 위한 깊이가 부족한 추론을 초래하여 결과적으로 부정확한 응답을 유도한다.

   - **과소 사고의 존재**: 실험 결과, LLM이 문제를 해결하면서 올바른 사고보다는 잘못된 사고로 빈번하게 전환하는 경향을 보였음이 밝혀졌다. 이로 인해 LLM이 정답을 도출하는 데 필요한 깊은 탐색이 부족하면 부정확한 결론에 이를 수 있다.

   - **실증적 과소 사고 결과**: 새로운 과소 사고 메트릭(ξUT)을 제안하여 LLM의 응답이 정답에 얼마나 기여하는지를 평가하는 체계를 구축하였다. 이를 통해 LLM이 생성한 토큰의 효율성을 측정하여 과소 사고 수준을 정량적으로 분석하였다.

   - **과소 사고 문제 완화**: 사고 전환 페널티(TIP)를 적용하는 새로운 디코딩 전략을 제안한다. 이 전략은 사고 전환에 대한 페널티를 부여하여 LLM이 한 가지 사고를 직면했을 때 충분히 탐구하도록 유도한다. TIP을 통해 어려운 문제에서 LLM의 정확도를 향상시킬 수 있음을 보여주었다.

   - **결론**: 본 연구가 LLM의 추론 과정에 대한 깊은 이해를 제공하며, 문제 해결 능력을 개선하기 위한 실용적인 접근 방식을 제안하여 AI의 발전에 기여하는 것을 목표로 한다.

2. **종합 요약 (한국어)**

   연구는 대규모 언어 모델(LLM)에 존재하는 '과소 사고' 현상을 분석하며, LLM이 유망한 추론 경로를 조기에 포기함으로써 성능이 제한됨을 보여준다. 실험을 통해 LLM에서 빈번한 사고 전환이 부정확한 응답으로 이어진다는 사실을 발견하고, 이에 따라 새로운 메트릭인 ξUT를 제안하여 토큰 효율성을 평가하였다. 마지막으로, 사고 전환 페널티(TIP)를 도입하여 모델이 보다 깊이 있는 사고를 할 수 있도록 유도하고, 이를 통해 복잡한 문제에 대한 해결 능력을 향상시킬 수 있음을 입증하였다. 이러한 연구 결과는 AI 모델의 문제 해결 능력을 한 단계 더 발전시키는 데 기여할 것으로 기대된다.