# HelpSteer2: Open-source dataset for training top-performing reward models
## TL;DR
## Summary
- [https://arxiv.org/pdf/2406.08673.pdf](https://arxiv.org/pdf/2406.08673.pdf)

### 1. 섹션별 요약
#### Introduction
최근 AI와 머신러닝 분야에서는 인간 피드백을 통한 강화학습(RLHF)이 중요성을 명확히 보여주고 있습니다. 여러 상용 및 오픈 소스 모델들이 선호도 데이터셋을 사용하여 모델 정렬을 개선해 왔으나, 정확한 데이터 세부 사항은 다소 부족한 실정입니다. 이 문제를 해결하기 위해 HelpSteer2라는 오픈 소스 헬프풀니스 데이터셋이 제안되었으며, 이것을 통해 복잡한 요구를 충족할 수 있는 최신 모델 정렬 패러다임 SteerLM 2.0이 등장했습니다.

#### Dataset
- **데이터셋 수집**: HelpSteer2 데이터셋은 다각적인 보상 모델을 훈련시켜 거대 언어 모델이 인간의 선호도에 맞추도록 설계되었습니다. 사용된 데이터는 다양한 출처에서 수집되었으며, 모델은 이러한 데이터를 사용하여 복잡한 다요소의 보상을 학습합니다.
- **보상 모델**: SteerLM 보상 모델은 선호도 데이터를 통해 응답의 평점을 예측하도록 훈련됩니다. 이는 기존의 이진 선호도 모델보다 더 세밀한 정보를 제공합니다.

#### Methods
- **SteerLM 2.0 모델**: SteerLM 2.0은 특정 속성 값을 지정하여 최적의 응답 분포를 생성하도록 학습됩니다. 이를 통해 모델은 더욱 정확하고 일관된 응답을 생성할 수 있습니다.
- **조건부 분포 최적화**: 최적의 SteerLM 모델은 Bayes 규칙을 활용하여 P(y|a,x)를 생성하며, 이는 주어진 속성 a와 프롬프트 x에 따라 응답 y를 생성합니다.

#### Experiments and Results
- **평가 지표**: MT Bench와 TruthfulQA 등을 통해 모델의 성능을 측정했습니다. SteerLM과 PPO(Proximal Policy Optimization), DPO(Direct Preference Optimization) 등 다양한 모델 정렬 기법이 비교되었습니다.
- **결과 요약**: SteerLM 2.0이 다양한 측정 항목에서 높은 성능을 보였으며, 특히 장문 응답 생성 및 상세 답변에 강점이 있습니다. HelpSteer2 데이터셋을 사용한 DPO 모델은 TruthfulQA와 Arena Hard에서 우수한 성능을 발휘했습니다.

#### Conclusions
이번 연구는 HelpSteer2 데이터셋을 통해 다요소 보상 모델을 훈련시키고 이를 활용한 SteerLM 2.0 모델이 복잡한 응답 요구를 효과적으로 충족할 수 있음을 보여주었습니다. 이를 통해 다양한 AI 응용 분야에서 높은 정렬성과 효율성을 기대할 수 있습니다.

### 2. 전체 요약
이 논문은 인간 피드백을 통한 강화학습의 중요성을 강조하며, 이를 위해 HelpSteer2라는 새로운 오픈 소스 데이터셋을 제안합니다. 데이터셋을 통해 여러 각도의 보상을 제공하고, 이를 통해 훈련된 SteerLM 2.0 모델이 더 정확하고 일관된 응답을 생성할 수 있도록 합니다. 이 모델은 다양한 평가 지표에서 우수한 성능을 보였으며, 복잡한 다요소 요구를 충족하는 데 특히 효과적입니다. 이를 통해 AI 시스템의 정렬성과 효율성을 개선하는 데 큰 기여를 할 것으로 기대됩니다.