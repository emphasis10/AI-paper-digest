# OmniPred: Language Models as Universal Regressors
## TL;DR
## Summary
- [https://arxiv.org/pdf/2402.14547.pdf](https://arxiv.org/pdf/2402.14547.pdf)

1. 각 섹션의 중요 내용 요약:

- **서론:**
  OmniPred는 모든 형태의 (x, y) 데이터에 대해 종단간 회귀를 수행할 수 있는 프레임워크입니다. Google Vizier의 대규모 데이터베이스를 활용하여 대규모로 학습할 경우 전통적인 회귀 모델을 능가할 수 있음을 보여줍니다.

- **관련 연구 및 모티베이션:**
  기존 회귀 모델들은 고정된 길이의 특징 벡터를 필요로 하며, 다양한 입력 공간을 지원하지 못합니다. 이에 따라 OmniPred는 다중 작업 학습을 통해 다양한 입력 공간과 목표의 이점들을 활용하여 전통적인 회귀 모델을 능가할 수 있음을 언급합니다.

- **방법론:**
  OmniPred의 방법론은 자유 형태의 텍스트 표현을 이용하여 회귀를 수행합니다. 다양한 입력 공간에서 조정없이 다양한 데이터에 대한 매우 정확한 예측이 가능하도록 설계되었습니다.

- **실험 및 결과:**
  실험에서는 OmniPred가 다양한 영역에서 다중 작업 학습을 통해 단일 작업에 비해 개선된 성능을 보임을 보여주며, 특히 새로운 데이터에 대한 적응력도 뛰어납니다. 이는 전통적인 데이터 구조에 제한받지 않는 텍스트와 토큰 기반 표현의 이점을 활용한 결과입니다.

2. 전반적 요약:

이번 연구는 OmniPred라는 프레임워크를 통해 언어 모델이 다양한 포맷의 데이터를 회귀할 수 있는 가능성을 제시하였고, 전통적 회귀 모델의 한계를 뛰어넘는 방법론을 제안하였습니다. OmniPred는 텍스트 기반으로 다중 작업 학습의 이점을 활용하여, 향후 AI와 기계 학습 분야에 새로운 가능성을 열어 줄 혁신적인 접근입니다.