# Almost Surely Safe Alignment of Large Language Models at Inference-Time
## TL;DR
## Summary
- [https://arxiv.org/pdf/2502.01208.pdf](https://arxiv.org/pdf/2502.01208.pdf)

### 1. 각 섹션 요약 (Korean)

**1. 서론**
이 논문은 최신 대형 언어 모델(LLM)의 안전을 보장하는 새로운 추론 시간 정렬 접근 방식을 소개합니다. 기존의 RLHF 기반 방법은 비효율적이며 과적합에 취약한 반면, 이 방법은 LLM의 잠재 공간에서 안전한 응답 생성을 제약된 마르코프 결정 프로세스(MDP)로 재구성합니다. 이를 통해, 안전성 보장을 형식적으로 선언할 수 있습니다.

**2. 배경**
LLM을 확률 동적 시스템으로 바라보며, 모델의 행동이 내부 매개변수와 입력에 따라 확률적으로 진화한다고 설명합니다. 이는 각각의 새로운 토큰이 생성되는 프로세스를 명확히 이해하는 데 도움이 됩니다.

**3. 이론적 통찰**
잠재 공간에서의 최적 정책을 계산할 수 있음과 동시에 해당 정책이 원래 토큰 공간에서도 유효하다는 것을 보여줍니다. 이로 인해 잠재 공간에서 최적화를 수행하는 것이 안전성과 정확성을 모두 만족시킬 수 있음을 증명합니다.

**4. 알고리즘 및 실용적 구현**
'InferenceGuard'라는 알고리즘을 제안하여, 안전한 응답 생성을 위한 두 가지 주요 접근 방식을 도입합니다. 하나는 완전한 응답이 생성된 후 안전 비용을 쿼리하는 경우에, 다른 하나는 직접 비용 쿼리를 활용하여 효율적인 최적화를 이룹니다.

**5. 실험 및 결과**
InferenceGuard에 대한 실험을 통해, 다양한 LLM 모델에서 높은 안전성 비율을 달성하면서도 성과의 균형을 유지했습니다. Alpaca-7B 모델에서는 91.04%, Beaver-7B-v3 모델에서는 100%의 안전 비율을 기록했습니다.

### 2. 전체 요약 (Korean)

이 논문은 효과적인 안전 정렬을 위한 새로운 접근 방식을 제안하고, 기존의 RLHF와 같은 방법들이 가진 한계를 극복하고자 합니다. 잠재 마르코프 결정 프로세스를 활용하여 안전한 응답 생성을 형식적으로 보장하였고, 'InferenceGuard'라는 알고리즘을 통해 이를 실제로 구현했습니다. 실험 결과, 기존의 방법들보다 더 높은 안전성을 보이며, LLM의 다양한 활용에 기여할 수 있음을 입증했습니다. 이 연구는 AI의 안전성과 효율성을 동시에 만족시킬 수 있는 가능성을 제시하며, 향후 AI 발전에 중요한 기여를 할 것으로 기대됩니다.