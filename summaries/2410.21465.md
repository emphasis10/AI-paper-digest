# ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM Inference
## TL;DR
## Summary
- [https://arxiv.org/pdf/2410.21465.pdf](https://arxiv.org/pdf/2410.21465.pdf)

### 1. 각 섹션의 요약

#### 초록
지속적으로 인기 있는 대형 언어 모델(LLM)의 사용 확대로, 시퀀스 길이에 따른 메모리 사용 증가와 각 토큰 생성에 필요한 접근 시간으로 인해 장기 전환 LLM의 처리 속도가 저하되고 있습니다. 이에 따라 ShadowKV라는 시스템을 제안하였습니다. 이 시스템은 메모리 사용량을 줄이고, 적절한 키-값 선택 전략을 통해 디코딩 지연을 최소화합니다.

#### 소개
대형 언어 모델은 확장하여 긴 문맥을 처리할 수 있는 능력을 증명해왔습니다. 그러나 이러한 긴 문맥 LLM을 지원하는 데에는 키-값(KV) 캐시에 대한 도전과제가 존재합니다. 기존의 해결 방법은 GPU 메모리 사용량을 줄이는 데 한계가 있으며, 일부는 정보 손실을 초래할 수도 있습니다. 이에 ShadowKV는 혁신적 방법으로 장기 전환 LLM의 추론 효율성을 높였습니다.

#### ShadowKV 시스템
ShadowKV는 낮은 차원으로의 키 캐시와 CPU로의 값 캐시 오프로딩을 통해 메모리 사용을 최적화하였습니다. 이 시스템은 스파스 주의력 기법을 사용하여 높은 정확도를 유지하며 처리량을 증가시킵니다. 실험 결과, ShadowKV는 최대 6배 큰 배치 크기를 지원하고 최대 3.04배의 처리량 증가를 이룰 수 있었습니다.

#### 실험과 평가
ShadowKV의 성능을 다양한 LLM 및 실제 배치 서비스 시나리오에서 평가하였습니다. ShadowKV는 6배까지 큰 배치 크기와 3배 이상의 처리량 증가를 제공합니다. 이는 무한한 GPU 메모리를 가정한 무한 배치 크기보다도 성능이 뛰어납니다.

#### 결론
ShadowKV는 LLM의 추론을 위한 높은 처리량을 제공하며, 기존 문제점들을 극복하고 메모리 사용을 줄이는 혁신적인 접근 방식을 제안합니다. ShadowKV는 다양한 모델에서 실험적으로 뛰어난 성능을 입증하며, 향후 LLM 추론의 발전에 기여할 수 있는 전망이 있습니다.

### 2. 전체 요약
ShadowKV는 장기 전환 대형 언어 모델의 처리 효율성을 극대화하기 위한 최적화 시스템입니다. 본 논문은 메모리 사용 최적화와 스파스 주의력 기법을 활용하여, 기존 해결책의 한계를 극복하면서도 처리량을 극대화하는 방법론을 제시합니다. 이는 긴 문맥을 처리할 때의 메모리 사용 문제와 느린 디코딩 속도 문제를 효과적으로 해결하며, 대규모 데이터 집합과 복잡한 문맥에서도 높은 정확도와 처리 성능을 유지할 수 있습니다. ShadowKV는 향후 LLM 추론의 효율성과 확장 가능성을 크게 개선할 수 있는 잠재력을 갖추고 있습니다.