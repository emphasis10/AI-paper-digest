# Dataset Reset Policy Optimization for RLHF
## TL;DR
## Summary
- [https://arxiv.org/pdf/2404.08495.pdf](https://arxiv.org/pdf/2404.08495.pdf)

이 논문은 "데이터셋 리셋 정책 최적화(Dataset Reset Policy Optimization, DR-PO)"라는 새로운 강화학습(RL) 알고리즘을 제안합니다. 이 방법은 인간의 피드백에서 학습하는 강화학습(RLHF)에 적용되며, 기존의 데이터셋을 온라인 정책 훈련 절차에 통합하여 더 효과적인 학습이 가능하도록 합니다.

### 주요 내용 요약

1. **서론**:
   - RLHF는 보상 함수를 명시적으로 정의하기 어려운 상황에서 유용합니다. 인간의 선호도 기반 피드백을 사용하여 보상 모델을 학습하고 이를 최적화하는 두 단계로 구성됩니다.

2. **알고리즘 제안 (DR-PO)**:
   - DR-PO는 오프라인 데이터셋을 온라인 RL 절차에 재사용하여 효율을 높이는 것을 목표로 합니다.
   - 데이터셋 리셋을 통해 정책 최적화가 진행될 때마다 특정 상태로 리셋하여 데이터 수집을 시작합니다. 이는 일반적인 초기 상태 분포에서 시작하는 것과 대조됩니다.

3. **이론적 보장**:
   - DR-PO는 자연 함수 근사와 유한 샘플 복잡성 하에서 이론적으로 검증 가능한 성능 보장을 제공합니다.
   - 정책이 오프라인 데이터에 의해 커버된 어떤 정책과도 최소한 동등한 성능을 달성할 수 있음을 보여줍니다.

4. **실험 결과**:
   - TL;DR 요약 및 Anthropic Helpful Harmful (HH) 데이터셋에서 DR-PO는 기존의 PPO 및 DPO 보다 우수한 성능을 보였습니다.
   - 이러한 실험은 DR-PO가 데이터셋 리셋을 통한 온라인 데이터 수집이 성능 향상에 기여함을 시사합니다.

5. **결론**:
   - DR-PO는 강화학습에서 데이터셋 리셋을 활용하여 학습 효율성을 높이는 새로운 접근 방식을 제공합니다.
   - 이 알고리즘은 특히 생성 모델을 RL로 미세 조정하는 경우에 유용할 수 있으며, 이론적으로나 실제적으로 강력한 성능을 보입니다.

이 논문은 RLHF의 이론적 작업을 진전시키고, 데이터셋 리셋을 활용하여 온라인 RL을 강화하는 새로운 방법을 제시함으로써, 인간 피드백을 기반으로 하는 학습 프로세스의 효율성과 효과를 개선합니다.