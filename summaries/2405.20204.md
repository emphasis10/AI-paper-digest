# Jina CLIP: Your CLIP Model Is Also Your Text Retriever
## TL;DR
## Summary
- [https://arxiv.org/pdf/2405.20204.pdf](https://arxiv.org/pdf/2405.20204.pdf)

### 1. 각 섹션의 요약

#### 1. 서론
논문은 기존의 CLIP(Contrastive Language-Image Pretraining) 모델이 이미지와 텍스트를 공통 임베딩 공간으로 매핑하는 방법에 집중하지만 텍스트 전용 작업에는 성능이 낮다는 점을 지적합니다. 따라서 텍스트 전용 및 멀티모달 작업 모두에서 높은 성능을 발휘할 수 있는 새로운 다중 작업 대조 훈련 방법을 제안합니다.

#### 2. 관련 연구
대조 학습은 텍스트 임베딩 및 텍스트-이미지 학습에 널리 사용되고 있으며, 이를 통해 정보 검색, 의미적 텍스트 유사성, 텍스트 클러스터링 등의 다양한 작업을 수행합니다. 여러 연구에서 이러한 모델의 성능을 개선하기 위한 방법들을 제시하였습니다.

#### 3. 모델 아키텍처
제안하는 모델은 CLIP의 이중 인코더 아키텍처를 채택했으며, 텍스트 인코더로 JinaBERT 아키텍처를, 이미지 인코더로 EVA02 아키텍처를 사용합니다. 이 아키텍처들은 긴 텍스트와 이미지를 효과적으로 처리할 수 있도록 설계되었습니다.

#### 4. 훈련
모델은 텍스트-이미지 매칭과 텍스트-텍스트 매칭 두 작업을 동시에 최적화하는 세 단계 훈련 방법을 사용합니다. 첫 번째 단계에서는 짧은 인간 생성 캡션과 텍스트 쌍을 사용하고, 두 번째 단계에서는 합성 이미지 캡션을 추가합니다. 마지막 단계에서는 강력한 부정 예제를 사용하여 텍스트 인코더의 성능을 높입니다.

#### 5. 실험과 결과
실험 결과 제안하는 Jina-CLIP-v1 모델은 텍스트-이미지 검색과 텍스트-텍스트 검색 모두에서 우수한 성능을 나타냅니다. 모델은 기존의 CLIP 모델보다 약 15% 이상의 성능 향상을 보였으며, 특히 검색 작업에서 약 22%의 성능 향상이 있었습니다.

#### 6. 결론
논문은 다중 작업, 세 단계 훈련 방법을 통해 멀티모달 모델이 텍스트 전용 작업에서도 높은 성능을 유지할 수 있음을 확인하였습니다. 제안된 Jina-CLIP-v1 모델은 텍스트-이미지 검색과 의미적 텍스트 유사성 및 텍스트 검색 작업에서 우수한 성능을 보였으며, 이는 통합 멀티모달 모델들이 다양한 작업에 대해 훌륭한 성능을 발휘할 수 있음을 시사합니다.

### 2. 전체 요약
이 논문은 기존 CLIP 모델의 한계를 극복하기 위해 텍스트 전용 작업과 멀티모달 작업 모두에서 높은 성능을 발휘할 수 있는 새로운 다중 작업 대조 훈련 방법을 제안합니다. 이를 통해 제안된 Jina-CLIP-v1 모델은 텍스트-이미지 및 텍스트-텍스트 검색에서 뛰어난 성능을 보였으며, 이는 통합 멀티모달 모델이 다양한 작업에 대응할 수 있음을 입증했습니다. 이 모델은 영어 텍스트에만 국한되어 있으며, 향후 연구에서는 다국어 환경으로 확장할 계획입니다.