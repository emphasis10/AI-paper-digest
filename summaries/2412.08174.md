# Can Graph Neural Networks Learn Language with Extremely Weak Text Supervision?
## TL;DR
## Summary
- [https://arxiv.org/pdf/2412.08174.pdf](https://arxiv.org/pdf/2412.08174.pdf)

**1. 주요 내용 요약**

- **서론**: 이 논문에서는 Graph Neural Networks (GNNs)를 대규모 자연어 모델(LLM)의 임베딩 공간과 정렬시킴으로써 GNN이 다양한 다운스트림 작업에 적응하도록 하는 멀티모달 프롬프트 학습 접근법인 Morpher를 제안합니다. 이러한 접근법은 매우 약한 텍스트 감독만 존재하는 상황에서 GNN의 언어 의존성을 학습할 수 있도록 합니다.

- **배경 및 문제 정의**: GNN의 언어 의존성 학습을 위해 멀티모달 프롬프트 학습 방법이 도입되었으며, 이 과정에서 그래프와 텍스트 프롬프트를 병합하는 방식으로 진행됩니다. 또한, 값을 조정하여 그래프 임베딩 공간을 텍스트 임베딩 공간으로 매핑하는 방법을 제시합니다.

- **방법론**: Morpher의 성능은 두 가지 프리트레인 방식인 GraphCL과 SimGRACE를 사용하여 평가됩니다. 더불어 ImprovedAIO와 비교하여 다단계 작업 수행에 있어 Morpher의 우수성을 입증합니다.

- **실험**: 다양한 실제 데이터를 기반으로 하는 실험 결과, Morpher는 극도로 약한 텍스트 감독 하에서 다양한 다운스트림 설정에서 우수한 성능을 보입니다. 특히, 기존 방법들과 비교하였을 때 더 적은 라벨 샘플만으로도 높은 성능을 나타냈습니다.

- **결론 및 논의**: Morpher는 매우 약한 텍스트 감독 하에서도 그래프와 언어 간의 멀티모달 프롬프트 조정이 가능하며, 그래프 텍스트 멀티모달 프롬프트 학습 패러다임의 가능성을 제안합니다. 이후 연구는 이러한 구조를 확장하여 더 많은 데이터 및 응용 작업으로의 전환 가능성을 탐구할 수 있습니다.

**2. 전체 요약**

이 연구는 기존의 그래프 신경망(GNN)과 대규모 자연어 모델(LLM)을 효과적으로 결합하는 멀티모달 프롬프트 학습 접근법, Morpher를 제안합니다. 이는 매우 약한 텍스트 데이터에도 불구하고 GNN이 임베딩 차원에서 언어 의존성을 학습하도록 합니다. Morpher는 ImprovedAIO보다 다양한 작업에서 우수한 성능을 보이며, 실험을 통해 기존의 최첨단 방법들보다 효율적이고 효과적임을 입증하였습니다. 이러한 성과는 GNN의 다운스트림 작업 적응성과 텍스트 감독 활용의 새로운 가능성을 열어줍니다.