# LLM Pruning and Distillation in Practice: The Minitron Approach
## TL;DR
## Summary
- [https://arxiv.org/pdf/2408.11796.pdf](https://arxiv.org/pdf/2408.11796.pdf)

### 1. 각 섹션 요약 및 주된 기여

#### Introduction (소개)
본 보고서는 가중치 가지치기(weight pruning)와 지식 증류(knowledge distillation)를 결합하여 Llama 3.1 8B와 Mistral NeMo 12B 모델을 각각 4B와 8B 파라미터로 압축하는 방법을 설명합니다. 이 방법은 원본 데이터셋에 대한 접근 없이도 교사 모델을 미세 조정(finetuning)하여 가능한 한 데이터 분포 불일치를 최소화합니다.

#### Methodology (방법론)
고수준의 접근법은 먼저 교사 모델을 미세 조정한 후 가지치기를 통해 모델을 압축하며, 이후 증류를 통해 손실된 모델 정확도를 회복하는 방식입니다. 중요한 차별점은 원본 데이터셋 없이 별도로 준비한 데이터셋으로 교사 모델을 미세 조정하는 '교사 보정(teacher correction)' 단계입니다.

#### Pruning (가지치기)
가지치기는 모델 크기를 줄이기 위한 강력한 기술로, 블록이나 채널 단위로 모델 가중치의 비영향 요소를 제거합니다. 가지치기 단계에서 각 레이어, 뉴런, 주의(attention) 헤드 및 임베딩 차원의 중요성을 계산하여 이에 따른 중요도 순위를 산출합니다.

#### Retraining with Distillation (증류를 통한 재교육)
증류를 통한 재교육은 가지치기 후 손실된 정확도를 회복하는 프로세스입니다. 두 가지 방법이 존재하는데, 하나는 기존의 라벨을 활용한 학습이고, 다른 하나는 더 큰 모델(교사 모델)로부터 작은 모델(학생 모델)로 지식을 전이하는 방법입니다. 주로 KL Divergence 손실 함수를 사용하여 증류를 진행합니다.

#### Training Details (훈련 세부사항)
기존 Llama 3.1 8B와 Mistral NeMo 12B 모델은 가진 독점 데이터셋으로 사전 학습되었으며, 우리는 접근이 불가한 상태에서 이를 미세 조정하여 증류와 가지치기에 활용합니다.

#### Analysis (분석) 및 Evaluation (평가)
가지치기 후 증류를 거친 모델이 폭넓은 벤치마크 테스트에서 강력한 성능을 나타냅니다. 특히, 폭 가지치기(width pruning)가 깊이 가지치기(depth pruning)에 비해 일관된 이점을 증명했습니다. 여러 조건에서 교사 보정이 최적의 가지치기와 증류에 필수적임을 확인했습니다.

### 2. 전체적인 요약

보고서의 주요 기여는 Llama 3.1 8B 및 Mistral NeMo 12B 모델을 가중치 가지치기와 지식 증류를 통해 각각 4B와 8B 파라미터로 성공적으로 압축한 점입니다. 이를 통해 데이터 소비와 훈련 시간을 획기적으로 줄이는 동시에, 압축된 모델의 성능을 유지할 수 있음을 입증했습니다. 특히, 단계적인 가지치기와 증류 과정에서 교사 보정의 중요성을 강조하며, 모델의 실험적 성능 향상을 다수의 벤치마크를 통해 확인했습니다.