# Great Models Think Alike and this Undermines AI Oversight
## TL;DR
## Summary
- [https://arxiv.org/pdf/2502.04313.pdf](https://arxiv.org/pdf/2502.04313.pdf)

1. **각 섹션 요약**:
   - **서론**:
     머신러닝 모델의 능력이 향상되어 왔으며, 이는 주로 훈련 데이터 양의 증가 덕분이다. 최근에는 인간의 선호도나 전문적 작업에 대한 주석을 활용한 사후 훈련에 대한 관심이 높아지고 있다. 그러나 이는 느리고 비용이 많이 드는 과정이다. 따라서 LLM(대형 언어 모델)을 사용하여 훈련 데이터를 주석 처리하는 방법이 대안으로 제시된다.

   - **모델 유사성 측정 방법론**:
     두 모델 간의 유사성을 정량화하기 위한 새로운 확률적 지표인 CAPA(Chance Adjusted Probabilistic Alignment)가 소개된다. 이 지표는 다른 모델의 오류를 비교하여 기능적 유사성을 측정한다. 이 방법은 모델의 출력 확률을 포함하여 서로의 예측 간의 차이를 다룬다.

   - **LLM 판별자와 유사성**:
     LLM이 다른 모델의 주석을 사용하여 훈련할 때, 유사성은 성능 향상에 중요한 역할을 한다. 특히 강력한 모델이 약한 모델의 주석으로 훈련될 경우, 두 모델 간의 차이가 클수록 성능 향상이 더 효과적이다. 이는 '약한-강한 일반화(weak-to-strong generalization)' 현상을 통해 나타난다.

   - **모델의 오류가 유사해지는 경향**:
     모델의 능력이 향상됨에 따라 그들의 오류는 점점 더 유사해지는 경향이 있다. 이는 AI 감독의 신뢰성을 저하시킬 수 있는 우려를 낳는다. 다양한 모델의 접근은 오류의 상관관계를 줄임으로써 편향을 줄이며, AI 감독에서의 성과를 향상시킬 수 있다.

   - **결론 및 미래 작업 방향**:
     모델 유사성 측정의 중요성이 강조되며, CAPA 지표를 사용하여 AI 감독에서의 여러 차원에서의 시사점을 탐구한다. AI 감독이 더욱 중요한 역할을 할 것으로 예상되기 때문에, 모델의 오류가 점점 더 유사해지는 경향은 안전성에 대한 우려를 불러일으킨다.

2. **전체 요약**:
   이 논문은 대형 언어 모델의 감독을 위한 새로운 접근 방식을 제안하고, CAPA라는 새로운 유사성 지표를 개발했다. 이를 통해 모델 간의 유사성이 AI 감독의 여러 측면에서 미치는 영향을 조사하며, 이들 간의 오류가 점점 비슷해지는 트렌드가 AI의 안전성에 미치는 위험을 강조한다. 특히, 서로 다른 모델 간의 유사성 감소가 AI 감독의 편향을 줄이는 데 중요함을 보여주며, 이러한 통찰력은 AI 기술을 더욱 발전시키는 데 기여할 수 있다.