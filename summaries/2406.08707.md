# mOSCAR: A Large-scale Multilingual and Multimodal Document-level Corpus
## TL;DR
## Summary
- [https://arxiv.org/pdf/2406.08707.pdf](https://arxiv.org/pdf/2406.08707.pdf)

#### 1. Introduction
이 논문은 대규모 멀티모달 언어 모델(mLLMs)의 훈련과 관련된 내용을 다루고 있습니다. mLLMs는 텍스트와 이미지 데이터를 대량으로 사용하여 훈련됩니다. 기존 연구는 캡션과 같은 데이터만 사용하였으나, 최근 연구에서는 텍스트와 이미지가 interleaved된 시퀀스를 포함하여 훈련하는 것이 더 나은 결과를 가져온다는 것을 밝혔습니다. 그러나 기존 데이터셋은 주로 영어로 이루어져 있고, 다국어 데이터셋은 제한적입니다. 이를 해결하기 위해 본 논문에서는 163개 언어로 구성된 mOSCAR라는 대규모 다국어 멀티모달 데이터셋을 소개합니다.


#### 2. Related Work
이전 연구들은 주로 영어 기반 데이터셋과 캡션 데이터만을 사용해 멀티모달 모델을 훈련해 왔습니다. Flamingo 모델은 이를 넘어서는 새 방식을 제안했고, 다른 연구들도 이를 다국어로 확장하려는 시도를 하고 있지만 아직 부족한 실정입니다.

#### 3. Dataset Creation Pipeline
mOSCAR 데이터셋은 웹 기반 크롤링을 통해 수집된 315M 개의 문서와 1.2B 개의 이미지를 포함합니다. 이 데이터셋은 철저한 필터링과 평가 과정을 통해 품질과 다양성을 보장합니다.

#### 4. Training a Multilingual Multimodal Language Model
다양한 언어로 구성된 텍스트-이미지 데이터 중 일부는 캡션 데이터를 이용하여 훈련하였습니다. 또한 mOSCAR 데이터를 포함한 훈련은 모델의 몇 가지 이동학습 성능을 크게 향상시켰습니다.

#### 5. Evaluation Setup
모델은 여러 이미지-텍스트 멀티링구얼 태스크 및 벤치마크에서 평가되었습니다. XVNLI, MaRVL 같은 추론 및 시각적 질문응답, 출처 요약, 자막 생성 등의 다양한 태스크에서 평가되었습니다.

#### 6. Results
mOSCAR 데이터를 사용하여 추가로 훈련된 모델은 캡션 데이터만을 사용한 모델에 비해 더 높은 점수를 기록했습니다. 특히 몇 가지 이동학습 성능에서 큰 차이를 보였습니다.

#### 7. Conclusion, Limitations and Societal Impacts
본 논문에서 소개한 mOSCAR는 기존의 다른 데이터셋들과 비교하여 다국어와 멀티모달 성능을 크게 향상시켰음을 확인했습니다. 하지만 몇 가지 한계점과 사회적 영향을 고려해야 합니다.

### 전체 요약
본 논문은 다국어와 멀티모달 데이터를 사용하여 훈련한 AI 모델(mOSCAR)이 기존 방식에 비해 성능이 향상된다는 것을 보여줍니다. 주된 기여는 세계 각국의 언어로 다양한 데이터를 포함한 대규모 데이터셋(mOSCAR)을 소개하고 이를 통해 멀티모달 모델의 성능을 향상시키는데 있습니다. 연구 결과는 특히 이동학습 성능에서 큰 개선을 나타내며, 이는 다양한 태스크와 벤치마크에서 평가되었습니다. 최종적으로, mOSCAR 데이터셋을 사용한 모델은 더 나은 성능을 보였으며, 이는 멀티모달 및 다국어 머신러닝 연구에 큰 진전을 이룰 수 있음을 시사합니다.