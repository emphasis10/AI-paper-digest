# Pegasus-1 Technical Report
## TL;DR
## Summary
- [https://arxiv.org/pdf/2404.14687.pdf](https://arxiv.org/pdf/2404.14687.pdf)

이 논문은 Pegasus-1이라는 최신 다모달 언어 모델을 소개하며, 이 모델은 자연어를 통한 비디오 내용의 이해 및 상호작용에 특화되어 있습니다. Pegasus-1은 비디오 데이터의 독특한 도전을 해결하기 위해 설계되었으며, 공간적 및 시간적 정보를 해석하는 능력을 갖추고 있습니다. 주요 내용은 다음과 같습니다:

1. **모델 아키텍처**: Pegasus-1은 세 가지 주요 구성 요소로 구성됩니다.
   - **비디오 인코더 모델**: 비주얼 및 오디오 입력에서 멀티모달 임베딩을 생성합니다.
   - **비디오-언어 정렬 모델**: 비디오 임베딩과 언어 임베딩을 매핑하여 비디오와 텍스트 표현이 일치하도록 합니다.
   - **대형 언어 모델(디코더 모델)**: 정렬된 임베딩과 사용자 프롬프트를 사용하여 상황에 맞는 텍스트 출력을 생성합니다.

2. **훈련**: Pegasus-1은 비디오 데이터의 다양한 모달리티를 조화롭게 이해하기 위해 설계된 멀티모달 기초 모델입니다. 1,000만 개 이상의 다양한 비디오에 대한 상세한 설명을 주석으로 달아 사전 훈련을 실시하였으며, 이후 명령 조정 단계를 통해 모델의 사용자 지시에 대한 반응성을 미세 조정합니다.

3. **벤치마크 결과**: Pegasus-1은 비디오 대화, 제로샷 비디오 질문 응답, 비디오 요약 벤치마크에서 새로운 최고 기록을 달성하였습니다. 이 모델은 복잡한 비디오 데이터를 이해하는 능력을 일반화하여 다양한 상황에서 우수한 성능을 보여줍니다.

4. **장단점 및 활용 가능성**: Pegasus-1은 탁월한 비디오 이해 및 생성 능력을 보유하고 있으나, 아직 완전히 신뢰할 수 있거나 일관성 있는 수준은 아니며, 이에 대한 지속적인 개선이 필요합니다.

Pegasus-1은 비디오 데이터의 복잡성을 효과적으로 처리하고, 다양한 비디오 기반 작업에서 뛰어난 성능을 발휘할 잠재력을 가진 혁신적인 다모달 언어 모델입니다. 이는 특히 비디오와 언어의 상호 작용이 중요한 응용 분야에서 큰 영향을 미칠 수 있습니다.

## Similar Papers
- [MiniGPT4-Video: Advancing Multimodal LLMs for Video Understanding with Interleaved Visual-Textual Tokens](2404.03413.md)
- [PLLaVA : Parameter-free LLaVA Extension from Images to Videos for Video Dense Captioning](2404.16994.md)
- [LLM In-Context Recall is Prompt Dependent](2404.08865.md)
- [Gemini vs GPT-4V: A Preliminary Comparison and Combination of Vision-Language Models Through Qualitative Cases](2312.15011.md)
- [Video-MME: The First-Ever Comprehensive Evaluation Benchmark of Multi-modal LLMs in Video Analysis](2405.21075.md)
- [SPIQA: A Dataset for Multimodal Question Answering on Scientific Papers](2407.09413.md)
- [SlowFast-LLaVA: A Strong Training-Free Baseline for Video Large Language Models](2407.15841.md)
- [MMBench-Video: A Long-Form Multi-Shot Benchmark for Holistic Video Understanding](2406.14515.md)
- [MMIU: Multimodal Multi-image Understanding for Evaluating Large Vision-Language Models](2408.02718.md)
