# Power Scheduler: A Batch Size and Token Number Agnostic Learning Rate Scheduler
## TL;DR
## Summary
- [https://arxiv.org/pdf/2408.13359.pdf](https://arxiv.org/pdf/2408.13359.pdf)

### 논문 요약(Korean Summary)

#### 1. Introduction (소개)
- **요약**: 논문은 초거대 언어 모델(LLM)의 사전 학습을 위한 최적의 학습률을 찾는 어려움을 설명합니다. 기존의 코사인 학습률 스케줄러가 여러 모델에서 효과적이나, 미리 정의된 학습 단계 수가 필요하여 중간 체크포인트와 연속 학습에 있어 문제가 발생한다고 지적합니다.
- **주요 기여**: 최적 학습률을 예측하기 위한 새로운 학습률 스케줄러인 Power 스케줄러를 제안하여, 배치 크기와 토큰 수에 무관하게 적용할 수 있도록 하였습니다.

#### 2. Background (배경)
- **요약**: Maximum Update Parametrization (µP)을 이용하여 작은 프록시 모델에서 대규모 모델로의 학습률 전이 연구를 설명합니다. Warmup-Stable-Decay (WSD) 스케줄러의 세 단계(웜업, 안정, 감쇠)를 소개합니다.
- **주요 기여**: µP가 모델 간 학습률 전이에 효율적이며, WSD 스케줄러를 통해 안정적인 학습을 가능하게 하였습니다.

#### 3. Optimal Learning Rate Search (최적 학습률 탐색)
- **요약**: 다양한 배치 크기와 토큰 수에 대한 최적 학습률의 관계성을 연구하였고, 최적 학습률이 토큰 수가 증가함에 따라 감소함을 발견했습니다. 이를 통해 학습률과 배치 크기, 토큰 수의 관계를 모형화했습니다.
- **주요 기여**: 최적 학습률이 배치 크기와 토큰 수의 거듭제곱 관계에 있다는 점을 밝히며, 이는 µP를 이용해 다양한 모델 크기에서 전이 가능함을 입증했습니다.

#### 4. Power Scheduler (파워 스케줄러)
- **요약**: PowerLR 스케줄러를 제안하여 배치 크기와 토큰 수에 영향을 받지 않고 다양한 상황에서 최적 학습률을 전이할 수 있게 하였습니다. 이는 학습 단계를 미리 정의할 필요 없이 학습률을 설정할 수 있는 장점을 가지고 있습니다.
- **주요 기여**: 다양한 상황에서 Power 스케줄러가 기존의 WSD, 코사인 스케줄러와 비교하여 우수하거나 유사한 성능을 보임을 실험적으로 입증했습니다.

#### 5. Pre-Training Experiments (사전 학습 실험)
- **요약**: 1B 및 3B 매개변수 모델을 다양한 학습률 스케줄러를 이용해 실험하였고, Power 스케줄러가 여러 언어 모델링 및 다운스트림 작업에서 꾸준히 더 나은 성능을 보임을 확인했습니다.
- **주요 기여**: Power 스케줄러가 다양한 조건에서도 최적의 성능을 발휘할 수 있다는 점을 입증하며, 이는 큰 모델에서도 동일한 성능을 유지할 수 있습니다.

#### 6. Conclusion (결론)
- **요약**: 연구를 통해 학습률, 배치 크기, 토큰 수 간의 관계를 체계적으로 연구하였고, 새로운 Power 스케줄러를 제안하여 다양한 상황에서 최고의 성능을 보임을 확인했습니다.
- **주요 기여**: Power 스케줄러는 안정적인 성능을 유지하면서도 배치 크기와 토큰 수에 독립적인 최적 학습률을 제공합니다.

### 전체 요약 (Overall Summary)
이 논문은 초거대 언어 모델의 사전 학습을 위한 최적의 학습률을 찾는 문제를 해결하기 위해 새로운 학습률 스케줄러인 Power 스케줄러를 제안합니다. 기존의 학습률 스케줄러인 코사인과 WSD가 가진 문제점을 개선하여, 학습 단계 수를 미리 정의할 필요 없이, 다양한 배치 크기와 토큰 수에 무관하게 최적의 학습률을 적용할 수 있습니다. 연구는 µP를 이용한 학습률 전이 실험을 통해, Power 스케줄러가 다양한 모델 크기에서도 안정적인 성능을 유지함을 입증하였습니다. 실험 결과, Power 스케줄러는 다양한 언어 모델링 및 다운스트림 작업에서 기존 스케줄러와 비교하여 우수한 성능을 보였습니다. 이를 통해 초거대 언어 모델의 학습 효율성을 크게 향상시킬 수 있음을 보여주었습니다.