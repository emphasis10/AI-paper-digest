# SPARC: Subspace-Aware Prompt Adaptation for Robust Continual Learning in LLMs
## TL;DR
## Summary
- [https://arxiv.org/pdf/2502.02909.pdf](https://arxiv.org/pdf/2502.02909.pdf)

1. **섹션 요약**

- **소개 (Introduction)**: 대형 언어 모델(LLM)이 자연어 처리에 뛰어난 능력을 보여주며, 다양한 작업과 도메인에서 전이를 가능하게 하는 특성을 강조하고 있습니다. 그러나 LLM은 정적 데이터 세트에만 효과적이며, 진화하는 환경에서는 새로운 정보를 점진적으로 통합하고 이전 지식을 유지해야 하는 도전 과제가 존재합니다.

- **배경 (Background)**: LLM의 지속적 학습에서 발생할 수 있는 문제인 '재앙적 망각(catastrophic forgetting)'을 포함한 기법들에 대해 설명하고 있으며, 간단한 재조정으로는 이미 학습한 내용을 잃는 문제가 발생할 수 있음을 밝힙니다.

- **소프트 프롬프트와 소프트 임베딩 (Soft Prompts and Soft Embeddings)**: 소프트 프롬프트가 정의되고, 고정된 LLM에 추가하여 특정 작업에 맞는 학습을 가능하게 할 뿐만 아니라, 기존 모델의 내부 가중치는 변경하지 않고도 작업에 적합하게 변화하도록 합니다. 이를 통해 학습할 수 있는 파라미터 수를 줄이면서도 효율성을 유지할 수 있습니다.

- **PCA 기반 서브스페이스 식별 (PCA-based Subspace Identification)**: 주성분 분석(PCA)을 이용하여 데이터의 차원을 줄이고 가장 중요한 패턴을 추출하여 작업별 서브스페이스를 형성하는 과정을 설명합니다. 이로써 작업 간의 유사성을 비교할 수 있습니다.

- **서브스페이스 겹침 (Subspace Overlap)**: 코사인 유사성을 통해 서브스페이스의 겹침을 정량화하고, 새로운 작업에 대해 기존 프롬프트를 재사용할 수 있는지를 평가하는 방법론을 다룹니다.

- **SPARC 프레임워크 (SPARC: Subspace-Aware Prompt Tuning for Continual Learning)**: SPARC는 저차원 공간에서 프롬프트를 튜닝하여 새로운 작업에 적응하며, 기계의 내장 구조를 변경하지 않도록 하여 사전 학습된 지식이 손실되지 않도록 합니다. 이 과정에서 PCA 기반의 서브스페이스 분석과 정규화된 발산을 통해서 작업 간 충돌을 방지합니다.

- **결과 (Results)**: SPARC 프레임워크가 다양한 도메인에서 잠재력을 보여주며, 기존 학습 지식을 97% 이상 유지하면서도 새로운 작업에 대해 강력한 전이 능력을 발휘하는 결과를 나타냅니다. 이는 모델 파라미터의 0.04%만 조정하여 달성되었습니다.

- **결론 (Conclusions)**: SPARC는 지속적 학습을 위한 강력하고 효율적인 기법으로, 이전 지식을 보호하면서도 새로운 작업에 대한 적응력을 보장하는 것을 목표로 하고 있습니다. LoRA와의 통합을 통해 필요 시 높은 성능을 유지할 수 있도록 하였습니다.

2. **전체 요약**: 
이 논문은 대형 언어 모델의 지속적 학습 문제를 해결하기 위해 SPARC라는 프레임워크를 제안합니다. 이 프레임워크는 PCA 기반의 서브스페이스 분석을 통해 새로운 작업에 적합한 프롬프트를 생성하고, 기존의 정보를 잃지 않도록 하는 것을 목표로 하며, 결과적으로 효율적인 파라미터 조정과 높은 성능 유지를 동시에 달성합니다. SPARC는 0.04%의 파라미터만을 조절하여 이전 작업의 97% 이상의 지식을 유지하면서 새로운 작업을 성공적으로 학습하는 성과를 보여줍니다. 

이와 같은 접근은 AI의 지속적 발전을 위한 길잡이가 될 수 있습니다.