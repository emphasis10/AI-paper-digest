# DoRA: Weight-Decomposed Low-Rank Adaptation
## TL;DR
## Summary
- [https://arxiv.org/pdf/2402.09353.pdf](https://arxiv.org/pdf/2402.09353.pdf)
- [https://discuss.pytorch.kr/t/dora-lora-weight-decomposed-low-rank-adaptation/3528](https://discuss.pytorch.kr/t/dora-lora-weight-decomposed-low-rank-adaptation/3528)

#### 서론
- **요약:** 일반 도메인 데이터셋으로 사전 훈련된 모델들은 다양한 애플리케이션에 상당한 일반화 능력을 보여줍니다. 많은 모델과 데이터셋의 크기가 증가함에 따라, 모델의 모든 매개 변수를 재훈련하는 전체 미세 조정(Full fine-tuning; FT)의 비용이 커지고 있습니다. 이러한 문제를 해결하기 위해 LoRA와 같은 매개 변수 효율적 미세조정(PEFT) 방법들이 도입되었으나, 여전히 FT와 비교할 때 학습 능력의 격차가 있습니다.
- **혁신적 파트:** 본 논문은 사전 훈련된 가중치를 크기와 방향의 두 요소로 분해하여 미세 조정하는 새로운 방법인 DoRA를 제안합니다. DoRA는 LoRA를 이용하여 방향성 요소를 효율적으로 업데이트하며 FT와 유사한 학습 능력을 보이면서도 추가적인 추론 비용을 필요로 하지 않습니다.

#### 관련 연구
- **요약:** 대규모 모델을 미세조정하는 데 높은 비용을 줄이기 위해 PEFT 방법이 설계되었습니다. 기존의 PEFT 방법은 Adaptor 기반, Prompt 기반 그리고 LoRA 같은 계산 부담을 늘리지 않는 방법으로 분류될 수 있습니다. 그러나 LoRA와 같은 방법들은 FT에 비해 학습 능력에 차이가 있음을 발견했습니다.
- **혁신적 파트:** 이 연구는 특히 LoRA와 그 변종들과 함께 본 연구에서 제안된 방법인 DoRA의 효과성을 광범위한 실험을 통해 검증하여 LoRA를 상회하는 결과를 보여줍니다.

#### LoRA와 FT의 패턴 분석
- **요약:** LoRA는 사전 훈련된 가중치의 저 "내재 랭크(intrinsic rank)" 업데이트를 가정하고, 두 개의 저랭크 행렬의 곱을 사용하여 사전 훈련된 가중치를 점진적으로 업데이트합니다. 본 논문에서는 이러한 가정을 바탕으로, 가중치를 크기와 방향성으로 분해하고, 이러한 분해를 통해 LoRA와 FT 사이의 학습 능력 차이를 탐구합니다.

### 2. 종합 요약

본 논문은 매개 변수 효율성이 높은 미세 조정 방법인 DoRA(Weight-Decomposed Low-Rank Adaptation)를 제안합니다. DoRA는 사전 훈련된 가중치를 크기와 방향으로 분해하고, LoRA를 활용하여 방향성 요소만을 효율적으로 업데이트하여 FT에 가까운 학습 능력을 달성합니다. 이를 통해 DoRA는 다양한 다운스트림 작업에서 LoRA를 일관되게 상회하는 성능을 보여주며, 특히 상식 추론, 시각적 지시 조정, 이미지/비디오-텍스트 이해 등에서 성과를 나타냅니다. DoRA는 학습 능력과 훈련 안정성을 향상시키면서 추가적인 추론 비용 없이 LoRA의 한계를 극복합니다. 본 연구는 PEFT 분야에 새로운 방향을 제시하며, 더 광범위한 학습 태스크에 효율적으로 적용될 수 있는 기회를 열어줍니다.