# Reconstructing Hand-Held Objects in 3D
## TL;DR
## Summary
- [https://arxiv.org/pdf/2404.06507.pdf](https://arxiv.org/pdf/2404.06507.pdf)

이 연구는 손으로 들고 있는 물체를 3D로 재구성하는 새로운 방법에 관한 것입니다. 특히, 손이 물체의 대부분을 가리는 상황에서도 단일 RGB 이미지로부터 물체의 3D 형태를 정확하게 파악할 수 있는 기술을 개발했습니다. 이 방법은 두 가지 주요 아이디어에 기반합니다: 첫째, 3D로 추정된 손의 정보를 활용해 물체의 위치와 크기를 추정하며, 둘째, 대상 물체의 범주가 상대적으로 제한되어 있기 때문에, 이를 이용하여 더 정확한 3D 재구성을 달성합니다. 연구팀은 MCC-Hand-Object (MCC-HO) 모델을 제안하며, 이 모델은 RGB 이미지와 추정된 3D 손 정보를 입력으로 받아 물체와 손의 3D 기하학적 구조를 동시에 재구성합니다. 그 후, 대규모 언어/비전 모델을 활용해 이미지 속 물체와 일치하는 3D 모델을 검색하고, 이를 네트워크로부터 추론된 기하학적 구조에 딱 맞도록 정렬하는 Retrieval-Augmented Reconstruction (RAR) 방법을 사용합니다. 실험 결과, 이 방법은 실험실 환경과 인터넷 데이터셋 모두에서 최신 기술 대비 우수한 성능을 보였으며, RAR을 통해 자동으로 손-물체 상호작용 이미지의 3D 라벨을 얻을 수 있음을 보여줍니다.

### 요약

손으로 들고 있는 물체를 3D로 재구성하는 것은 컴퓨터 비전과 로보틱스에서 중요한 문제입니다. 이 연구에서는 두 가지 주요 발견을 기반으로 하는 새로운 접근 방식을 제시합니다. 첫째, 이미지에서 3D 손을 추정하는 최신 기술의 발전을 활용하여, 손이 물체를 가리는 경우에도 물체의 위치와 크기를 정확하게 추론할 수 있습니다. 둘째, 대상 물체가 일상에서 자주 사용되는 제한된 범주에 속한다는 점을 이용하여, 이러한 물체들의 3D 모델을 효율적으로 검색하고 재구성하는 방법을 개발했습니다. MCC-HO 모델과 RAR 방법을 통해, 단일 RGB 이미지로부터 손과 물체의 3D 구조를 정확하게 재구성할 수 있으며, 이는 다양한 데이터셋에서 우수한 성능을 입증했습니다. 이 연구는 3D 손-물체 상호작용 재구성 분야에서 중요한 진전을 나타내며, 향후 더 많은 연구와 응용을 위한 기반을 마련합니다.

## Similar Papers
- [Shape of Motion: 4D Reconstruction from a Single Video](2407.13764.md)
- [OmniNOCS: A unified NOCS dataset and model for 3D lifting of 2D objects](2407.08711.md)
- [Lessons from Learning to Spin "Pens"](2407.18902.md)
- [Real3D: Scaling Up Large Reconstruction Models with Real-World Images](2406.08479.md)
- [SparseCraft: Few-Shot Neural Reconstruction through Stereopsis Guided Geometric Linearization](2407.14257.md)
- [Expressive Whole-Body 3D Gaussian Avatar](2407.21686.md)
- [Sampling 3D Gaussian Scenes in Seconds with Latent Diffusion Models](2406.13099.md)
- [PointInfinity: Resolution-Invariant Point Diffusion Models](2404.03566.md)
- [GS-LRM: Large Reconstruction Model for 3D Gaussian Splatting](2404.19702.md)
