# Superposition Prompting: Improving and Accelerating Retrieval-Augmented Generation
## TL;DR
## Summary
- [https://arxiv.org/pdf/2404.06910.pdf](https://arxiv.org/pdf/2404.06910.pdf)

### 1. 각 섹션 요약

**1. 서론 (Introduction)**  
논문은 대형 언어 모델(LLMs)이 긴 문맥을 처리할 때 발생하는 몇 가지 주요 문제점을 설명합니다. 특히, 시퀀스 길이에 따라 모델의 추론 비용이 기하급수적으로 증가하고, 불필요한 문맥 정보가 출력 품질을 저하시킬 수 있다는 "산만 현상"을 제기합니다. 이를 해결하기 위해 저자들은 "중첩 프롬프트(Superposition Prompting)"라는 새로운 방법론을 제안합니다. 이 방법론은 사전 학습된 LLM에 추가 학습 없이 직접 적용될 수 있으며, 효율성과 정확도를 동시에 향상시킵니다.

**2. 관련 연구 (Related Work)**  
이 섹션에서는 LLM을 사용한 검색 증강 생성(RAG) 및 긴 문맥 처리의 효율성을 개선하기 위한 기존 연구들을 다룹니다. 저자들은 특히 최근 연구에서 중요한 문서 위치에 따라 모델의 성능이 달라질 수 있음을 강조하며, 이러한 문제를 해결하기 위한 다양한 방법들을 논의합니다.

**3. 제안된 방법 (Proposed Method)**  
제안된 방법론인 중첩 프롬프트는 문서와 질의를 독립적인 경로로 병렬 처리하고, 질의와 관련이 없는 경로를 제거하여 효율성을 극대화하는 것입니다. 구체적으로, 프롬프트 경로 캐싱과 병렬화를 통해 추론 속도를 높이고, 경로 가지치기를 통해 과도한 정보로부터 모델을 보호합니다.

**4. 실험 (Experiments)**  
저자들은 다양한 질문-응답 데이터셋에서 제안된 방법의 이점을 실험적으로 증명합니다. 실험 결과는 중첩 프롬프트가 기존 방법들에 비해 시간 효율성을 크게 향상시키고, 문맥이 길어질수록 정확도를 개선한다는 것을 보여줍니다. 예를 들어, NaturalQuestions-Open 데이터셋에서 중첩 프롬프트는 정확도를 43% 향상시키면서 계산 시간을 93배 줄였습니다.

**5. 결론 (Conclusion)**  
결론적으로, 중첩 프롬프트는 LLM 기반 RAG 작업에서 사용자가 관찰하는 응답 지연을 줄이고 긴 시퀀스 모델링의 정확성을 높이는 데 효과적이라는 것을 입증했습니다. 미래 연구에서는 중첩 프롬프트의 세부 조정을 통해 추가적인 성능 향상을 탐구하고, RAG 외의 다른 설정에서도 이 아이디어를 일반화하는 방법을 조사할 것을 제안하고 있습니다.

### 2. 전체 요약
이 논문은 긴 문맥을 처리하는 대형 언어 모델의 비효율성을 해결하기 위한 혁신적인 방법인 중첩 프롬프트(Superposition Prompting)를 제안합니다. 이 방법은 기존 LLM에 추가적인 훈련 없이 바로 적용 가능하며, 문서와 질의를 병렬로 처리하여 불필요한 문맥을 제거하고, 모델의 정확도와 효율성을 동시에 개선할 수 있습니다. 다양한 질문-응답 데이터셋에서 실험한 결과, 중첩 프롬프트가 기존 방법들에 비해 응답 지연을 크게 줄이고, 긴 문맥에서의 정확도를 높이는 것으로 나타났습니다.

## Similar Papers
- [KV-Runahead: Scalable Causal LLM Inference by Parallel Key-Value Cache Generation](2405.05329.md)
- [Nearest Neighbor Speculative Decoding for LLM Generation and Attribution](2405.19325.md)
- [SeaKR: Self-aware Knowledge Retrieval for Adaptive Retrieval Augmented Generation](2406.19215.md)
- [PrimeGuard: Safe and Helpful LLMs through Tuning-Free Routing](2407.16318.md)
- [Improved Modelling of Federated Datasets using Mixtures-of-Dirichlet-Multinomials](2406.02416.md)
- [SnapKV: LLM Knows What You are Looking for Before Generation](2404.14469.md)
- [Simplified and Generalized Masked Diffusion for Discrete Data](2406.04329.md)
- [LazyLLM: Dynamic Token Pruning for Efficient Long Context LLM Inference](2407.14057.md)
- [Position: Foundation Agents as the Paradigm Shift for Decision Making](2405.17009.md)
