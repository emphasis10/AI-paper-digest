# Critique-out-Loud Reward Models
## TL;DR
## Summary
- [https://arxiv.org/pdf/2408.11791.pdf](https://arxiv.org/pdf/2408.11791.pdf)

### 요약 및 분석:

#### 1. 논문 각 섹션 요약:

**1. 서론 (Introduction):**
이 논문은 보상 모델에서 언어 모델(LM)을 활용하는 방법을 탐구합니다. 기존 보상 모델은 보상 점수를 예측하는 데 있어 LLM(Large Language Model)의 생성 능력을 활용하지 않았기 때문에 제한적이었습니다. 이를 해결하기 위해, Critique-out-Loud (CLoud) 보상 모델을 제안하며, 이 모델은 보상을 예측하기 전에 보조 응답의 자연어 비평을 생성합니다. 이를 통해 사용자가 요구하는 품질에 대한 추론을 명시적으로 할 수 있습니다.

**2. 방법 (Methods):**
CLoud 보상 모델의 훈련 방법론을 설명합니다. 기존의 보상 모델의 훈련 방법을 확장하여, LLM의 언어 생성 능력을 포함한 새로운 보상 모델을 훈련합니다. 훈련 데이터셋은 사용자 프롬프트와, 선택된 응답과 거부된 응답, 그리고 응답 품질에 대한 비평으로 구성됩니다. CLoud는 이러한 비평 정보를 바탕으로 보상을 예측합니다.

**3. 실험 및 결과 (Results):**
CLoud 보상 모델이 기존의 보상 모델보다 뛰어난 성능을 보임을 보여줍니다. 예를 들어, RewardBench에서 8B와 70B 기본 모델 모두에서 4.65 및 5.84 퍼센트 포인트의 정확도 향상을 보였습니다. 또한, ArenaHard에서 Best-of-N 방법론으로 사용될 때, CLoud 모델이 더 나은 성과를 보이는 것으로 나타났습니다.

**4. 논의 (Discussion):**
모델의 성능을 분석하며, self-consistency 및 on-policy 훈련의 중요성에 대해 논의합니다. self-consistency는 특히 짧은 추론 단계의 문제에서 유리하며, on-policy 훈련은 모델의 성능을 크게 향상시키는 것으로 나타났습니다.

**5. 결론 (Conclusion):**
이 논문은 CLoud 보상 모델이 기존의 보상 모델을 능가하며, 강화 학습에서의 인간 피드백 (RLHF)의 새로운 가능성을 보여줍니다. 언어 생성 능력을 포함한 새로운 보상 모델의 방향을 제시하며, 이는 향후 연구에 대한 기초를 제공합니다.

### 전체 요약:
이 논문은 기존 보상 모델이 언어 모델의 생성 능력을 활용하지 못했다는 한계를 극복하고자, Critique-out-Loud (CLoud) 보상 모델을 제안합니다. 이 모델은 응답의 품질을 예측하기 전에 비평을 생성하여, 보다 정확하고 명시적인 추론을 가능하게 합니다. 실험 결과, CLoud 모델은 기존의 보상 모델보다 더 높은 정확도와 성능을 보였으며, 특히 짧은 추론 단계에서 유리한 self-consistency 방법론을 제시하였습니다. 또한, on-policy 훈련이 모델의 성능을 크게 향상시키는 것으로 나타났습니다. 이 연구는 향후 AI 및 강화 학습 분야에서 인간 피드백을 효과적으로 활용할 수 있는 새로운 방향을 제시합니다.

이를 통해, CLoud 모델은 기존의 방법론에 비해 높은 성능을 보이면서도, 언어 생성 능력을 활용한 명시적인 추론 과정을 통해 향후 연구와 적용 가능성을 높이고 있습니다.