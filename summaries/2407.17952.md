# BetterDepth: Plug-and-Play Diffusion Refiner for Zero-Shot Monocular Depth Estimation
## TL;DR
## Summary
- [https://arxiv.org/pdf/2407.17952.pdf](https://arxiv.org/pdf/2407.17952.pdf)

### 논문의 각 섹션 요약 및 주요 내용

#### 1. 서론
**요약:** 
단일 이미지에서 깊이 정보를 추출하는 기술인 단일카메라 깊이 추정(MDE, Monocular Depth Estimation)은 자율 주행, 증강 현실 등 다양한 응용 분야에 중요합니다. MDE는 전통적인 스테레오 비전이나 구조화된 조명 시스템보다 더 많은 도전 과제를 포함합니다. 최근 확산 모델(Diffusion Model)의 발전은 MDE에서 우수한 성능을 보여줬지만, 여전히 데이터의 다양성 부족과 라벨의 완전성을 극복하는 데 어려움이 있습니다.

**주요 기여 및 혁신:** 
- BetterDepth라는 새로운 모델을 제안하여 기하학적으로 정확한 MDE 성능을 달성하고, 세부 정보를 잘 포착하는 능력을 갖추었습니다.
- 제안된 모델은 사전 학습된 MDE 모델의 예측을 깊이 조건으로 삼아 디퓨젼 방식을 활용해 세부 정보를 반복적으로 보정합니다.

#### 2. 관련 연구
**요약:** 
제안된 연구는 두 가지 주요 접근방식, 즉 제로샷 깊이 추정(Zero-Shot MDE) 및 확산 기반 MDE(Diffusion-Based MDE)에 대한 기존 연구를 탐구합니다. 주요 제로샷 MDE 연구들은 다양한 데이터셋을 혼합하거나 대규모 비지도 학습 데이터를 활용합니다. 반면, 확산 기반 MDE는 세부 정보 추출 능력이 뛰어나지만, 기하학적으로 복잡한 장면에서는 성능이 제한됩니다.

#### 3. BetterDepth 프레임워크
**요약:** 
BetterDepth는 사전 학습된 피드포워드 MDE 모델과 조건부 라텐트 확산 모델을 결합한 것입니다. 이 모델은 사전 학습된 MDE 모델로부터 얻은 기하학적 사전 지식을 활용하고, 디퓨젼 모델을 학습해 세부 정보를 보정합니다. 새로운 학습 전략으로는 글로벌 정렬 및 국부 패치 마스킹을 포함하여 세부 정보를 학습하면서도 사전 학습된 모델에 충실하게 학습하도록 합니다.

#### 4. 실험 결과
**요약:** 
기제안된 BetterDepth 모델은 다양한 공용 데이터셋과 자유형 장면에서 기존의 피드포워드 및 확산 기반 MDE 방법보다 우수한 성능을 보였습니다. 특히 훈련 샘플 수가 적은 경우에도 BetterDepth는 뛰어난 성능을 유지합니다. 예제 비교에서는 다른 모델들보다 공간 레이아웃을 정확히 복원하고 세부 정보를 잘 포착하는 모습을 보였습니다.

#### 5. 결론
**요약:**
BetterDepth는 사전 학습된 MDE 모델의 영구력을 활용하여 제로샷 MDE 성능을 보장하고, 디퓨젼 모델로 세부 정보를 보정하는 혁신적인 방식으로 단일카메라 깊이 추정 문제를 개선하였습니다. 이는 훈련 샘플 수가 적어도 강력한 성능을 끌어내며, 다른 MDE 모델을 재훈련 없이도 성능을 향상시킬 수 있는 유연성을 제공합니다.

### 전체 요약
**BetterDepth**는 기존의 제로샷 MDE 방법의 일반화 성능과 확산 모델의 세부 정보 추출 능력을 결합한 새로운 접근 방식입니다. 이 모델은 조건부 라텐트 확산 모델로 설계되어, 사전 학습된 MDE 모델에서 얻은 기하학적 지식에 의존하고, 세부 정보를 반복 보정하여 학습합니다. 이 모델은 처리 속도가 빠르고, 소규모 데이터셋에서도 효과적으로 훈련할 수 있어 다양한 공용 데이터셋 및 자유형 장면에서 뛰어난 성능을 보입니다. 따라서 BetterDepth는 단일카메라 깊이 추정 문제에서 새로운 기준을 제시하며, 다른 모델을 재훈련 없이도 성능을 향상시킬 수 있습니다.

## Similar Papers
- [Depth Anywhere: Enhancing 360 Monocular Depth Estimation via Perspective Distillation and Unlabeled Data Augmentation](2406.12849.md)
- [SparseCraft: Few-Shot Neural Reconstruction through Stereopsis Guided Geometric Linearization](2407.14257.md)
- [MonoPatchNeRF: Improving Neural Radiance Fields with Patch-based Monocular Guidance](2404.08252.md)
- [Learning Temporally Consistent Video Depth from Video Diffusion Priors](2406.01493.md)
- [Evaluating Alternatives to SFM Point Cloud Initialization for Gaussian Splatting](2404.12547.md)
- [No Training, No Problem: Rethinking Classifier-Free Guidance for Diffusion Models](2407.02687.md)
- [GFlow: Recovering 4D World from Monocular Video](2405.18426.md)
- [Real3D: Scaling Up Large Reconstruction Models with Real-World Images](2406.08479.md)
- [Fine-gained Zero-shot Video Sampling](2407.21475.md)
