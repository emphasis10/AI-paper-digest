# TPI-LLM: Serving 70B-scale LLMs Efficiently on Low-resource Edge Devices
## TL;DR
## Summary
- [https://arxiv.org/pdf/2410.00531.pdf](https://arxiv.org/pdf/2410.00531.pdf)

## 1. 각 섹션의 주요 내용 요약

### Introduction
대규모 언어 모델 (LLM)은 점점 더 클라우드에서 엣지 디바이스(예: 노트북, 스마트폰)로 이동하고 있습니다. 이는 사용자 프라이버시와 관련된 문제를 해결하는 동시에 엣지 디바이스의 제한된 메모리와 컴퓨팅 능력을 고려합니다. TPI-LLM은 엣지 디바이스에서 효율적으로 대규모 모델을 실행할 수 있는 새로운 텐서 병렬 추론 시스템입니다.

### Observations and Motivations
엣지 디바이스에서 텐서 병렬 처리와 모델 병렬 처리를 비교하고, 텐서 병렬 처리가 더욱 효율적임을 발견했습니다. 특히, 링크 지연(latency)이 주요 병목 현상이며, 이를 해결하기 위해 스타 기반의 allreduce 알고리즘을 구현했습니다.

### TPI-LLM Framework with Sliding Window Memory Scheduling
TPI-LLM 시스템은 사용자 디바이스에서 프롬프트를 유지하여 프라이버시를 보호하고, 슬라이딩 윈도우 메모리 스케줄링 기법을 통해 효율적으로 메모리를 관리합니다. 이를 통해 메모리 부족 문제를 해결하고 더 큰 모델을 원활하게 실행할 수 있습니다.

### Experiments
실험 결과, TPI-LLM은 Transformer, Accelerate, Galaxy 등과 비교해 현저히 낮은 메모리 사용량과 지연 시간을 보였습니다. 특히, Llama 2-70B 모델에서 90% 이상 메모리 사용량을 줄이며, 3.1GB의 메모리로 실행 가능합니다.

### Conclusion
TPI-LLM은 엣지 디바이스에서 대규모 언어 모델을 효율적으로 실행할 수 있는 텐서 병렬 추론 시스템으로, 프라이버시를 유지하면서 시간과 메모리를 크게 절감합니다. 실험을 통해 성능과 효율성을 입증하였습니다.

## 2. 종합 요약

TPI-LLM 연구는 낮은 자원 환경에서도 대규모 언어 모델을 효과적으로 실행할 수 있는 새로운 텐서 병렬 추론 시스템(TPI-LLM)을 제안합니다. 본 논문은 LLM을 엣지 디바이스에서 실행할 때의 주요 문제인 메모리 제한과 컴퓨팅 능력 문제를 해결하기 위한 방법으로, 스타 기반 allreduce 알고리즘과 슬라이딩 윈도우 메모리 스케줄링 기법을 소개합니다.

TPI-LLM은:
1. 사용자 데이터를 로컬에서 유지하여 개인정보를 보호하고
2. 링크 지연 문제를 해결하기 위해 스타 기반 allreduce 알고리즘을 도입하며
3. 슬라이딩 윈도우 메모리 스케줄링을 통해 메모리 부족 문제를 해결합니다.

실험 결과, TPI-LLM은 기존의 추론 시스템에 비해 현저히 낮은 지연 시간과 메모리 소모를 보여줍니다. 예를 들어, Llama 2-70B 모델에서 기존 시스템 대비 최대 90% 이상의 메모리를 절약하며, 실행 시간을 크게 단축시켰습니다.

이 연구는 엣지 디바이스에서의 LLM 실행을 가능하게 하여, 보다 프라이버시 친화적이고 효율적인 AI 기술 발전에 기여할 것입니다.