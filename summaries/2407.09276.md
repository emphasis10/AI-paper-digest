# H2O-Danube3 Technical Report
## TL;DR
## Summary
- [https://arxiv.org/pdf/2407.09276.pdf](https://arxiv.org/pdf/2407.09276.pdf)

### 1. 각 섹션 요약

#### Abstract (초록)
이 논문에서는 H2O-Danube3라는 소형 언어 모델 시리즈를 소개합니다. H2O-Danube3-4B와 H2O-Danube3-500M 모델이 있으며, 두 모델은 각각 6조 및 4조 토큰으로 사전 학습되었습니다. 이 모델들은 주로 고품질 웹 데이터를 사용하여 세 단계로 나누어 학습되었습니다. H2O-Danube3는 다양한 학술, 채팅, 세부 조정 벤치마크에서 높은 성능을 보이며, 현대 스마트폰에서도 효율적으로 실행될 수 있습니다. 모든 모델은 Apache 2.0 라이선스로 공개되어 더 넓은 청중에게 언어 모델을 민주화하는 데 기여합니다.

#### Introduction (소개)
소형 언어 모델은 특히 소비자 하드웨어 및 에지 장치에서 효율적인 추론을 목표로 하는 오늘날의 오픈 소스 언어 모델 환경에서 중요한 위치를 차지하고 있습니다. H2O-Danube3는 이전 연구를 확장하여 학습된 모델로, 6조 및 4조 토큰으로 학습된 H2O-Danube3-4B 및 H2O-Danube3-500M으로 구성되어 있습니다. 이 보고서에서는 모델의 아키텍처, 학습 절차 및 세부 조정 과정을 설명하고, 다양한 벤치마크를 통해 성능을 평가합니다. H2O-Danube3 모델은 특히 챗봇 애플리케이션, RAG 애플리케이션, 특정 세부 조정, 연구 또는 모바일 장치에서의 오프라인 애플리케이션과 같은 다양한 용도에서 중요한 역할을 할 수 있습니다.

#### Model Architecture (모델 아키텍처)
H2O-Danube3는 Llama 모델 아키텍처를 사용하여 설계된 디코더 전용 언어 모델입니다. 주요 설계 원칙은 Llama 2와 Mistral에서 차용되었으며, 각 레이어의 모양과 총 매개변수 수를 결정하는 맞춤 매개변수가 포함되어 있습니다. 모델은 Mistral 토크나이저를 사용하며, 최대 8,192 길이의 컨텍스트를 학습할 수 있습니다. H2O-Danube3-4B는 39.6억 개의 학습 가능한 매개변수를 가지고 있으며, H2O-Danube3-500M은 에지 장치 또는 특정 세부 조정 작업에 적합한 5억 개의 매개변수를 가지고 있습니다.

#### Training (학습)
모델은 주로 영어 텍스트를 세 단계로 나누어 학습되었습니다. 각 단계에서는 노이즈가 많은 웹 데이터의 비율을 줄이고, 더 높은 품질의 데이터 비율을 증가시켰습니다. 첫 번째 데이터 단계는 90.6%의 웹 데이터로 구성되며, 두 번째 단계에서는 81.7%, 세 번째 단계에서는 51.6%로 감소합니다. 동시에 지시 데이터, 위키피디아, 학술 텍스트, 합성 텍스트 및 기타 고품질 텍스트 데이터의 비율이 증가합니다. 각 단계별 세부 데이터 비율은 도표로 표시됩니다. 또한 모델의 채팅 세부 조정 버전인 H2O-Danube3-4B-Chat 및 H2O-Danube3-500M-Chat도 제공됩니다.

#### Evaluation (평가)
H2O-Danube3는 학술 벤치마크, 채팅 벤치마크 및 세부 조정 벤치마크의 다양한 측면에서 평가되었습니다. 학술 벤치마크에서는 H2O-Danube3-4B가 강력한 성능을 보이며, 여러 벤치마크에서 경쟁 모델을 능가합니다. 채팅 벤치마크에서는 채팅 성능을 맹검 평가하여 ELO 점수를 계산하고, RAG 벤치마크에서는 질문-응답 작업 성능을 평가합니다. 세부 조정 벤치마크에서는 특정 과제에 모델을 쉽게 적응시킬 수 있는 능력을 평가합니다.

#### Model Quantization (모델 양자화)
에지 장치에서 모델을 보다 효율적으로 사용하기 위해 H2O-Danube3의 양자화 버전이 도입되었습니다. 양자화된 모델은 성능의 최소 손실로 크기를 줄일 수 있습니다. 양자화 방법과 성능의 차이를 도표로 제공합니다.

#### Conclusions (결론)
H2O-Danube3는 다양한 벤치마크에서 경쟁력 있는 성능을 보이는 소형 언어 모델 시리즈입니다. 이 모델들은 챗봇, 특정 과제의 세부 조정, 연구 및 모바일 장치에서의 오프라인 애플리케이션과 같은 다양한 용도에서 중요한 역할을 할 수 있습니다.

### 2. 전체 요약

본 논문에서는 H2O-Danube3라는 신개념 소형 언어 모델 시리즈를 소개합니다. H2O-Danube3는 H2O-Danube3-4B와 H2O-Danube3-500M 두 가지 모델로 구성되어 있으며, 각각 6조 및 4조 토큰으로 학습되었습니다. 고품질의 웹 데이터를 사용하여 세 단계로 학습된 이 모델은 챗봇 응용 프로그램, 특정 용도의 세부 조정, 연구 및 모바일 장치에서의 오프라인 응용 프로그램 등 다양한 사용 사례에서 뛰어난 성능을 발휘합니다. 기술적으로는 Llama 및 Mistral의 설계 원칙을 따르며, Mistral 토크나이저를 사용합니다. 또한 양자화된 버전을 통해 에지 장치에서도 효율적으로 작동할 수 있습니다. H2O-Danube3는 학술, 채팅 및 세부 조정 벤치마크에서 높은 성능을 보이며 오픈 소스로 제공되어 널리 활용될 수 있습니다.