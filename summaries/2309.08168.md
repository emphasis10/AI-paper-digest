# Draft & Verify: Lossless Large Language Model Acceleration via Self-Speculative Decoding
## TL;DR
## Summary
- [https://arxiv.org/pdf/2309.08168.pdf](https://arxiv.org/pdf/2309.08168.pdf)

### 요약

#### 1. 각 섹션의 중요한 내용 요약

##### 1.1 서론 (Introduction)
이 논문은 보조 모델을 사용하지 않고도 대형 언어 모델 (LLMs)의 추론 속도를 가속화할 수 있는 새로운 추론 방법인 '자체 추론 디코딩'을 제안합니다. 이 방법은 초안을 생성하는 단계와 이를 검증하는 단계로 이루어져 있습니다. 모델 초안은 낮은 품질이지만 빠르게 생성되고, 검증 단계에서 원본 LLM을 사용하여 초안을 검증하여 최종 출력이 원본 LLM과 동일하도록 합니다. 이 방법은 추가적인 뉴럴 네트워크 훈련이나 메모리 증설 없이 사용 가능하며, 비용효율적인 솔루션입니다.

##### 1.2 관련 연구 (Related Work)
LLMs의 추론 가속화와 관련된 기존 연구를 검토하고, 이러한 방법들의 한계점을 논의합니다. 특히, 많은 기존 연구는 모델 압축 (Quantization, Pruning) 또는 재훈련이 필요하며, 이는 추가적인 메모리와 계산 비용을 초래할 수 있습니다.

##### 1.3 방법론 (Methodology)
자체 추론 디코딩 방법은 두 가지 단계- 초안 생성 및 검증-로 나뉩니다. 초안 생성 단계에서는 선택된 중간 레이어를 건너뛰어 빠르게 초안을 생성합니다. 그런 다음 검증 단계에서 원본 LLM을 사용하여 초안을 검증합니다. 이 방법은 초안 생성 단계의 효율성을 높이기 위해 적응형 초안 종료 메커니즘(Adaptive Draft-Exiting Mechanism)을 도입하여 컴퓨팅 리소스를 효율적으로 사용하고, 레이어 건너뛰기 전략을 베이지안 최적화를 통해 설정합니다.

##### 1.4 평가 (Evaluation)
다양한 LLM 모델과 데이터셋을 사용하여 제안된 방법의 성능을 평가합니다. LLaMA-2와 그 변형 모델을 이용한 벤치마크 결과, 최고 1.99배의 속도 향상을 보여주었습니다.

##### 1.5 한계 (Limitations)
이 방법의 한계로는 베이지안 최적화가 오프라인에서 여러 시간 소요될 수 있다는 점과, 지나치게 많은 레이어를 건너뛰면 수용률이 급격히 낮아져 속도가 오히려 감소될 수 있다는 점이 있습니다. 이를 보완하기 위해 추가적인 모델 압축 전략이나 적응형 디코딩 방법을 제안합니다.

#### 2. 전체 요약
이 논문에서는 보조 모델 없이 대형 언어 모델의 추론 속도를 가속화하는 '자체 추론 디코딩' 방법을 제안합니다. 이 방법은 두 단계로 이루어집니다: 초안 생성 단계를 통해 빠르게 낮은 품질의 초안을 생성하고, 검증 단계에서 원본 모델을 이용하여 최종 출력을 검증하는 방식입니다. 이를 통해 기존 대비 최대 1.99배의 속도 향상 효과를 달성했습니다. 또한, 이 방법은 추가적인 뉴럴 네트워크 훈련이나 메모리 증설 없이 사용 가능하여 비용효율적인 솔루션을 제공합니다. 이 논문은 다양한 모델과 데이터셋을 사용하는 실험을 통해 제안된 방법의 효율성과 실현 가능성을 입증하고, 추가적으로 사용할 수 있는 여러 최적화 전략을 소개합니다.

## Similar Papers
- [Benchmarking Mental State Representations in Language Models](2406.17513.md)
- [CLLMs: Consistency Large Language Models](2403.00835.md)
- [Medusa: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads](2401.10774.md)
- [Language Models are Super Mario: Absorbing Abilities from Homologous Models as a Free Lunch](2311.03099.md)
- [Cost-Effective Hallucination Detection for LLMs](2407.21424.md)
- [Unlocking Efficiency in Large Language Model Inference: A Comprehensive Survey of Speculative Decoding](2401.07851.md)
- [Fast Feedforward Networks](2308.14711.md)
- [Beyond the Speculative Game: A Survey of Speculative Execution in Large Language Models](2404.14897.md)
- [Accelerating Large Language Model Decoding with Speculative Sampling](2302.01318.md)
