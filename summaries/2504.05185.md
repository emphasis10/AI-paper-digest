# Concise Reasoning via Reinforcement Learning
## TL;DR
## Summary
- [https://arxiv.org/pdf/2504.05185.pdf](https://arxiv.org/pdf/2504.05185.pdf)

1. 논문의 각 섹션 요약

   - **소개 (Introduction)**: 대형 언어 모델(LLM)이 발달했지만, 추론 모델은 많은 토큰을 소모하여 높은 계산 비용과 자원 요구를 초래했다는 내용이 강조됩니다. 저자들은 강화 학습(RL) 중 긴 응답이 정확성을 필연적으로 향상시킨다는 가정을 의심하며, 간결성과 정확성의 자연스러운 상관관계를 밝혀내고자 합니다.

   - **배경과 문제점 (Background and Issues)**: 여러 연구에서 긴 응답은 정확성 증가와 관련이 있지만, 지나친 길이는 오히려 성능 감소를 일으킨다는 점을 분석합니다. 긴 체인 오브 쏘트(COT)를 감소시켜 효율성을 높일 수 있는 이론적 분석과 강화 학습을 통한 두 단계 학습 방법을 제안합니다.

   - **강화 학습의 두 단계 (Two-Phase Reinforcement Learning Strategy)**: 첫 번째 단계에서는 도전적인 문제를 통한 학습을 통해 모델의 추론 능력을 개선합니다. 두 번째 단계에서는 해결할 수 있는 문제들을 통해 응답의 간결성을 강화하면서 정확성을 유지하거나 향상시킵니다.

   - **실험 결과 (Experimental Results)**: 두 단계의 강화 학습 접근법이 모델의 반응 길이를 줄이면서도 정확성을 유지하거나 개선함을 보였습니다. 또한 온도 설정에 대해 모델의 성능 강건성을 평가하여 추가적인 RL 훈련이 강건성을 향상시킨다는 결과를 제시합니다.

   - **결론 (Conclusion)**: 새로운 강화 학습 접근법이 응답의 간결성을 유지하며 정확성을 향상시킬 수 있음을 보고하며, 이는 자원 효율성과 계산 효율성을 동시에 제공함을 강조합니다.

2. 전반적인 요약

   이 논문은 LLM의 강화 학습을 활용하여 응답의 길이를 줄이는 동시에 정확성을 유지하거나 향상시키는 새로운 접근 방식을 제안합니다. 긴 체인의 쓸모없는 길이가 아닌 간결한 응답이 오히려 정확도와 효과성을 높일 수 있음을 다양한 실험을 통해 입증하였습니다. 논문의 핵심 기여는 대규모 데이터셋 없이도 두 단계의 RL 훈련을 통해 간결성을 효과적으로 강화할 수 있음을 보여준 것입니다.