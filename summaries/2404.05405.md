# Physics of Language Models: Part 3.3, Knowledge Capacity Scaling Laws
## TL;DR
## Summary
- [https://arxiv.org/pdf/2404.05405.pdf](https://arxiv.org/pdf/2404.05405.pdf)

**1. 서론**

이 논문은 언어 모델의 크기가 지식 저장 능력에 어떻게 영향을 미치는지에 대한 정량적인 분석을 제시합니다. 특히 언어 모델이 저장할 수 있는 지식의 양이 모델의 크기와 어떻게 상관 관계가 있는지에 대한 스케일링 법칙을 소개합니다. 또한 다양한 요소(아키텍처, 양자화, 훈련 기간 등)가 지식 저장 능력에 어떤 영향을 미치는지 조사합니다.

**2. 지식 정의 및 데이터셋**

지식은 (이름, 속성, 값)의 튜플로 정의되며, 예를 들어 ('Anya Forger', 'birthday', '10/2/1996')와 같이 구체적인 정보를 포함합니다. 다양한 크기의 모델을 통해 이러한 지식 피스를 학습하고, 이를 통해 얻어진 지식을 평가하는 과정을 설명합니다.

**3. 스케일링 법칙**

언어 모델이 저장할 수 있는 지식의 양은 모델의 크기와 선형적으로 비례하지 않음을 발견했습니다. 예를 들어, 7B 모델은 이론적으로 14B 비트의 지식을 저장할 수 있으며, 이는 영어 위키백과와 교과서를 합친 것보다 많은 양입니다. 또한, 모델의 아키텍처와 훈련 데이터의 양자화 방법이 지식 저장 능력에 미치는 영향을 분석합니다.

**4. 훈련 시간과 아키텍처의 영향**

훈련 시간이 충분하지 않을 경우, 지식 피스를 충분히 반복하여 접근하지 못하면 모델의 지식 저장 능력이 저하될 수 있습니다. 또한 다양한 모델 아키텍처를 비교 분석하여 GPT-2 아키텍처가 지식 저장에 있어 다른 아키텍처와 비슷하거나 더 우수할 수 있음을 보여줍니다.

**5. 결론**

언어 모델의 지식 저장 능력을 정량화하는 새로운 접근 방식을 통해, 모델 선택, 훈련 데이터 준비 및 이론적 연구에 대한 실질적인 통찰을 제공합니다. 이러한 스케일링 법칙은 향후 언어 모델의 설계와 최적화에 중요한 기준을 제시합니다.

이 내용을 바탕으로 전체적인 요약을 제공하겠습니다.

본 논문은 언어 모델의 크기가 그 모델이 저장할 수 있는 지식의 양과 어떤 관계가 있는지에 대한 스케일링 법칙을 제시합니다. 연구는 다양한 언어 모델 아키텍처와 훈련 변수가 지식 저장 능력에 미치는 영향을 분석하며, 모델 크기와 지식 저장 용량 간의 비선형 관계를 밝혀냈습니다. 예를 들어, 7B 파라미터 모델은 이론적으로 14B 비트의 지식을 저장할 수 있는 능력을 가지고 있으며, 이는 영어 위키백과와 교과서의 지식을 합친 것보다 많은 양입니다.

더욱이, 이 논문은 훈련 시간과 모델의 아키텍처가 지식 저장 능력에 중요한 영향을 미친다는 것을 보여줍니다. 충분한 훈련 없이는 모델이 학습한 지식의 양이 현저히 감소할 수 있으며, GPT-2 아키텍처는 다른 아키텍처와 비교하여 우수한 지식 저장 능력을 보였습니다. 이러한 발견은 언어 모델의 선택과 훈련 방법에 있어 중요한 지침을 제공하며, 이론적 연구와 실용적 적용에 있어 유용한 통찰을 제공합니다.