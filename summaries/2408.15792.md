# Efficient LLM Scheduling by Learning to Rank
## TL;DR
## Summary
- [https://arxiv.org/pdf/2408.15792.pdf](https://arxiv.org/pdf/2408.15792.pdf)

### 요약

**논문의 주제**: 이 논문은 대형 언어 모델(LLM)의 효율적인 스케줄링을 다루며, 특히 요청의 출력 길이를 예측함으로써 처리 시간을 최적화하는 방법을 제안합니다.

#### Introduction (소개)

대형 언어 모델(LLM)은 많은 인터넷 서비스 및 애플리케이션의 핵심이 되었습니다. 그러나 수많은 사용자 요청을 처리해야 하는 상황에서 FCFS(First-come-first-serve) 스케줄링 방식은 Head-Of-Line (HOL) 차단을 초래해 서비스 품질을 저하시킬 수 있습니다. 이 연구는 요청의 정확한 생성 길이를 예측하는 것은 불가능하지만, 상대적인 순위를 예측하여 요청을 효과적으로 스케줄링할 수 있음을 제시합니다.

#### Related Work (관련 연구)

이전 연구에서는 주로 FCFS 스케줄링을 사용하며, 이는 HOL 차단 문제를 해결하지 못했습니다. 일부 연구는 생성 길이를 예측하려고 시도했지만 정확도가 떨어졌습니다. 이 논문은 상대적인 순위를 사용하여 스케줄링을 최적화하는 새로운 접근법을 제안합니다.

#### Problem Formulation (문제 정의)

생성 길이의 정확한 예측보다 상대적인 순위 예측이 더 중요합니다. Kendall의 타우(Kendall’s Tau)를 사용하여 예측된 스케줄과 이상적인 SJF/SRTF 스케줄 사이의 유사성을 측정합니다. 높은 유사성은 실질적으로 더 낮은 대기 시간으로 이어집니다.

#### Method (방법)

**Generation Length Ranking Predictor (생성 길이 순위 예측기)**:
- 작은 보조 모델(예: OPT-125M)을 사용해 요청의 생성 길이 순위를 예측합니다. 이 모델은 거의 비용이 들지 않습니다.

**Request Scheduling with Rankings (순위 기반 요청 스케줄링)**:
- 학습된 순위를 기반으로 요청을 스케줄링합니다. 이 방식은 온라인, 오프라인 모두 적용 가능하며, 실시간으로 요청을 처리합니다.

#### Evaluation (평가)

다양한 작업에서 제안된 방법이 기존 방법보다 성능이 뛰어남을 확인했습니다.
- 챗봇 서비스에서는 대기 시간이 2.8배 감소했습니다.
- 합성 데이터 생성에서는 처리량이 6.5배 증가했습니다.

#### Limitations (한계)

Kendall의 타우를 사용한 순위 측정에는 한계가 있습니다. 일부 경우에는 늦은 요청이 과도하게 대기하는 문제도 발생할 수 있습니다. 이러한 한계를 극복하기 위해 추가적인 연구가 필요합니다.

#### Conclusion (결론)

간단하고 저비용인 이 방법은 실제 LLM 서비스 시스템에 쉽게 통합될 수 있으며, 서비스 품질을 개선하고 비용을 절감할 수 있습니다. 챗봇 서비스에서 대기 시간은 2.8배 감소하고, 합성 데이터 생성에서는 처리량이 6.5배 증가했습니다.

---

### 전체 요약

이 논문은 대형 언어 모델(LLM)의 효율적인 요청 스케줄링 방법을 제안합니다. 요청의 정확한 길이를 예측하는 것은 어려울 수 있지만, 상대적인 순위를 예측하는 방식으로 대기 시간을 줄이고 처리량을 증가시킬 수 있음을 보였습니다. 제안된 방법은 작은 보조 모델을 통해 요청의 순위를 예측하고, 이를 기반으로 실시간으로 요청을 스케줄링합니다. 이 방법은 챗봇 서비스와 합성 데이터 생성 등 다양한 애플리케이션에서 기존 방법보다 성능이 뛰어나며, 실제 시스템에 쉽게 통합될 수 있는 간단하고 저비용의 솔루션을 제공합니다.