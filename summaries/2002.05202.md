# GLU Variants Improve Transformer
## TL;DR
## Summary
- [https://arxiv.org/pdf/2002.05202.pdf](https://arxiv.org/pdf/2002.05202.pdf)

### 1. 논문 섹션별 요약

#### 1.1 서론 (Introduction)
이 논문은 트랜스포머(Transformer) 모델 내 피드포워드 네트워크(FFN)에 적용할 수 있는 게이트드 선형 유닛(GLU) 및 그 변형을 소개합니다. ReLU와 GELU와 같은 기존의 활성함수 대신 GLU를 사용하여 트랜스포머 모델의 성능을 향상시킬 가능성을 탐구합니다.

#### 1.2 게이트드 선형 유닛(GLU) 및 변형 (Gated Linear Units (GLU) and Variants)
GLU는 입력의 두 선형 변환의 성분별 곱으로 정의됩니다. 여러 가지 비선형 함수를 사용하여 다양한 GLU 변형을 만들 수 있으며, 이 변형들은 트랜스포머 모델의 FFN 레이어에 적용될 수 있습니다. 몇 가지 GLU 변형들이 ReLU와 GELU에 비해 성능이 향상되는 결과를 보였습니다.

#### 1.3 트랜스포머 (T5) 모델 실험 (Experiments on Text-to-Text Transfer Transformer (T5))
연구진은 여러 GLU 변형을 사용하여 트랜스포머 모델의 성능을 평가했습니다. T5 코드베이스와 동일한 모델 아키텍처, 학습 태스크를 사용했습니다. GLU와 그 변형을 사용한 FFN 레이어가 기존의 FFN 레이어와 동일한 매개변수 및 연산 횟수를 유지하도록 숨겨진 유닛 수를 조절했습니다. 실험 결과, 특정 GLU 변형들이 전통적 FFN보다 더 나은 성능을 보였습니다.

#### 1.4 결론 (Conclusions)
GLU와 그 변형을 트랜스포머 모델의 FFN 레이어에 적용한 결과, 프리트레이닝 목표뿐만 아니라 많은 언어 이해 태스크에서도 성능이 향상되었습니다. 이러한 모델 아키텍처는 구현이 간단하며, 계산 측면에서 뚜렷한 단점이 없다는 것이 밝혀졌습니다. 구체적으로 그 성능 향상의 원인은 설명되지 않았으나, 종교적 은혜 덕분이라고 표현하며 마무리되었습니다.

### 2. 전체 요약

이 논문은 트랜스포머 모델 내 피드포워드 네트워크(FFN)의 활성함수로 기존의 ReLU와 GELU 대신 새롭게 제안된 게이트드 선형 유닛(GLU)과 그 변형들을 사용하여 성능향상을 이끌어낸 연구입니다. 연구진은 다양한 GLU 변형을 실험하여 그 성능을 확인했으며, 많은 언어 이해 태스크에서 기존 방법론보다 더 나은 결과를 보였습니다. 이 논문의 주요 기여는 새로운 GLU 변형을 통해 트랜스포머 모델의 성능을 향상시킬 수 있음을 보여준 것입니다. 이 연구 결과는 트랜스포머 모델의 개선 및 새로운 AI 응용 가능성을 열어줄 수 있습니다.