# Evolutionary Optimization of Model Merging Recipes
## TL;DR
## Summary
- [https://arxiv.org/pdf/2403.13187](https://arxiv.org/pdf/2403.13187)

이 논문에서는 기존의 진화 알고리즘을 활용하여 강력한 기초 모델을 자동으로 생성하는 새로운 접근 방식을 제시합니다. 모델 병합이 뛰어난 비용 효율성을 가지고 있지만 현재는 인간의 직관과 도메인 지식에 의존하고 있어 잠재력이 제한되어 있습니다. 저자들은 이러한 제약을 극복하기 위해 진화 알고리즘을 사용하여 다양한 오픈 소스 모델들의 효과적인 조합을 자동으로 발견하는 방법을 제안합니다.

### 주요 내용 요약

1. **서론 및 배경**:
   - 모델 병합은 추가 훈련 없이도 여러 LLM을 하나의 아키텍처로 결합함으로써 새로운 모델을 개발하는 매우 비용 효율적인 접근 방식을 제공합니다. 이 방식은 기존 모델들을 전략적으로 결합하여 새로운 능력을 탐색하고자 합니다.

2. **진화 알고리즘의 적용**:
   - 진화 알고리즘은 다양한 모델의 파라미터 공간(가중치)과 데이터 흐름 공간(추론 경로) 모두를 탐색할 수 있는 능력을 가지고 있습니다. 이를 통해 모델의 조합을 최적화하고, 더 효과적인 모델 병합 솔루션을 발견할 수 있습니다.

3. **일본어 LLM과 수학 능력의 결합**:
   - 연구자들은 일본어 LLM에 수학적 추론 능력을 추가하여, 기존 모델들보다 더 나은 성능을 보이는 새로운 모델을 개발하였습니다. 이 모델은 일본어 벤치마크에서 최고 성능을 달성하였고, 명시적으로 해당 작업에 대해 훈련되지 않았음에도 불구하고 우수한 결과를 보였습니다.

### 혁신적인 부분
이 연구의 가장 혁신적인 점은 기존의 진화 알고리즘을 통해 다양한 오픈 소스 모델들을 자동으로 효과적으로 결합할 수 있는 방법을 제시하였다는 점입니다. 이 방법은 모델 개발의 비용 효율성을 크게 향상시킬 뿐만 아니라, 다양한 모델들의 장점을 결합하여 새로운 도메인에 적용 가능한 강력한 모델을 생성할 수 있습니다.

## Similar Papers
- [An Image is Worth More Than 16x16 Patches: Exploring Transformers on Individual Pixels](2406.09415.md)
- [Unveiling Encoder-Free Vision-Language Models](2406.11832.md)
- [LLM-jp: A Cross-organizational Project for the Research and Development of Fully Open Japanese LLMs](2407.03963.md)
- [VoCo-LLaMA: Towards Vision Compression with Large Language Models](2406.12275.md)
- [LAMBDA: A Large Model Based Data Agent](2407.17535.md)
- [Commonsense-T2I Challenge: Can Text-to-Image Generation Models Understand Commonsense?](2406.07546.md)
- [Autoregressive Model Beats Diffusion: Llama for Scalable Image Generation](2406.06525.md)
- [Mamba: Linear-Time Sequence Modeling with Selective State Spaces](2312.00752.md)
- [Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache Consumption](2407.18003.md)
