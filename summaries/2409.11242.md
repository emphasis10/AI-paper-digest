# Measuring and Enhancing Trustworthiness of LLMs in RAG through Grounded Attributions and Learning to Refuse
## TL;DR
## Summary
- [https://arxiv.org/pdf/2409.11242.pdf](https://arxiv.org/pdf/2409.11242.pdf)

### 논문 요약 - 섹션별 요약 및 주요 기여와 혁신 부분
#### Introduction (소개)
본 논문은 거대 언어 모델(LLMs)이 발생시키는 "Hallucination" 문제를 다루고 있습니다. Hallucination은 모델이 그럴듯한 그러나 실제로는 틀린 정보를 생성하는 것을 의미합니다. RAG(Retrieval-Augmented Generation) 시스템을 통해 이 문제를 완화하고자 하며, 이를 위한 신뢰성 평가 지표인 `TRUST-SCORE`를 제안합니다.

#### Problem Description (문제 정의)
LLMs는 외부 정보 없이 내부적으로 저장된 지식을 활용하여 응답을 생성하는데, 이로 인해 비정확한 응답을 생성하기 쉽습니다. 본 논문은 이러한 문제를 해결하기 위해 모델의 응답이 문서에 기반하도록 하는 평가 방법을 제시합니다.

#### Task Setup (작업 설정)
RAG 시스템은 대규모 문서 집합과 관련 문서를 찾아주는 검색기, 그리고 응답을 작성하는 LLM으로 구성됩니다. 본 논문은 이러한 시스템을 평가하는 새로운 방법론을 제안합니다.

#### When is refusal expected? (거절이 기대될 때)
LLMs가 제공된 정보로 질문에 답할 수 없을 때 거절하는 능력을 평가하는 새로운 방법을 소개합니다. 이 부분은 거절해야 할 질문에 대해 모델이 적절히 대응할 수 있는지 평가합니다.

#### Hallucination in LLM in RAG (RAG에서의 LLM Hallucination)
마무리되지 않은 응답, 과도한 응답, 잘못된 인용 등 다양한 Hallucination 유형을 정의하고 설명합니다.

#### Metrics for LLM-in-RAG (RAG에서의 LLM 평가 지표)
`TRUST-SCORE`는 여러 차원에서 LLM의 신뢰성을 평가합니다. 주어진 문서에 기반하여 응답할 질문과 거절할 질문을 구분하는 능력, 정확한 응답 비율, 인용의 정확성 등이 포함됩니다.

#### The Trust-Align Framework (Trust-Align 프레임워크)
`Trust-Align` 프레임워크는 문서 기반 응답을 생성하기 위해 LLM을 조정합니다. 이 프레임워크는 다양한 질문과 문서 세트를 포함한 데이터셋을 사용하여 모델을 학습시킵니다.

#### Experimental Setup and Results (실험 설정 및 결과)
LLMs의 성능을 평가하기 위해 다양한 실험을 실시했습니다. 결과적으로 Direct Preference Optimization (DPO) 기법을 통해 모델의 Hallucination 빈도를 줄이는 데 성공하였습니다.

#### Analysis (분석)
RAG 시스템에서 발생하는 Hallucination의 근원과 DPO가 이러한 문제를 줄이는 방법을 철저히 분석합니다. 또한, 모델 성능을 비교할 수 있는 새로운 지표를 제시합니다.

#### Different Data Synthesis Techniques (다양한 데이터 합성 기법)
다양한 데이터 합성 기법의 효용성을 분석하며, 이를 통해 거절 샘플을 포함한 데이터셋의 중요성을 강조합니다.

#### Generalizability Analysis (일반화 가능성 분석)
모델의 일반화 능력을 평가하기 위해 다양한 데이터셋을 사용한 실험을 실시하고 분석합니다. 결과적으로, 제안된 모델이 높은 신뢰도를 보임을 확인했습니다.

#### Related Works (관련 연구)
기존 연구들에서 다루는 RAG와 LLM의 신뢰성을 비교하고, 이번 연구가 기존 연구 대비 어떠한 면에서 차별화되는지 설명합니다.

#### Conclusion (결론)
본 연구는 LLM의 RAG 환경에서 발생하는 Hallucination 문제를 해결하기 위한 다양한 접근법을 제안하고 평가했습니다. TRUST-SCORE와 Trust-Align 프레임워크를 통해 더 신뢰할 수 있는 응답을 생성하는 모델을 만드는 데 기여했습니다.

### 전체 요약
거대 언어 모델(LLMs)이 생성하는 정보의 신뢰성을 높이기 위해 본 논문에서는 Retrieval-Augmented Generation (RAG) 프레임워크를 활용한 방법론을 제안합니다. TRUST-SCORE와 Trust-Align 프레임워크를 통해 문서에 기반한 응답을 생성하고 다양한 실험을 통해 모델의 정확성과 신뢰성을 평가했습니다. 이를 통해 Hallucination 문제를 대폭 줄일 수 있음을 실험적으로 입증하였으며, 이는 AI 기술의 발전과 신뢰성 확보에 중요한 기여를 할 수 있습니다.