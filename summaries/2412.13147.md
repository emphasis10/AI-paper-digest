# Are Your LLMs Capable of Stable Reasoning?
## TL;DR
## Summary
- [https://arxiv.org/pdf/2412.13147.pdf](https://arxiv.org/pdf/2412.13147.pdf)

죄송합니다. 요청을 완료하려면 문서 전체를 읽어야 할 필요가 있습니다. 문서 내용을 검색한 후 각 섹션의 중요한 내용이 발견될 경우 요약하겠습니다. ### 1. 각 섹션의 중요한 내용 요약:

#### 초록
- 대형 언어 모델(LLM)의 복잡한 추론 능력을 평가하기 위한 기존 지표의 한계점을 지적하며, G-Pass@k라는 새로운 평가 지표를 제안합니다. G-Pass@k는 모델의 최적 성능 및 안정성을 다차원적으로 평가하는 메트릭이며, 이와 함께 최신 수학 문제들을 포함한 'LiveMathBench'라는 동적 벤치마크도 소개합니다.

#### 소개
- 복잡한 수학적 문제 해결은 LLM의 핵심 능력으로 여겨집니다. LLM의 성능 지표인 Greedy Accuracy와 Pass@K 등의 지표는 실제 응용에서 예상되는 일관성을 충분히 반영하지 못하고 있습니다.

#### 새로운 평가 메트릭: G-Pass@k
- G-Pass@k는 문제 해결 능력과 성능의 안정성을 동시에 평가하는 지표로 설계되었습니다. 이 지표는 LLM이 여러 샘플링에서 문제를 일관되게 해결할 수 있는지를 측정하는 데 그 초점을 둡니다.

#### 성능 분석 및 결과
- LiveMathBench와 공개 벤치마크에서 다양한 LLM들을 평가한 결과, 현재의 LLM들은 여전히 컴페티션 수준의 수학 문제 해결에 도전 과제를 안고 있으며, 모든 모델은 고난도의 문제에서는 여전히 많은 성능 하락을 보입니다.

#### 결론
- LLM의 실제적인 추론 능력을 강화하기 위해서는 더 강력한 평가법이 필요하며, G-Pass@k와 같은 지표가 이러한 필요성을 해결할 수 있는 가능성을 보여줍니다.

### 2. 전체 요약:
이 논문에서는 LLM의 복잡한 추론을 보다 효과적으로 평가하기 위해 G-Pass@k라는 새로운 평가 지표를 제안하고, 'LiveMathBench'와 같이 최신 수학 문제를 포함한 동적 벤치마크를 통해 모델들의 성능을 종합적으로 분석합니다. 분석 결과, 현재의 LLM은 여전히 높은 복잡성의 문제 해결에서는 많은 한계를 보이며, 이러한 점들을 개선하기 위해 새로운 평가 기준의 필요성을 강조하고 있습니다.