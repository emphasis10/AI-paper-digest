# Temporal Preference Optimization for Long-Form Video Understanding
## TL;DR
## Summary
- [https://arxiv.org/pdf/2501.13919.pdf](https://arxiv.org/pdf/2501.13919.pdf)

### 1. 논문의 각 섹션 요약

#### 서론
이 논문은 비디오 대규모 다중 모드 모델(video-LMM)의 시간적 연결성 향상을 위해 설계된 새로운 후처리 프레임워크인 시간적 선호 최적화(TPO)를 제안합니다. TPO는 두 가지 세부 수준, 즉 특정 비디오 구간에 집중하는 국소적 시간 연결성과 전체 비디오 시퀀스에서 확장된 시간적 의존성을 포착하는 포괄적 시간 연결성을 통해 시간적 이해를 크게 개선합니다.

#### 시간적 선호 최적화(TPO)
TPO는 비디오 입력을 받아 시간적으로 더욱 연결된 출력을 생성하는 모델을 목표로 합니다. 이 과정을 통해, 인간의 주석이나 고도로 발달된 교사 모델에 대한 의존을 최소화하면서 시간적 추론을 강화합니다.

#### 성능 평가
세 가지 벤치마크—LongVideoBench, MLVU, Video-MME—에서의 실험 결과, TPO는 두 가지 최첨단 비디오-LMM에서 효과성을 입증했으며, 특히 LLaVA-Video-TPO 모델이 Video-MME 벤치마크에서 7B 모델 중 선두를 차지하였습니다.

#### 데이터 집합과 프레퍼런스 최적화
TPO는 10,000개의 쿼리와 그에 따른 선호 응답을 포함하는 데이터 집합을 기반으로 자동화된 자가 학습 접근법을 통해 시간적 선호 데이터를 생성합니다. 이로 인해 비디오-LMMs의 시간적 이해가 향상됩니다.

### 2. 전체 요약

이 논문은 비디오 대규모 다중 모드 모델의 고도화된 시간적 이해를 목표로 TPO라는 새로운 후처리 프레임워크를 제안합니다. TPO는 두 가지 시간적 데이터 수준을 사용하여 모델의 시간적 추론 능력을 강화하며, 인간의 주석 없이도 학습되도록 설계되었습니다. 실험 결과, TPO는 세 가지 다양한 벤치마크에서 모델의 성능을 꾸준히 향상시켰으며, 특히 LLaVA-Video-TPO가 높은 수준의 시간적 이해력을 발휘하도록 돕습니다.