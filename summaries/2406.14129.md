# Towards Event-oriented Long Video Understanding
## TL;DR
## Summary
- [https://arxiv.org/pdf/2406.14129.pdf](https://arxiv.org/pdf/2406.14129.pdf)

### 1. 연구 논문의 각 섹션 요약

#### Abstract (초록)
이 논문은 이벤트 지향적 비디오 이해를 평가하기 위한 새로운 벤치마크인 "Event-Bench"를 소개합니다. 이 벤치마크는 기존 데이터셋과 인간 주석을 기반으로 구축되었으며, 이벤트 집중형 비디오 주석 데이터를 사용해 비용 효율적으로 비디오 다중모드 대형 언어 모델(Video MLLMs)을 향상시키는 방법인 VIM을 제안합니다. 실험 결과, 최고 성능 모델인 GPT-4o는 전체 정확도에서 53.33%를 달성하며, 최첨단 모델을 뛰어넘었습니다.

#### Introduction (서론)
비디오 이해는 AI 모델이 시각적 세계를 인식하는 데 중요한 능력입니다. 기존 벤치마크는 비디오의 풍부한 이벤트가 부족하여, 몇 프레임만으로도 답을 추론할 수 있는 단편적 편향(short-cut bias)을 겪고 있습니다. 이에 따라 연구진은 이벤트 지향적 비디오 이해를 평가하기 위해 Event-Bench를 소개하였습니다. Event-Bench는 6개의 이벤트 관련 작업과 2190개의 테스트 인스턴스를 포함하여 비디오 이벤트 이해 능력을 종합적으로 평가합니다.

#### Related Work (관련 연구)
기존 연구들은 비디오 이해와 관련된 여러 벤치마크를 제안해왔으며, 주요 연구 분야로는 시간적 추론, 상황적 추론, 복합적 추론 등이 있습니다. 그러나 대부분의 비디오는 짧고 다양성이 부족합니다. Event-Bench는 이러한 한계를 극복하며, 이벤트 지향적 평가가 가능하도록 설계되었습니다.

#### Methodology (방법론)
연구진은 VIM(Video Instruction Merging)이라는 비용 효율적 방법을 통해 이벤트 집중형 비디오 주석 데이터를 생성하여 모델을 향상시켰습니다. VIM은 비슷한 비디오들을 병합하여 하나의 새로운 비디오를 구성함으로써 더 복잡한 이벤트를 포함한 데이터 세트를 만듭니다. 이렇게 생성된 데이터는 비디오 다중모드 대형 언어 모델을 훈련시키는 데 사용됩니다. 또한, 새로운 모델 아키텍처를 통해 이미지와 비디오 입력을 모두 처리할 수 있습니다.

#### Experimental Setup (실험 설정)
실험은 다양한 오픈 소스 및 독점 비디오 MLLMs을 사용하여 설정되었으며, 새로운 Event-Bench를 통해 평가되었습니다. 각 모델은 8에서 32 프레임을 동일하게 샘플링한 입력을 받아 최고의 성능을 기록했습니다. 실험 결과, GPT-4o 모델이 가장 높은 평균 정확도를 기록하였습니다.

#### Results (결과)
모델 성능은 Table 3에 요약되어 있으며, 이벤트 추론 작업에서 이미지 MLLMs 및 비디오 MLLMs 성능이 전반적으로 저조한 것으로 나타났습니다. Task 별로 보면, 복합적 이벤트 이해보다는 전반적 이해가 더 어렵다는 점이 관찰되었습니다. 특히 새롭게 주석된 에피소드적 추론 작업에서 대부분의 모델이 낮은 성능을 보였습니다. 오픈 소스 모델 중에서는 ST-LLM, PLLaVA, 그리고 VideoChat2가 우수한 성능을 기록했습니다.

#### Discussion (토론)
프레임 수와 모델 성능의 관계를 분석한 결과, 프레임 수가 많을수록 모델의 성능이 향상되는 경향이 있음을 발견했습니다. 그러나 모든 오픈 소스 모델이 항상 더 많은 프레임을 통해 성능이 개선되는 것은 아니었습니다. 또한, 비디오와 이미지 데이터를 결합한 훈련 전략이 모델 성능을 크게 향상시켰습니다.

#### Conclusion (결론)
이벤트 지향적 비디오 이해를 평가하기 위해 Event-Bench를 제안하며, 이 벤치마크는 비디오 MLLMs의 종합적인 평가를 가능하게 하였습니다. 또한, VIM을 통해 비용 효율적으로 이벤트 집중형 비디오 데이터를 생성하여 모델 성능을 향상시켰습니다. 향후 연구에서는 오디오 및 텍스트와 같은 다른 모달리티를 추가하여 더욱 다양한 이벤트 이해 능력을 평가할 계획입니다.

### 2. 전체 요약

이 논문은 AI가 비디오에서 이벤트를 이해하는 능력을 평가하기 위해 새로운 벤치마크인 Event-Bench를 제안합니다. Event-Bench는 6개의 이벤트 관련 과제와 2190개의 테스트 인스턴스로 구성되어 있습니다. 또한, VIM이라는 비용 효율적 방법을 통해 이벤트 집중형 비디오 주석 데이터를 생성하여 비디오 다중모드 대형 언어 모델을 향상시켰습니다. 실험 결과, 다양한 오픈 소스 및 독점 모델 중 GPT-4o가 가장 높은 성능을 기록하였으며, 이는 이벤트 지향적 데이터의 중요성을 강조합니다. 향후 연구는 비디오 외의 오디오 및 텍스트 등의 다양한 모달리티를 포함할 계획입니다.