# NoProp: Training Neural Networks without Back-propagation or Forward-propagation
## TL;DR
## Summary
- [https://arxiv.org/pdf/2503.24322.pdf](https://arxiv.org/pdf/2503.24322.pdf)

1. 각 섹션 요약:

- **서론**: 이 논문은 전통적인 역전파(back-propagation) 방법론이 요구하는 계산 복잡성과 생물학적 불가능성에 대해 언급하며 이의 대안으로 역전파와 순전파 없이 신경망을 학습하는 새로운 방법론, NoProp을 소개합니다. NoProp은 확산(diffusion) 및 흐름 일치(flow matching) 방법을 기반으로 하며, 각 층이 독립적으로 학습을 수행하여 잡음이 있는 목표를 정화하는 방식으로 작동합니다.
  
- **방법론**: NoProp 방법론은 레이어별로 독립적인 학습을 하여 잡음이 있는 라벨을 기반으로 목표 라벨을 예측하는 방식으로 작동합니다. 이 과정에서 레이어는 서로 다른 시간 단계에서 다양한 확률적 과정을 통해 독립적으로 작동하며, 이는 확산 모델의 특정한 변형입니다.

- **실험 및 결과**: MNIST, CIFAR-10, CIFAR-100 데이터셋에서 NoProp의 성능은 기존의 역전파 및 다른 역전파 없는 방법과 비교했을 때 유사하거나 더 뛰어난 결과를 보여주었습니다. 특히 NoProp은 메모리 사용량을 줄이고 학습의 효율성을 증가시켰습니다.

- **결론**: NoProp은 확산 모델 기반의 잡음 정화(denoising) 기법을 활용하여 기존의 역전파 없는 방법들보다 단순하고 안정적이며 계산 효율이 높은 학습 알고리즘임을 강조합니다. 이 새로운 관점의 학습 방식은 심층 학습 모델을 훈련하는 새로운 가능성을 열어주며, 미래 연구에 영감을 줄 것이라 기대됩니다.

2. 전체 요약:

이 논문은 전통적인 역전파를 사용하지 않는 NoProp이라는 새로운 학습 방법론을 제안하고 있으며, 이는 각 층에서 노이즈가 있는 목표를 독립적으로 정화하는 기법을 사용합니다. MXNIST, CIFAR-10, CIFAR-100과 같은 데이터셋에서 NoProp의 성능은 기존의 방법과 유사하거나 뛰어났으며, 특히 메모리 사용량과 계산 효율성 면에서 큰 이점을 보여줍니다. 최종적으로 NoProp은 심층 학습에서 새로운 차원의 가능성을 제공하며, 이는 미래의 AI 연구와 개발에 새로운 시각을 제공합니다.