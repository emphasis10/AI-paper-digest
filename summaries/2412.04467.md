# VisionZip: Longer is Better but Not Necessary in Vision Language Models
## TL;DR
## Summary
- [https://arxiv.org/pdf/2412.04467.pdf](https://arxiv.org/pdf/2412.04467.pdf)

1. 각 섹션 요약:

- **서론**: 최근 비전-언어 모델(Vision Language Models, VLMs)의 발전은 비주얼 토큰(Visual Token)의 길이를 늘림으로써 성능을 향상시켜왔으나, 이로 인한 계산 비용 증가 문제도 생겼습니다. 이러한 문제를 해결하기 위해, 저자는 VisionZip이라는 방법을 도입하여 중요 정보만을 담는 토큰을 선별하고 비주얼 토큰의 중복성을 줄입니다. 이 방법은 이미지와 비디오 이해 작업에 폭넓게 적용 가능하며, 다중 대화와 같은 실제 상황에서도 효과적입니다.

- **방법론**: VisionZip은 기존의 비주얼 토큰 생성 방식에서 발견된 중복성을 줄이는 방식을 제안합니다. 이는 비주얼 토큰을 평균적으로 분석하여, 주목도가 높은 토큰을 선택하는 방식으로 이루어집니다. 선택된 토큰들은 비슷한 정보를 가진 토큰들과 결합시켜 그 정보를 최적화합니다. 이를 통해 효율성을 높이면서도 성능을 유지합니다.

- **결과**: VisionZip은 여러 벤치마크에서 기존의 최첨단(SOTA) 방법보다 최소 5% 높은 성능 이득을 보여줍니다. 또한, 모델 추론 속도를 크게 높이고, LLaVA-Next 13B 모델이 LLaVA-Next 7B 모델보다 더 빠르면서도 더 나은 성과를 낼 수 있게 합니다.

- **토론**: 실험 결과, 비주얼 토큰의 중복을 제거하면 오히려 성능이 향상됨을 확인했습니다. 이전 방법과 비교했을 때, VisionZip은 중복을 효과적으로 제거하고, 성능 저하 없이 토큰 수를 줄일 수 있는 가능성을 보여줍니다. 이 방법은 다중 대화 및 실제 애플리케이션 시나리오에 유리합니다.

- **결론**: 이 연구는 VisionZip이 비주얼 토큰을 효과적으로 줄여 계산 효율성을 크게 향상시키면서도 모델 성능을 유지할 수 있음을 증명합니다. 또한, 비전 인코더의 비효율적 토큰 중복성을 개선함으로써 더 나은 VLM 성능 개발의 새로운 방향성을 제시합니다.

2. 전체 요약:
VisionZip은 비주얼 토큰의 중복성을 줄임으로써 비전-언어 모델의 효율성을 극대화하는 혁신적 방법입니다. 이 방법은 기존의 장황한 비주얼 토큰 길이 문제를 해결하며, 다중 대화 및 실제 상황에서의 적용 가능성과 성능을 입증했습니다. VisionZip은 앞으로의 VLM 발전 방향에 큰 기여를 할 수 있는 가능성을 보여줍니다.