# LLM-Pruner: On the Structural Pruning of Large Language Models
## TL;DR
## Summary
- [https://arxiv.org/pdf/2305.11627.pdf](https://arxiv.org/pdf/2305.11627.pdf)

### 1. 섹션별 요약

#### 1.1. 서론
최근 대형 언어 모델(LLM)은 언어 이해와 생성에서 뛰어난 능력을 보여주고 있지만, 큰 모델 크기와 관련된 배포 및 추론의 어려움이 있습니다. LLM-Pruner는 이러한 문제를 해결하기 위해 제안된 새로운 프레임워크입니다. 이 방법은 작업에 구애받지 않고 원본 학습 데이터에 대한 의존도를 최소화하여 LLM을 압축하는 방법을 설계했습니다.

#### 1.2. 관련 연구
기존의 언어 모델 압축 기술은 주로 네트워크 가지치기, 지식 증류, 양자화 등 여러 기술로 나뉩니다. 기존 방법들은 주로 특정 작업이나 도메인에 초점을 맞추고 있는 반면, LLM-Pruner는 작업 독립적인 압축을 목표로 합니다.

#### 1.3. 방법론
LLM-Pruner는 구조적 가지치기를 통해 중요도를 고려해 비핵심 구조체를 제거하는 방식으로 작동합니다. 이는 모델의 성능 저하를 최소화하게 설계되었습니다. 또한 많은 데이터가 필요하지 않은 빠른 튜닝 기법을 사용해 모델의 성능을 효과적으로 회복시킵니다.

#### 1.4. 결과
실험 결과, 세 가지 대형 언어 모델(LLaMA, Vicuna, ChatGLM)을 대상으로 구조적 가지치기를 수행했을 때, 매개 변수를 20% 제거해도 원래 모델 성능의 94.97%를 유지할 수 있었습니다. 또한 대다수의 데이터셋에서 5.4B LLaMA 모델이 ChatGLM-6B 모델보다 더 뛰어난 성능을 보였습니다.

#### 1.5. 논의
LLM-Pruner는 다양한 작업에서 사용할 수 있는 다목적 해결사로서의 LLM 기능을 유지하면서 데이터 의존성을 줄이고 빠른 압축이 가능한 프레임워크입니다. 높은 압축 비율을 적용하면 성능 저하가 발생할 수 있지만, LLM-Pruner는 이러한 과제를 해결하기 위한 좋은 출발점입니다.

#### 1.6. 결론
LLM-Pruner는 대형 언어 모델의 구조적 가지치기에 관한 첫 프레임워크로, 모델의 다목적 해결사 기능을 유지하면서 데이터 의존성을 줄이고 짧은 시간에 압축할 수 있는 혁신적인 접근법을 제안합니다.

### 2. 전체 요약
이 논문은 LLM-Pruner라는 새로운 프레임워크를 제안하여 작업에 구애받지 않고 대형 언어 모델(LLM)을 효율적으로 압축하는 방법을 연구합니다. 기존의 방법들이 특정 작업이나 도메인에 집중되어 있는 반면, LLM-Pruner는 다양한 작업에서 사용될 수 있는 모델의 다목적 기능을 유지하면서 압축을 수행합니다.

LLM-Pruner는 구조적 가지치기를 사용하여 중요하지 않은 구조체를 제거하고, 모델의 성능 저하를 최소화합니다. 최소한의 데이터(50K 샘플)만을 사용해 모델의 성능을 빠르게 회복할 수 있습니다. 실험 결과, LLaMA, Vicuna, ChatGLM 같은 모델에서 매개 변수를 20% 제거해도 성능의 94.97%를 유지하는 뛰어난 성능을 보였습니다.

이 연구는 대형 언어 모델을 효율적으로 압축할 수 있는 가능성을 보여주며, 모델의 데이터 종속성을 줄이고 짧은 시간에 압축을 완료할 수 있는 혁신적인 방법을 제시합니다.

이를 통해 대형 언어 모델을 다양한 애플리케이션에 더 효과적으로 적용할 수 있는 기반을 마련하게 되었습니다.