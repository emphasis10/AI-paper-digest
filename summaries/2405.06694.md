# SUTRA: Scalable Multilingual Language Model Architecture
## TL;DR
## Summary
- [https://arxiv.org/pdf/2405.06694.pdf](https://arxiv.org/pdf/2405.06694.pdf)

### 주요 내용 요약

1. **서론 및 배경**:
   - 이 논문에서는 SUTRA라는 다중 언어 대규모 언어 모델 아키텍처를 소개합니다. SUTRA는 50개 이상의 언어를 이해하고, 추론하며, 텍스트를 생성할 수 있는 능력을 가지고 있으며, 언어별 처리와 개념 학습을 분리하여 확장성과 효율성을 제공합니다. 이를 통해 기존 모델보다 MMLU 벤치마크에서 20-30% 향상된 성능을 보입니다.

2. **방법론**:
   - SUTRA는 개념 학습과 언어 학습을 분리하여 다중 언어 정렬 및 학습을 용이하게 합니다. 이를 위해 Mixture of Experts (MoE) 프레임워크를 사용하여 개념 및 언어 처리의 효율성을 높입니다. SUTRA는 인터넷에 연결되어 최신 정보를 바탕으로 사실적이고 최신의 응답을 제공합니다.

3. **실험**:
   - SUTRA는 9개의 계획 도메인에 걸쳐 7개의 대규모 언어 모델을 실험 분석하였으며, 특히 고유의 언어 처리 메커니즘을 통해 다중 언어 작업에서 높은 성능을 보였습니다. MMLU 벤치마크에서 SUTRA는 기존 모델들보다 우수한 성능을 보여주었으며, 특히 비영어권 언어에서 성능 차이가 적었습니다.

### 혁신적인 부분
SUTRA의 혁신성은 개념 학습을 언어 학습과 분리하여 다중 언어 처리의 효율성을 높이고, MoE 프레임워크를 통해 컴퓨팅 자원을 효율적으로 사용함으로써 확장성과 성능을 동시에 제공하는 데 있습니다. 특히, SUTRA는 다양한 언어에서 일관된 성능을 유지하여 글로벌 AI 기술의 접근성을 높이고, 다중 언어 AI 모델의 새로운 기준을 세웠습니다.