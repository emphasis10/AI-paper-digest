# Can Large Language Models Understand Symbolic Graphics Programs?
## TL;DR
## Summary
- [https://arxiv.org/pdf/2408.08313.pdf](https://arxiv.org/pdf/2408.08313.pdf)

### 섹션별 요약

#### 1. 서론 (Introduction)
이 논문은 대형 언어 모델(LLMs)의 평가 과제를 소개하며, 주로 심볼릭 그래픽스 프로그램을 이용한 이해 능력을 중점으로 다룹니다. 심볼릭 그래픽스 프로그램은 2D 이미지나 3D 모형 등의 그래픽 콘텐츠를 생성하는 프로그램입니다. 이 논문은 LLM이 이러한 프로그램을 "이해"할 수 있는지를 평가하기 위해서 새로운 벤치마크인 SGP-Bench를 도입합니다. 주요 목표는 LLM이 심볼릭 프로그램을 통해 그래픽 콘텐츠를 머릿속으로 상상할 수 있는 능력을 시험하는 것입니다.

#### 2. 관련 연구 (Related Work)
기존의 연구들은 주로 코드 이해나 생성을 위한 벤치마크 및 평가 기준에 집중되어 있습니다. 이 섹션에서는 다양한 벤치마크와 데이터셋을 소개하며, 주로 AI와 LLM의 코드 생성 및 이해 능력에 중점을 두고 있습니다. 또한, LLM의 비주얼 언어 처리 능력을 평가하기 위한 데이터셋도 논의됩니다.

#### 3. 심볼릭 그래픽스 프로그램의 의미 이해 (Semantic Understanding of Symbolic Graphics Programs)
이 섹션은 심볼릭 그래픽스 프로그램의 의미를 이해하는 과제를 정의하고, LLM이 이러한 프로그램을 읽고 그래픽스를 상상할 수 있는지 여부를 평가합니다. 이 논문은 그래픽스 프로그램이 단순한 텍스트 코드보다는 더 복잡한 의미를 담고 있기 때문에, 단순히 코드만 이해하는 것 이상으로 LLM의 능력을 확장해야 한다고 주장합니다.

#### 4. 메소드 (Methodology)
이 섹션에서는 새로운 벤치마크인 SGP-Bench를 이용해 LLM을 평가하는 방법을 소개합니다. SGP-Bench는 심볼릭 그래픽스 프로그램을 기반으로 질문과 프로그램-그래픽 대응 관계를 활용한 평가 체계를 갖추고 있습니다. 또한, 심볼릭 인스트럭션 파인튜닝(SIT)이라는 새로운 방법론을 도입해 LLM의 그래픽 프로그램 이해 능력을 강화합니다. 작은 데이터로도 SIT가 성과를 내는 것을 확인했습니다.

#### 5. 결과 (Results)
다양한 LLM을 SGP-Bench를 통해 평가한 결과를 제공합니다. 평가된 모델들은 기술기업 및 연구기관에서 개발된 오픈소스와 상용 모델들로 구성되어 있습니다. 결과적으로, 심볼릭 프로그램 이해 능력에서 큰 차이가 나타났으며, 잘 알려진 추론 모델들이 더 좋은 성과를 냈습니다. SIT를 통해 모델의 이해 능력이 향상되는 것도 확인했습니다.

#### 6. 결론 (Conclusion)
이 논문은 LLM의 새로운 평가 기준을 제시하며, 심볼릭 그래픽스 프로그램 이해 능력을 측정하는 SGP-Bench와 이를 향상시키는 SIT 기법을 도입했습니다. 이 연구는 LLM의 잠재적 능력을 깊이 이해하는데 기여하며, 향후 다양한 평가 과제가 개발될 수 있도록 영감을 제공합니다. 향후 연구에서는 심볼릭 그래픽스 프로그램의 이해와 성능 향상을 위한 메커니즘을 더 탐구할 예정입니다.

### 전체 요약
이 논문은 대형 언어 모델(LLM)의 이해 능력을 평가하기 위한 새로운 벤치마크와 방법론을 제안합니다. 특히 심볼릭 그래픽스 프로그램을 사용하여 LLM의 이해 능력을 측정하며, 이를 통해 LLM이 시각적 콘텐츠를 상상하거나 예측할 수 있는지를 평가합니다. 이를 위한 SGP-Bench와 심볼릭 인스트럭션 파인튜닝(SIT)이라는 새로운 평가 및 개선 방법을 도입하였습니다. 실험 결과, 여러 LLM 모델들이 심볼릭 프로그램 이해에서 성과를 내었으며, SIT를 통해 성능이 향상되는 것도 확인되었습니다. 이 연구는 LLM의 능력을 더 깊이 이해하고 평가할 수 있는 새로운 기준을 제시하며, 향후 연구에 중요한 기초 자료가 될 것입니다.