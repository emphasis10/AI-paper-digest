# Layerwise Recurrent Router for Mixture-of-Experts
## TL;DR
## Summary
- [https://arxiv.org/pdf/2408.06793.pdf](https://arxiv.org/pdf/2408.06793.pdf)

## 1. 섹션별 요약

### 1. 소개 (Introduction)
본 논문은 대규모 언어 모델(LLM)의 확장과 관련된 문제를 다룹니다. 특히, Mixture-of-Experts(MoE) 아키텍처가 어떻게 모델의 크기를 증가시키면서도 계산 비용을 크게 증가시키지 않는지 설명합니다. 현재의 MoE 모델이 파라미터 비효율성을 나타내는 문제를 보완하기 위해 RMoE (Layerwise Recurrent Router for Mixture-of-Experts)를 소개합니다. 이 시스템은 다양한 계층들의 라우팅 결정을 연결해주는 Gated Recurrent Unit(GRU)을 활용합니다.

### 2. 방법론 (Methodology)
RMoE는 연속적인 계층 간의 라우팅 의사 결정을 연결하여 기초 모델의 효율성을 높입니다. GRU를 통해 라우터들이 서로의 정보를 공유함으로써 더 나은 토큰-전문가 조합을 이끌어내고, 이는 파라미터 효율성을 증가시킵니다. 이 방법론의 주요 구성 요소로는 GRU와 이를 활용한 계층별 연속 라우팅 메커니즘이 있습니다.

### 3. 실험 설정 (Experiment Setup)
모델 아키텍처는 24개 계층과 Rotary Embedding, SwiGLU 활성화 함수 등을 포함합니다. 각 토큰은 16명의 전문가 중 4명을 활성화합니다. 각 모델은 대략 910M의 파라미터를 가지며, 8개의 NVIDIA-A100 GPU에서 훈련됩니다. 평가 데이터로는 ARC-Easy, Hellaswag, PIQA 등이 사용되었습니다.

### 4. 주요 결과 (Main Results)
실험 결과, RMoE는 기존의 SMoE와 다른 다양한 기준 모델보다 더 나은 성능을 보였습니다. RMoE는 특히 토큰-전문가 조합과 관련된 파라미터 효율성을 크게 개선시켰습니다. 또한, 다양한 XMoE 설정에서도 GRU 라우터를 통해 성능 향상을 확인했습니다.

### 5. 한계점 (Limitations)
현재 RMoE는 MMLU나 GSM8K와 같은 유명한 도전적 작업들에서 충분히 테스트되지 않았습니다. 향후 연구에서는 성능을 향상시키기 위한 z-loss 도입 및 소프트맥스 온도 변화와 같은 방법들을 탐구할 계획입니다. 또한, MoE의 라우터에 대한 더 직접적인 그래디언트 분석이 필요할 것으로 보입니다.

## 2. 전체 요약 (Overall Summary)
본 논문은 Layerwise Recurrent Router for Mixture-of-Experts (RMoE)를 통해 대규모 언어 모델의 파라미터 효율성을 크게 개선할 수 있는 방법을 제안합니다. RMoE는 Gated Recurrent Unit (GRU)을 활용하여 연속적인 계층 간의 라우팅 결정을 연결함으로써, 기존의 MoE 시스템의 주요 문제점을 해결합니다. 실험 결과, RMoE는 다양한 기준 모델보다 우수한 성능을 보였으며, 특히 토큰-전문가 조합에서 효율성을 극대화했습니다. 하지만 아직 유명한 도전적 작업들에서의 테스트가 부족하며, 향후 연구에서는 추가적인 성능 개선을 위한 다양한 방법들을 탐색할 예정입니다.