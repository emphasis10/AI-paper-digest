# MMIU: Multimodal Multi-image Understanding for Evaluating Large Vision-Language Models
## TL;DR
## Summary
- [https://arxiv.org/pdf/2408.02718.pdf](https://arxiv.org/pdf/2408.02718.pdf)

### 주요 내용 요약

#### 1. 소개
이 논문은 다중 이미지를 이해할 수 있는 대규모 비전-언어 모델(LVLMs)을 평가하기 위한 **MMIU (Multimodal Multi-image Understanding)** 벤치마크를 소개합니다. MMIU는 전체 장면을 보다 세밀하게 이해하기 위해 다중 이미지를 처리할 수 있는 능력이 중요함을 강조합니다.

#### 2. 관련 연구
여기에서 제시된 MMIU 벤치마크는 기존의 단일 이미지-텍스트 페어가 아닌 다중 이미지를 포함한 데이터 세트를 활용하여 모델의 성능을 평가하는 데 중점을 둡니다. 이는 모델이 다중 이미지를 통해 더 풍부한 정보를 분석하고 종합할 수 있어야 함을 보여줍니다.

#### 3. 방법론
MMIU 벤치마크는 77,000장의 이미지와 11,000개의 다중 선택 질문을 포함하여 가장 포괄적인 평가를 제공합니다. 다중 이미지 간의 관계를 기반으로 7가지 유형의 이미지 관계와 52개의 다양한 태스크를 평가하도록 설계된 점이 특징입니다.

#### 4. 실험 설정
24개의 주요 LVLMs를 평가한 결과, 최신 모델인 GPT-4o조차도 55.7%의 정확도밖에 달성하지 못했습니다. 이는 다중 이미지 과제를 해결하는 데 큰 도전이 있음을 시사합니다. 또한, 단일 이미지 이해를 바탕으로 다중 이미지 과제를 처리할 수 있는 능력이 있음을 발견했습니다.

#### 5. 결과 및 분석
LVLMs는 다중 이미지 시나리오에서의 의미적 콘텐츠 이해에는 강하지만, 다중 이미지 맥락에서의 시간적 및 공간적 관계 이해에는 약점을 보였습니다. 이로 인해 특정 임무(예: 3D 인식 및 시간적 추론 과제)에서 모델 성능이 저조했습니다. 과제 학습 난이도 분석 결과 단순한 감독 학습(SFT)으로는 정렬, 검색 및 대규모 이미지가 포함된 과제를 과적합시키기 어려움이 밝혀졌습니다.

#### 6. 결론
- **기여 1**: MMIU 벤치마크는 다양한 복잡한 다중 이미지 과제를 평가할 수 있는 포괄적인 평가 스위트를 도입하여 다중 이미지 이해에 중요한 공백을 메웁니다.
- **기여 2**: 평가 결과 현재의 LVLMs, 특히 GPT-4와 같은 독점 모델도 공간적 이해를 포함한 다중 이미지 과제 해결에 큰 도전을 맞이하고 있음을 보여줍니다.
- **기여 3**: 이 논문은 다양한 분석 도구를 사용하여 현재 모델의 성능 격차와 한계를 확인하고, 향후 모델 및 데이터 개선을 위한 유용한 통찰력을 제공합니다.

### 종합 요약
이 논문은 다중 이미지를 이해할 수 있는 대규모 비전-언어 모델을 평가하기 위한 새로운 평가 벤치마크인 MMIU를 소개합니다. MMIU는 7가지 유형의 이미지 관계와 52개의 태스크를 포함하며, 최신 모델인 GPT-4o조차도 55.7%의 정확도밖에 달성하지 못할 정도로 어려운 평가 환경입니다. 이 논문의 주요 기여는 MMIU 벤치마크를 통해 현재 모델의 부족한 부분을 파악하고, 향후 모델 개선을 위해 중요한 통찰력을 제공하는 것입니다. 모델은 의미적 관계 이해에는 강하지만 시간적 및 공간적 추론 능력에는 약점을 보였으며, 대규모 이미지 과제 해결과 관련된 많은 과제에서 어려움을 겪고 있습니다. 이를 통해 LVLMs 연구와 개발을 진전시키고, 향후 더 정교한 다중 이미지 사용자 인터랙션을 실현하는 데 기여할 것입니다.