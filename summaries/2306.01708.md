# TIES-Merging: Resolving Interference When Merging Models
## TL;DR
## Summary
- [https://arxiv.org/pdf/2306.01708.pdf](https://arxiv.org/pdf/2306.01708.pdf)

논문 "TIES-MERGING: Resolving Interference When Merging Models"에서는 미세조정된 다중 모델을 하나의 멀티태스크 모델로 통합하는 새로운 방법을 제안합니다. 이 방법은 기존의 모델 통합 기법들이 무시하는 모델 간 파라미터의 상호 간섭 문제를 해결하는 데 초점을 맞춥니다.

### 주요 내용 요약

1. **서론 및 배경**:
   - 사전 훈련된 모델을 특정 작업에 대해 미세조정하는 것은 성능 향상, 빠른 수렴, 효율적인 샘플 활용 등의 이점을 제공하지만, 각 작업에 대해 별도의 미세조정 모델을 관리하는 것은 비효율적입니다. 멀티태스크 학습은 이러한 문제를 해결할 수 있지만, 비용이 많이 들고 모든 작업에 동시에 접근해야 한다는 단점이 있습니다.

2. **TIES-MERGING 방법론**:
   - 파라미터의 미세조정으로 인한 '태스크 벡터'를 생성하고, 이를 기반으로 세 가지 주요 단계를 통해 모델을 통합합니다. 첫째, 중요도가 낮은 파라미터를 재설정합니다. 둘째, 파라미터 값의 부호 충돌을 해결합니다. 마지막으로, 동의된 부호에 따라 파라미터를 평균화합니다.

3. **성능 평가**:
   - 다양한 모달리티, 모델 크기 및 벤치마크에서 TIES-MERGING은 기존 방법보다 우수한 성능을 보여줍니다. 특히, 도메인 내 평가에서 평균 2.3%, 도메인 간 일반화에서 T5-large 모델을 사용할 때 4.4%의 성능 향상을 달성했습니다.

### 혁신적인 부분
이 연구의 혁신적인 점은 모델 간의 상호 간섭, 특히 중복 파라미터와 부호 불일치로 인한 간섭을 해결하는 새로운 방법을 도입한 것입니다. 이는 모델 통합의 성능을 크게 향상시키며, 멀티태스크 모델의 효율성과 일반화 능력을 개선합니다.

이 논문은 모델 통합 분야에서 중요한 진전을 나타내며, 향후 다양한 NLP 작업에 활용될 수 있는 강력하고 효과적인 방법을 제시합니다.

## Similar Papers
- [Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning](2205.05638.md)
- [SPAFIT: Stratified Progressive Adaptation Fine-tuning for Pre-trained Large Language Models](2405.00201.md)
- [DELLA-Merging: Reducing Interference in Model Merging through Magnitude-Based Sampling](2406.11617.md)
- [Tied-Lora: Enhancing parameter efficiency of LoRA with weight tying](2311.09578.md)
- [OpenELM: An Efficient Language Model Family with Open Training and Inference Framework](2404.14619.md)
- [RULE: Reliable Multimodal RAG for Factuality in Medical Vision Language Models](2407.05131.md)
- [Transfer Learning for Structured Pruning under Limited Task Data](2311.06382.md)
- [MultiLoRA: Democratizing LoRA for Better Multi-Task Learning](2311.11501.md)
- [TabReD: A Benchmark of Tabular Machine Learning in-the-Wild](2406.19380.md)
