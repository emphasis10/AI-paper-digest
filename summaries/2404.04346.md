# Koala: Key frame-conditioned long video-LLM
## TL;DR
## Summary
- [https://arxiv.org/pdf/2404.04346.pdf](https://arxiv.org/pdf/2404.04346.pdf)

Koala 연구는 길이가 긴 비디오에서 짧은 비디오에 이르기까지 다양한 비디오를 이해하는 데 초점을 맞추고 있습니다. 기존의 비디오 대규모 언어 모델(vLLMs)은 짧은 비디오 클립에서 훈련되었기 때문에 긴 비디오를 정확하게 이해하고 관련 질문에 답하는 데 어려움을 겪습니다. Koala는 이러한 문제를 해결하기 위해 제안된 경량화되고 자기 감독된 접근 방식으로, 사전 훈련된 vLLMs를 더 긴 비디오로 일반화할 수 있는 학습 가능한 시공간 쿼리를 도입합니다. 이 접근 방식은 시각적 토큰을 조건으로 하는 두 가지 새로운 토크나이저를 통해 짧은 비디오 순간과 긴 비디오 순간을 모두 이해할 수 있게 합니다. Koala는 HowTo100M 데이터셋에서 제안된 방식을 훈련시키고 EgoSchema 및 SeedBench 벤치마크에서 제로샷 긴 비디오 이해 능력을 시연하여 기존 대규모 모델을 3~6%의 절대 정확도로 능가하는 성능을 보였습니다.

### 주요 내용 요약

- **Koala 소개**: 길이가 긴 비디오를 이해하기 위한 새로운 경량화된 자기 감독 접근 방식. 사전 훈련된 vLLMs를 활용하여 더 긴 비디오에 대한 이해를 일반화합니다.
- **핵심 기술**: 학습 가능한 시공간 쿼리를 도입하여 사전 훈련된 비디오 토크나이저의 기능을 확장합니다. 이를 통해 긴 비디오 전반에 걸친 시공간 맥락을 집계할 수 있습니다.
- **성능**: 제안된 Koala 방식은 제로샷 긴 비디오 이해 태스크에서 기존 대규모 모델들을 상당한 정확도로 능가합니다.
- **응용**: Koala는 다양한 길이의 비디오에 걸쳐 효과적으로 작동하며, 긴 비디오에서의 정확한 이해 및 질문 답변에 기여할 수 있습니다.

### 종합 요약

Koala 연구는 사전 훈련된 비디오 대규모 언어 모델(vLLMs)의 한계를 극복하고자 합니다. 특히, 짧은 비디오 클립에서만 훈련된 모델이 긴 비디오를 정확하게 이해하는 데 어려움을 겪는 문제를 해결하기 위해, 학습 가능한 시공간 쿼리를 사용하여 모델이 긴 비디오의 맥락을 효과적으로 집계하고 이해할 수 있도록 합니다. 이 접근 방식은 비디오 이해 및 관련 질문에 대한 답변의 정확도를 크게 향상시키며, 다양한 길이의 비디오를 처리할 수 있는 유연성을 제공합니다. Koala는 비디오 분석 및 이해와 관련된 다양한 응용 분야에서 유용하게 사용될 수 있습니다.