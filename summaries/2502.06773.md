# On the Emergence of Thinking in LLMs I: Searching for the Right Intuition
## TL;DR
## Summary
- [https://arxiv.org/pdf/2502.06773.pdf](https://arxiv.org/pdf/2502.06773.pdf)

### 논문 요약

1. **소개 및 문제 정의**
   - 최근의 AI 발전은 대규모 언어 모델(LLM)을 대규모 추론 모델(LRM)로 변모시키고 있습니다. 이는 LRM이 추론 시 더 많은 시간과 계산을 수행하여 고품질의 출력을 생성하기 때문입니다. 본 연구는 LLM의 추론 및 사고 과정을 훈련시키기 위한 알고리즘적 프레임워크를 모색하는 것을 목적으로 합니다.

2. **RLSP 프레임워크 소개**
   - 연구진은 LLM에 사고하는 능력을 부여하기 위해 자기 대전을 통한 강화학습(RLSP)이라는 사후 훈련 프레임워크를 제안합니다. RLSP는 세 가지 단계로 구성됩니다: 
     1) 인간 또는 합성된 추론 과정 시연 시 고도의 지도 기반 미세 조정(SFT),
     2) 다양한 탐색 행위를 권장하는 탐색 보상 신호 사용,
     3) 보상 해킹을 방지하면서 해의 정확성을 보장하는 결과 검증기를 활용한 RL 훈련.

3. **실험결과 및 기여**
   - RLSP 프레임워크는 간단한 탐색 보상만으로도 백트래킹, 아이디어 탐색, 검증 등 다양한 발현적 행동을 보였습니다. 이 프레임워크는 여러 모델 계열, 크기, 도메인 전반에 걸쳐 발현적 행동을 가능하게 했습니다.

4. **결론 및 미래 연구 방향**
   - RLSP는 LLM의 복잡한 추론 능력 발달에 중요한 역할을 할 가능성이 있습니다. 향후 더 큰 규모의 실험과 분석이 필요하며, 이는 모델이 보다 고차원적인 추론 능력, 예를 들어 추상화 및 이론 생성, 해결되지 않은 문제 해결 등을 달성하는 데 필요한 훈련 레시피를 찾는 데에 집중할 것입니다.

### 종합 요약
이 논문은 LLM이 더 높은 품질의 출력을 생성하도록 추론이나 사고 능력을 갖추기 위한 RLSP라는 새로운 프레임워크를 제안합니다. RLSP는 LLM이 다양한 도메인에서 더 복잡한 추론 능력을 발휘할 수 있게 하며, 특히 백트래킹이나 아이디어 탐색, 자기 검증 등 인간적 사고 과정과 유사한 행동을 가능하게 합니다. 이 연구는 모델 훈련에 대한 새로운 접근 방식을 제시하며, LLM이 더욱 발전된 사고 능력을 가질 수 있는 길을 열고 있습니다.