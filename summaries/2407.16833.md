# Retrieval Augmented Generation or Long-Context LLMs? A Comprehensive Study and Hybrid Approach
## TL;DR
## Summary
- [https://arxiv.org/pdf/2407.16833.pdf](https://arxiv.org/pdf/2407.16833.pdf)

### 1. 섹션별 요약

#### 1.1 서론 (Introduction)
논문은 대형 언어 모델(LLM)의 긴 맥락을 효율적으로 처리하기 위해 검색 증강 생성(Retrieval Augmented Generation, RAG)과 긴 맥락(LC, Long Context) LLM을 비교 연구합니다. 최근의 LLM들은 긴 맥락을 직접 이해하는 데 뛰어난 능력을 보이지만, RAG는 훨씬 저렴한 비용 등 여러 장점을 가지고 있습니다. 이에 따라 두 접근법을 결합한 SELF-ROUTE 라는 모델을 제안하며, 이는 모델 자체 반성을 통해 쿼리를 라우팅해 계산 비용을 대폭 절감하면서도 LC와 유사한 성능을 유지합니다.

#### 1.2 관련 연구 (Related Work)
기존 연구들은 LLM이 긴 맥락을 처리하는 방법, RAG의 장점, 그리고 긴 맥락 모델을 평가하는 방법 등을 다룹니다. 특이하게도, 여러 연구들에서 긴 맥락 모델은 여전히 RAG보다 성능이 떨어지는 것으로 알려졌으나, 본 논문은 최근의 LLM들을 사용해 이를 해결하려 합니다.

#### 1.3 RAG와 LC 벤치마킹 (Benchmarking RAG versus LC)
10가지 공개 데이터셋을 통해 RAG와 LC를 벤치마킹한 결과, LC가 거의 모든 설정에서 RAG보다 뛰어난 성능을 보였습니다. 그럼에도 불구하고 RAG는 훨씬 낮은 계산 비용 덕에 여전히 유용하게 사용될 수 있음을 보여주었습니다.

#### 1.4 SELF-ROUTE 모델 제안 (Proposing SELF-ROUTE)
SELF-ROUTE 모델은 모델 자체가 쿼리가 해결될 수 있는지 반영하는 방법으로, 쿼리를 RAG 또는 LC에 라우팅합니다. 이를 통해 비용은 절감하면서도 성능 차이를 최소화할 수 있음을 보였습니다. 구체적으로, 비용을 65%까지 줄이면서 Gemini와 GPT-4O에서 유사한 성능을 유지할 수 있었습니다.

#### 1.5 실험 결과 (Experimental Results)
SELF-ROUTE 모델은 대부분의 쿼리를 RAG를 통해 해결하면서 LC와 유사한 성능을 달성했습니다. 특히, 긴 텍스트가 많은 데이터셋에서는 비용 절감 효과가 더욱 두드러졌습니다.

#### 1.6 결론 (Conclusion)
논문은 RAG와 LC의 성능 및 비용 절감을 비교 분석했으며, SELF-ROUTE 모델이 두 접근법의 장점을 결합해 유사한 성능을 유지하면서도 비용을 대폭 절감할 수 있음을 입증했습니다. 이는 긴 맥락을 처리하는 LLM 응용 프로그램의 실용적 가이드라인을 제공하며, 향후 연구에 중요한 통찰을 제공합니다.

### 2. 종합 요약
이 논문은 기존의 검색 증강 생성(RAG) 및 긴 맥락(LC) 대형 언어 모델(LLM) 접근법을 비교하고, 두 접근법의 장점을 결합한 SELF-ROUTE 모델을 제안합니다. SELF-ROUTE 모델은 모델 자체 반성을 통해 쿼리를 RAG 또는 LC로 라우팅하여 계산 비용을 절감하면서도 유사한 성능을 유지합니다. 실험 결과, SELF-ROUTE는 비용을 65%까지 절감하면서도 LC와 유사한 성능을 보였으며, 긴 텍스트가 많은 데이터셋에서 특히 효과적임을 입증했습니다. 이 논문은 긴 맥락을 처리하는 LLM의 실용적 응용에 중요한 통찰을 제공하며, 향후 RAG 기술 최적화 연구에 방향을 제시합니다.

## Similar Papers
- [Sibyl: Simple yet Effective Agent Framework for Complex Real-world Reasoning](2407.10718.md)
- [Evaluating Open Language Models Across Task Types, Application Domains, and Reasoning Types: An In-Depth Experimental Analysis](2406.11402.md)
- [Does Fine-Tuning LLMs on New Knowledge Encourage Hallucinations?](2405.05904.md)
- [Can Long-Context Language Models Subsume Retrieval, RAG, SQL, and More?](2406.13121.md)
- [NATURAL PLAN: Benchmarking LLMs on Natural Language Planning](2406.04520.md)
- [Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache Consumption](2407.18003.md)
- [LLM In-Context Recall is Prompt Dependent](2404.08865.md)
- [SnapKV: LLM Knows What You are Looking for Before Generation](2404.14469.md)
- [Speculative RAG: Enhancing Retrieval Augmented Generation through Drafting](2407.08223.md)
