# LVD-2M: A Long-take Video Dataset with Temporally Dense Captions
## TL;DR
## Summary
- [https://arxiv.org/pdf/2410.10816.pdf](https://arxiv.org/pdf/2410.10816.pdf)

### 1. 각 섹션 요약

#### 서론
AI 및 머신러닝을 통해 긴 영상을 생성하는 것은 영화 제작 등 다양한 분야에서 중요합니다. 이전의 비디오 생성 모델들은 주로 짧은 클립을 다루었지만, 긴 영상을 생성하기 위해서는 일관된 시간성과 큰 움직임을 처리하는 모델이 필요합니다. 본 연구에서는 많은 양의 긴 영상을 걸러내고 그것을 재캡션하는 자동화 파이프라인을 개발했습니다.

#### 관련 연구
현재까지 대부분의 영상-언어 데이터셋은 짧은 비디오 클립에 초점을 맞췄으며, 긴 비디오에 대한 밀집된 묘사가 부족했습니다. 이를 해결하기 위해 대규모 고품질 긴 영상을 포함한 데이터셋이 필요합니다.

#### 방법론
비디오 필터링과 긴 영상 재캡션을 위한 자동화된 파이프라인을 소개합니다. 비디오 LLMM과 같은 도구를 활용하여 고품질 긴 영상을 선택하고, LLaVA-v1.6-34B와 같은 언어 모델을 사용해 긴 영상에 시간적으로 밀집된 캡션을 생성합니다.

#### 결과 및 논의
LVD-2M 데이터셋을 활용해 긴 영상 생성 모델을 미세 조정한 결과, 기존의 모델 대비 더 동적이고 텍스트와 잘 맞는 영상을 생성하는 데 우수한 성능을 보였습니다. 

#### 결론
본 연구는 LVD-2M이라는 대규모 데이터셋을 구축하여 긴 영상 생성의 한계를 극복했으며, 이를 통해 동적 모션을 가진 긴 영상을 생성하는 데 기여했습니다. 데이터셋은 2백만 개의 영상으로 구성되어 있으며, 각 영상에는 시간에 따라 변하는 장면을 설명하는 밀집된 캡션이 부여되어 있습니다.

### 2. 전체 요약
본 연구는 긴 영상 생성의 어려움을 해결하기 위한 새로운 데이터셋, LVD-2M을 소개합니다. 이 데이터셋은 최소 10초 이상의 긴 영상을 포함하며, 컷 없이 큰 움직임을 갖는 다양한 콘텐츠로 이루어져 있습니다. 연구에서는 새로운 비디오 필터링과 계층적 캡션 생성 방법을 도입하여 고품질의 긴 영상 데이터를 선택하고 이들에 시간적으로 밀집된 캡션을 부여합니다. 실험 결과, 본 연구의 데이터셋을 활용해 미세 조정한 모델은 더 우수한 시간적 일관성과 동적 모션을 보여주었습니다. 이러한 데이터를 통해 AI 기반의 긴 영상 생성 기술 발전에 중요한 기여를 할 수 있을 것으로 기대됩니다.