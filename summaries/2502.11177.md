# The Mirage of Model Editing: Revisiting Evaluation in the Wild
## TL;DR
## Summary
- [https://arxiv.org/pdf/2502.11177.pdf](https://arxiv.org/pdf/2502.11177.pdf)

1. 논문의 각 섹션 요약:

- 서론: 이 논문은 대형 언어 모델(LLM)에 대한 모델 편집이 이론적으로는 거의 완벽한 결과를 보이고 있지만, 실제 환경에서의 효과는 밝혀지지 않았다고 지적합니다. 이를 해결하기 위해 모델 편집을 질문 응답(QA) 과제에서 평가하는 새로운 기준(QAEdit)을 제안하고 있습니다.

- QAEdit의 설계: QAEdit는 세 가지 널리 사용되는 QA 데이터셋에서부터 파생된 데이터셋입니다. QA 데이터셋에서 모델의 이전 오류에 대한 성능을 평가하여 현실적인 평가를 가능하게 합니다.

- 문제의식: 이전 연구의 성과가 부풀려져 있을 가능성이 있으며, 실세계 평가에서의 성능 저하 원인을 규명하려고 합니다. 이를 위해 입력, 생성 전략, 출력 절단, 평가 메트릭 등 주요 모듈을 분석합니다.

- 성능 분석: 여러 평가 프레임워크를 통해 모델 편집의 실제 효과를 재검토합니다. 결과적으로, 현재의 편집 방법들이 실제 조건에서 얼마나 잘 작동하는지를 평가합니다. 실제 환경에서의 연속 편집 실험에서 현재 방법들이 잘 확장되지 않는다는 것을 보여줍니다.

2. 논문의 주요 기여 및 혁신:
- QAEdit라는 새로운 벤치마크를 도입하여 실세계 QA 과제에 맞춘 현실적 평가 프로토콜을 설정했습니다.
- 이론적 성과와 현실 세계 시나리오에서의 현저한 성능 차이를 보여줍니다.
- 현재의 평가 관행에서 문제점과 그 근본 원인을 규명합니다.

3. 전체 요약:
이 논문은 대형 언어 모델의 모델 편집을 현실적 시나리오에서 평가하는 프레임워크를 제안하며, 기존 연구 성과가 현실환경에서 부풀려진 것일 수 있음을 보여줍니다. 새로운 QAEdit 벤치마크를 통해 문제의 복잡성을 강조하고, 이론적 성과와 실세계 적용 가능성 사이의 차이를 공론화합니다.