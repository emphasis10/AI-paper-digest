# From RAG to Memory: Non-Parametric Continual Learning for Large Language Models
## TL;DR
## Summary
- [https://arxiv.org/pdf/2502.14802.pdf](https://arxiv.org/pdf/2502.14802.pdf)

1. 각 섹션 요약:

- 소개 및 배경: 매일 변하는 세상에서 지식을 지속적으로 흡수하고 통합하며 활용할 수 있는 능력은 인간의 지능에서 가장 중요한 특징 중 하나입니다. AI 시스템도 이러한 기능을 모방하여 인간 수준의 유용성을 얻어야 합니다. 이 논문에서는 'HippoRAG 2'라는 프레임워크를 통해, 기존의 정보 검색 방식이 진화된 장기 기억과 연결 관계라는 인간의 특성을 구현하는 방법을 소개합니다.

- 관련 연구: 지속학습을 통해 대형 언어 모델(LLM)에 새로운 지식을 쉽게 통합하며 이전 정보를 보존할 수 있는 다양한 기술에 대해 소개됩니다. HippoRAG 2는 기존 HippoRAG 기법을 개선하여, 지식 그래프(KG)를 활용한 효과적인 검색과 그 개선점들을 제안합니다.

- HippoRAG 2의 구성: HippoRAG 2는 OpenIE와 PPR 기법을 활용해 효과성을 증대시키며, 검색된 정보를 활용하여 더 이상 관련성 없는 데이터를 제거하고 온라인 검색을 활성화합니다. 이 기법은 기존의 방법들보다 모든 메모리 과제에서 향상된 성능을 제공합니다.

- 실험 결과: HippoRAG 2는 기존의 RAG 방법보다 더 우수한 사실 기억, 이해력, 연관성 기억 성능을 보여주었으며, 이를 통해 대형 언어 모델을 위한 인간과 유사한 비수치적 지속 학습 시스템의 발전 가능성을 제시합니다.

- 결론: HippoRAG 2는 기존 RAG 시스템의 한계를 해결하고 인간의 장기 기억의 효과성을 보다 가까이 재현하기 위해 다양한 혁신을 제안합니다. 이 연구는 장기 기억과 지속 학습을 위한 새로운 연구 경로를 열고 있습니다.

2. 전체 요약:

이 논문은 인간 장기 기억의 동적이고 연결된 특성을 모방하여 대형 언어 모델의 지속 학습 문제를 해결하기 위한 방법으로 'HippoRAG 2'를 제안합니다. HippoRAG 2는 지식 그래프와 향상된 검색 기법을 사용하여 기존의 방식보다 더 나은 지식 통합 및 검색 기능을 제공하며, 이를 통해 인공지능 시스템을 인간 수준의 장기 기억 능력에 한 발 더 다가가게 합니다.