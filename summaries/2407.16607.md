# Data Mixture Inference: What do BPE Tokenizers Reveal about their Training Data?
## TL;DR
## Summary
- [https://arxiv.org/pdf/2407.16607.pdf](https://arxiv.org/pdf/2407.16607.pdf)

### 1. 섹션 요약

#### 1.1 서론
이 논문은 언어 모델(특히 GPT 시리즈)의 사전 훈련 데이터에 포함된 데이터의 구성 비율을 분석하는 새로운 접근 방식을 소개합니다. 주된 목표는 BPE(Byte-Pair Encoding) 토크나이저의 병합 규칙을 이용해 데이터의 구성 비율을 추론하는 것입니다. 이 방법을 통해 모델의 훈련 데이터가 어떤 구성 요소로 이루어졌는지를 밝힐 수 있습니다.

#### 1.2 데이터 혼합 추론 기법
데이터 혼합 추론 기법은 다양한 언어와 프로그래밍 언어, 그리고 데이터 소스의 비율을 추론하는 데 사용됩니다. 이 논문에서는 BPE 토크나이저의 병합 규칙 목록을 이용해 데이터를 분석하고, 이를 통해 각 데이터 카테고리의 비율을 수학적으로 추정합니다.

#### 1.3 실험 및 결과
논문에서는 자연 언어, 프로그래밍 언어, 데이터 소스 등 다양한 데이터 혼합을 사용해 실험을 진행했습니다. 그 결과, 높은 정확도로 데이터 비율을 추정할 수 있음을 확인했습니다. 특히 상용 토크나이저(GPT-2, GPT-3.5, GPT-4O 등)에 대해 적용한 결과, 그동안 공개되지 않았던 많은 정보를 밝혀냈습니다.

#### 1.4 논의 및 방어 기법
논문에서는 제시된 기법에 대한 가능한 방어 기법들(예: 병합 규칙 변경, 프리토크나이제이션 규칙 숨기기 등)의 효과를 논의합니다. 그러나 이러한 방어 기법들은 대부분 효과가 제한적일 것으로 보입니다.

#### 1.5 결론
이 연구는 BPE 토크나이저의 병합 규칙을 이용해 모델의 사전 훈련 데이터 구성을 밝혀내는 새로운 방법을 제시합니다. 이를 통해 사전 훈련 데이터의 설계 결정을 더 투명하게 만들고, 데이터 편향성 등의 문제를 외부에서 감사할 수 있는 가능성을 열어줍니다.

---

### 2. 전체 요약
이 논문은 BPE 토크나이저의 병합 규칙을 사용하여 언어 모델의 사전 훈련 데이터 구성을 추론하는 새로운 방법을 제시합니다. 다양한 실험을 통해 높은 정확도로 데이터 비율을 추정할 수 있는 방법을 검증했으며, 상용 토크나이저의 데이터 구성 비율을 밝혀냈습니다. 이를 통해 데이터의 투명성을 높이고, 편향성을 감사할 수 있는 가능성을 제공합니다.

## Similar Papers
- [Tuning Language Models by Proxy](2401.08565.md)
- [FP8-LM: Training FP8 Large Language Models](2310.18313.md)
- [Data Mixing Made Efficient: A Bivariate Scaling Law for Language Model Pretraining](2405.14908.md)
- [The Good, The Bad, and The Greedy: Evaluation of LLMs Should Not Ignore Non-Determinism](2407.10457.md)
- [Harvesting Textual and Structured Data from the HAL Publication Repository](2407.20595.md)
- [Is Programming by Example solved by LLMs?](2406.08316.md)
- [Aya 23: Open Weight Releases to Further Multilingual Progress](2405.15032.md)
- [HyperCLOVA X Technical Report](2404.01954.md)
- [Granite Code Models: A Family of Open Foundation Models for Code Intelligence](2405.04324.md)
