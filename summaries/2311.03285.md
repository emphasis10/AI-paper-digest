# S-LoRA: Serving Thousands of Concurrent LoRA Adapters
## TL;DR
## Summary
- [https://arxiv.org/pdf/2311.03285.pdf](https://arxiv.org/pdf/2311.03285.pdf)

### 논문 요약: S-LORA: Serving Thousands of Concurrent LoRA Adapters

#### 1. 소개 (Introduction)
이 논문은 수많은 LoRA 어댑터를 동시에 제공할 수 있는 S-LORA 시스템을 소개합니다. S-LORA는 수많은 LoRA 어댑터를 효율적으로 관리하고, 높은 처리량을 제공하며, 단일 또는 여러 GPU에서 작동할 수 있습니다. LoRA 어댑터는 대형 언어 모델(LLM)을 다양한 작업에 맞게 미세 조정할 때 사용되는 저랭크 적응 방법입니다.

#### 2. 배경 (Background)
LLM은 자연어 처리 및 일반적인 작업에서 우수한 성능을 발휘합니다. 그러나 LLM을 여러 작업에 맞게 미세 조정하는 것은 큰 비용이 듭니다. LoRA는 매개변수를 효율적으로 미세 조정하는 방법으로, LLM의 적응을 위해 소수의 매개변수만 업데이트합니다. 그러나 많은 LoRA 어댑터를 동시에 제공하는 문제는 잘 연구되지 않았습니다.

#### 3. 방법론 (Methodology)
S-LORA는 다음과 같은 주요 기술을 포함합니다:
1. **통합 페이징 (Unified Paging)**: S-LORA는 통합 메모리 풀을 사용하여 동적 어댑터 가중치와 다양한 시퀀스 길이를 가진 KV 캐시 텐서를 관리합니다. 이는 메모리 단편화를 줄이고 배치 크기를 증가시킵니다.
2. **이종 배치 처리 (Heterogeneous Batching)**: S-LORA는 다양한 랭크를 가진 어댑터의 배치 처리를 최적화하기 위해 맞춤형 CUDA 커널을 사용합니다.
3. **텐서 병렬 처리 (Tensor Parallelism)**: S-LORA는 여러 GPU에 걸쳐 효율적으로 병렬 처리를 수행하기 위해 새로운 텐서 병렬 처리 전략을 도입합니다.

#### 4. 실험 (Experiments)
S-LORA는 Llama 모델 시리즈(7B, 13B, 30B, 70B)에서 실험을 통해 우수한 성능을 입증했습니다. S-LORA는 단일 GPU 또는 여러 GPU에서 수천 개의 LoRA 어댑터를 작은 오버헤드로 제공할 수 있습니다. 기존의 최첨단 라이브러리인 HuggingFace PEFT와 비교했을 때, S-LORA는 처리량을 최대 30배 향상시키고, 제공 가능한 어댑터 수를 수백 배 증가시켰습니다.

#### 5. 결론 (Conclusion)
S-LORA는 수많은 LoRA 어댑터를 효율적으로 제공할 수 있는 시스템으로, 다양한 작업에 맞게 미세 조정된 모델을 대규모로 제공할 수 있는 가능성을 열었습니다. 향후 연구에서는 더 많은 어댑터 방법을 지원하고, 커널을 추가로 최적화하며, 여러 CUDA 스트림을 사용하여 기본 모델과 LoRA 계산을 병렬화할 계획입니다.

### 전체 요약
S-LORA는 수많은 LoRA 어댑터를 동시에 제공할 수 있는 효율적인 시스템입니다. 이 시스템은 통합 메모리 풀, 맞춤형 CUDA 커널, 새로운 텐서 병렬 처리 전략을 통해 높은 처리량을 제공하며, 단일 또는 여러 GPU에서 작동할 수 있습니다. 실험 결과, S-LORA는 기존의 최첨단 라이브러리보다 최대 30배 높은 처리량을 제공하고, 제공 가능한 어댑터 수를 수백 배 증가시켰습니다. 이는 다양한 작업에 맞게 미세 조정된 대규모 언어 모델의 효율적인 제공을 가능하게 합니다.

## Similar Papers
- [SPEED: Speculative Pipelined Execution for Efficient Decoding](2310.12072.md)
- [Fast Distributed Inference Serving for Large Language Models](2305.05920.md)
- [FlexGen: High-Throughput Generative Inference of Large Language Models with a Single GPU](2303.06865.md)
- [Efficient Memory Management for Large Language Model Serving with PagedAttention](2309.06180.md)
- [Parrot: Efficient Serving of LLM-based Applications with Semantic Variable](2405.19888.md)
- [LLoCO: Learning Long Contexts Offline](2404.07979.md)
- [BASS: Batched Attention-optimized Speculative Sampling](2404.15778.md)
- [RAFT: Adapting Language Model to Domain Specific RAG](2403.10131.md)
- [KV-Runahead: Scalable Causal LLM Inference by Parallel Key-Value Cache Generation](2405.05329.md)
