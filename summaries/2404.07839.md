# RecurrentGemma: Moving Past Transformers for Efficient Open Language Models
## TL;DR
## Summary
- [https://arxiv.org/pdf/2404.07839.pdf](https://arxiv.org/pdf/2404.07839.pdf)

문서를 분석한 결과, 이 논문은 RecurrentGemma라는 새로운 언어 모델을 소개합니다. 이 모델은 기존의 트랜스포머 모델을 뛰어넘는 효율성을 제공하며, Google의 Griffin 아키텍처를 기반으로 하고 있습니다. 주요 특징은 메모리 사용을 줄이고 긴 시퀀스에 대한 효율적인 추론을 가능하게 하는 고정 크기의 상태를 사용한다는 것입니다.

### 1. 서론
RecurrentGemma는 Griffin 아키텍처를 사용하여 전역 주의 메커니즘 대신 선형 재귀와 지역 주의를 혼합하여 시퀀스를 모델링합니다. 이로 인해 메모리 사용량이 줄어들고, 긴 시퀀스에 대한 추론이 효율적으로 이루어집니다.

### 2. 모델 아키텍처
모델은 입력 임베딩을 모델 너비의 제곱근에 해당하는 상수로 곱하고, 출력 임베딩과 연결하는 단일 변경 사항을 제외하고 기존 Griffin 아키텍처를 따릅니다. 이 아키텍처는 매우 효율적인 언어 모델링을 가능하게 하여 다양한 언어 작업에서 경쟁력 있는 성능을 제공합니다.

### 3. 훈련 세부 사항
RecurrentGemma는 웹 문서, 수학 및 코드로 구성된 주로 영어 데이터를 사용하여 2조 토큰에 대해 사전 훈련을 받았습니다. 이는 Gemma-2B 모델이 사용한 3조 토큰보다 적은 양입니다. 또한, 이 모델은 RLHF 알고리즘을 사용하여 특정 대화 형식을 따르도록 미세 조정되었습니다.

### 4. 평가
자동 벤치마크 및 인간 평가를 통해 평가된 RecurrentGemma는 Gemma-2B와 비슷한 성능을 보였습니다. 인간 평가에서는 Mistral 7B 모델과 비교하여 지시사항을 따르는 작업에서 43.7%의 승률을 기록했습니다.

### 5. 추론 속도 벤치마크
RecurrentGemma는 긴 시퀀스에 대해 더 빠른 추론 속도를 제공하며, 이는 모델의 상태 크기가 고정되어 있기 때문입니다. 이는 메모리 요구 사항을 줄이고, 더 큰 배치 크기에서 추론을 수행할 수 있게 합니다.

### 결론
RecurrentGemma는 Gemma의 성능을 유지하면서 긴 시퀀스에서 더 높은 처리량을 달성합니다. 이는 리소스가 제한된 환경에서 고성능 소형 언어 모델의 새로운 응용을 가능하게 할 것입니다.