# Theia: Distilling Diverse Vision Foundation Models for Robot Learning
## TL;DR
## Summary
- [https://arxiv.org/pdf/2407.20179.pdf](https://arxiv.org/pdf/2407.20179.pdf)

### 1. 섹션별 내용 요약

#### 1.1 서론

Theia는 여러 비전 기반 모델(VFMs)을 결합하여 로봇 학습에 필요한 시각적 표현을 풍부하게 하는 비전 기반 로봇 학습 모델입니다. 이 모델은 다양한 시각적 과제를 위한 여러 VFMs에서 지식을 추출하여 다운스트림 로봇 학습에서 향상된 성능을 제공합니다.

#### 1.2 관련 연구
- **시각적 표현 학습:** 다양한 데이터를 사용하여 로봇 정책을 위한 시각적 표현을 학습합니다.
- **비전 기반 모델:** 단일 또는 복수의 시각 과제를 해결하기 위한 대규모 데이터 기반 모델입니다.
- **지식 증류:** 큰 모델의 지식을 더 작은 모델로 압축하는 방법으로, 기존의 VFM을 더욱 작고 효율적인 형태로 변환합니다.

#### 1.3 방법론
- **개요:** Theia는 여러 VFM의 지식을 작은 모델로 증류하여 로봇 학습에 필요한 공간적 표현을 만듭니다.
- **구조:** 입력 이미지에서 시각 인코더를 통해 풍부한 표현을 생성하고, 이를 통해 다운스트림 로봇 학습 과제를 수행합니다.

#### 1.4 실험
- **모델 성능 분석:** Theia는 기존 모델들과 비교하여 더 적은 계산 비용으로도 향상된 성능을 제공합니다.
- **공간적 토큰 vs. [CLS] 토큰:** 공간적 토큰이 [CLS] 토큰보다 로봇 학습에 더 효과적이라는 결과를 보여줍니다.

#### 1.5 정성적 시각화
Theia의 예측된 표현이 원래의 VFM 표현과 비교했을 때도 합리적인 결과를 나타냅니다. 이는 다양한 시각 이해를 결합하는 것이 유용함을 보여줍니다.

#### 1.6 결론
Theia는 여러 VFMs에서 지식을 증류하여 로봇 학습에 필요한 풍부한 시각적 표현을 생성하며, 이를 통해 향상된 다운스트림 성능을 제공합니다. 실험 결과, 높아진 특징 노름의 엔트로피와 더 나은 로봇 학습 성능 사이의 강한 상관관계를 발견하였습니다.

### 2. 전체 요약

Theia는 CLIP, DINOv2, ViT와 같은 여러 비전 기반 모델(VFMs)의 지식을 결합하여 풍부한 시각적 표현을 생성하고 이를 통해 로봇 학습 성능을 강화하는 새로운 로봇 비전 모델입니다. Theia는 작은 모델이지만 다양한 시각적 과제에 대한 깊은 이해를 포함하고 있으며, 더 나은 다운스트림 로봇 학습 성능과 계산 효율성을 제공합니다. 이를 통해 기존 모델보다 더 적은 데이터와 자원으로도 높은 성능을 달성할 수 있습니다. 또한, 특징 노름 엔트로피와 로봇 학습 성능 사이의 상관관계를 발견함으로써 향후 연구에 유용한 인사이트를 제공합니다.

## Similar Papers
- [Vision Transformers Need Registers](2309.16588.md)
- [LLaRA: Supercharging Robot Learning Data for Vision-Language Policy](2406.20095.md)
- [iVideoGPT: Interactive VideoGPTs are Scalable World Models](2405.15223.md)
- [4M-21: An Any-to-Any Vision Model for Tens of Tasks and Modalities](2406.09406.md)
- [Make-An-Agent: A Generalizable Policy Network Generator with Behavior-Prompted Diffusion](2407.10973.md)
- [An Image is Worth 32 Tokens for Reconstruction and Generation](2406.07550.md)
- [MaGGIe: Masked Guided Gradual Human Instance Matting](2404.16035.md)
- [Model Stock: All we need is just a few fine-tuned models](2403.19522.md)
- [Scaling (Down) CLIP: A Comprehensive Analysis of Data, Architecture, and Training Strategies](2404.08197.md)
