# Prism: A Framework for Decoupling and Assessing the Capabilities of VLMs
## TL;DR
## Summary
- [https://arxiv.org/pdf/2406.14544.pdf](https://arxiv.org/pdf/2406.14544.pdf)

### 요약 - 논문 각 섹션 요약

#### 도입
논문에서는 대형 언어 모델(LLM)의 발전과 함께 비전 언어 모델(VLM)도 크게 발전했음을 강조합니다. VLM은 대규모 멀티모달 데이터를 기반으로 훈련되어 다양한 작업에서 뛰어난 성능을 보이며, 물체 위치 지정, 광학 문자 인식부터 도표 및 다이어그램 이해, 기하학적 문제 해결에 이르기까지 다양합니다. VLM이 일반적인 시각적 질문을 해결하려면 두 가지 중요한 능력, 즉 '지각'과 '추론'이 필요합니다.

#### 방법론
##### Prism 아키텍처
Prism은 시각 질문을 해결하는 과정을 두 단계(지각 모듈과 추론 모듈)로 나누는 모듈형 설계로 특징지어집니다. 지각 모듈(대부분은 VLM)이 이미지를 기반으로 시각 정보를 추출하고 이를 텍스트로 표현합니다. 추론 모듈(대부분은 LLM)은 텍스트 기반 추론을 통해 질문에 대한 답변을 생성합니다.

##### 적합한 벤치마크 분석
Prism을 활용하여 VLM의 지각 및 추론 능력을 평가할 때 다양한 멀티모달 벤치마크를 선택해야 합니다. MMStar를 주요 벤치마크로 사용하며, 이는 시각 정보 없어도 해결될 수 있는 질문들을 배제하고 데이터 누출을 최소화하면서 복잡한 질문을 해결하는 과정에서 지각과 추론이 모두 요구되도록 설계되었습니다.

#### 평가 결과
##### Prism Captioner 성능
PrismCaptioner는 강력한 LLM을 외부 추론 모듈로 사용하여 다양한 멀티모달 벤치마크에서 출중한 성능을 보입니다. Llama3를 통합한 7B 변형이 특히 경쟁력 있는 시각-언어 해석기로 자리잡고 있습니다. 결과적으로 PrismCaptioner는 기존의 엔드 투 엔드 모델보다 성능이 뛰어납니다.

#### 논의
##### 한계와 폭넓은 영향
Prism 프레임워크는 단순히 분석 도구에 그치지 않고 다양한 시각-언어 작업 해결에도 활용될 수 있습니다. 대규모 VLM 훈련에 필요한 대규모 다중 모달 데이터와 계산 자원을 절약할 수 있으며, 맞춤형 응용 프로그램이나 제한된 훈련 데이터가 있는 작업에 유망한 접근 방식입니다. 그러나 사회적 영향도 고려해야 하며, 프리즘은 멀티모달 응용 프로그램 개발의 문턱을 낮추어 잠재적으로 해로운 AI 시스템 개발에 악용될 수 있는 위험을 내포하고 있습니다. 데이터 기반 방법론은 종종 편향을 계승할 수 있으므로 이러한 편향성의 영향을 신중하게 고려해야 합니다.

### 전체 요약
이 논문은 비전 언어 모델(VLM)과 대형 언어 모델(LLM)의 지각과 추론 능력을 분리하여 평가하기 위해 프리즘(Prism) 프레임워크를 소개합니다. Prism은 시각 정보를 추출하는 지각 모듈과 이를 기반으로 답변을 생성하는 추론 모듈로 구성되어 있으며, MMStar 벤치마크를 사용하여 평가되었습니다. PrismCaptioner는 특히 뛰어난 성능을 보이며, 기존 모델보다 비용 효율적이며 성능도 뛰어납니다. 또한, Prism은 대규모 VLM 훈련의 비용 문제를 해결하고 맞춤형 응용 프로그램 개발에 유용하지만, 잠재적으로 해로운 AI 시스템 개발에 악용될 수 있다는 점에서 사회적 영향도 고려해야 합니다.

## Similar Papers
- [AlignGPT: Multi-modal Large Language Models with Adaptive Alignment Capability](2405.14129.md)
- [MMBench-Video: A Long-Form Multi-Shot Benchmark for Holistic Video Understanding](2406.14515.md)
- [VLMEvalKit: An Open-Source Toolkit for Evaluating Large Multi-Modality Models](2407.11691.md)
- [ML-Mamba: Efficient Multi-Modal Large Language Model Utilizing Mamba-2](2407.19832.md)
- [Imp: Highly Capable Large Multimodal Models for Mobile Devices](2405.12107.md)
- [InternLM-XComposer-2.5: A Versatile Large Vision Language Model Supporting Long-Contextual Input and Output](2407.03320.md)
- [OmniCorpus: A Unified Multimodal Corpus of 10 Billion-Level Images Interleaved with Text](2406.08418.md)
- [ANAH-v2: Scaling Analytical Hallucination Annotation of Large Language Models](2407.04693.md)
- [Ferret-v2: An Improved Baseline for Referring and Grounding with Large Language Models](2404.07973.md)
