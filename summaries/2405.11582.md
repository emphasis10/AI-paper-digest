# SLAB: Efficient Transformers with Simplified Linear Attention and Progressive Re-parameterized Batch Normalization
## TL;DR
## Summary
- [https://arxiv.org/pdf/2405.11582.pdf](https://arxiv.org/pdf/2405.11582.pdf)

### 요약

이 논문은 "SLAB: Efficient Transformers with Simplified Linear Attention and Progressive Re-parameterized Batch Normalization"라는 제목으로, Transformers 모델의 계산 효율성을 향상시키기 위한 새로운 방법론을 제안합니다. 이 방법론은 주로 두 가지 혁신적인 요소를 포함합니다: 점진적인 재파라미터화 배치 정규화(PRepBN)와 단순화된 선형 주의(SLA) 모듈입니다.

#### 1. 소개
- Transformers는 자연어 처리와 컴퓨터 비전 작업에서 널리 사용되는 모델 구조입니다.
- 그러나 높은 계산 비용으로 인해 자원 제약이 있는 장치에서 사용하기 어렵습니다.
- 본 연구는 Transformer의 효율성을 높이기 위해 정규화 층과 주의 메커니즘의 계산 병목 현상을 조사합니다.

#### 2. 관련 연구
- 기존 연구는 주의 메커니즘의 계산 복잡성을 줄이기 위해 다양한 방법을 시도했습니다.
- LayerNorm을 BatchNorm으로 대체하여 계산 효율성을 높이려는 시도도 있었습니다.
- 그러나 BatchNorm을 사용할 경우 성능이 저하되고 학습이 불안정해지는 문제가 있습니다.

#### 3. 방법론
1. **점진적인 재파라미터화 배치 정규화 (PRepBN)**:
   - LayerNorm을 BatchNorm으로 점진적으로 대체하여 학습을 안정화하고 성능을 유지합니다.
   - 새로운 재파라미터화 공식인 RepBN을 도입하여 학습 안정성과 성능을 향상시킵니다.

2. **단순화된 선형 주의 (SLA) 모듈**:
   - ReLU를 커널 함수로 사용하고 깊이별 컨볼루션을 통합하여 지역 특징을 강화합니다.
   - 기존의 선형 주의 메커니즘보다 더 효율적이며 성능을 유지합니다.

#### 4. 실험
- 다양한 백본 아키텍처와 벤치마크에서 제안된 방법을 평가했습니다.
- 이미지 분류, 객체 검출, 언어 모델링 등 여러 작업에서 성능을 검증했습니다.
- 예를 들어, ImageNet-1K 데이터셋에서 SLAB-Swin 모델은 83.6%의 정확도를 달성했으며, 이는 기존 모델보다 더 낮은 지연 시간으로 이루어진 성과입니다.

#### 5. 결론
- 본 연구는 Transformer의 효율성을 높이기 위해 정규화 층과 주의 메커니즘을 개선한 방법을 제안했습니다.
- 점진적인 재파라미터화 배치 정규화와 단순화된 선형 주의 모듈을 도입하여 성능을 유지하면서 계산 효율성을 크게 향상시켰습니다.
- 다양한 실험을 통해 제안된 방법의 유효성을 입증했습니다.

### 전체 요약
이 논문은 Transformer 모델의 효율성을 높이기 위해 새로운 방법론을 제안합니다. 주요 기여는 두 가지로, 첫째, LayerNorm을 BatchNorm으로 점진적으로 대체하는 PRepBN 방법을 통해 학습 안정성과 성능을 유지하면서 계산 효율성을 높였습니다. 둘째, SLA 모듈을 통해 주의 메커니즘의 계산 복잡성을 줄이고 성능을 유지했습니다. 제안된 방법은 다양한 실험에서 우수한 성능을 보였으며, 특히 ImageNet-1K 데이터셋에서 높은 정확도와 낮은 지연 시간을 달성했습니다. 이를 통해 Transformer 모델의 실용성을 한층 더 높일 수 있음을 입증했습니다.