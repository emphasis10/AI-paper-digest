# Trans-LoRA: towards data-free Transferable Parameter Efficient Finetuning
## TL;DR
## Summary
- [https://arxiv.org/pdf/2405.17258.pdf](https://arxiv.org/pdf/2405.17258.pdf)

### 1. 섹션별 요약

1. **서론 (Introduction)**
   - **내용 요약**:
     - 최근 언어 모델링의 발전으로 대형 언어 모델 (LLMs)이 개발되었고, 이들이 일반 언어 작업에서 높은 성과를 보이고 있음.
     - 그러나 LLM을 특정 작업에 맞게 미세 조정함으로써 성능을 더욱 높일 수 있음.
     - 이를 위해 대부분 낮은 수의 추가 파라미터만을 훈련시키는 방법인 파라미터 효율적인 미세 조정 (PEFT) 방법이 사용됨.
     - 본 연구에서는 LoRA (Low-Rank Adapters) 모델이 특정 작업에 대해 미세 조정되었을 때 이 모델을 다른 기본 모델로 전이할 수 있는 방법인 'Trans-LoRA'를 제안함.

2. **관련 연구 (Related Work)**
   - **내용 요약**:
     - PEFT와 지식 증류 (Knowledge Distillation) 그리고 합성 데이터를 활용한 기존 연구들을 리뷰함.
     - PEFT 방법들은 다양한 접근 방식으로 선행 모델을 최소한으로 변경하면서도 높은 성능을 유지함.
     - 지식 증류는 더 큰 모델(교사)의 지식을 더 작은 모델(학생)으로 전이하는 방식으로, 데이터가 없는 상황에서 적용이 어려움.
     - 본 연구에서는 합성 데이터를 활용해 PEFT 모델을 전이하는 첫 연구임을 강조함.

3. **Trans-LoRA**
   - **내용 요약**:
     - 소스 모델과 타깃 모델 간의 LoRA 모듈을 거의 데이터 없이 전이하는 방법을 제안.
     - 소수의 '시드' 예제를 활용하여 합성 데이터를 생성하고, 이를 통해 타깃 모델을 훈련시킴.
     - 이 과정에서 생성된 합성 데이터가 실제 훈련 데이터와 통계적으로 구별되지 않도록 설계됨.

4. **지식 증류와 합성 데이터를 통한 성능 전이 (Capabilities transfer through knowledge distillation on synthetic data)**
   - **내용 요약**:
     - GAN (Generative Adversarial Networks)에서 영감을 받은 합성 데이터 시뮬레이터를 구축하여 데이터 증류에 활용.
     - 소스 모델의 데이터를 기반으로 합성 데이터를 생성하고, 이를 통해 타깃 모델을 훈련시킴.
     - 합성 데이터가 실제 데이터와 유사하도록 필터링하는 과정을 통해 효과적인 전이가 가능함.

5. **실험 (Experiments)**
   - **내용 요약**:
     - 다양한 모델과 작업에서 Trans-LoRA의 성능을 검증.
     - BBH, MMLU, GSM8K 등 여러 데이터셋을 사용하여 실험 결과 도출.
     - 실험 결과, Trans-LoRA를 통해 전이된 모델이 원래의 LoRA 모델보다 높은 성능을 보임.
     - 데이터 필터링 및 시드 샘플 수의 조정이 성능에 미치는 영향을 분석함.

6. **결론 및 제한 사항 (Conclusions and Limitations)**
   - **내용 요약**:
     - Trans-LoRA를 활용하면 거의 데이터 없이도 LoRA 모델의 전이가 가능함을 입증함.
     - 본 연구는 PEFT 모델의 전이 가능성을 보여준 첫 연구이며, 이는 향후 연구에 영감을 줄 것임.
     - 합성 데이터 생성과 관련된 추가 계산 비용이 있을 수 있으며, 이를 줄일 방안을 제시함.
     - 또한 일부 작업에서 합성 데이터 생성기가 작업을 잘 이해하지 못하는 한계가 있으며, 이를 완화하는 방안을 논의함.

### 2. 전체 요약

이 논문은 파라미터 효율적인 미세 조정 (PEFT) 방법인 Trans-LoRA를 소개합니다. Trans-LoRA는 대형 언어 모델 (LLMs)에서 특정 작업에 맞게 미세 조정된 LoRA 모델을 다른 기본 모델로 거의 데이터 없이 전이할 수 있는 방법입니다. 이를 위해 소수의 시드 예제를 사용하여 합성 데이터를 생성하고, 이를 통해 타깃 모델을 훈련시켜 소스 모델과 동등하거나 더 나은 성능을 달성합니다. 이 방법은 특히 상업적 클라우드 서비스 환경에서 매우 실용적입니다.

본 연구는 관련된 다양한 PEFT, 지식 증류, 합성 데이터 생성 연구들을 바탕으로 구축되었으며, 다양한 실험을 통해 그 효과성을 입증합니다. 또한, 합성 데이터의 질을 높이기 위해 필터링 과정을 도입하고, 시드 샘플의 개수를 조정하여 성능을 최적화합니다. 결론적으로, Trans-LoRA는 원 데이터에 접근할 필요 없이도 모델 전이가 가능토록 하여, 실무에서의 활용 가능성을 크게 높여줍니다.

## Similar Papers
- [Self-MoE: Towards Compositional Large Language Models with Self-Specialized Experts](2406.12034.md)
- [Yuan 2.0-M32: Mixture of Experts with Attention Router](2405.17976.md)
- [Scaling Granite Code Models to 128K Context](2407.13739.md)
- [Similarity is Not All You Need: Endowing Retrieval Augmented Generation with Multi Layered Thoughts](2405.19893.md)
- [Large Language Model Confidence Estimation via Black-Box Access](2406.04370.md)
- [OpenBezoar: Small, Cost-Effective and Open Models Trained on Mixes of Instruction Data](2404.12195.md)
- [RAG Foundry: A Framework for Enhancing LLMs for Retrieval Augmented Generation](2408.02545.md)
- [DoRA: Weight-Decomposed Low-Rank Adaptation](2402.09353.md)
- [Concise Thoughts: Impact of Output Length on LLM Reasoning and Cost](2407.19825.md)
