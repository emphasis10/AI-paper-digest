# Evaluating the IWSLT2023 Speech Translation Tasks: Human Annotations, Automatic Metrics, and Segmentation
## TL;DR
## Summary
- [https://arxiv.org/pdf/2406.03881.pdf](https://arxiv.org/pdf/2406.03881.pdf)

### 요약

이 종이는 IWSLT 2023 평가 캠페인의 일부로서 다양한 언어 쌍에 대한 말의 번역(ST) 시스템 평가 방법론을 다룹니다. 평가 방법론은 인간 평가와 자동 평가 메트릭의 비교를 통해 다각도로 진행되었고, 새로운 ST 평가 프로토콜을 개발하는 데 중요한 기여를 합니다.

### 각 섹션 요약

#### 1. Introduction (서론)
- **내용 요약:** 인간 평가의 중요성과 번역 시스템 개발에서 완전한 표준 역할. 텍스트 번역에 집중된 기존 연구와 비교해 말 번역 평가의 복잡성을 다룸.
- **중요 포인트:** 말 번역의 특수성(자동 구분 오류, 소음 데이터, 대화 언어 처리 등) 강조.
  
#### 2. Task Description (작업 설명)
- **내용 요약:** IWSLT 2023 평가 작업 개요. 다양한 작업 조건 (오프라인, 다국어, 동시 번역)의 데이터 사용과 자동 평가 프로토콜 소개.
- **중요 포인트:** 각 작업 조건마다 다양하게 사용되는 시스템과 기술 비교, 여러 스피커 및 각기 다른 발음, 자발적 발언 등 복잡한 상황도 포함됩니다.

#### 3. Evaluation Strategy (평가 전략)
- **내용 요약:** 인간 평가 절차와 자동 재분할 방법을 통한 평가 전략 설명.
- **중요 포인트:** 자동 재분할을 통한 시스템 출력의 일관성 유지와 세그먼트 문맥 제공으로 인간 평론가의 평가 신뢰성 확보.
  
#### 4. DA and Automatic Metrics(DA와 자동 메트릭)
- **내용 요약:** chrF와 DA 점수 간의 상관관계 분석. 높은 상관 관계 지수와 통계적 유의미성을 확인.
- **중요 포인트:** chrF가 COMET보다 신뢰도가 높을 수 있다는 지적과 더욱 높은 자동 메트릭 성능도 여전히 필요함.

#### 5. MQM (Multidimensional Quality Metric)
- **내용 요약:** MQM 점수와 다른 스코어 간의 상관 관계 분석.
- **중요 포인트:** DA 점수의 신뢰성을 추가로 검증하며, DA 점수가 MQM 점수와 잘 맞는다는 결론 확인.

#### 6. Conclusion (결론)
- **내용 요약:** 말 번역에서 인간 평가의 중요성을 강조하며, 새로운 평가 절차의 신뢰성 및 COMET의 가능성을 확인.
- **중요 포인트:** 더욱 신뢰할만한 평가 체계를 구축하기 위한 추가 연구 필요.

### 전체 요약
이 연구는 다양한 평가 조건하에 인간 평가와 자동 메트릭의 비교를 통해 IWSLT 2023에서 수행된 말 번역 시스템의 성능을 평가합니다. 자동 재분할과 세그먼트 문맥을 통해 평가의 신뢰성을 높이고, 인간 평가의 중요성을 확인했습니다. COMET 메트릭이 chrF보다 높은 성능을 보였으나, 더 많은 실험이 필요합니다. 이 연구는 지속적인 인간 평가와 신뢰할 수 있는 자동 메트릭의 병행 운영이 필요함을 시사합니다.