# FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning
## TL;DR
## Summary
- [https://arxiv.org/pdf/2307.08691.pdf](https://arxiv.org/pdf/2307.08691.pdf)

### 요약

#### 1. 논문 개요
FlashAttention-2는 GPU의 비대칭 메모리 계층 구조를 활용하여 기존의 Attention 메커니즘을 개선한 알고리즘입니다. 이 논문은 Attention 연산의 비효율적인 작업 분할을 해결하여 속도와 효율성을 대폭 향상시켰습니다. FlashAttention-2는 특히 긴 시퀀스 길이를 처리할 때 더 높은 GPU 자원 점유율을 달성하고, 최적화된 행렬 곱셈(GEMM) 연산의 이론적 최대 FLOPs/s의 50-73%에 도달합니다.

#### 2. 주요 기여 및 혁신
1. **비-행렬 곱셈 FLOPs의 감소**: 알고리즘을 조정하여 비-행렬 곱셈 FLOPs를 줄임으로써 GPU의 행렬 곱셈 처리 유닛의 성능을 극대화했습니다.
2. **Attention 연산의 병렬화**: 시퀀스 길이, 배치 크기, 헤드 수 차원에서 Forward 및 Backward 연산을 병렬화하여 GPU 자원 점유율을 높였습니다.
3. **Thread Block 내 작업 분할 개선**: Thread Block 내에서 Warp 간 통신을 줄이고 공유 메모리 접근을 최소화하여 성능을 향상시켰습니다.

#### 3. 각 섹션 요약

**1. 서론**
Transformer 모델의 컨텍스트 길이를 늘리는 것은 중요한 도전 과제입니다. FlashAttention은 메모리 사용량을 선형으로 줄이고 실행 시간을 크게 단축했지만, 여전히 최적화된 행렬 곱셈(GEMM) 연산에 비해 비효율적입니다. FlashAttention-2는 이러한 비효율성을 해결하여 더욱 빠르고 효율적인 Attention 메커니즘을 제공합니다.

**2. 배경**
GPU의 성능 특성과 실행 모델에 대해 설명하고, 표준 Attention 구현과 FlashAttention의 동작 방식을 소개합니다. FlashAttention은 시퀀스 길이 N에 대해 메모리 사용량을 선형으로 줄이고, 실행 시간을 크게 단축합니다.

**3. FlashAttention-2: 알고리즘, 병렬성 및 작업 분할**
FlashAttention-2는 알고리즘을 미세 조정하여 비-행렬 곱셈 FLOPs를 줄이고, Forward 및 Backward 패스를 시퀀스 길이 차원에서 병렬화합니다. 또한, Thread Block 내 작업을 Warp 간에 효율적으로 분할하여 성능을 극대화합니다.

**4. 실증적 검증**
FlashAttention-2는 FlashAttention 대비 약 2배의 속도 향상을 보이며, 이론적 최대 처리량의 73%에 도달합니다. GPT-스타일 모델을 학습할 때 최대 225 TFLOPs/s의 속도를 달성합니다.

**5. 논의 및 향후 방향**
FlashAttention-2는 기존의 FlashAttention보다 2배 빠르며, 더 긴 시퀀스 길이의 모델을 동일한 비용으로 학습할 수 있게 합니다. 향후 H100 GPU, AMD GPU 등 다양한 장치와 새로운 데이터 유형에 적용할 계획입니다.

### 전체 요약
FlashAttention-2는 Transformer 모델의 Attention 메커니즘을 획기적으로 개선한 알고리즘입니다. 비-행렬 곱셈 FLOPs를 줄이고, 병렬성과 작업 분할을 최적화하여 GPU 자원 점유율을 크게 향상시켰습니다. 이를 통해 긴 시퀀스 길이를 처리하는 모델의 학습 및 추론 속도를 크게 향상시킬 수 있습니다. FlashAttention-2는 특히 대규모 모델 학습 시 뛰어난 성능을 발휘하며, 기존 FlashAttention 대비 약 2배의 속도 향상을 보입니다.

## Similar Papers
- [ByteTransformer: A High-Performance Transformer Boosted for Variable-Length Inputs](2210.03052.md)
- [FlashAttention-3: Fast and Accurate Attention with Asynchrony and Low-precision](2407.08608.md)
- [FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness](2205.14135.md)
- [FlashDecoding++: Faster Large Language Model Inference on GPUs](2311.01282.md)
- [AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration](2306.00978.md)
- [Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality](2405.21060.md)
- [Mamba: Linear-Time Sequence Modeling with Selective State Spaces](2312.00752.md)
- [2BP: 2-Stage Backpropagation](2405.18047.md)
- [Efficient Memory Management for Large Language Model Serving with PagedAttention](2309.06180.md)
