# LVLM-Intrepret: An Interpretability Tool for Large Vision-Language Models
## TL;DR
## Summary
- [https://arxiv.org/pdf/2404.03118.pdf](https://arxiv.org/pdf/2404.03118.pdf)

#### 1. 소개
LVLM-Interpret는 대형 비전-언어 모델(LVLM)의 내부 메커니즘을 이해하고 분석할 수 있는 상호작용 애플리케이션입니다. 이 도구는 이미지 패치의 해석 가능성을 높이고, 모델의 출력이 이미지에 얼마나 잘 기반하고 있는지를 평가할 수 있도록 설계되었습니다. 사용자는 이 애플리케이션을 통해 모델을 체계적으로 조사하고 시스템의 한계를 발견하여 성능 향상을 도모할 수 있습니다.

#### 2. 관련 연구
최근 LLM(대형 언어 모델)들은 놀라운 이해력과 추론 능력을 보이며, 인간의 지시에 따라 출력을 생성할 수 있는 능력을 입증했습니다. 이를 바탕으로 GPT-4V, Qwen-VL, Gemini, LLaVA 등의 연구가 시각적 이해 능력을 추가하여 텍스트와 시각적 작업을 모두 수행할 수 있는 LVLM을 개발했습니다. 그러나 이러한 모델은 여전히 환각 현상(사실이 아닌 정보를 생성하는 문제)에 취약합니다. 이러한 문제를 해결하기 위해 다양한 해석 가능성 도구와 메커니즘이 개발되었습니다.

#### 3. 인터페이스 및 해석 기능
LVLM-Interpret는 Gradio를 사용하여 개발되었으며, 멀티모달 채팅을 위한 표준 레이아웃을 따릅니다. 사용자는 이미지를 업로드하고 멀티모달 쿼리를 발행할 수 있으며, 기본 이미지 편집 기능을 통해 입력 이미지를 변형하여 모델을 조사할 수 있습니다. 모델이 응답을 생성하면 주의 가중치(attention weights)를 저장하고, 이를 시각화하여 사용자가 모델 출력을 해석할 수 있도록 합니다.

- **층별 주의 시각화**: 사용자는 특정 헤드와 층(layer)에 대한 주의 값을 시각화하여 이미지 패치와 쿼리 토큰, 그리고 응답 토큰 간의 상호작용을 조사할 수 있습니다.
- **관련성 지도(Relevancy Map)**: 이 지도는 모델의 결정 과정에서 입력의 각 요소가 출력에 얼마나 기여하는지 시각화하여 해석 가능성을 높입니다.
- **인과 해석(Causal Interpretation)**: 주의 메커니즘의 인과 해석을 통해 주의 값을 기반으로 인과 관계를 설명할 수 있습니다.

#### 4. 사례 연구
MMVP(Multimodal Visual Patterns) 벤치마크 데이터셋을 사용하여 LLaVA 모델을 분석했습니다. 이 데이터셋은 시각적으로 명확한 차이가 있음에도 CLIP 모델에 의해 유사하게 인식되는 이미지 쌍을 다룹니다. 사례 연구를 통해 LLaVA 모델이 텍스트와 이미지 토큰에 어떻게 주의하는지 조사했습니다. 그 결과, LLaVA는 텍스트 토큰에 더 많은 주의를 기울이는 경우가 많았으며, 이는 모델이 이미지 내용과 관계없이 쿼리에 따라 응답을 변경할 수 있게 만듭니다. 반면, 이미지 토큰에 더 많은 관련성을 보이는 경우 질문의 표현 방식에 상관없이 일관된 응답을 보였습니다.

#### 5. 결론 및 향후 방향
LVLM-Interpret는 대형 비전-언어 모델의 응답을 해석할 수 있는 상호작용 도구로, 다양한 해석 가능성 기능을 통해 모델의 내부 메커니즘을 탐구하고 실패 사례를 파악할 수 있습니다. 향후 연구에서는 여러 방법을 통합하여 모델 응답의 이유를 설명할 수 있는 더 포괄적인 메트릭을 개발할 계획입니다.

### 전체 요약
LVLM-Interpret는 대형 비전-언어 모델의 내부 메커니즘을 이해하고 분석할 수 있는 상호작용 애플리케이션입니다. 이를 통해 사용자는 모델의 출력을 해석하고 시스템의 한계를 발견하여 성능을 향상시킬 수 있습니다. 이 도구는 다양한 해석 기능을 제공하며, 사례 연구를 통해 LLaVA 모델의 응답 패턴을 조사했습니다. LVLM-Interpret는 모델의 신뢰성을 높이고, 향후 연구에서는 더욱 포괄적인 해석 방법을 개발할 계획입니다.