# Knesset-DictaBERT: A Hebrew Language Model for Parliamentary Proceedings
## TL;DR
## Summary
- [https://arxiv.org/pdf/2407.20581.pdf](https://arxiv.org/pdf/2407.20581.pdf)

### 1. 각 섹션 요약

#### 서론 (Introduction)
이 논문은 **Knesset-DictaBERT** 모델을 소개합니다. 이는 이스라엘 의회의 의사록 기반의 히브리어 언어 모델로, **DictaBERT** 구조에 기초하여 학습되었습니다. Knesset-DictaBERT는 MLM (Masked Language Modeling) 작업에서 성능을 크게 개선하여 의회 언어의 이해도를 높였습니다.

#### 방법론 (Methodology)
**DictaBERT** 모델을 Knesset Corpus 데이터셋을 활용하여 MLM 작업을 위한 미세 조정(fine-tuning)을 진행했습니다. 모델 초기화는 Hugging Face의 **dicta-il/dictabert** 체크포인트를 사용했고  , 데이터셋 준비와 전처리를 통해 효율적인 로딩 및 처리를 했습니다. 데이터셋은 학습(train), 검증(validation), 테스트(test) 세트로 분류되었으며, 텍스트 샘플을 고정 길이의 토큰 덩어리로 분할하여 모델을 준비했습니다.

#### 데이터셋과 전처리 (Dataset and Data preprocessing)
**Knesset Corpus**는 이스라엘 의회 의사록을 포함한 대규모 히브리어 데이터셋으로, 3200만 문장과 3억 8400만 토큰을 포함합니다. 데이터는 학습, 검증, 테스트 세트로 나누어졌고, 텍스트 샘플은 자동 토크나이저를 사용해 토큰화되었습니다.

#### 학습 절차와 하이퍼파라미터 (Training procedure and hyperparameters)
모델은 NCCL 백엔드를 사용하는 SLURM 환경에서 여러 GPU를 활용해 분산 학습되었습니다. 학습 설정은 장치당 배치 크기 32로 설정되었고, AdamW 옵티마이저의 학습률은 1e−4, 가중치 감소는 0.01로 설정되었습니다. 모델은 2개의 에폭 동안 학습되었으며, 혼합 정밀도(fp16) 모드를 사용해 메모리 사용량을 줄이고 학습 속도를 높였습니다.

#### 실험 결과 (Experiments and Results)
**Knesset-DictaBERT** 모델은 퍼플렉시티(perplexity)와 정확도로 평가되었습니다. Knesset-DictaBERT는 퍼플렉시티 6.60을 기록하여 원본 DictaBERT 모델의 퍼플렉시티 22.87보다 월등히 나은 성능을 보였습니다. 또한, 마스크된 토큰 예측 정확성에서 모든 지표(top-1, top-2, top-5)에서 원본 모델을 능가했습니다.

#### 결론 및 향후 연구 (Conclusion and Future Work)
연구는 **Knesset-DictaBERT** 모델이 히브리어 의회 언어를 이해하고 생성하는 데 뛰어난 성능을 보인다는 결론을 내렸습니다. 향후 연구로는 추가적인 히브리어 데이터셋에 대한 평가와 다른 언어 모델들의 Knesset Corpus에 대한 미세 조정을 고려하고 있습니다.

### 2. 전체 요약
이 논문은 **Knesset-DictaBERT**라는 새로운 히브리어 언어 모델을 소개합니다. 이 모델은 이스라엘 의회의 의사록을 기반으로 학습되었으며, 기존의 **DictaBERT** 모델보다 의회 언어를 더 잘 이해하고, 생성하는 데 있어 높은 성능을 보입니다. 주요 기여점은 의회 언어의 특성을 잘 반영하는 데이터셋을 활용한 미세 조정과 효율적인 학습 절차입니다. 전체 성능 평가에서 **Knesset-DictaBERT**는 퍼플렉시티와 정확성 모든 측면에서 원본 모델보다 우수한 결과를 나타냈습니다.

이 연구는 히브리어 NLP 모델 개발에 있어 중요한 진전을 이루었으며, 향후 다른 히브리어 데이터셋 및 언어 모델에 대한 적용 가능성을 제시합니다.