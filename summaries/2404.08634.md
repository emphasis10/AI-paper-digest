# Pre-training Small Base LMs with Fewer Tokens
## TL;DR
## Summary
- [https://arxiv.org/pdf/2404.08634.pdf](https://arxiv.org/pdf/2404.08634.pdf)

여기서 소개된 연구는 "Inheritune"이라는 새로운 방식으로 소형 기초 언어 모델을 사전 훈련하는 것을 중점적으로 다루고 있습니다. Inheritune은 기존의 대형 언어 모델로부터 몇 개의 트랜스포머 블록(레이어)을 상속받고, 이를 매우 작은 비율(0.1%)의 원래 사전 훈련 데이터로 추가 훈련함으로써 소형 모델을 구축합니다. 이 방법은 데이터와 계산 효율성이 매우 뛰어나며, 단일 A6000 GPU를 사용하여 반나절 이내에 훈련을 완료할 수 있습니다.

### 주요 내용 요약

1. **서론**:
   - 대규모 언어 모델의 사전 훈련은 자원과 데이터의 질 측면에서 도전적입니다. 이 연구에서는 적은 데이터로 소형 기초 언어 모델을 효과적으로 사전 훈련할 수 있는 새로운 방법을 제시합니다.

2. **방법론 (Inheritune)**:
   - Inheritune은 레퍼런스 모델에서 소수의 트랜스포머 블록을 상속받고, 이를 사용하여 타겟 모델을 구축한 다음, 한정된 데이터로 사전 훈련을 수행하는 방식입니다.
   - 이는 기존 모델의 특정 층만을 사용하여 새로운 소형 모델을 초기화하고, 매우 제한된 데이터를 이용하여 추가적인 사전 훈련을 진행합니다.

3. **실험 설정 및 결과**:
   - 1B 토큰만을 사용하여 1.5B 파라미터의 소형 모델을 구축하고, 이 모델이 다양한 벤치마크 데이터셋에서 기존의 소형 모델과 경쟁력 있는 성능을 보여줍니다.
   - Inheritune은 다양한 크기의 소형 모델을 실험적으로 평가하고, 모델 크기에 따른 성능 변화를 분석합니다.

4. **토론 및 결론**:
   - Inheritune을 통해 소형 모델은 적은 데이터와 계산 자원을 사용하여도 높은 효율성과 경쟁력 있는 성능을 달성할 수 있습니다.
   - 이 연구는 향후 소형 모델의 사전 훈련 방법론에 대한 새로운 방향을 제시하며, 데이터 효율성과 환경 지속 가능성을 향상시키는 데 기여할 수 있습니다.

이 연구는 소형 언어 모델의 개발과 사전 훈련에 있어서 경제적이고 효율적인 새로운 방법을 제시함으로써, 리소스가 제한된 환경에서도 효과적인 모델을 구축할 수 있는 가능성을 열어줍니다. 이 방법은 특히 데이터와 계산 자원이 부족한 상황에서 유용할 수 있습니다.