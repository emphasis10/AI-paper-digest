# DISTILLM: Towards Streamlined Distillation for Large Language Models
## TL;DR
## Summary
- [https://arxiv.org/pdf/2402.03898.pdf](https://arxiv.org/pdf/2402.03898.pdf)

이 연구 논문은 "DISTILLM: Towards Streamlined Distillation for Large Language Models"라는 제목으로, 대규모 언어 모델의 지식 증류를 개선하기 위한 새로운 프레임워크인 DISTILLM을 소개합니다. 이 프레임워크는 더 효율적이고 효과적인 증류 방법을 제공하기 위해 고안되었습니다.

### 주요 내용 요약

1. **서론 및 배경**:
   - 대규모 언어 모델의 학습과 운용은 높은 비용을 수반하며, 이를 해결하기 위해 지식 증류 기법이 널리 사용됩니다. DISTILLM은 기존 증류 방식의 한계를 극복하고자 새로운 목적 함수와 효율적인 샘플 활용 방식을 도입합니다.

2. **DISTILLM의 주요 구성**:
   - **Skew KLD 손실 함수**: 이 새로운 손실 함수는 최적화의 안정성을 향상시키고 근사 오류를 최소화하여, 빠른 수렴과 우수한 성능을 달성합니다.
   - **적응형 오프-폴리시 접근 방식**: 학생 생성 출력(SGO)을 효과적으로 활용하여 샘플 효율을 높이고, 훈련-추론 불일치를 최소화합니다.

3. **성능 및 효율성 평가**:
   - DISTILLM은 다양한 생성 작업에서 최신의 증류 기법보다 우수한 성능을 보여주며, 훈련 속도도 2.5배에서 4.3배 빨라졌음을 확인했습니다.

### 혁신적인 부분
DISTILLM의 혁신적인 점은 두 가지 주요 기술의 도입입니다. 첫째, Skew KLD 손실 함수는 이론적 기반을 갖춘 새로운 목적 함수로, 기존 증류 목적 함수가 가진 문제점을 해결합니다. 둘째, 적응형 오프-폴리시 접근 방식은 샘플 효율을 높이고 증류 과정의 효율성을 개선합니다.

이 논문은 대규모 언어 모델의 증류 기술을 한 단계 발전시키며, 향후 다양한 언어 처리 작업에 큰 영향을 미칠 것으로 기대됩니다.

## Similar Papers
- [Towards Fast Multilingual LLM Inference: Speculative Decoding and Specialized Drafters](2406.16758.md)
- [A Careful Examination of Large Language Model Performance on Grade School Arithmetic](2405.00332.md)
- [Direct Preference Knowledge Distillation for Large Language Models](2406.19774.md)
- [Is Programming by Example solved by LLMs?](2406.08316.md)
- [QMoE: Practical Sub-1-Bit Compression of Trillion-Parameter Models](2310.16795.md)
- [HalluciBot: Is There No Such Thing as a Bad Question?](2404.12535.md)
- [Who Says Elephants Can't Run: Bringing Large Scale MoE Models into Cloud Scale Production](2211.10017.md)
- [KAN: Kolmogorov-Arnold Networks](2404.19756.md)
- [PLaD: Preference-based Large Language Model Distillation with Pseudo-Preference Pairs](2406.02886.md)
