# MINT-1T: Scaling Open-Source Multimodal Data by 10x: A Multimodal Dataset with One Trillion Tokens
## TL;DR
## Summary
- [https://arxiv.org/pdf/2406.11271.pdf](https://arxiv.org/pdf/2406.11271.pdf)

### 요약 및 전체 요약

---

#### 1. 각 섹션 요약 및 설명 

**소개 (Introduction)**  

MINT-1T 프로젝트는 현재까지 가장 방대한 다중 모달 인터리브 데이터셋을 구축했습니다. 이 데이터셋은 총 1조 개의 텍스트 토큰과 30억 개의 이미지를 포함하며, HTML, PDF 및 ArXiv와 같은 다양한 출처로부터 수집되었습니다. 이 프로젝트는 오픈 소스 다중 모달 인터리브 데이터셋의 부족을 해결하고자 기획되었습니다.

---

**데이터셋 구성 (Dataset Construction)**  

MINT-1T 데이터셋은 HTML, PDF, ArXiv 문서 등 다양한 형식에서 다중 모달 문서를 소싱하여 구축됩니다. 이 섹션은 문서의 소싱, 저품질 문서 필터링, 데이터 중복 제거 및 부적절한 콘텐츠 제거 방법에 대해 설명합니다. 데이터셋에는 9220억 개의 HTML 토큰, 1060억 개의 PDF 토큰, 그리고 90억 개의 ArXiv 토큰이 포함됩니다.

---

**모델 실험 (Model Experiments)**  

MINT-1T에서 학습된 LMMs(대형 다중 모달 모델)는 기존의 가장 우수한 공개 데이터셋인 OBELICS를 능가할 잠재력을 가지며, 약 10배 더 큰 규모를 제공합니다. MINT-1T의 HTML 서브셋은 VQA(시각 질문 응답) 태스크에서 OBELICS보다 더 나은 성능을 보였으나 캡셔닝 벤치마크에서는 더 낮은 성능을 보였습니다. 그러나 전체 MINT-1T 데이터 소스를 사용한 모델은 대부분의 인-컨텍스트 학습 벤치마크에서 OBELICS를 능가합니다.

---

**결론 (Conclusion)**  

MINT-1T는 오픈 소스 다중 모달 인터리브 데이터셋의 중요한 구성 요소로, 대형 다중 모달 모델 학습에 중요한 리소스를 제공합니다. 이 데이터셋은 중요한 연구 자원을 제공하며, 추가 연구에서는 더 큰 서브셋에서 모델을 학습하고 데이터 품질을 향상시키기 위한 필터링 방법을 개발하며, 다양한 출처에서 다중 모달 시퀀스를 큐레이션할 것을 제안합니다.

---

#### 2. 전체 요약

MINT-1T 프로젝트는 다중 모달 인터리브 데이터셋을 구축하여 텍스트와 이미지 데이터를 대규모로 활용하면서 다양한 출처의 데이터를 조합했습니다. PDF와 ArXiv와 같은 다양한 형식의 데이터를 추가하여 데이터의 다양성과 품질을 높였으며, 이를 통해 LMMs 모델의 성능을 개선하는 데 중점을 두었습니다. 실험 결과, MINT-1T에서 학습된 모델은 기존의 OBELICS 데이터셋을 사용한 모델보다 대부분의 인-컨텍스트 학습 벤치마크에서 더 나은 성능을 보였습니다. 이러한 결과는 MINT-1T 데이터셋이 연구 커뮤니티에서 중요한 리소스로 활용될 수 있음을 보여줍니다.

### 요약 및 설명 끝

#### 주요 기여 및 혁신 부분

MINT-1T의 주요 기여는 기존의 다중 모달 인터리브 데이터셋의 한계를 넘어서는 대규모 데이터셋 구축과 다양한 출처로부터 고품질의 데이터를 수집했다는 점입니다. 이는 모델의 성능을 크게 향상시키며, 데이터의 다양성과 규모를 확장함으로써 연구 커뮤니티에 매우 중요한 자원을 제공합니다. 특히, PDF와 ArXiv 문서를 포함하여 데이터의 범위와 품질을 높였다는 점에서 큰 혁신이 있습니다.

## Similar Papers
- [LongVideoBench: A Benchmark for Long-context Interleaved Video-Language Understanding](2407.15754.md)
- [DataComp-LM: In search of the next generation of training sets for language models](2406.11794.md)
- [Multimodal Table Understanding](2406.08100.md)
- [Large Scale Transfer Learning for Tabular Data via Language Modeling](2406.12031.md)
- [SF-V: Single Forward Video Generation Model](2406.04324.md)
- [StableSemantics: A Synthetic Language-Vision Dataset of Semantic Representations in Naturalistic Images](2406.13735.md)
- [Test of Time: A Benchmark for Evaluating LLMs on Temporal Reasoning](2406.09170.md)
- [Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone](2404.14219.md)
- [ATHAR: A High-Quality and Diverse Dataset for Classical Arabic to English Translation](2407.19835.md)
