# VSSD: Vision Mamba with Non-Casual State Space Duality
## TL;DR
## Summary
- [https://arxiv.org/pdf/2407.18559.pdf](https://arxiv.org/pdf/2407.18559.pdf)

**요약:**

1. **서론**
   본 논문은 비전 상태 공간 모델(Visual State Space Model, VSSD)을 새로운 방식으로 제안합니다. 기존 상태 공간 모델의 인과성 문제를 해결하고, 비인과적 데이터에서의 성능을 향상시킵니다.

2. **관련 연구**
   기존의 비전 트랜스포머와 상태 공간 모델의 한계를 극복하기 위해 다양한 연구가 이루어졌습니다. 주요 연구들은 계산 복잡성을 줄이고 성능을 높이는 데 중점을 두었습니다.

3. **제안하는 방법론**
   - **비인과적 상태 공간 모델(NC-SSD)**: 기존의 인과적 모델을 비인과적 방식으로 변환하여 이미지 데이터에 적합하도록 만들었습니다.
   - **VSSD 모델**: NC-SSD를 기반으로 하여 비전 상태 공간 이중성(VSSD) 모델을 제안합니다. 이를 통해 비인간적 데이터에서도 높은 성능을 발휘합니다.

4. **실험 결과**
   - **분류 성능**: ImageNet-1K 데이터셋에서 기존 모델들을 능가하는 성능을 보였습니다. 특히, 비전 상태 공간 모델 기반의 VMamba보다 약간 높은 정확도를 보였습니다.
   - **세그멘테이션 성능**: ADE20K 데이터셋에서의 단일 및 다중 스케일 테스트에서 높은 mIoU를 기록하였습니다.

5. **결론 및 향후 연구**
   제안한 VSSD 모델은 CNN, ViT, SSM 기반 모델보다도 향상된 성능을 보였습니다. 향후 다양한 비전 응용 분야에 적용될 가능성이 큽니다.

**주요 기여 및 혁신적인 부분:**

1. **비인과적 상태 공간 모델(NC-SSD)의 도입**: 기존의 인과성을 제거하여 비인과적 데이터에 적합한 모델을 개발.
2. **VSSD 모델 제안**: 다양한 기술, 예를 들어 Depth-Wise Convolution과 Feed-Forward Network를 포함하여 모델 성능을 최적화.

---

**전체 요약:**

본 논문에서는 비전 상태 공간 모델(Visual State Space Model, VSSD)을 혁신적으로 개선하여 제안하였습니다. 주요 기여는 기존의 인과성을 제거한 비인과적 상태 공간 모델(NC-SSD)을 도입하고, 이를 기반으로 비전 상태 공간 이중성(VSSD) 모델을 선보인 것입니다. VSSD 모델은 CNN, ViT, SSM 기반 모델보다도 높은 성능을 보이며, 특히 ImageNet-1K 및 ADE20K 데이터셋에서 탁월한 분류 및 세그멘테이션 성능을 보였습니다. 이러한 결과는 다양한 비전 응용 분야에 중요한 기여를 할 것으로 예상됩니다.

## Similar Papers
- [MambaVision: A Hybrid Mamba-Transformer Vision Backbone](2407.08083.md)
- [4-bit Shampoo for Memory-Efficient Network Training](2405.18144.md)
- [ML-Mamba: Efficient Multi-Modal Large Language Model Utilizing Mamba-2](2407.19832.md)
- [EVLM: An Efficient Vision-Language Model for Visual Understanding](2407.14177.md)
- [Sigma: Siamese Mamba Network for Multi-Modal Semantic Segmentation](2404.04256.md)
- [ConvLLaVA: Hierarchical Backbones as Visual Encoder for Large Multimodal Models](2405.15738.md)
- [Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality](2405.21060.md)
- [cosFormer: Rethinking Softmax in Attention](2202.08791.md)
- [CLIP meets Model Zoo Experts: Pseudo-Supervision for Visual Enhancement](2310.14108.md)
