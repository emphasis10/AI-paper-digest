# WalledEval: A Comprehensive Safety Evaluation Toolkit for Large Language Models
## TL;DR
## Summary
- [https://arxiv.org/pdf/2408.03837.pdf](https://arxiv.org/pdf/2408.03837.pdf)

### 중요한 내용 요약

#### 1. 도입 (Introduction)
이 논문은 대형 언어 모델(LLM)의 안전성을 평가하기 위한 포괄적인 도구킷인 WALLEDEVAL을 소개합니다. 이 도구킷은 다양한 모델을 지원하며 35개 이상의 안전성 벤치마크를 포함합니다. 이 프레임워크는 LLM 및 심판(judge) 벤치마킹을 지원하며, 맞춤형 변이를 통해 다양한 텍스트 스타일 변이에 대한 안전성을 테스트할 수 있습니다. 또한, WALLEDGUARD라는 새로운 콘텐츠 모더레이션 도구와 문화적 맥락에서 과장된 안전성을 평가하기 위한 SGXSTEST를 도입합니다.

#### 2. 프레임워크 설계 (Framework Design)
WALLEDEVAL 프레임워크는 데이터셋 로더(HuggingFaceDataset), LLM 로더(HF_LLM), 심판 로더(LLMasaJudge)의 세 가지 주요 클래스 조합으로 구성됩니다. 이를 통해 LLM 벤치마킹(Dataset → LLM → Judge → Score), 심판 벤치마킹(Dataset → Judge → Score), 및 MCQ 벤치마킹(Dataset → Template → LLM → Judge → Score)을 실행할 수 있습니다.

#### 3. LLM 및 심판 평가 (Evaluating LLMs and Judges)
WALLEDEVAL은 유해 행동과 거부 행동이라는 두 가지 유형의 LLM 행동을 벤치마킹합니다.
- **유해 행동**: 데이터셋의 각 위험한 샘플을 LLM에게 프롬프트하여 심판이 그 응답이 안전한지 여부를 점수화합니다.
- **거부 행동**: LLM이 과장된 안전성을 보이는지 여부를 평가합니다. 이는 안전한지 아닌지에 대한 문항으로 구성된 MCQ에 대한 모델의 선택을 점수화합니다.

#### 4. WALLEDGUARD 및 SGXSTEST
- **WALLEDGUARD**: LlamaGuard-3 에 비해 약 16배 작은 새로운 콘텐츠 모더레이션 도구로, 영어 벤치마크에서 강력한 성능을 보입니다.
- **SGXSTEST**: 싱가포르 문화 맥락에서 과장된 안전성을 평가하기 위한 데이터셋으로, 10가지 위험 범주에 걸친 100쌍의 안전-비안전 프롬프트를 포함합니다.

#### 5. 실험 및 토론 (Experiments & Discussions)
여러 테스트를 통해 모델의 유해성과 거부 행동을 벤치마킹하며, 여러 모델(Open-weight & Closed-weight) 및 심판에 대한 다각적인 테스트 결과를 제공합니다. 실험 결과, Llama 및 Gemma 모델이 뛰어난 안전성을 보였으며, WALLEDGUARD가 높은 성능을 보였습니다.

#### 6. 결론 (Conclusion)
WALLEDEVAL은 LLM 및 콘텐츠 모더레이터를 다양한 안전성 평가지표로 벤치마킹하는 도구로, 모델의 유해성과 거부 행동을 테스트하는 데 유용합니다. 새로운 콘텐츠 모더레이터 WALLEDGUARD와 문화적으로 맞춤화된 SGXSTEST도 함께 도입합니다.

### 전체 요약
WALLEDEVAL은 다양한 대형 언어 모델(LLM)의 안전성을 평가하기 위한 포괄적인 도구킷으로, 35개 이상의 안전성 벤치마크를 포함합니다. 이 프레임워크는 데이터셋과 LLM, 심판을 조합하여 LLM의 유해 행동과 거부 행동을 벤치마킹합니다. WALLEDGUARD는 새로운 콘텐츠 모더레이션 도구로, 과장된 안전성을 평가하는 SGXSTEST와 함께 소개되었습니다. 실험 결과, Llama 및 Gemma 모델이 뛰어난 성능을 보였으며, WALLEDGUARD도 높은 성과를 보였습니다.

## Similar Papers
- [Ruby Teaming: Improving Quality Diversity Search with Memory for Automated Red Teaming](2406.11654.md)
- [Mission Impossible: A Statistical Perspective on Jailbreaking LLMs](2408.01420.md)
- [WildGuard: Open One-Stop Moderation Tools for Safety Risks, Jailbreaks, and Refusals of LLMs](2406.18495.md)
- [SeaLLMs 3: Open Foundation and Chat Multilingual Large Language Models for Southeast Asian Languages](2407.19672.md)
- [Towards Fast Multilingual LLM Inference: Speculative Decoding and Specialized Drafters](2406.16758.md)
- [Privacy Preserving Prompt Engineering: A Survey](2404.06001.md)
- [Improving Text Embeddings for Smaller Language Models Using Contrastive Fine-tuning](2408.00690.md)
- [The Instruction Hierarchy: Training LLMs to Prioritize Privileged Instructions](2404.13208.md)
- [ShieldGemma: Generative AI Content Moderation Based on Gemma](2407.21772.md)
