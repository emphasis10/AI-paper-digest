# Jamba-1.5: Hybrid Transformer-Mamba Models at Scale
## TL;DR
## Summary
- [https://arxiv.org/pdf/2408.12570.pdf](https://arxiv.org/pdf/2408.12570.pdf)

### 섹션별 요약 및 설명

#### 1. 요약 (배경 정보 포함)
- **초록 (Abstract)**
  - Jamba-1.5는 Transformer-Mamba 혼합 아키텍처로 만들어진 대규모 언어 모델로, 높은 처리량과 낮은 메모리 사용을 자랑합니다. Jamba-1.5-Large 모델은 94B의 활성 파라미터를 가지고 있으며, 256K 토큰의 컨텍스트 길이에서도 뛰어난 성능을 발휘합니다. 비용 효율적인 추론을 지원하기 위해 ExpertsInt8이라는 새로운 정량화 기술을 도입해, 품질 손실 없이 8개의 80GB GPU에서 실행됩니다.

- **소개 (Introduction)**
  - Jamba-1.5는 Jamba 아키텍처에 기반한 두 개의 대규모 언어 모델입니다. Transformer와 Mamba 계층을 혼합하고 전문화된 기술인 Mixture-of-Experts (MoE) 모듈을 사용해 효율성과 성능을 조화롭게 달성합니다. 특히 256K 토큰의 긴 컨텍스트에서 뛰어난 성능을 보여줍니다.

- **모델 아키텍처 (Model Architecture)**
  - Jamba-1.5-Large 모델은 94B의 활성 파라미터와 398B의 총 파라미터를 가지고 있는 모델입니다. 8개의 Transformer와 Mamba 계층을 포함한 블록 구조로 이루어져 있으며, MoE가 적용된 MLP 계층을 사용합니다. 이 모델은 적응적 정량화 기술을 통해 긴 컨텍스트에서도 효율적으로 동작합니다.

- **학습 및 사후 학습 (Training and Post-training)**
  - Jamba-1.5-Large는 단계별로 훈련되며, 다국어 데이터와 길이가 긴 문서로 구성된 훈련 데이터를 사용해 중간 훈련 단계를 거쳐 최종적으로 다양한 기능과 대화 능력을 부여하는 사후 학습을 합니다. 지도 형식의 미세 조정을 통해 모델의 장기 컨텍스트 이해와 다양한 기술 습득을 동시에 추구합니다.

- **평가 (Evaluation)**
  - 다양한 벤치마크를 통해 Jamba-1.5 모델의 성능을 평가한 결과, 최신 공개 모델과 비교해 우수한 성능을 보이며, 특히 긴 컨텍스트 처리에서 뛰어난 성과를 나타냅니다. 또한 멀티태스크, 다국어 이해 능력에서도 경쟁 모델보다 높은 성능을 보입니다.

- **다국어 능력 (Multilingual Capabilities)**
  - Jamba-1.5는 다양한 언어에서도 우수한 성능을 발휘합니다. MMLU 데이터셋에서 평가된 결과, Jamba-1.5-Mini 모델은 비교 모델보다 좋은 성과를 내고, Jamba-1.5-Large 모델도 높은 다국어 능력을 보여줍니다.

- **결론 (Conclusion)**
  - Jamba-1.5-Large와 Mini 모델은 학술 벤치마크, 챗봇 평가, 그리고 긴 컨텍스트 평가에서 훌륭한 성과를 거둡니다. 특히 긴 컨텍스트에서의 개선된 지연 시간과 처리량은 이 모델의 큰 장점 중 하나입니다. 모델 가중치는 커뮤니티와 함께 공유하여 지속적인 발전을 목표로 합니다.


### 전체 요약

Jamba-1.5는 Transformer와 Mamba 혼합 아키텍처를 사용하는 대규모 언어 모델입니다. 이 모델은 긴 컨텍스트에서도 뛰어난 성능을 발휘하며, 효율적인 메모리 사용과 향상된 처리량을 제공합니다. Jamba-1.5-Large 모델은 94B의 활성 파라미터와 398B의 총 파라미터를 가지고 있어, 256K 토큰의 긴 컨텍스트에서도 우수한 성능을 보입니다. 새로운 정량화 기술인 ExpertsInt8을 통해 비용 효율적인 추론을 지원하며, 다양한 벤치마크와 평가에서 최신 공개 모델과 비교해 뛰어난 결과를 나타냅니다. 특히 다국어 능력에서도 높은 성과를 보이며, 고급 대화 기능을 제공합니다. 이 모델은 커뮤니티와 공유되어 지속적인 발전을 추구합니다.

Jamba-1.5의 주요 기여는 긴 컨텍스트 처리 능력, 효율적인 메모리 사용, 그리고 다양한 벤치마크에서의 뛰어난 성능입니다. 혁신적인 정량화 기술과 혼합 아키텍처를 통해 긴 컨텍스트에서도 높은 성능을 유지하며, 다양한 언어와 태스크에서 우수한 결과를 보입니다. 이러한 특성은 Jamba-1.5 모델을 다양한 응용 분야에 적용할 수 있게 합니다.