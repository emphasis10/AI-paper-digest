# Poisoned LangChain: Jailbreak LLMs by LangChain
## TL;DR
## Summary
- [https://arxiv.org/pdf/2406.18122.pdf](https://arxiv.org/pdf/2406.18122.pdf)

### 섹션별 요약 (논문의 주요 내용)

#### 1. 초록 (Abstract)
이 논문은 간접 탈옥 공격의 개념을 제안하고, LangChain을 이용한 Retrieval-Augmented Generation (RAG)을 통해 이를 구현했습니다. 독이 든 외부 지식 베이스를 활용하여 대형 언어 모델(LLM)에서 악의적 비준수 대화를 생성하게 하였습니다. 이 방법은 여섯 가지 대형 언어 모델에 대해 테스트되었으며, 세 가지 주요 탈옥 문제에서 높은 성공률을 기록했습니다.

#### 2. 소개 (Introduction)
AI 및 대형 언어 모델(LLM)이 일상생활에 깊이 통합되고 있습니다. 하지만 이러한 모델들은 "탈옥"이라 불리는 보안 취약성을 가지고 있습니다. 탈옥 공격은 보안 메커니즘을 회피하고 부적절한 콘텐츠를 생성하게 하는 명령을 통해 이루어집니다. 이에 따라 모델의 방어 능력을 강화하는 것이 중요해졌습니다.

#### 3. 관련 연구 (Related Work)
대형 언어 모델을 대상으로 한 탈옥 공격은 주로 "탈옥 명령어"를 통해 모델 출력을 조작하는 방법을 사용합니다. Retrieval-Augmented Generation (RAG)은 외부 지식 소스를 활용하여 모델 응답의 정확성을 높이는 기술로, 주로 다양한 응용 프로그램에서 사용됩니다. RAG는 검색기, 지식 데이터베이스, LLM으로 구성됩니다.

#### 4. 방법론 (Method)
포이즌드 랭체인(Poisoned LangChain, PLC) 구축 과정을 설명합니다. 이는 세 가지 주요 단계로 구성됩니다: 랭체인 구축, 악성 데이터베이스 생성, 키워드 트리거링. 그리고 실험 모델로는 여섯 가지 중국 대형 언어 모델이 사용되었습니다.

#### 5. 예비 실험 (Preliminary Experiments)
세 가지 유형의 악성 콘텐츠를 통해 PLC의 효과를 평가했습니다: 위험한 행동 선동, 화학 물질 오용, 불법 차별적 행동. 실험 결과, PLC는 높은 성공률로 공격을 수행할 수 있음을 보였습니다. 이는 모델 별로 성공률이 다르긴 하지만, 전반적으로 높은 탈옥 성공률을 기록했습니다.

#### 6. 결론 및 향후 작업 (Conclusion and Future Work)
PLC는 실제 시나리오에서 매우 효과적임을 입증했으며, 이는 향후 방어 전략을 개발하는 데 중요한 기초를 제공합니다. 향후 연구에서는 비악성 지식 베이스를 원격으로 독화하는 방법을 탐구할 계획입니다.

### 전체 요약

이 논문은 Retrieval-Augmented Generation (RAG) 기술을 이용한 LangChain을 통해 간접 탈옥 공격을 구현한 연구입니다. 간접 탈옥 공격은 독이 든 외부 지식 베이스를 사용하여 대형 언어 모델이 악의적이고 비준수한 대화를 생성하게 합니다. 이는 OpenAI의 GPT 시리즈나 Meta의 LLaMA 시리즈와 같은 대형 언어 모델에서 사용되며, 모델의 보안 취약점을 탐지하고 방어 메커니즘을 강화하는 데 중요한 기초를 제공합니다. PLC는 여섯 가지 중국 대형 언어 모델에 대해 테스트되었으며, 세 가지 탈옥 시나리오에서 높은 성공률을 기록했습니다. 향후 연구는 비악성 지식 베이스를 원격으로 독화하는 방법과 새로운 방어 전략을 탐구할 예정입니다.

## Similar Papers
- [AgentPoison: Red-teaming LLM Agents via Poisoning Memory or Knowledge Bases](2407.12784.md)
- [Can LLMs be Fooled? Investigating Vulnerabilities in LLMs](2407.20529.md)
- [Merging Improves Self-Critique Against Jailbreak Attacks](2406.07188.md)
- [The Art of Refusal: A Survey of Abstention in Large Language Models](2407.18418.md)
- [Best Practices and Lessons Learned on Synthetic Data for Language Models](2404.07503.md)
- [A False Sense of Safety: Unsafe Information Leakage in 'Safe' AI Responses](2407.02551.md)
- [Phi-3 Safety Post-Training: Aligning Language Models with a "Break-Fix" Cycle](2407.13833.md)
- [RAG and RAU: A Survey on Retrieval-Augmented Language Model in Natural Language Processing](2404.19543.md)
- [Searching for Best Practices in Retrieval-Augmented Generation](2407.01219.md)
