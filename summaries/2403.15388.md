# LLaVA-PruMerge: Adaptive Token Reduction for Efficient Large Multimodal Models
## TL;DR
## Summary
- [https://arxiv.org/pdf/2403.15388.pdf](https://arxiv.org/pdf/2403.15388.pdf)

### AI 및 기계 학습 논문 요약

#### 1. 각 섹션의 중요 내용 요약
논문을 크게 섹션별로 요약하였습니다. 각 섹션의 주요 내용을 이해하기 쉽게 설명하겠습니다.

##### 서론 (Introduction)
이 섹션에서는 대규모 멀티모달 모델(LMM)의 필요성과 도입 배경을 설명합니다. LMM은 텍스트 생성뿐만 아니라 시각적 정보를 종합하여 강력한 추론 능력을 제공합니다. 하지만 시각적 토큰의 수가 많아질수록 연산 비용이 기하급수적으로 증가한다는 문제점을 지적합니다.

##### 관련 연구 (Related Work)
LMM의 효율성을 높이기 위한 기존 연구들을 설명합니다. 기존 연구들은 주로 모델의 백본 크기를 줄이는 방식으로 효율성을 추구했지만, 시각적 토큰의 수를 줄이는 접근은 부족했다고 지적합니다.

##### 방법론: 토큰 프루-머징 (Method: Token Pru-Merging)
이 섹션에서는 시각적 토큰 감소를 위한 새로운 방법인 Token Pru-Merge를 소개합니다. 이 방법은 중요 토큰 선별(AITS)과 유사 키 클러스터링(Token Supplement)을 통해 시각적 정보를 손실 없이 효율적으로 처리할 수 있게 합니다.

##### 실험 결과 (Experiments)
LLaVA-1.5와 같은 최신 LMM에 Token Pru-Merge을 적용하여 다양한 벤치마크 테스트에서 유사한 성능을 유지하면서도 시각적 토큰 수를 크게 줄일 수 있음을 보여줍니다. 특히 비디오와 이미지에서 높은 효율성을 입증하였습니다.

##### 결론 (Conclusion)
이 섹션에서는 중요한 시각적 토큰을 유지하면서 효율성을 향상시키는 Token Pru-Merge 방법을 통해 연산 비용을 크게 절감할 수 있음을 다시 한번 강조합니다. 이 연구는 LMM의 성능과 효율성 간의 균형을 모색하는 데 있어 중요한 기여를 했다고 평가됩니다.

#### 2. 논문의 전체 요약

이 논문은 대규모 멀티모달 모델(LMM)의 효율성을 높이기 위해 새로운 토큰 감소 방법인 Token Pru-Merge를 제안합니다. LLM(대규모 언어 모델)과 시각적 인코더를 결합한 LMM은 강력한 추론 능력을 제공하지만, 시각적 토큰의 수가 많아질수록 연산 비용이 기하급수적으로 증가하는 문제가 있습니다. 이를 해결하기 위해, 논문에서는 중요 시각 토큰을 선택하고 클러스터링하여 불필요한 토큰을 삭제하는 방법을 제안합니다.

주요 기여는 Adaptive Important Token Selection (AITS)와 Token Supplement (TS)를 통해 시각적 정보를 효율적으로 처리하면서도 성능을 유지하는 것입니다. 실험 결과, LLaVA-1.5와 같은 최신 LMM에 이 방법을 적용하여 다양한 벤치마크 테스트에서 성능 저하 없이 시각적 토큰을 평균 14배 줄일 수 있음을 입증하였습니다. 이 연구는 경제적 비용과 효율성 간의 균형을 맞추는 데 중요한 발전을 이루었으며, LMM의 실질적 적용 가능성을 크게 확대했습니다.