# LLM See, LLM Do: Guiding Data Generation to Target Non-Differentiable Objectives
## TL;DR
## Summary
- [https://arxiv.org/pdf/2407.01490.pdf](https://arxiv.org/pdf/2407.01490.pdf)

### 1. 섹션별 내용 요약

#### 1. 서론 (Introduction)
이 논문은 합성 데이터를 활용하여 대규모 언어 모델(LLM)의 특성과 선호도를 어떻게 조정할 수 있는지에 대해 다룹니다. 고품질 레이블 데이터의 높은 비용과 데이터 커버리지 확대의 어려움 때문에, 기존 데이터의 최적화 연구가 많이 진행되고 있습니다. 이 논문에서는 합성 데이터를 통한 '수동 속성 상속(passive inheritance)' 및 '능동 속성 상속(active inheritance)'을 통해 모델의 생성을 원하는 속성으로 유도할 수 있는지 탐구합니다.

#### 2. 관련 연구 (Related Work)
LLM 순환성(LLM circularity)에 대한 연구로, LLM이 다른 모델에 영향을 미치는 연구와 데이터 증폭 및 다양성을 상실하게 하는 연구가 있습니다. 이 논문은 기존 연구와 달리 다양한 LLM으로 생성된 합성 데이터가 모델의 특성(사회적 편향, 유해성 등)에 미치는 영향을 조사합니다.

#### 3. 방법론 (Methodology)
논문의 방법론 부분에서는 합성 데이터를 통한 모델 학습 방법과 데이터 속성 측정에 대해 설명합니다. '수동 속성 상속'을 연구하여 합성 데이터 통합의 결과를 체계적으로 분석하고, '능동 속성 상속'을 통해 특정 속성을 갖도록 모델 생성을 유도하는 방법을 제안합니다.

#### 4. 실험 (Experiments)
두 모델(LLaMa2-7B, Mixtral-8x7B)을 대상으로 합성 데이터를 사용하여 실험을 진행했습니다. 다양한 메트릭을 사용해 특성이 상속되는 방식을 프로파일링(profiling)하고, 조정된 데이터로 모델의 특성을 조정할 수 있는지 확인합니다.

#### 5. 결과 (Results)
수동 속성 상속 실험에서는 모델이 합성 데이터에 민감하게 반응하며, 능동 속성 상속을 통해 데이터 샘플링을 조정해 원하는 특성을 강화하고 독성을 줄일 수 있음을 확인했습니다. 예를 들어, 독성 감소 실험에서는 독성 점수가 20-40% 감소했습니다.

#### 6. 토론 (Discussion)
결과를 바탕으로 모델 속성에 대한 통찰력을 제공합니다. 데이터 샘플링을 통해 LLM의 다양한 속성을 조정할 수 있음을 증명하며, 능동 속성 상속은 특히 비분화(non-differentiable) 목표를 달성하는 데 유용합니다.

#### 7. 결론 (Conclusion)
합성 데이터가 LLM에 미치는 영향을 조사하고, 능동 속성 상속 전략이 모델 생성을 원하는 속성으로 유도하는 데 효과적임을 확인했습니다. 이 연구는 합성 데이터 사용의 비의도적인 결과를 이해하고, 모델을 이상적인 생성 프로파일로 조정하는 데 기여합니다.

### 2. 전체 요약
이 논문은 합성 데이터를 활용해 LLM(대규모 언어 모델)의 특정 속성을 조정하고 최적화하는 방법을 탐구합니다. 기존 방법의 한계를 극복하기 위해 '수동 속성 상속'과 '능동 속성 상속' 접근법을 사용하여 모델 특성을 원하는 대로 조정하는 방법을 실험하고 제안합니다. 실험 결과, 합성 데이터를 통해 모델의 유해성을 줄이거나, 텍스트의 길이와 다양한 속성을 과도하게 조정할 수 있음을 확인했습니다. 능동 속성 상속은 비분화 목표를 달성하기 위한 안정적이고 해석 가능한 방법으로, 모델의 속성을 원하는 방식으로 유도하는 데 유용합니다. 이 연구는 합성 데이터 사용의 비의도적인 결과를 이해하고, 이를 통해 AI 모델을 더 효과적으로 개선하는데 중요한 기여를 합니다.

## Similar Papers
- [The Art of Saying No: Contextual Noncompliance in Language Models](2407.12043.md)
- [Back to Basics: Revisiting REINFORCE Style Optimization for Learning from Human Feedback in LLMs](2402.14740.md)
- [Towards Reliable Latent Knowledge Estimation in LLMs: In-Context Learning vs. Prompting Based Factual Knowledge Extraction](2404.12957.md)
- [Confidence Regulation Neurons in Language Models](2406.16254.md)
- [On LLMs-Driven Synthetic Data Generation, Curation, and Evaluation: A Survey](2406.15126.md)
- [LoRA Land: 310 Fine-tuned LLMs that Rival GPT-4, A Technical Report](2405.00732.md)
- [PLaD: Preference-based Large Language Model Distillation with Pseudo-Preference Pairs](2406.02886.md)
- [CodecLM: Aligning Language Models with Tailored Synthetic Data](2404.05875.md)
- [Meta-Rewarding Language Models: Self-Improving Alignment with LLM-as-a-Meta-Judge](2407.19594.md)
