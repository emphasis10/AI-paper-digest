# Tied-Lora: Enhancing parameter efficiency of LoRA with weight tying
## TL;DR
## Summary
- [https://arxiv.org/pdf/2311.09578.pdf](https://arxiv.org/pdf/2311.09578.pdf)

### 요약

#### 1. 소개
이 논문은 **Tied-LoRA**라는 새로운 패러다임을 소개합니다. 이는 Low-rank Adaptation (LoRA)의 매개변수 효율성을 높이기 위해 **weight tying**과 **선택적 학습**을 결합한 방법입니다. 다양한 과제와 두 가지 언어 모델을 사용한 실험을 통해 성능과 학습 가능한 매개변수 수 사이의 최적의 균형을 찾아내는 것을 목표로 합니다.

#### 2. 주요 기여
1. **Tied-LoRA 구성**: 다양한 Tied-LoRA 구성을 제안하여 LoRA의 매개변수 효율성을 높입니다.
2. **실험적 검증**: 실제 환경과 유사한 다양한 과제에서 Tied-LoRA 구성의 성능을 연구합니다.
3. **최적 구성 제안**: 성능을 유지하면서 매개변수를 줄이기 위한 TL6(vB®uA®) 구성을 제안합니다. 이는 LoRA와 비교해 성능 차이가 1~2% 내외이면서도 매개변수 사용량은 12.5%에 불과합니다.

#### 3. 방법론
**Tied-LoRA**는 큰 언어 모델을 저차원으로 미세 조정하기 위해 weight tying과 선택적 학습을 사용합니다. 이러한 구성은 모든 계층에 걸쳐 매개변수를 공유함으로써 학습 가능한 매개변수의 수를 크게 줄입니다. 이는 저차원 경로가 비선형성을 도입하지 않기 때문에 기본 모델 가중치에 통합할 수 있어 추론 시간에 추가적인 지연을 발생시키지 않습니다.

#### 4. 실험
**다양한 과제**에서 Tied-LoRA 구성의 성능을 평가합니다:
- **추출형 질문 응답**: SQuADv1 데이터셋 사용
- **요약**: DialogSum 데이터셋 사용
- **상식적 자연어 추론 (NLI)**: HellaSwag 데이터셋 사용
- **번역**: IWSLT 2017 독일어-영어 번역 데이터셋 사용
- **수학적 추론**: GSM8K 벤치마크 사용

실험 결과, TL6(vB®uA®) 구성은 대부분의 과제에서 LoRA와 비슷한 성능을 보였으며, 번역 과제에서는 LoRA를 능가하는 성과를 보였습니다.

#### 5. 결론 및 미래 연구 방향
이 논문은 **Tied-LoRA**가 LoRA의 매개변수 효율성을 크게 향상시키는 방법임을 입증합니다. 특히 번역 과제에서 12.5%의 매개변수만 사용하면서도 LoRA를 능가하는 성능을 보였습니다. 앞으로 더 큰 기본 모델을 사용하여 Tied-LoRA 방법의 확장성과 효율성을 평가할 계획입니다.

### 전체 요약
**Tied-LoRA**는 LoRA의 매개변수 효율성을 높이기 위해 weight tying과 선택적 학습을 결합한 혁신적인 접근 방식입니다. 이 방법은 다양한 실제 과제에서 적은 매개변수로도 LoRA와 유사한 성능을 유지할 수 있음을 보여줍니다. 특히 번역 과제에서는 12.5%의 매개변수만으로도 더 나은 성과를 보였습니다. 향후 연구는 더 큰 모델과 다양한 과제에서 Tied-LoRA의 효율성을 검증할 계획입니다.

## Similar Papers
- [LoRA: Low-Rank Adaptation of Large Language Models](2106.09685.md)
- [Parameter-Efficient Fine-Tuning with Discrete Fourier Transform](2405.03003.md)
- [The Unreasonable Ineffectiveness of the Deeper Layers](2403.17887.md)
- [PiSSA: Principal Singular Values and Singular Vectors Adaptation of Large Language Models](2404.02948.md)
- [OLoRA: Orthonormal Low-Rank Adaptation of Large Language Models](2406.01775.md)
- [DoRA: Weight-Decomposed Low-Rank Adaptation](2402.09353.md)
- [TIES-Merging: Resolving Interference When Merging Models](2306.01708.md)
- [Q-GaLore: Quantized GaLore with INT4 Projection and Layer-Adaptive Low-Rank Gradients](2407.08296.md)
- [SPAFIT: Stratified Progressive Adaptation Fine-tuning for Pre-trained Large Language Models](2405.00201.md)
