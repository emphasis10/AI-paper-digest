# Fourier Position Embedding: Enhancing Attention's Periodic Extension for Length Generalization
## TL;DR
## Summary
- [https://arxiv.org/pdf/2412.17739.pdf](https://arxiv.org/pdf/2412.17739.pdf)

### 1. 각 섹션의 중요한 내용 요약

#### 서론
이 논문은 언어 모델(LM)에서 길이 일반화를 개선하기 위해 로터리 위치 임베딩(RoPE)을 개선하는 방법을 제안합니다. 주로 디스크리트 신호 처리(DSP) 이론을 사용하여 RoPE 기반 주의력 메커니즘을 분석하고, 이로 인해 생기는 주파수 왜곡이 RoPE의 주기성을 악화시키는 문제를 다룹니다.

#### 비기존의 디스크리트 푸리에 변환
RoPE는 비정형 디스크리트 푸리에 변환을 통해 주기적 주의력을 구현하지만, 선형 층과 활성화 함수, 불충분하게 훈련된 주파수 성분으로 인한 스펙트럼 손상이 발생합니다.

#### 제안: 푸리에 위치 임베딩(FoPE)
FoPE는 RoPE의 문제점을 보완하여 주의력의 주기적 확장을 개선하고 길이 일반화를 높입니다. FoPE는 각 차원을 푸리에 시리즈로 모델링하여 다양한 주파수 정보를 분리하고, 훈련이 불충분한 주파수 성분을 제거합니다.

#### 실험
FoPE는 여러 모델 크기 및 데이터셋에서 RoPE와 ALiBi보다 뛰어난 성능을 보입니다. 다양한 실험 및 분석을 통해 FoPE의 효과를 검증합니다.

#### 결론
FoPE는 주의력의 주파수 도메인 특성을 강화하여 길이 일반화를 개선합니다. 이를 통해 다양한 과제와 데이터셋에서 우수한 성능을 입증하였습니다.

### 2. 논문의 전체 요약
이 논문은 RoPE의 한계를 넘어 언어 모델의 길이 일반화를 개선하는 새로운 방법론을 제안하고, 이를 위해 퓨리에 위치 임베딩(FoPE)을 제안합니다. FoPE는 주파수 도메인에서의 문제점을 해결하며 기존의 RoPE와 ALiBi 방법론보다 우수한 성능을 보여줍니다. 이 연구는 주로 디스크리트 신호 처리 이론에 기반하여 주의력의 주기적 확장을 최적화하고, 모델의 고유한 주파수 특성을 강화함으로써 다양한 길이에서의 일반화를 달성합니다.