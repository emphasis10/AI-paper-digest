# Chain of Thought Empowers Transformers to Solve Inherently Serial Problems
## TL;DR
## Summary
- [https://arxiv.org/pdf/2402.12875.pdf](https://arxiv.org/pdf/2402.12875.pdf)

### 주요 내용 요약

1. **서론 및 배경**:
   - 이 논문에서는 대규모 언어 모델(LLMs)이 수학적 문제 해결 및 코드 생성 등의 복잡한 추론 작업에서 뛰어난 능력을 보인다고 설명합니다. 이러한 능력의 핵심은 중간 단계를 생성하는 것입니다. 이 연구는 "chain of thought(CoT)"가 디코더 전용 트랜스포머의 표현력을 향상시키는 이유를 이론적으로 분석합니다.

2. **방법론**:
   - 트랜스포머가 CoT를 사용하면 더 많은 직렬 계산을 수행할 수 있으며, 이를 통해 복잡한 문제를 해결할 수 있습니다. CoT 없이 트랜스포머는 병렬 계산이 어려운 작업을 처리할 수 없지만, CoT를 사용하면 이러한 한계를 극복할 수 있습니다.

3. **실험**:
   - 트랜스포머가 모듈러 덧셈, 순열 구성, 반복 제곱, 회로 값 문제 등의 네 가지 핵심 문제를 해결하는 능력을 평가합니다. CoT를 사용하면 이러한 문제에서 트랜스포머의 성능이 크게 향상됨을 보여줍니다.

### 혁신적인 부분
이 논문의 혁신성은 트랜스포머가 CoT를 사용하여 더 많은 직렬 계산을 수행할 수 있도록 함으로써 복잡한 문제를 해결하는 능력을 크게 향상시킨다는 점에 있습니다. 이는 특히 병렬 계산이 어려운 문제에서 CoT가 트랜스포머의 표현력을 확장할 수 있음을 이론적으로 증명합니다.

## Similar Papers
- [Let's Think Dot by Dot: Hidden Computation in Transformer Language Models](2404.15758.md)
- [Improve Mathematical Reasoning in Language Models by Automated Process Supervision](2406.06592.md)
- [The Illusion of State in State-Space Models](2404.08819.md)
- [Reasoning in Large Language Models: A Geometric Perspective](2407.02678.md)
- [LiteSearch: Efficacious Tree Search for LLM](2407.00320.md)
- [Transformers Can Represent $n$-gram Language Models](2404.14994.md)
- [Transformers Can Do Arithmetic with the Right Embeddings](2405.17399.md)
- [TernaryLLM: Ternarized Large Language Model](2406.07177.md)
- [How Smooth Is Attention?](2312.14820.md)
