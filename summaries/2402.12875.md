# Chain of Thought Empowers Transformers to Solve Inherently Serial Problems
## TL;DR
## Summary
- [https://arxiv.org/pdf/2402.12875.pdf](https://arxiv.org/pdf/2402.12875.pdf)

### 1. 섹션 별 주요 내용 요약

**서론**
이 논문은 체인 오브 쏘트(Chain of Thought, CoT)가 어떻게 대형 언어 모델(LLMs)의 추론 능력을 향상시키는지를 연구합니다. CoT는 모델이 중간 단계의 시퀀스를 생성하게 하여 복합적인 문제를 해결하는 데 있어 필수적입니다.

**표현력의 이론적 배경**
CoT를 갖춘 변환기(Transformer)는 문제를 푼 후 자동 회귀적으로 중간 단계를 생성하여 복잡한 문제를 해결할 수 있습니다. 이 중간 단계들은 CoT가 없을 때보다 더 많은 직렬 계산을 가능하게 합니다. CoT는 낮은 깊이에서 Serial 계산 능력을 증가시켜, 평행 계산만 할 수 있는 것에서 향상된 결과를 보입니다.

**결론 및 혁신적인 기여**
CoT는 모델이 문제 풀이의 중간 단계를 생성하도록 하여, 결과적으로 더 복잡한 문제까지도 해결할 수 있도록 하는 것이 그 주요 기여입니다. 특히, CoT는 평행 계산을 넘어, 순차적 계산이 필요한 문제들에도 적용 가능함을 보여주었습니다.

### 2. 전반적인 요약

이 논문은 CoT가 대형 언어 모델의 추론 능력을 어떻게 확장시키는지를 설명합니다. CoT를 통한 중간 과정의 생성은 단순히 모델에게 지시하는 것만으로도 성능 향상의 큰 부분을 차지합니다. 이는 모델이 더욱 복잡한 문제를 해결할 수 있도록 하며, 직렬 문제에서도 비약적인 계산 능력 향상을 제공합니다.

이 요약은 AI의 발전에 있어 중요한 통찰을 제공하며, 이러한 설명을 통해 보다 나은 방향으로 AI 발전을 도모할 수 있을 것입니다.