# LongDPO: Unlock Better Long-form Generation Abilities for LLMs via Critique-augmented Stepwise Information
## TL;DR
## Summary
- [https://arxiv.org/pdf/2502.02095.pdf](https://arxiv.org/pdf/2502.02095.pdf)

### 1. 각 섹션의 주요 내용 요약 (한국어)

#### 서론
최근 대규모 언어 모델(LLM)의 발전으로 긴 텍스트를 처리하는 능력이 크게 향상되었지만, 고품질의 장기 산출물 생성에는 여전히 난제가 남아 있습니다. 이러한 장기 산출물은 학술 논문, 소설, 법률 계약, 코드 생성에 필수적입니다. 기존 방법들은 결과 감독을 통해 개선을 시도했지만, 중간 단계에 대한 세부적인 피드백이 부족하여 일관성과 품질 저하에 기여하고 있습니다.

#### LongDPO 제안
LongDPO는 장기 생성 능력을 향상시키기 위해 고안된 방법입니다. 이 방법은 단계별 감독을 통해 좋지 않은 선택을 개선하고 일관성 있는 결과를 유지합니다. Monte Carlo Tree Search (MCTS) 기법을 활용해 단계별 선호 쌍을 수집하고, 외부 비판적 피드백을 통합하여 저품질 후보를 개선합니다.

#### 방법론
LongDPO는 두 가지 주요 구성 요소로 나누어집니다: 1) 단계별 선호 데이터 수집, 2) 수집된 선호 데이터를 활용한 DPO 훈련. MCTS를 사용하여 수집된 선호 쌍과 전역 메모리 풀을 통해 사실적 일관성을 유지하는 방법을 설명합니다.

#### 결과
LongDPO의 실험 결과는 긴 형식의 텍스트 생성 성능이 크게 향상되었음을 보이며, 일반적인 벤치마크에서도 손실 없는 성능을 유지합니다. 이 방법은 특히 인간 피드백과 잘 일치하는 결과를 생성하는 데 기여하였습니다.

#### 결론
LongDPO는 대규모 언어 모델의 장기 생성 능력을 향상시킬 수 있는 효과적인 방법으로 평가되었습니다. 이 방법은 법률, 코드 생성 등의 분야에서도 유망한 결과를 보여줄 것으로 기대됩니다.

### 2. 전체 요약
LongDPO는 대규모 언어 모델의 장기 형식 생성 능력을 향상시키기 위해 개발된 새로운 접근 방식입니다. 기존의 결과 감독 방식을 넘어 단계별 감독을 도입하여 일관성과 품질을 높이는 데 성공했습니다. MCTS를 활용하여 상세하고 일관된 데이터를 수집하고, 외부 비판적 피드백을 통해 저품질 후보의 개선을 도모합니다. 전체적으로 LongDPO는 긴 형식 생성에서의 성능을 향상시키며, 일반 작업에서도 손실 없는 결과를 유지함으로써 인간의 피드백과 더 밀접하게 연관된 출력을 생성할 수 있는 가능성을 보여줍니다.