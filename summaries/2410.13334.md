# Do LLMs Have Political Correctness? Analyzing Ethical Biases and Jailbreak Vulnerabilities in AI Systems
## TL;DR
## Summary
- [https://arxiv.org/pdf/2410.13334.pdf](https://arxiv.org/pdf/2410.13334.pdf)

### 요약

#### 소개
이 연구는 대형 언어 모델(LLM)이 가지고 있는 잠재적인 안전 문제를 탐구합니다. 악성 입력이 모델을 해킹하는 '탈옥' 현상을 조사하며, 개발자들이 이를 막기 위해 적용한 다양한 안전 조치가 오히려 모델을 해킹하는데 사용될 수 있는 의도적 편향을 도입하고 있다는 것을 보여줍니다.

#### 배경 및 관련 연구
LLM의 안전 정렬을 위한 기존 방법에는 데이터 필터링, 감독된 미세 조정 및 인간 피드백을 통한 강화 학습이 포함됩니다. 그러나 이러한 방법들이 '탈옥' 공격으로 이어질 수 있는 취약성을 노출시키도록 하는 문제점을 지닙니다.

#### PCJailbreak 방법론
PCJailbreak 방법은 정치적 정당성을 위한 고정된 형식을 사용하여 탈옥을 시도합니다. 이는 스케일링 문제를 해결할 수 있으며, 주의력 기반 공격과 달리 본질적인 의미를 상당히 유지하도록 설계되었습니다..

#### PCDefense
PCDefense는 추가적인 모델 없이 편향을 조정하여 탈옥을 막는 방법입니다. 이는 비용 효율적이며 모델의 성능을 유지하면서 안전을 보장하는 데 효과적입니다.

#### 실험 결과
여러 모델에 대한 실험 결과에 따르면 의도적으로 주입된 편향이 탈옥 시도에 대한 취약성에 영향을 미치는 것으로 나타났습니다. 특히 최신 GPT-4o 모델은 마이너리티와 기득권 그룹 사이에 탈옥 성공률에 상당한 차이를 보입니다.

#### 결론 및 기여도
이 연구는 LLM의 안전 기준을 맞추기 위한 의도적 편향이 오히려 해킹에 사용될 수 있는 모순적인 결과를 유발할 수 있음을 강조합니다. 연구진은 PCJailbreak와 PCDefense라는 도구를 통해 이러한 문제를 해결하려고 시도합니다. 코드를 오픈 소스로 제공하여 연구 커뮤니티가 이 작업을 확장할 수 있도록 지원합니다.

---

### 전체 요약

이 논문은 대형 언어 모델의 안전성 및 탈옥 가능성에 관한 연구입니다. 정치적 정당성을 위한 의도적 편향이 보안 취약점을 유발할 수 있음을 보여주며, 이를 해결하기 위한 PCJailbreak와 PCDefense 방법론을 제안합니다. 연구 결과는 LLM의 미래 보안 조치 설계에 중요한 정보를 제공합니다.