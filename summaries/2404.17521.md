# Ag2Manip: Learning Novel Manipulation Skills with Agent-Agnostic Visual and Action Representations
## TL;DR
## Summary
- [https://arxiv.org/pdf/2404.17521.pdf](https://arxiv.org/pdf/2404.17521.pdf)

이 논문은 새로운 로봇 조작 학습 프레임워크인 Ag2Manip을 소개합니다. 이 프레임워크는 도메인간의 격차를 해소하고 로봇 조작을 위한 새로운 기술을 배우는 데 집중합니다. 주요 내용은 다음과 같습니다.

1. **서론 및 관련 작업**:
   - 로봇 시스템이 전문가의 지시 없이 새로운 조작 기술을 자동으로 학습하는 능력은 변화하는 작업과 환경에 적응하는 데 중요합니다.
   - 이전 연구들은 인간 중심의 비디오 데이터셋에서 시각적 표현을 개발하는 데 집중했으며, 이 데이터는 작업의 본질과 시각적 프레임 사이의 시간적 역학을 포착하는 데 도움이 됩니다.

2. **Ag2Manip 방법론**:
   - **에이전트 불특정 시각 표현**: 인간과 로봇 간의 도메인 격차를 극복하기 위해, 인간과 로봇을 비디오 프레임에서 제거하고 이를 다시 채워 넣어 학습합니다.
   - **에이전트 불특정 행동 표현**: 로봇의 행동을 일반적인 프록시 에이전트로 추상화하여, 조작 학습을 탐색 및 상호작용의 두 단계로 나눕니다.

3. **강화 학습 및 보상 형성**:
   - 목표 이미지를 사용하여 로봇이 특정 목표를 달성하도록 학습하는 모델-프리, 목표 조건부 강화 학습(GCRL) 프레임워크를 사용합니다.
   - 정책 최적화는 Proximal Policy Optimization (PPO)를 사용하여, 로봇의 행동을 목표로 이끌어내는 데 초점을 맞춥니다.

4. **로봇 특정 행동 재타게팅**:
   - 프록시 에이전트의 궤적을 실제 로봇의 조작 가능한 동작으로 변환하는 재타게팅 정책을 사용하여 실제 로봇에 적용합니다.

5. **시뮬레이션 및 실험**:
   - 다양한 시뮬레이션 환경에서의 Ag2Manip의 효과를 보여주는 종합적인 평가를 수행하며, 로봇이 다양한 조작 작업에서 높은 성공률을 달성함을 입증합니다.

6. **결론**:
   - Ag2Manip은 전문가의 시연 없이도 로봇이 다양한 조작 기술을 습득할 수 있도록 지원하는 새로운 프레임워크를 제공하며, 로봇의 자율성과 적응성을 크게 향상시킬 수 있는 가능성을 보여줍니다.

이 연구는 로봇이 새로운 작업을 습득하고 다양한 환경에서 유연하게 작동할 수 있도록 하는데 기여하는 새로운 접근 방식을 제시합니다.