# AdaMix: Mixture-of-Adaptations for Parameter-efficient Model Tuning
## TL;DR
## Summary
- [https://arxiv.org/pdf/2205.12410.pdf](https://arxiv.org/pdf/2205.12410.pdf)

AdaMix는 PLM(사전 학습된 언어 모델)의 파라미터 효율적인 파인 튜닝(PEFT)을 위한 방법을 제시합니다. 이 방법은 트랜스포머의 각 층에 소개된 다양한 적응 모듈의 혼합을 조정함으로써 대부분의 PLM 가중치를 고정하면서 PEFT의 효율성을 높입니다. 예를 들어, AdaMix는 Houlsby 등의 어댑터나 Hu 등의 LoRA처럼 낮은 등급 분해 행렬의 혼합을 활용할 수 있습니다. AdaMix는 기존의 PEFT 방법과 비교하여 완전 관리되는 NLU 및 NLG 작업의 성능을 향상시키는 것을 목표로 합니다.

AdaMix는 PLM의 단 0.1%에서 0.2%의 파라미터만을 조정함에도 불구하고, 기존의 PEFT 방법 및 전체 모델 파인 튜닝을 능가하는 성능을 보여주었습니다. 코드와 모델은 공개적으로 제공됩니다.

기술 용어를 평이한 한국어로 번역해보겠습니다:

- PLM(Pre-trained Language Models, 사전 학습된 언어 모델): 대량의 데이터를 사용해 사전에 학습된 언어 모델입니다. 이러한 모델은 다양한 자연어 처리(NLP) 작업에 적용될 수 있습니다.
- PEFT(Parameter-efficient Fine-tuning, 파라미터 효율적인 파인 튜닝): 기존 모델의 작은 부분만을 조정하여 새로운 작업에 적응시키는 과정입니다. 이는 모델의 저장 및 계산 비용을 줄이는 데 도움이 됩니다.
- NLU(Natural Language Understanding, 자연어 이해): 텍스트의 의미를 이해하고 해석하는 능력을 개발하는 것을 목표로 하는 NLP의 한 분야입니다.
- NLG(Natural Language Generation, 자연어 생성): 주어진 데이터로부터 자연스러운 언어 텍스트를 생성하는 과정입니다.
- 어댑터와 LoRA(Low-rank Adaptation, 낮은 등급 적응): 트랜스포머 모델에 새로운 파라미터를 추가하거나 기존 가중치를 조정하여 모델이 새로운 작업에 더 잘 적응하도록 하는 기술입니다.

AdaMix는 이러한 적응 모듈들의 혼합을 통해, 각 입력 예제에 대해 모델 가중치의 부분 집합만을 활성화함으로써, 계산 비용을 증가시키지 않으면서도 모델의 성능을 향상시킬 수 있습니다. 이는 NLU 및 NLG 작업에 대해 기존 PEFT 방법보다 우수한 성능을 달성하는 것을 가능하게 합니다.

## Similar Papers
- [LoRA: Low-Rank Adaptation of Large Language Models](2106.09685.md)
- [Mixture of A Million Experts](2407.04153.md)
- [SPAFIT: Stratified Progressive Adaptation Fine-tuning for Pre-trained Large Language Models](2405.00201.md)
- [MoRA: High-Rank Updating for Parameter-Efficient Fine-Tuning](2405.12130.md)
- [LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models](2310.08659.md)
- [OLoRA: Orthonormal Low-Rank Adaptation of Large Language Models](2406.01775.md)
- [Pushing Mixture of Experts to the Limit: Extremely Parameter Efficient MoE for Instruction Tuning](2309.05444.md)
- [DoRA: Weight-Decomposed Low-Rank Adaptation](2402.09353.md)
- [RE-AdaptIR: Improving Information Retrieval through Reverse Engineered Adaptation](2406.14764.md)
