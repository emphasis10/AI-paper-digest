# OpenRFT: Adapting Reasoning Foundation Model for Domain-specific Tasks with Reinforcement Fine-Tuning
## TL;DR
## Summary
- [https://arxiv.org/pdf/2412.16849.pdf](https://arxiv.org/pdf/2412.16849.pdf)

1. 각 섹션 요약 및 논문의 주요 공헌과 혁신 부분 요약:

- **서론**:
  이 논문은 OpenAI의 강력한 추론 모델이 도메인별 작업으로 확장될 수 있음을 보여주는 Reinforcement Fine-Tuning(RFT)의 가능성을 소개합니다. 이 기술은 단순한 패턴 모방을 넘어서는 새로운 정밀 조정 방법을 제공하며, 도메인 특화 모델을 생성하는 데 활용됩니다.

- **방법론**:
  RFT는 도메인별 데이터를 활용하여 질문 증대, 추론 과정 데이터 합성, 및 소수의 샘플을 사용하는 상황 학습(ICL) 기법을 통해 모델을 정밀 조정합니다. 이를 통해 도메인 특화 추론 모델의 성능을 향상시킵니다.

- **데이터 증강**:
  데이터 증강은 질문의 변형을 통해 데이터의 다양성을 높이며, 이에 따라 모델의 학습을 확장할 수 있게 도와줍니다.

- **SFT 기반 모방**:
  강력한 추론 모형을 교사 모델로 사용하여 도메인별 데이터에 적합한 고품질의 추론 과정을 합성하고 이를 통해 정책 모델을 초기화합니다.

- **실험 및 결과**:
  극소수의 도메인별 샘플로도 OpenRFT가 유의미한 성능 향상을 달성했으며, 이는 광범위한 도메인에서의 추론 모델의 활용 가능성을 넓혔습니다.

2. 전체 요약:

이 논문은 도메인 특화 작업에 적합한추론 모델을 개발하기 위해 Reinforcement Fine-Tuning (RFT)을 도입하였습니다. OpenRFT라고 명명된 이 접근법은 도메인별 적합성을 강화하기 위해 질문 증강, 추론 과정 데이터 합성, 소수의 샘플 기반의 상황 학습을 활용합니다. 이 방식은 단순히 패턴을 모방하는 기존 방법을 넘어 합리적이며 자주적 학습을 가능하게 함으로써, 보다 인간과 닮은 일반화 능력을 보여줍니다. 실험 결과, OpenRFT는 적은 수의 도메인별 샘플만으로도 높은 성능을 보였으며, 이는 추론 모델의 적용 범위를 더욱 광범위하게 확장할 수 있음을 시사합니다.