# Meltemi: The first open Large Language Model for Greek
## TL;DR
## Summary
- [https://arxiv.org/pdf/2407.20743.pdf](https://arxiv.org/pdf/2407.20743.pdf)

### 섹션 요약

#### 1. Introduction
논문에서는 그리스어를 위한 최초의 대규모 언어 모델(Large Language Model, LLM)인 Meltemi 7B와 이에 기반한 지시 모델인 Meltemi 7B Instruct의 개발을 다룹니다. 주로 영어와 중국어가 주도하는 LLM 분야에서의 불균형을 해결하고자 지속적인 사전 훈련을 통해 그리스어 데이터로 모델을 발전시켰습니다. 이 모델은 다양한 번역, 대화 및 추론 작업을 포괄하는 평가 벤치마크도 포함합니다.

#### 2. Our Method
Meltemi 7B는 Mistral 모델을 바탕으로 그리스어 코퍼스 400억 토큰으로 훈련되었습니다. 레이블이 없는 대규모 텍스트 데이터를 수집하고 필터링하는 과정을 거쳤고, 이를 통해 그리스어와 영어, 병렬 데이터의 토크나이저를 확장시켰습니다. ORPO 알고리즘과 번역된 선호 데이터로 대화형 모델을 구축하였고, 이를 통해 지시 모델 Meltemi 7B Instruct를 개발했습니다.

#### 3. Pretraining Data
수집된 훈련 데이터는 그리스어 434억, 영어 105억, 병렬 데이터 6.3억 토큰으로 구성되었습니다. 데이터 품질을 높이기 위해 다양한 필터링 정책과 도구를 사용하여 데이터를 정제하고 중복을 제거했습니다.

#### 4. Evaluation
개발된 모델들은 그리스어 벤치마크와 영어 벤치마크에서 평가되었습니다. Meltemi 7B Instruct는 그리스어 벤치마크에서 평균 20.2%의 성능 향상을 보였으나, 영어 벤치마크에서는 성능이 6% 감소했습니다. 이는 훈련 데이터의 스타일 차이와 모델 용량에 기인합니다. 유사한 연구에서도 이와 같은 경향이 나타났습니다.

#### 5. Discussion and Conclusions
논문에서는 Meltemi 7B가 지속적인 사전 훈련을 통해 그리스어를 위한 최초의 개방형 LLM을 개발했음을 밝히며, 지시 모델 Meltemi 7B Instruct를 통해 그리스어 관련 작업에서 뛰어난 성능을 보임을 확인했습니다. 이 모델은 문화적 특성과 지역적 법규 및 관습을 반영하여 공개 LLM의 필요성을 강조합니다. 지속적인 데이터 통합 및 모델 업데이트를 위한 환경적이고 경제적인 배포 계획의 중요성도 논의되었습니다.

### 전체 요약
이 논문은 그리스어를 위한 최초의 개방형 대규모 언어 모델 Meltemi 7B와 이를 기반으로 한 지시 모델 Meltemi 7B Instruct의 개발 과정을 다룹니다. 이 모델들은 그리스어 및 병렬 데이터로 사전 훈련되어 다양한 번역, 대화 및 추론 작업에서의 성능을 개선했습니다. 그리스어 관련 작업에서는 대체로 높은 성능을 보였으나, 영어 관련 작업에서는 성능이 다소 낮아졌습니다. 이 연구는 다양한 문화적 특성과 지역적 요구사항을 반영한 중소 자원 언어를 위한 개방형 LLM의 필요성을 강조하고 있습니다.