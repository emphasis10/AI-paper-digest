# EELBERT: Tiny Models through Dynamic Embeddings
## TL;DR
## Summary
- [https://arxiv.org/pdf/2310.20144.pdf](https://arxiv.org/pdf/2310.20144.pdf)

해당 논문은 대형 언어 모델의 압축에 관한 연구로, 특히 트랜스포머 기반 모델(BERT와 같은)의 입력 임베딩 레이어를 동적(즉시 계산되는) 임베딩으로 대체하여 모델 크기를 줄이는 방법에 초점을 맞추고 있습니다. 이 방법은 GLUE 벤치마크에서의 실험을 통해 기존의 BERT 모델들과 비교하여 미미한 성능 저하로 상당한 모델 크기 축소를 달성할 수 있음을 보여줍니다. 특히 가장 작은 모델인 UNO-EELBERT는 BERT-tiny 대비 15배 작은 크기(1.2MB)에도 불구하고 GLUE 점수에서 4% 미만의 차이를 보여, 작은 크기의 모델이 여전히 유용할 수 있음을 시사합니다.

### 1. 소개
언어 이해 시스템은 주로 BERT, T5, mT5, RoBERTa와 같은 강력한 사전 훈련된 언어 모델에 기반을 두고 있습니다. 이러한 모델은 다양한 자연 언어 처리(NLP) 태스크를 해결하는 데 사용되지만, 모델의 크기가 큰 문제점이 있습니다. 이에 따라, 모델을 축소하여 리소스가 제한된 환경(예: 스마트폰, 시계)에서도 모델을 효과적으로 사용할 수 있는 방법에 대한 연구가 활발히 진행되고 있습니다.

### 2. 관련 연구
기존의 연구들은 모델 프루닝, 양자화, 지식 증류 등 다양한 모델 압축 기술을 다루고 있습니다. 본 연구는 모델의 큰 어휘를 표현하는 데 필요한 리소스를 줄이는 새로운 접근 방식을 제안합니다.

### 3. EELBERT 모델링
EELBERT는 BERT와 같은 트랜스포머 기반 모델의 입력 임베딩 레이어를 동적 임베딩 계산 방식으로 대체하여 모델 크기를 줄입니다. 동적 임베딩은 런타임에 해시 함수를 사용하여 계산되며, 입력 임베딩 레이어가 차지하는 공간을 줄여 모델 크기를 감소시킵니다.

### 4. 실험 설정 및 결과
GLUE 벤치마크를 사용한 실험을 통해 EELBERT 모델의 효과를 평가했습니다. 결과는 EELBERT가 기존 BERT 모델과 비교하여 상당한 모델 크기 축소를 달성하면서도 GLUE 태스크에서 미미한 성능 저하만을 보였음을 나타냅니다.

### 5. 결론
EELBERT는 입력 임베딩 레이어를 동적 임베딩으로 대체함으로써 BERT와 같은 트랜스포머 기반 언어 모델의 크기를 현저히 줄일 수 있는 효과적인 방법을 제시합니다. 이 접근 방식은 리소스가 제한된 환경에서의 모델 배포에 특히 유용할 수 있으며, 기존의 모델 압축 기술과 결합하여 더 작은 크기의 모델을 생성하는 데 활용될 수 있습니다.

## Similar Papers
- [Enhancing Semantic Similarity Understanding in Arabic NLP with Nested Embedding Learning](2407.21139.md)
- [Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity](2101.03961.md)
- [ALBERT: A Lite BERT for Self-supervised Learning of Language Representations](1909.11942.md)
- [Jamba: A Hybrid Transformer-Mamba Language Model](2403.19887.md)
- [Transfer Learning for Structured Pruning under Limited Task Data](2311.06382.md)
- [Zero-Shot Tokenizer Transfer](2405.07883.md)
- [OLoRA: Orthonormal Low-Rank Adaptation of Large Language Models](2406.01775.md)
- [MLKV: Multi-Layer Key-Value Heads for Memory Efficient Transformer Decoding](2406.09297.md)
- [The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits](2402.17764.md)
