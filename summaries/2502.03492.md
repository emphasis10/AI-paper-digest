# Teaching Language Models to Critique via Reinforcement Learning
## TL;DR
## Summary
- [https://arxiv.org/pdf/2502.03492.pdf](https://arxiv.org/pdf/2502.03492.pdf)

### 논문 요약: AI와 머신러닝에 관한 연구

#### 1. 서론
대형 언어 모델(LLM)을 비판하도록 가르치는 것은 반복적으로 개선할 수 있는 시스템을 구축하는 데 필수적입니다. 그러나 정확한 판단과 실행 가능한 제안을 제공하는 능력에 제한이 있습니다. 이 연구에서는 코드 생성을 위한 LLM 비평가를 연구하고 Critic Training via Reinforcement Learning (CTRL)이라는 프레임워크를 제안합니다. CTRL은 비평 모델이 인간의 감독 없이 수정 성능을 극대화하는 피드백을 생성할 수 있도록 훈련합니다. 결과적으로 CTRL로 훈련된 비평가가 통과율을 유의미하게 개선함을 보여줍니다.

#### 2. 선행 연구 및 동기
반복 개선 방법의 성공은 피드백을 활용하여 솔루션을 개선할 수 있는 능력에 크게 의존합니다. 비평 능력은 해결책을 평가하고 실행 가능한 피드백을 제공하는 두 가지 주요 기능에 의해 결정됩니다. 이 논문은 비평 능력을 Markov 체인의 전환 동역학을 통해 특징지었습니다.

#### 3. 방법론
CTRL은 두 단계 훈련 접근 방식을 제안합니다: (1) 실행 피드백을 통해 고품질 비평을 합성하고, (2) 강화 학습을 통해 비평자를 다듬습니다. 이 비평 모델은 테스트 시 다양한 생성 모델과 짝을 이루어 솔루션을 반복적으로 다듬을 수 있도록 훈련됩니다.

#### 4. 실험
CTRL의 효과를 여러 프로그래밍 벤치마크에서 평가했습니다. CTRL은 자체 비평 방법 및보다 강력한 비평 모델을 사용하는 접근 방식을 모두 초월함을 입증했습니다. 비평 모델은 저조한 성능의 생성 모델을 유도하지만 유의미한 개선을 가져왔습니다.

#### 5. 결과
CTRL은 다른 비평 모델에 비해 106.1%까지 상대 통과율을 개선했습니다. 특히, 문제의 난이도가 높아질수록 CTRL의 효과가 큽니다.

#### 6. 결론
CTRL은 LLM 비평가를 훈련시켜 반복적인 수정의 효과적인 피드백을 제공할 수 있는 강화 학습 프레임워크로, 다수의 벤치마크에서 유의미한 개선을 보여줍니다. 미래 연구는 효율성과 안전성 최적화를 포함할 수 있습니다.

### 전체 요약
CTRL은 LLM 비평가가 피드백을 통해 반복적으로 솔루션을 개선할 수 있도록 하는 혁신적인 프레임워크입니다. 이 방법은 비평 모델이 주어진 생성 모델의 성능을 극대화하여 높은 통과율을 달성하는 것을 가능하게 합니다. CTRL의 연구 결과는 AI와 머신러닝 분야에서 비평가의 역할을 새롭게 조명하고 있으며, 향후 연구에서 보다 나은 자율 시스템 개발의 토대가 될 것입니다.