# Move-in-2D: 2D-Conditioned Human Motion Generation
## TL;DR
## Summary
- [https://arxiv.org/pdf/2412.13185.pdf](https://arxiv.org/pdf/2412.13185.pdf)

### 1. 각 섹션 요약 및 주요 기여점

**소개**: 본 논문에서는 인공지능과 기계 학습의 최신 발전을 활용하여 장면 이미지와 텍스트를 기반으로 인간 동작을 자연스럽게 생성하는 새로운 과제를 소개합니다. 이 과제는 2D 장면 이미지 및 텍스트 조건을 통해 인간의 동작을 생성함으로써, 3D 재구성 없이도 동작 생성을 보다 접근 가능하게 만듭니다.

**관련 연구**: 기존의 인간 중심 비디오 생성 연구는 사전 정의된 모션 시퀀스에 의존하여 다양한 모션을 생성하는 데 제한이 있었습니다. 이 논문은 텍스트 및 장면 이미지를 조건으로 하는 모션 가이드 시퀀스를 생성하여 인간 비디오 생성 프레임워크를 위한 안내로 사용합니다.

**Humans-in-Context Motion Dataset**: 인간의 동작을 이해하고 생성하기 위한 리서치를 촉진하기 위하여 대규모 데이터셋을 구축하였습니다. 이 데이터셋은 장면 문맥과 일치하는 움직임 시퀀스를 포함하지는 않지만, 다양한 얼굴 표정과 손 자세를 포함하게 확장하였습니다.

**접근방식**: 제안된 접근방식은 대규모 데이터셋을 활용하여 텍스트와 장면 이미지를 조건으로 하는 확산 기반 네트워크를 제안합니다. 이 모델을 통해 자동화된 인간 동작 생성 및 비디오 생성에서의 품질을 향상시킬 수 있습니다.

**Diffusion Model의 개요**: Diffusion 모델은 데이터를 노이즈를 추가하여 전진 과정 및 후진 과정을 통해 데이터 분포를 근사화합니다. 이 과정에서 배경 씬 이미지와 텍스트 프롬프트의 조건을 사용하는 것이 핵심입니다.

**Conditional Motion Diffusion**: 입력 조건을 기반으로 대상 인간 동작을 생성하여 움직임이 2D 배경 장면 이미지에 자연스럽게 투영되도록 합니다. 이는 SMPL 공간의 점들을 이미지 평면에 투영하기 위해 고정된 초점 거리를 가진 원근 카메라를 가정합니다.

**Multi-Conditional Transformer**: 텍스트 프롬프트와 장면 조건을 변환기 모형에 주입하여 입력 설명과 장면에 물리적으로 호환되는 움직임 시퀀스를 생성합니다.

**훈련 전략**: 두 단계 훈련을 통해 다양한 모션 시퀀스를 생성하고 카메라 효과와 인간의 모션을 분리합니다. 첫 번째 단계에서 장면 의미를 배우고 다양한 모션 시퀀스를 생성하며, 두 번째 단계에서 큰 움직임을 개선합니다.

**실험 및 결과**: 실험 결과, 제안된 모델이 기존 방법들보다 더 높은 품질 점수와 다양성을 갖춘 모션을 생성함을 보여주었습니다.

**결론**: 본 논문은 2D 배경 장면 이미지를 조건으로 하는 인간 동작 생성의 새로운 작업을 소개하며, 제안된 방법과 데이터셋을 활용하여 인공지능 기반 비디오 생성의 미래 발전에 기여합니다.

### 2. 전체 요약

본 논문은 인공지능 및 기계 학습을 활용하여 인간의 자연스러운 동작을 2D 장면 이미지 및 텍스트 조건에 맞춰 생성하는 새로운 문제를 제시합니다. 제안된 방법은 대규모 데이터셋을 기반으로 한 확산 기반 네트워크를 통해, 다양한 장면과 동작을 생성을 지원하며, 기존의 동작 생성 방식에 비해 향상된 품질과 다양성을 구현합니다. 논문에서는 이 모델이 인공지능 기반 비디오 생성에서의 품질 향상에 기여할 수 있는 가능성을 제시하고, 향후 발전 가능성을 남기고 있습니다.