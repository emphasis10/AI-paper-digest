# Explore the Limits of Omni-modal Pretraining at Scale
## TL;DR
## Summary
- [https://arxiv.org/pdf/2406.09412.pdf](https://arxiv.org/pdf/2406.09412.pdf)

### 1. 섹션별 요약 및 주요 기여
#### Abstract (초록)
이 연구는 모든 형태의 데이터를 이해하고 보편적인 표현을 학습할 수 있는 옴니모달(Intelligence) 지능을 구축하기 위한 접근법을 제안합니다. 이를 위해 다중 모달리티와 데이터 양을 확장할 수 있는 확장형 사전 학습 패러다임인 MiCo (Multimodal Context)를 개발했습니다. 이 모델은 단일 모달리티 인식 벤치마크, 크로스 모달리티 이해 작업, 다중 모달리티 대형 언어 모델 벤치마크에서 뛰어난 성능을 보였습니다. 

#### Introduction (서론)
AI 발전에서 확장 가능한 사전 학습은 일반 지능으로 가는 유망한 길로 주목받고 있습니다. CLIP 모델이 웹 전체의 텍스트-이미지 데이터를 수집하여 큰 영향을 미쳤지만, 현대의 다중 모달리티 시대에서는 음성, 비디오, 3D 콘텐츠와 같은 모달리티가 더 많이 사용되며 이에 따른 추가적인 도전 과제가 있습니다.

#### Related Work (관련 연구)
기존 연구들은 주로 시각-언어 사전 학습에 중점을 두었습니다. MCAN, VL-BERT, Oscar 등은 시각적 및 언어적 콘텐츠를 통합하며 기능을 향상시켰습니다. 또한 CLIP 모델 이후 다중 모달리티 학습의 필요성이 증가했습니다. CLIP 후속 연구들은 점점 더 많은 모달리티를 포함하도록 발전했습니다.

#### Methodology (방법론)
MiCo는 다양한 모달리티를 결합하여 공동 임베딩 공간으로 매핑합니다. 이를 위해 공통 백본 네트워크를 사용하고, 다중 모달리티 시퀀스를 결합하여 대조 학습 및 마스킹 모델링을 수행합니다.

#### Experiments (실험)
MiCo 모델은 단일 모달리티, 크로스 모달리티, 다중 모달리티 대형 언어 모델 벤치마크에서 검증되었습니다. 단일 모달리티 인식에서는 최고 성능을 기록했고, 크로스 모달리티 작업에서도 우수한 성과를 보였습니다.

#### Results (결과)
MiCo 모델은 총 37개의 새로운 최첨단 성과를 기록하며, 이는 기존 방법보다 20% 이상 개선된 성과입니다. 다량의 모달리티 데이터를 통합하여 더 나은 일반화 능력과 모달리티 확장성을 실현했습니다.

#### Conclusion (결론)
본 연구는 MiCo라는 새로운 프레임워크를 제안하여 향상된 시각 인식 능력과 다중 모달리티 능력을 갖춘 모델을 사전 학습했습니다. MiCo는 다중 모달리티 인지를 통해 인간 뇌의 멀티미디어 인지 과정을 시뮬레이션하는 중요한 시도입니다. 향후 추가적인 모달리티를 통합하여 능력을 강화할 계획입니다.

### 주요 기여 및 혁신
MiCo의 주요 기여는 다음과 같습니다:
1. **확장 가능성**: 다양한 모달리티와 데이터 양을 확장할 수 있는 능력
2. **비교 우위 점수**: 37개의 최신 성과를 기록하며, 기존 모델보다 성능이 뛰어남
3. **옴니모달 학습**: 다양한 모달리티를 결합한 공동 학습 프레임워크 제공
4. **인간 뇌 시뮬레이션**: 멀티미디어 인지 과정을 시뮬레이션하여 모델 성능 개선

### 2. 전체 요약
이 논문에서는 모든 형태의 데이터 모달리티를 이해하고 학습할 수 있는 모델인 MiCo를 제안했습니다. MiCo는 다양한 모달리티를 결합하여 공동 임베딩 공간으로 매핑하고, 대조 학습과 마스킹 모델링을 통해 단일 모달리티 인식, 크로스 모달리티 이해, 다중 모달리티 대형 언어 모델 벤치마크에서 최고 성능을 기록했습니다. 이는 기존 방법보다 20% 이상 개선된 성과로, 다중 모달리티 인지를 통해 인간 뇌의 멀티미디어 인지 과정을 시뮬레이션하는 중요한 시도입니다. 향후 연구에서는 추가적인 모달리티를 통합하여 모델의 능력을 더욱 강화할 계획입니다.

  