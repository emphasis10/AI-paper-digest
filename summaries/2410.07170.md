# One Initialization to Rule them All: Fine-tuning via Explained Variance Adaptation
## TL;DR
## Summary
- [https://arxiv.org/pdf/2410.07170.pdf](https://arxiv.org/pdf/2410.07170.pdf)

1. **각 섹션의 중요한 내용 요약 및 주 기여와 혁신 부분**:

   - **서론(Introduction)**: 본 논문은 "Explained Variance Adaptation (EVA)"이라는 새로운 방법을 제안하며, 이는 데이터 기반 초기화와 순위 재분배를 통해 LoRA(저차원 적응) 기법을 확장합니다. 이 방법론은 미니배치의 활성화 벡터에 대한 SVD(특잇값 분해)를 실행하여 LoRA 행렬을 초기화하고, 분산 설명량에 따라 모든 가중치 행렬에 순위를 재분배합니다. EVA는 다양한 도메인, 즉 언어기반 처리, 컴퓨터 비전 및 강화 학습의 다양한 작업에서 우수한 평균 성능을 입증했습니다.

   - **기술적 배경 및 관련 연구(Technical Background and Related Work)**: 본 섹션에서는 기존의 PEFT(파라미터 효율적 세부 조정) 방법, 특히 LoRA에 대해 설명하고, 이러한 기법들이 다양한 데이터 도메인에서 어떠한 희석화된 학습을 통해 기본 모델 파라미터를 일부만 사용함으로써 효율성을 달성하는지를 다룹니다.

   - **메소드(Methodology)**: EVA는 데이터 기반 초기화와 적응형 순위 할당의 이점들을 결합한 것이 특징입니다. LoRA는 주로 무작위 초기화와 고정된 순위를 사용하였지만, EVA는 데이터에서 직접 얻은 정보를 사용하여 LoRA 행렬을 초기화하고, 이는 학습 속도를 높이고 성능을 최적화합니다.

   - **실험(Experiments)**: EVA는 Llama-2-7B 및 Gemma-2-9B 등의 모델을 대상으로 실행되었으며, 이는 다양한 합리적 추론 및 수학적 추론 벤치마크에서 효율성과 성능을 높입니다. EVA는 LoRA 등 기존 방법보다 우수한 성능을 보였습니다.

   - **결론(Conclusion)**: EVA 방법론의 확장 가능성에 대해 논의하며, 미래 연구에서 기하급수적 효율성과 성능 향상을 위한 가능성을 제공합니다. 이는 기본 모델의 파라미터 수정을 최소화하면서도 최적의 성능을 가져올 수 있는 방법론으로 자리매김할 가능성을 가집니다.

2. **전체 요약**:

   본 논문은 "Explained Variance Adaptation (EVA)"이라는 새로운 방법을 제안하며, 이는 큰 언어 모델을 더 효과적으로 미세 조정할 수 있도록 합니다. 데이터 기반 초기화는 작업에 필요한 변동성을 설명할 수 있도록 LoRA 행렬의 순위를 재분배하며 최적의 성능을 달성합니다. EVA는 각종 도메인에서 기존의 방법보다 빠른 수렴과 높은 평균 성능을 보여줍니다. 이는 AI 및 기계 학습 발전에 기여할 잠재력이 있는 중요한 연구로 평가받고 있습니다.