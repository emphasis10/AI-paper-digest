# From Drafts to Answers: Unlocking LLM Potential via Aggregation Fine-Tuning
## TL;DR
## Summary
- [https://arxiv.org/pdf/2501.11877.pdf](https://arxiv.org/pdf/2501.11877.pdf)

### 1. 각 섹션의 주요 내용 요약

**초록**
이 논문은 Aggregation Fine-Tuning (AFT)이라는 감독 학습 기법을 소개합니다. AFT는 여러 개의 초안 응답을 통합하여 단일 정제 응답을 생성하게끔 모델을 학습시킵니다. 실험 결과, AFT를 사용한 모델들이 기존의 Supervised Fine-Tuning (SFT) 모델보다 성능이 대폭 향상되었음을 보여줍니다.

**1. 서론**
대형 언어 모델(LLMs)은 우수한 성능을 보이고 있으며, 학습 데이터와 모델 크기 증가에 힘입어 더 나은 결과를 내고 있습니다. AFT는 여러 초안 응답을 통합하여 최종 응답을 생성하는 새로운 접근법입니다. 이는 기존의 직접 매핑 방식과 다릅니다.

**2. 관련 연구**
기존의 SFT 방법들이 주로 사용자 쿼리를 참조 응답으로 직접 매핑하는데 반해, AFT는 여러 초안 응답을 바탕으로 최종 응답을 생성하는 과정을 포함합니다. 이를 통해 향상된 성능을 기대할 수 있습니다.

**3. 방법론**
AFT는 쿼리와 여러 초안 응답으로 구성된 데이터셋을 사용하여 학습됩니다. 이 과정에서 모델은 제안된 응답들을 통합한 최종 응답을 생성하게 됩니다. 이 방법론은 제안과 통합의 상호작용을 통해 성능을 향상시키는 구조를 가지고 있습니다.

**4. 실험 설정**
연구팀은 UltraFeedback 데이터셋을 사용하여 AFT의 성능을 평가했습니다. 이 데이터셋은 64,000개의 인스턴스를 포함하며, 각 인스턴스는 여러 응답을 가지고 있습니다.

**5. 실험 결과**
AFT 모델의 성능은 기존 SFT 모델에 비해 평균 12.9%에서 14.9% 향상되었습니다. 특히, AFT-on-policy 모델은 특히 우수한 성능을 보여주었습니다.

**6. 분석**
AFT는 학습 효율성을 높이고, 훈련 과정에서 더 안정적인 수렴을 더해줍니다. 초안 응답의 다양성과 품질이 집합된 응답의 질에도 큰 영향을 미친다는 점이 강조되었습니다.

**7. 결론**
AFT는 여러 초안 응답을 통합하여 최종 응답을 생성하는 방식으로, 기존의 SFT보다 나은 결과를 낳습니다. AFT는 모델의 잠재력을 최대한 활용할 수 있는 유연하고 비용 효율적인 접근 방식으로 자리 잡을 수 있습니다.

### 2. 전체 요약
이 논문은 Aggregation Fine-Tuning (AFT)라는 새로운 학습 기법을 제안하여 여러 초안 응답을 통합하여 최종 응답을 생성하는 방식을 설명했습니다. AFT는 기존의 감독 학습 방식과는 다른 접근법으로, 모델의 성능을 높이는 데 기여합니다. 실험 결과, AFT는 기존의 SFT보다 우수한 성능 향상을 보여주었으며, 이 방법론은 대형 언어 모델의 특정 요구를 충족시키는 데 적합하다고 결론짓습니다.