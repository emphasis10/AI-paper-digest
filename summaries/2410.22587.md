# Toxicity of the Commons: Curating Open-Source Pre-Training Data
## TL;DR
## Summary
- [https://arxiv.org/pdf/2410.22587.pdf](https://arxiv.org/pdf/2410.22587.pdf)

1. **논문의 각 섹션 요약 및 주요 기여와 혁신적 부분**

   - **서론**: 본 논문은 오픈 소스 대형 언어 모델(LLM)의 안전성과 투명성 향상을 위해 데이터 큐레이션 파이프라인을 제안합니다. 특히, 공공 도메인 데이터를 활용한 모델에서 유해 출력을 줄이는 방안을 탐구합니다. 공공 도메인 데이터는 웹 텍스트와는 다른 양상을 가지고 있어 독특한 도전과제를 안고 있습니다.
   
   - **연구 방법**: 저자들은 'ToxicCommons'라는 맞춤형 데이터셋을 구성하고, 다양한 차별 요소를 포함한 다섯 개 차원(인종/출신지, 성별/성차, 종교, 능력 기반 차별, 폭력)에 걸쳐 내용의 유해도를 분류합니다. 이 데이터셋으로 'Celadon'이라는 분류기를 훈련시켜 대규모 오픈 데이터에서 유해한 내용을 효율적으로 감지합니다.
   
   - **데이터 필터링 파이프라인**: 본 논문은 다양한 역사적인 공공 도메인 데이터셋을 사용하며, OCR 노이즈를 감소시키기 위해 'OCRonos'를 적용합니다. 필터링은 유해한 데이터를 걸러내고, 필요 시 텍스트를 재작성하여 독소를 제거하는 방법을 사용합니다.
   
   - **논의 및 결론**: 이 파이프라인과 데이터셋, 분류기는 공공 도메인 데이터에 대한 LLM의 유해성을 줄이는 효율적인 방법을 제공합니다. 또한, 노출된 인간 주석자의 위험을 줄이기 위해 주로 LLM을 활용한 어노테이션 과정을 사용하며, 이를 통해 공정성과 투명성을 높이는 데 기여할 수 있습니다.

2. **전체 요약**

   본 논문은 오픈 소스 AI 모델의 구성 요소로서 공공 도메인 데이터를 사용하는 데 있어 생기는 유해성을 감소시키기 위한 체계적 접근 방안을 제시합니다. 스스로 개발한 'ToxicCommons' 데이터셋과 'Celadon' 분류기를 통해 다섯 가지 스펙트럼의 유해 콘텐츠를 필터링하고, 필요 시 텍스트를 재작성하는 방법을 설명합니다. 이는 유해한 텍스트 데이터를 지속 가능한 방식으로 관리하여 LLM의 안전성과 신뢰성을 높이기 위한 노력의 일환입니다. 연구는 공공 도메인 데이터의 특별한 성격을 고려하면서 신중한 데이터 큐레이션의 필요성을 강조하며, 이에 대한 해결책을 제시함으로써 AI 커뮤니티에 기여하고자 합니다.