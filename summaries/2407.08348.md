# Skywork-Math: Data Scaling Laws for Mathematical Reasoning in Large Language Models -- The Story Goes On
## TL;DR
## Summary
- [https://arxiv.org/pdf/2407.08348.pdf](https://arxiv.org/pdf/2407.08348.pdf)

### 1. 섹션별 요약
#### 도입
이 논문은 대규모 언어 모델(LLMs)의 수학적 추론 능력을 향상시키기 위한 요인을 조사합니다. 저자들은 현대 LLM의 수학적 추론 능력에 대한 데이터 규모 법칙이 포화되지 않았다고 주장하며, 데이터 양이 증가함에 따라 모델 품질이 어떻게 향상되는지를 강조합니다. 이를 입증하기 위해, Skywork-Math 모델 시리즈를 소개하며, Skywork-MathQA 데이터셋을 사용하여 일반 7B LLM에서 감독학습(SFT)을 수행했습니다. 이 모델은 경쟁 수준의 MATH 벤치마크에서 51.2%, GSM8K 벤치마크에서 83.9%의 정확도를 달성했습니다.

#### 관련 연구
LLM의 정렬(alignment) 및 데이터의 양과 질에 초점을 맞추고 있습니다. 최근 연구들은 LLM 미세 조정에서 데이터 양이 중요한 역할을 한다고 입증했습니다. 그러나 최적의 미세 조정 데이터 크기는 과제에 따라 다르며, 질 높은 데이터 또한 중요합니다. 일부 연구는 다량의 미세 조정을 최소화하면서도 높은 성능을 달성할 수 있다고 주장합니다.

#### 방법론
이 논문은 두 단계의 데이터 합성 및 모델 SFT 파이프라인을 사용하여 Skywork-Math 모델 시리즈를 개발했습니다. 1단계에서는 일반적인 합성 문제를 생성하고, 2단계에서는 어려운 합성 문제를 생성하여 성능 저하를 완화합니다. 이를 위해 GPT-4를 사용해 250만 건의 합성 데이터를 생성했습니다.

#### 실험 분석
주요 실험 결과로, Skywork-Math 모델은 10B 매개변수 이하 모델 중 최고 성능을 보였습니다. 모델 성능 비교에서는 모델 정렬 기술과 데이터 합성 방법론의 중요성을 강조했습니다.

### 2. 전체 요약
이 논문은 대규모 언어 모델(LLMs)의 수학적 추론 능력을 향상시키기 위해 데이터 규모와 질의 중요성을 강조합니다. Skywork-Math 모델 시리즈는 250만 건의 데이터를 사용해 감독학습(SFT)을 수행하여 MATH와 GSM8K 벤치마크에서 높은 정확도를 기록했습니다. 두 단계의 데이터를 합성하여 성능을 최적화하는 방법론을 제안하며, LLM이 수학적 추론을 잘 수행하기 위해 데이터 양과 다각적인 데이터 증강 방법이 중요함을 보여줍니다.

이 요약을 기반으로 프레젠테이션을 제작할 수 있습니다. 논문의 주요 공헌과 혁신적인 부분을 한국어로 쉽게 이해할 수 있도록 설명하였습니다.