# Grounded 3D-LLM with Referent Tokens
## TL;DR
## Summary
- [https://arxiv.org/pdf/2405.10370.pdf](https://arxiv.org/pdf/2405.10370.pdf)

### 요약
이 논문은 **Grounded 3D-LLM**이라는 모델을 제안하여 3D 장면 이해를 위한 통합적 모델링을 다루고 있습니다. 이 모델은 **참조 토큰(referent tokens)**을 사용하여 3D 장면과 언어 구를 연결하여 다양한 3D 비전 작업을 언어 형식으로 통합합니다.

### 각 섹션 요약

#### 1. 서론
3D 장면 이해를 위한 통합 모델 개발은 중요한 과제입니다. 기존 모델들은 특정 작업에 맞춰져 있으며, 3D 공간 추론에 한계가 있습니다. 이 논문에서는 대형 언어 모델(LLM)의 생성적 모델링 기능을 활용하여 여러 3D 비전 작업을 통합하는 Grounded 3D-LLM을 제안합니다.

#### 2. 관련 연구
2D와 3D의 비전-언어 모델을 검토하고, 기존의 모델들이 문장 수준에서 이미지를 텍스트와 일치시키는 방식에 대해 논의합니다. 이 논문은 더 세밀한 구 수준에서의 장면-텍스트 대응을 목표로 합니다.

#### 3. 방법론
- **Vision-Language Correspondence**: 세 가지 수준(문장, 단어, 구)에서 비전-언어 정렬을 탐구합니다.
- **Grounded 3D-LLM**: 참조 토큰을 사용하여 3D 장면과 언어를 연결하는 방식으로, 3D 포인트 클라우드 인코더와 교차 모달 인터랙터를 사전 학습한 후, 다양한 3D 비전 작업을 위한 다중 작업 지시 튜닝을 수행합니다.
- **Grounded Language Data**: 기존 데이터셋을 사용하여 구 수준의 장면-텍스트 데이터를 생성합니다.

#### 4. 실험
- **Setup**: ScanNet 데이터셋을 사용하여 단일 객체 및 다중 객체 그라운딩, 객체 탐지, 3D QA 및 밀집 캡션 작업을 평가합니다.
- **Main Results**: Grounded 3D-LLM은 기존 모델들보다 높은 성능을 보이며, 다양한 작업에서 탁월한 결과를 보여줍니다.
- **Ablation Studies**: CLASP 사전 학습이 참조 토큰의 해석 능력을 향상시키는 것을 보여줍니다.

#### 5. 한계 및 결론
Grounded 3D-LLM은 다양한 3D 비전 작업을 통합하는 데 있어 유망한 접근법을 제시하지만, 생성적 접근 방식의 한계도 존재합니다. 향후 연구에서는 장면-텍스트 데이터의 확장을 통해 이 격차를 줄이는 것을 목표로 합니다.

### 전체 요약
**Grounded 3D-LLM**은 참조 토큰을 활용하여 3D 장면과 언어 구를 연결함으로써, 3D 비전 작업을 통합적으로 처리할 수 있는 새로운 접근법을 제시합니다. 이 모델은 다양한 3D 비전 작업에서 뛰어난 성능을 보이며, 실험을 통해 그 유효성을 입증했습니다. 향후 연구 방향은 더 많은 장면-텍스트 데이터의 통합을 통해 성능을 더욱 향상시키는 것입니다.