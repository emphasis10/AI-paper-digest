# Model Stock: All we need is just a few fine-tuned models
## TL;DR
## Summary
- [https://arxiv.org/pdf/2403.19522.pdf](https://arxiv.org/pdf/2403.19522.pdf)

이 연구 논문은 "Model Stock: All we need is just a few fine-tuned models"라는 제목으로, 대규모 사전 훈련된 모델들을 효율적으로 미세조정하는 새로운 방법을 제안합니다. 이 방법은 인-디스트리뷰션(ID)과 아웃-오브-디스트리뷰션(OOD) 작업에서 모두 강력한 성능을 제공합니다. 기존의 여러 미세조정 모델을 평균화하는 방식과 달리, 이 연구에서는 소수의 모델만을 사용하여 최종 가중치를 달성함으로써 더 높은 정확도를 제공합니다.

### 주요 내용 요약

1. **서론 및 배경**:
   - 사전 훈련/미세조정 패러다임이 강력한 성능을 낼 수 있음을 설명하며, 특히 미세조정 단계가 모델의 성능과 분포 이동에 대한 강인성에 중요한 영향을 미친다고 강조합니다.

2. **Model Stock의 주요 구성**:
   - 기존의 Model Soup 방법과 달리 Model Stock은 미세조정된 모델의 가중치를 평균화하는 대신 가중치 공간의 기하학적 특성을 활용하여 두 개의 미세조정 모델만을 사용합니다. 이 접근 방식은 계산 효율성을 대폭 개선하며, 과정을 간소화합니다.

3. **성능 평가**:
   - CLIP 아키텍처를 기반으로 한 미세조정 모델을 사용하여 ID 및 OOD 작업에 대한 성능을 평가하였고, 기존 방법들과 비교하여 뛰어난 성능을 보여줍니다. 특히 ImageNet에서 87.8%의 top-1 정확도를 달성하며 다섯 가지 분포 이동 벤치마크에서 평균 74.9%의 성능을 보였습니다.

### 혁신적인 부분
Model Stock의 혁신성은 미세조정된 가중치의 기하학적 분포를 이용하여 최적화된 가중치를 합성하는 방법에 있습니다. 이는 몇 개의 미세조정 모델만을 사용하여도 높은 성능을 유지할 수 있게 하며, 기존의 여러 모델을 사용하는 방식보다 훨씬 효율적입니다.

이 논문은 미세조정 과정을 효율적으로 개선하는 새로운 방법을 제시하며, 광범위한 NLP 작업에서의 응용 가능성을 보여 줍니다. 이 연구는 사전 훈련/미세조정 패러다임을 이해하고 최적화하는 데 있어 중요한 기초를 제공할 것입니다.