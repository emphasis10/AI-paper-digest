# Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture
## TL;DR
## Summary
- [https://arxiv.org/pdf/2301.08243.pdf](https://arxiv.org/pdf/2301.08243.pdf)

### 주요 내용 요약

#### 1. 서론
- **문제 정의**: 기존의 이미지 기반 자가 지도 학습 방법론은 두 가지로 나뉜다: 불변성 기반 방법과 생성 기반 방법. 이들은 각각 고유한 한계를 지니고 있다.
- **제안하는 방법**: 이 논문에서는 손으로 제작된 데이터 증강 없이 고도로 의미 있는 이미지 표현을 학습할 수 있는 이미지 기반 공동 임베딩 예측 아키텍처(I-JEPA)를 소개한다. 
- **핵심 아이디어**: 하나의 컨텍스트 블록에서 같은 이미지의 다양한 타겟 블록의 표현을 예측하는 것이다. 여기서 핵심은 의미 있는 타겟 블록을 충분히 큰 규모로 샘플링하고, 충분히 정보가 풍부한 컨텍스트 블록을 사용하는 것이다.

#### 2. 배경
- **자가 지도 학습**: 시스템이 입력 간의 관계를 포착하도록 학습하는 대표적인 접근법.
- **공동 임베딩 아키텍처**: 유사한 입력에 대해 유사한 임베딩을 출력하는 것을 목표로 하며, 대표적인 예로 대조 학습이 있다.
- **생성적 아키텍처**: 하나의 신호로부터 다른 신호를 직접 재구성하는 방법.
- **공동 임베딩 예측 아키텍처**: 임베딩 공간에서 예측을 수행하는 아키텍처로, 예측 네트워크를 사용하여 신호의 임베딩을 예측한다.

#### 3. 방법론
- **I-JEPA**: 단일 컨텍스트 블록을 사용하여 다양한 타겟 블록의 표현을 예측.
- **타겟 생성**: 이미지 블록의 표현을 타겟으로 사용하며, 랜덤으로 샘플링된 여러 블록의 표현을 예측.
- **컨텍스트 생성**: 단일 컨텍스트 블록을 샘플링하고, 예측할 타겟 블록과 중복되는 부분을 제거.
- **예측 및 손실 함수**: 컨텍스트 인코더의 출력을 바탕으로 타겟 블록의 표현을 예측하며, 예측된 표현과 실제 표현 간의 L2 거리 평균을 손실로 사용.

#### 4. 관련 연구
- **기존 연구**: 다양한 이미지 표현 학습 방법들이 있으며, 대표적으로 마스킹 및 재구성, 대조 학습 등이 있다.
- **차별점**: I-JEPA는 손으로 제작된 데이터 증강 없이도 고도의 의미 있는 표현을 학습할 수 있도록 설계되었다.

#### 5. 이미지 분류
- **평가 결과**: I-JEPA는 손으로 제작된 데이터 증강 없이도 높은 성능을 보이며, 특히 ImageNet-1K에서 다른 방법들보다 우수한 성능을 보인다.
- **전이 학습**: 다양한 이미지 분류 작업에서 우수한 성능을 보임.

#### 6. 로컬 예측 작업
- **성능**: I-JEPA는 객체 수 세기 및 깊이 예측과 같은 저수준 작업에서도 우수한 성능을 보임.

#### 7. 확장성
- **모델 효율성**: I-JEPA는 다른 방법들보다 적은 연산으로 우수한 성능을 보이며, 특히 큰 모델에서도 효율적임.
- **데이터 크기 및 모델 크기 확장**: 데이터셋 크기와 모델 크기를 확장하면 성능이 향상됨.

#### 8. 예측기 시각화
- **시각화 결과**: I-JEPA의 예측기가 위치 불확실성을 정확히 포착하고 고수준 객체 부분을 올바르게 예측.

#### 9. 실험
- **예측 공간**: 픽셀 공간 대신 임베딩 공간에서의 예측이 성능에 중요한 영향을 미침.
- **마스킹 전략**: 다중 블록 마스킹 전략이 의미 있는 표현 학습에 도움.

#### 10. 결론
- **요약**: I-JEPA는 손으로 제작된 데이터 증강 없이도 고도로 의미 있는 이미지 표현을 학습할 수 있는 효율적이고 확장 가능한 방법을 제안. 이는 다양한 컴퓨터 비전 작업에서 뛰어난 성능을 보임.

---

### 전체 요약
이 논문은 손으로 제작된 데이터 증강 없이도 고도로 의미 있는 이미지 표현을 학습할 수 있는 새로운 방법인 I-JEPA를 소개한다. 기존의 자가 지도 학습 방법론의 한계를 극복하기 위해, I-JEPA는 단일 컨텍스트 블록을 사용하여 동일한 이미지 내의 다양한 타겟 블록의 표현을 예측하는 접근 방식을 채택한다. 이는 의미 있는 타겟 블록을 충분히 큰 규모로 샘플링하고, 정보가 풍부한 컨텍스트 블록을 사용함으로써 달성된다. 실험 결과, I-JEPA는 ImageNet-1K와 같은 대규모 데이터셋에서 우수한 성능을 보이며, 객체 수 세기 및 깊이 예측과 같은 저수준 작업에서도 뛰어난 성능을 나타낸다. 또한, 모델의 효율성과 확장성 면에서 기존 방법들보다 우수하며, 데이터셋 크기와 모델 크기를 확장함에 따라 성능이 향상되는 것을 확인했다. 이러한 결과는 I-JEPA가 다양한 컴퓨터 비전 작업에서 매우 효과적인 접근 방식임을 보여준다.

## Similar Papers
- [An Image is Worth More Than 16x16 Patches: Exploring Transformers on Individual Pixels](2406.09415.md)
- [Vision Transformers Need Registers](2309.16588.md)
- [MambaVision: A Hybrid Mamba-Transformer Vision Backbone](2407.08083.md)
- [Theia: Distilling Diverse Vision Foundation Models for Robot Learning](2407.20179.md)
- [SIGMA: Sinkhorn-Guided Masked Video Modeling](2407.15447.md)
- [4M-21: An Any-to-Any Vision Model for Tens of Tasks and Modalities](2406.09406.md)
- [Diffusion Models Without Attention](2311.18257.md)
- [Affine-based Deformable Attention and Selective Fusion for Semi-dense Matching](2405.13874.md)
- [DiM: Diffusion Mamba for Efficient High-Resolution Image Synthesis](2405.14224.md)
