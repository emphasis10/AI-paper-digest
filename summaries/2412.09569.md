# JuStRank: Benchmarking LLM Judges for System Ranking
## TL;DR
## Summary
- [https://arxiv.org/pdf/2412.09569.pdf](https://arxiv.org/pdf/2412.09569.pdf)

### 1. 주요 섹션 요약

**초록 (Abstract)**:
- 본 논문은 빠르게 발전하는 생성형 AI의 다양한 모델 및 구성 간의 체계적인 비교의 필요성을 강조하고, 이를 극복하기 위한 방안으로 LLM(대형 언어 모델)을 활용한 심사(판단) 기법을 제안합니다. LLM 심사는 시스템 간의 순위를 판단하고 인간 기반의 순위와 비교함으로써 심사 체계의 품질을 평가합니다.

**서론 (Introduction)**:
- 대형 언어 모델의 평가에서 LLM을 심사자로 활용하는 경향이 증가하고 있습니다. 이러한 평가 방식은 고품질의 LLM 심사 자체를 평가하는 것이 필요하며, 이 연구는 시스템 전반의 순위 평가에서 LLM 심사가 얼마나 정확한지를 중점으로 다루고 있습니다.

**문제 정의 (Task Formulation)**:
- 연구는 사용자 지침에 대한 여러 시스템의 응답 품질을 판단하는 LLM 심사의 메커니즘을 제안하고, 이 메커니즘을 통해 시스템의 전반적인 품질 순위를 매기는 방법을 설명합니다.

**실험 설정 (Experimental Setup)**:
- 연구에서는 LLM 기반 심사의 정확도를 다양한 시스템의 응답 집합을 통해 분석하고, 이를 분석하기 위한 다양한 집계 방법을 사용합니다. 최종 순위는 다른 데이터 집합에서 얻은 황금 기준 순위와 비교합니다.

**결과 (Results)**:
- JuStRank라는 시스템 평가 기준을 도입해 LLM 심사의 시스템 순위 판단과 관련된 성능과 행동을 평가하였습니다. JuStRank를 사용하면 LLM 심사의 시스템 간 순위 매기는 능력을 확인할 수 있습니다.

**결론 (Conclusion)**:
- 연구에서는 LLM 심사자의 시스템 순위 매기는 첫 번째 대규모 평가를 수행하였고, JuStRank라는 기준을 통해 LLM의 판별력을 인간 기반의 순위와 비교하였습니다.

### 2. 전체 요약

이 논문은 AI와 머신러닝의 발전 속에서 다양한 모델과 설정을 체계적으로 비교하고 선택하기 위한 새로운 접근법으로 대형 언어 모델을 활용한 심사 체계를 제안합니다. JuStRank라는 새로운 시스템 평가 기준을 통해 LLM 심사가 시스템 순위 매기는 데 있어 얼마나 효과적인지를 확인하고자 했으며, 더욱 정확한 시스템 순위를 위해 LLM의 판별력과 편견을 분석했습니다. JuStRank는 사용자가 다양한 모델과 구성을 비교할 때 필요한 정보를 제공합니다.