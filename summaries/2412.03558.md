# MIDI: Multi-Instance Diffusion for Single Image to 3D Scene Generation
## TL;DR
## Summary
- [https://arxiv.org/pdf/2412.03558.pdf](https://arxiv.org/pdf/2412.03558.pdf)

## 섹션별 요약

- **소개 및 배경**: 단일 이미지로부터 3D 장면을 생성하는 것은 제한된 공간적 정보를 가지고 있어 도전적입니다. 기존 모델들은 주로 데이터베이스에 3D 모델을 저장하거나, 3D 데이터셋에서 훈련된 신경망을 통해 새로운 이미지를 추론하여 장면을 재구성합니다. 이러한 방법들은 새로운 환경에서의 정확성과 일반화 능력이 부족합니다.

- **MIDI 개요**: 본 논문은 MIDI라는 새로운 패러다임을 소개하며, 단일 이미지에서 다중 인스턴스 확산 모델을 사용하여 3D 장면을 생성하는 방법을 제안합니다. MIDI는 사전 훈련된 이미지-3D 오브젝트 생성 모델을 확장하여 다중 인스턴스 주의를 통해 물체 간 상호작용을 효과적으로 캡처합니다.

- **다중 인스턴스 주의 및 확산 모델**: 다중 인스턴스 주의는 장면 내 여러 3D 인스턴스 간의 상호작용을 모델링하는 데 중요합니다. 이를 통해 공간적 관계를 정확하게 나타낼 수 있습니다. 또한, MIDI는 글로벌 장면 이미지와 개별 객체 이미지를 고려하여 물체 간 교차 주의를 통해 학습합니다.

- **훈련 및 실험 결과**: MIDI는 합성 데이터, 실제 이미지, 다양한 스타일의 장면 이미지에서 최첨단 성능을 보여줍니다. 다중 인스턴스 확산 모델은 높은 일반화 능력을 제공하며, 입력 이미지에 더 잘 맞는 3D 장면을 생성합니다. 실험 결과로 MIDI의 정밀도 및 일관성이 입증되었습니다.

- **한계 및 추가 논의**: MIDI는 작은 객체의 해상도가 낮아질 수 있는 문제를 가지고 있으며, 훈련 데이터의 상호작용 관계가 단순하여 복잡한 상호작용을 생성하는 데 한계가 있습니다. 이를 해결하기 위해 다양한 상호작용과 공간적 관계를 포함하는 훈련 데이터가 필요합니다.

## 전체 요약

MIDI는 단일 이미지에서 다중 인스턴스를 효과적으로 생성할 수 있는 새로운 3D 장면 생성 방법을 제안합니다. 기존 모델의 한계를 극복하기 위해 다중 인스턴스 주의 메커니즘을 도입하여 객체 간의 상호작용과 공간적 일관성을 강화하였습니다. MIDI는 실험을 통해 우수한 성능을 입증하며, 입력 이미지에 맞춰 정확한 3D 장면을 생성할 수 있는 가능성을 보여줍니다.