# VeRA: Vector-based Random Matrix Adaptation
## TL;DR
## Summary
- [https://arxiv.org/pdf/2310.11454.pdf](https://arxiv.org/pdf/2310.11454.pdf)

### 1. 각 섹션의 요약 및 주요 공헌

#### Introduction (소개)
- **요약**: 대형 언어 모델의 효율적인 적응이 왜 중요한지를 설명하며, 이를 위한 새로운 방법론인 VeRA(Vector-based Random Matrix Adaptation)를 소개합니다.
- **주요 공헌**:
  - 메모리 및 파라미터 효율성을 극대화하여 기존의 LoRA(Low-rank Adaptation) 방법보다 오히려 더 나은 성능을 보여줍니다.
  
#### Related Work (관련 연구)
- **요약**: LoRA와 같은 기존의 저차 적응 방법들을 리뷰하며, VeRA가 이러한 방법들보다 뛰어난 점을 강조합니다.
- **기술 용어**:
  - LoRA (Low-rank Adaptation): 대형 언어 모델을 효율적으로 미세 조정하기 위한 방법.
  - VeRA: 랜덤 행렬과 스케일링 벡터를 사용하는 새로운 적응 방법.

#### Methodology (방법론)
- **요약**: VeRA의 방법론을 상세히 설명합니다. Low-rank 행렬을 고정시키고, 각 레이어에 대해 작은 스케일링 벡터만 학습하는 방식을 채택합니다.
- **주요 공헌**:
  - 매우 적은 수의 학습 파라미터로도 높은 성능을 유지할 수 있습니다.

#### Experiments (실험)
- **요약**: 여러 벤치마크 (GLUE, E2E)와 이미지 분류 작업에서 VeRA를 테스트하고, LoRA와 비교하여 우수한 성능을 입증합니다.
- **주요 공헌**:
  - GLUE, E2E 벤치마크 및 이미지 분류 작업에서 VeRA가 LoRA보다 적은 학습 파라미터로 더 나은 성능을 보임.

#### Results (결과)
- **요약**: 구체적인 수치 결과를 통해 VeRA의 우수성을 입증합니다.
- **주요 공헌**:
  - GLUE 벤치마크에서 1024 레이어를 학습할 때, VeRA가 평균 85.2 점을 기록.
  - E2E 벤치마크에서 VeRA는 BLEU 점수 기준 LoRA보다 높은 점수를 기록 (70.3 vs 70.1).
  
#### Discussion (토론)
- **요약**: VeRA의 성능, 장점, 그리고 향후 연구 방향을 토론합니다.
- **주요 공헌**:
  - VeRA는 매우 적은 파라미터로도 양호한 성능을 유지하여, 메모리와 계산 리소스를 절약할 수 있습니다.

### 2. 전체 요약

이 논문은 대형 언어 모델의 미세 조정을 위해 새로운 방법론인 VeRA(Vector-based Random Matrix Adaptation)를 제안합니다. VeRA는 기존의 LoRA 방법보다 훨씬 적은 수의 학습 파라미터로도 유사하거나 더 나은 성능을 보여줍니다. 이는 대부분의 파라미터를 고정된 랜덤 행렬로 대체하고, 각 레이어에 대한 작은 스케일링 벡터만 학습하기 때문입니다.

여러 벤치마크와 실험 결과에 따르면, VeRA는 메모리와 계산 리소스를 효율적으로 활용하면서도 높은 성능을 유지할 수 있습니다. 이러한 혁신적인 접근 방식은 대형 언어 모델의 개인화와 다양한 응용 분야에서 중요한 기여를 할 것으로 기대됩니다. 

이러한 점들을 염두에 두고, VeRA는 기존의 방법들에 비해 매우 혁신적인 해결책을 제시하며, 앞으로 더욱 발전할 가능성이 큽니다.