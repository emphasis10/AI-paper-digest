# OptLLM: Optimal Assignment of Queries to Large Language Models
## TL;DR
## Summary
- [https://arxiv.org/pdf/2405.15130.pdf](https://arxiv.org/pdf/2405.15130.pdf)

#### 1. 서론 (Introduction)

이 논문은 대형 언어 모델(LLM)을 비용과 성능의 균형을 맞추어 최적으로 할당하는 문제를 다루고 있습니다. 많은 회사들이 LLM 서비스를 API를 통해 제공하고 있으며, 각 모델은 성능과 비용 면에서 차이가 있습니다. 따라서 사용자에게 가장 적합한 LLM을 선택하는 것이 중요한 과제가 됩니다. 이를 해결하기 위해 OptLLM이라는 프레임워크를 제안합니다. 이 프레임워크는 쿼리별로 가장 적합한 LLM을 선택하여 비용과 성능을 최적화합니다.

#### 2. 관련 연구 (Related Work)

OptLLM은 여러 가지 기존 연구와 비교됩니다. 특히, 비용과 성능을 예측하여 쿼리를 적절한 LLM에 할당하는 다른 프레임워크들과의 비교가 이루어집니다. 다른 연구와 달리, OptLLM은 멀티라벨 분류 모델을 사용하여 예측의 신뢰성을 높이고, 적은 양의 데이터(1%)만으로 예측 모델을 구성합니다.

#### 3. 문제 정의 (Problem Formulation)

이 논문은 쿼리 할당 문제를 다목적 최적화 문제로 정의합니다. 목표는 비용을 최소화하고 정확도를 최대화하는 것입니다. 쿼리는 각 LLM의 토큰 가격에 따라 비용이 산출되며, 각 쿼리에 대해 LLM의 정확도와 비용을 고려하여 할당됩니다.

#### 4. 제안된 접근법 (Proposed Approach)

OptLLM은 예측 컴포넌트와 최적화 컴포넌트로 구성됩니다. 예측 컴포넌트는 멀티라벨 분류 모델을 사용하여 각 쿼리에 대해 후보 LLM이 성공적으로 처리할 가능성을 예측합니다. 예측의 불확실성을 처리하기 위해, 부트스트랩 샘플 예측을 가중 평균하여 집계하고, 표준 편차를 계산하여 불확실성을 정량화합니다. 최적화 컴포넌트는 초기 최적 해를 생성한 후, 파괴 및 재구성 과정을 통해 비지배 해를 반복적으로 생성합니다.

#### 5. 실험 결과 (Experimental Results)

다양한 NLP 및 도메인 특화 과제를 대상으로 한 실험에서, OptLLM은 비용을 줄이면서 정확도를 높일 수 있음을 입증했습니다. OptLLM은 가장 좋은 개별 LLM과 같은 수준의 정확도를 유지하면서 비용을 2.40%에서 49.18%까지 절감할 수 있습니다. 다른 다목적 최적화 알고리즘과 비교했을 때, OptLLM은 정확도를 2.94%에서 69.05%까지 향상시키거나, 비용을 8.79%에서 95.87%까지 절감할 수 있습니다.

#### 6. 결론 (Conclusion)

OptLLM은 쿼리를 적절한 LLM에 자동으로 할당하는 효과적이고 효율적인 프레임워크를 제안합니다. 실험 결과는 OptLLM이 다른 기준 방법들에 비해 효율성과 효과성 면에서 우수함을 보여줍니다. OptLLM의 소스 코드와 실험 결과는 GitHub에서 제공됩니다.

### 전체 요약

이 논문은 대형 언어 모델(LLM)을 비용과 성능을 최적화하여 할당하는 문제를 다룹니다. 이를 위해, OptLLM이라는 프레임워크를 제안합니다. OptLLM은 멀티라벨 분류 모델을 사용하여 각 쿼리에 대해 후보 LLM의 성능을 예측하고, 파괴 및 재구성 과정을 통해 비지배 해를 반복적으로 생성하여 최적화합니다. 다양한 실험 결과는 OptLLM이 비용을 절감하면서 높은 정확도를 유지할 수 있음을 보여줍니다. OptLLM은 다른 다목적 최적화 알고리즘보다 뛰어난 성능을 보이며, LLM의 실제 적용에 있어 실질적인 이점을 제공합니다.


## Similar Papers
- [How Far Can We Go with Practical Function-Level Program Repair?](2404.12833.md)
- [On Speeding Up Language Model Evaluation](2407.06172.md)
- [Automated Data Visualization from Natural Language via Large Language Models: An Exploratory Study](2404.17136.md)
- [SambaNova SN40L: Scaling the AI Memory Wall with Dataflow and Composition of Experts](2405.07518.md)
- [Agentless: Demystifying LLM-based Software Engineering Agents](2407.01489.md)
- [ByteTransformer: A High-Performance Transformer Boosted for Variable-Length Inputs](2210.03052.md)
- [AutoCoder: Enhancing Code Large Language Model with \textsc{AIEV-Instruct}](2405.14906.md)
- [Searching for Best Practices in Retrieval-Augmented Generation](2407.01219.md)
- [Sentiment Analysis of Lithuanian Online Reviews Using Large Language Models](2407.19914.md)
