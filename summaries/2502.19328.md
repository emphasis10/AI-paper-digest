# Agentic Reward Modeling: Integrating Human Preferences with Verifiable Correctness Signals for Reliable Reward Systems
## TL;DR
## Summary
- [https://arxiv.org/pdf/2502.19328.pdf](https://arxiv.org/pdf/2502.19328.pdf)

1. 각 섹션의 중요한 내용을 요약하고 메인 기여 및 혁신 부분을 설명하겠습니다.

- **서론**: 이 연구는 기존의 보상 모델이 주로 인간의 선호도에 집중하는 한계를 지적하며, 검증 가능한 정확성 신호를 통합한 '주체적 보상 모델링'(agentic reward modeling)을 제안합니다.

- **REWARDAGENT 설계 및 구현**: REWARDAGENT는 각각 사실성과 지시문 준수도를 검증하는 에이전트를 포함하는 구조로 설계되었습니다. 이는 인간 선호 보상 모델과의 통합을 통해 더욱 신뢰성 있는 보상을 제공합니다.

- **실험 결과**: REWARDAGENT는 다양한 테스트 베드에서 뛰어난 성능을 보여주며, 기존의 보상 모델을 능가하고, 실습 데이터와 함께 LLM 트레이닝에서 우수한 성과를 보였습니다.

- **결론 및 제안**: REWARDAGENT의 효과를 입증한 다양한 실험 내용이 있으며, 이 연구는 AI 분야에서도 응용될 수 있는 잠재력을 가지고 있음을 강조합니다.

2. 전체 요약:

이 논문은 보상 시스템 개발에 있어서 인간 선호도와 검증 가능한 정확성 신호를 결합하여 '주체적 보상 모델링'이라는 새로운 접근 방식을 제안합니다. 이를 통해 REWARDAGENT를 구현했으며, 이는 기존의 보상 모델보다 뛰어난 성능을 발휘하고, 대규모 언어 모델의 훈련에 효과적임을 실험을 통해 확인했습니다. 연구는 더욱 발전된 보상 시스템 개발을 위한 방향을 제시하고 있습니다.