# WildBench: Benchmarking LLMs with Challenging Tasks from Real Users in the Wild
## TL;DR
## Summary
- [https://arxiv.org/pdf/2406.04770.pdf](https://arxiv.org/pdf/2406.04770.pdf)

### 1. 섹션별 요약 및 주요 기여와 혁신적인 부분

#### 개요 (Abstract)
이 논문은 WildBench라는 자동화 평가 프레임워크를 소개하며, 이를 사용하여 자연어 처리 모델(LLMs)에 대한 성능을 실제 사용자 질문 기준으로 평가합니다. WildBench는 약 1,024개의 사실 기반의 다양한 과제를 포함하며, 두 가지 주요 평가 지표인 WB-Reward와 WB-Score를 사용하여 모델의 성능을 분석합니다. 이 평가 지표는 인간의 평가와 높은 상관관계를 보이며, 보다 신뢰도 높은 자동화 평가를 제공합니다.

#### 소개 (Introduction)
AI 기반 자연어 처리 모델들이 다양한 실제 과제를 해결하는 능력은 매우 중요하지만, 이러한 모델의 성능을 효과적으로 평가하는 것은 여전히 어려운 과제입니다. 전통적인 벤치마크는 다중 선택 질문을 통해 모델의 추론 능력을 평가하지만, 실제 사용자가 제기하는 개방형 문제를 다루기에는 부족합니다. Chatbot Arena와 같은 플랫폼은 인간의 선호도를 수집하여 모델의 성능을 평가하지만, 높은 노동 비용과 실시간 평가의 어려움 등의 한계가 있습니다. 이를 해결하기 위해 자동화된 평가 프레임워크인 WildBench가 제안되었습니다.

#### 데이터 수집 (Data Collection)
WildBench의 데이터는 WildChat 프로젝트를 통해 수집된 실제 사용자-챗봇 대화 기록을 기반으로 합니다. 모델은 이러한 데이터를 분석하여 각 과제의 난이도를 평가하며, 모든 과제는 최종적으로 수작업 검토를 거칩니다. 이를 통해 WildBench의 과제 분포는 자연스러우며 다양한 사용 사례를 반영합니다.

#### 평가 방법 (Evaluation Methods)
WildBench는 두 가지 주요 평가 지표인 WB-Reward와 WB-Score를 사용합니다. WB-Reward는 모델 간의 페어와이즈 비교를 통해 성능을 평가하며, 모델의 응답 길이 편향을 줄이기 위해 길이 페널티 방식을 도입했습니다. WB-Score는 개별 모델의 응답 품질을 빠르고 비용 효율적으로 평가합니다. 이 두 지표는 인간의 평가와 높은 상관관계를 보이고 있어, 모델의 성능을 보다 정확하게 반영할 수 있습니다.

#### 결과 분석 (Results and Analysis)
WildBench의 리더보드 분석을 통해 다양한 모델의 성능을 비교할 수 있으며, 각 모델의 강점과 약점을 식별할 수 있습니다. 대표적인 예로, GPT-4-Turbo와 Claude 3 Opus는 모든 과제 범주에서 우수한 성능을 보이는 반면, Llama-3-8B-Inst와 Yi-1.5-34B-chat은 코딩 및 수학 관련 과제에서 다소 약한 성능을 보였습니다.

#### 결론 및 미래 방향 (Conclusion and Future Directions)
WildBench는 실제 사용자 질문을 기준으로 LLM을 평가하기 위해 설계된 프레임워크로, 지속적으로 새로운 예제를 추가하여 모델의 진화하는 능력을 반영합니다. 주요 지표인 WB-Reward와 WB-Score는 기존의 평가 지표보다 더 높은 상관관계를 보이며, 다양한 모델에 대한 성능 분석을 제공합니다. 이를 통해 LLM의 실제 사용 능력을 보다 정확하게 평가할 수 있습니다.

### 2. 전체 요약
WildBench는 실제 사용자 질문을 기반으로 LLM의 성능을 평가하기 위해 고안된 자동화 평가 프레임워크입니다. 이 프레임워크는 1,024개의 다양한 과제를 포함하며, 두 가지 주요 지표인 WB-Reward와 WB-Score를 통해 모델의 성능을 분석합니다. 이러한 평가 지표는 인간의 판단과 높은 상관관계를 보이며, 보다 신뢰성 높은 자동화 평가를 제공합니다. 이를 통해 다양한 모델의 강점과 약점을 식별할 수 있으며, 모델의 실사용 능력을 정확하게 평가할 수 있습니다. WildBench는 지속적으로 새로운 데이터를 추가하여 진화하는 모델의 능력을 반영하고자 합니다.

## Similar Papers
- [WildVision: Evaluating Vision-Language Models in the Wild with Human Preferences](2406.11069.md)
- [MMAU: A Holistic Benchmark of Agent Capabilities Across Diverse Domains](2407.18961.md)
- [The Art of Saying No: Contextual Noncompliance in Language Models](2407.12043.md)
- [Perplexed by Perplexity: Perplexity-Based Data Pruning With Small Reference Models](2405.20541.md)
- [Searching for Best Practices in Retrieval-Augmented Generation](2407.01219.md)
- [WildGuard: Open One-Stop Moderation Tools for Safety Risks, Jailbreaks, and Refusals of LLMs](2406.18495.md)
- [Magpie: Alignment Data Synthesis from Scratch by Prompting Aligned LLMs with Nothing](2406.08464.md)
- [SimPO: Simple Preference Optimization with a Reference-Free Reward](2405.14734.md)
- [ChatGLM-Math: Improving Math Problem-Solving in Large Language Models with a Self-Critique Pipeline](2404.02893.md)
