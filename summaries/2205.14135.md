# FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness
## TL;DR
## Summary
- [https://arxiv.org/pdf/2205.14135.pdf](https://arxiv.org/pdf/2205.14135.pdf)

### 논문 요약: FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness

#### 1. 소개
Transformer 모델은 긴 시퀀스를 처리할 때 시간과 메모리 면에서 비효율적입니다. 이 문제를 해결하기 위해 FlashAttention은 IO 인식(입출력 인식)을 도입하여 메모리 읽기/쓰기 횟수를 줄이고 효율성을 높였습니다.

#### 2. 배경
Transformer의 주요 구성 요소인 self-attention은 시간 및 메모리 복잡도가 시퀀스 길이에 따라 제곱으로 증가합니다. 기존의 근사 attention 방법은 정확도를 희생하여 계산 복잡도를 줄이려 했지만 실질적인 속도 향상을 보여주지 못했습니다.

#### 3. 제안된 접근 방법
FlashAttention은 타일링(tiling)과 재계산(recomputation)을 사용하여 메모리 접근 횟수를 줄이고 효율적으로 exact attention을 계산합니다. 이 방법은 GPU 고대역폭 메모리(HBM)와 온칩 SRAM 간의 데이터 전송을 최소화하여 성능을 향상시킵니다.

#### 4. 실험 및 평가
1. **훈련 속도**: FlashAttention은 BERT-large 모델 훈련을 15% 더 빠르게 수행하며, GPT-2는 기존 방법들보다 최대 3배 더 빠르게 훈련할 수 있습니다.
2. **모델 품질**: FlashAttention은 긴 시퀀스를 모델링하여 모델의 품질을 향상시킵니다. GPT-2의 퍼플렉시티(perplexity)를 0.7 포인트 향상시키고, 긴 문서 분류 작업에서 6.4 포인트의 향상을 보여줍니다.
3. **벤치마킹**: FlashAttention은 시퀀스 길이 최대 64K까지 메모리와 런타임 성능을 측정하여 기존 방법들보다 최대 3배 더 빠릅니다.

#### 5. 결론
FlashAttention은 메모리와 시간 효율성을 개선하여 Transformer 모델의 성능을 향상시킵니다. 이 방법은 더 긴 시퀀스를 처리할 수 있어 새로운 응용 가능성을 열어줍니다.

### 전체 요약
FlashAttention 논문은 Transformer 모델의 긴 시퀀스 처리에서의 비효율성을 해결하기 위해 IO 인식(입출력 인식) 알고리즘을 제안합니다. 타일링과 재계산을 통해 메모리 접근 횟수를 줄여 훈련 속도와 모델 품질을 향상시키는 FlashAttention은 기존 방법들보다 최대 3배 더 빠른 성능을 보여줍니다. 이 방법은 Transformer 모델이 더 긴 시퀀스를 처리할 수 있게 하여 새로운 응용 가능성을 열어줍니다.

## Similar Papers
- [FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning](2307.08691.md)
- [FlashAttention-3: Fast and Accurate Attention with Asynchrony and Low-precision](2407.08608.md)
- [Mamba: Linear-Time Sequence Modeling with Selective State Spaces](2312.00752.md)
- [Learning to (Learn at Test Time): RNNs with Expressive Hidden States](2407.04620.md)
- [SliceGPT: Compress Large Language Models by Deleting Rows and Columns](2401.15024.md)
- [Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality](2405.21060.md)
- [SparseGPT: Massive Language Models Can Be Accurately Pruned in One-Shot](2301.00774.md)
- [Consistency Flow Matching: Defining Straight Flows with Velocity Consistency](2407.02398.md)
- [Better & Faster Large Language Models via Multi-token Prediction](2404.19737.md)
