# Amuro & Char: Analyzing the Relationship between Pre-Training and Fine-Tuning of Large Language Models
## TL;DR
## Summary
- [https://arxiv.org/pdf/2408.06663.pdf](https://arxiv.org/pdf/2408.06663.pdf)

### 1. 섹션별 요약 및 주요 기여와 혁신 요약

#### Abstract (초록)
이 논문은 대형 언어 모델(LLM)로 이루어진 "사전 학습 후 정렬" 패러다임의 관계를 탐구합니다. 여러 중간 사전 학습 체크포인트에 대해 미세 조정을 수행하여 사전 학습과 미세 조정 간의 상호작용을 검토합니다. 결과는 사전 학습이 미세 조정 후에야 드러나는 잠재적 개선을 가져오며, 미세 조정이 특정한 도메인을 잊게 할 수 있음을 밝힙니다. 또한 제시된 평가 프롬프트에 대한 모델의 민감성도 논의됩니다.  

#### Introduction (서론)
대형 언어 모델은 일반적인 자연어 처리 작업의 범위를 크게 변화시켰으며, 데이터 수집과 모델 훈련에 새로운 패러다임을 도입했습니다. 이 연구는 모델의 사전 학습과 미세 조정이 결과 모델에 미치는 영향을 탐구하며, 사전 학습이 미세 조정 결과에 어떤 영향을 미치는지 알아봅니다. 특히, 모델이 학습하는 과정에서 어떤 정보가 중요한지에 대해 분석하여 모델 개발자에게 유용한 통찰을 제공합니다.

#### Background (배경)
이 섹션은 대형 언어 모델 훈련의 핵심 구성 요소인 사전 학습, 미세 조정, 지시 훈련에 대해 설명합니다. 또한, 모델 정렬의 개념과 관련 연구들을 소개하며, 사전 학습 및 미세 조정의 효율성을 탐구하는 실험의 필요성을 강조합니다.

#### Model Training and Fine-Tuning (모델 훈련 및 미세 조정)
사전 학습 단계에서 모델은 방대한 텍스트 코퍼스를 기반으로 훈련됩니다. 사후 미세 조정 단계에서는 모델을 특정 작업에 맞추어 성능을 최적화합니다. 이 과정에서 중간 체크포인트들을 통해 모델의 능력 변화를 평가합니다. 사전 학습 및 미세 조정된 모델의 성능을 비교하여 각 단계에서의 모델 학습 내용을 분석합니다.

#### Experiments (실험 설정 및 방법)
주요 실험 모델은 OLMo-1B로, 사전 학습 도중의 여러 체크포인트를 통한 미세 조정을 비교 분석합니다. 각 체크포인트와 데이터셋에 대해 슈퍼바이즈드 및 지시 기반 미세 조정을 수행하고, 결과 모델의 성능을 평가합니다. 이를 통해 모델이 사전 학습 도중 무엇을 배우는지, 미세 조정을 통해 어떤 성능 변화를 보이는지 탐구합니다.

#### Results and Discussion (결과 및 논의)
실험 결과, 계속적인 사전 학습이 모델 성능을 개선하며, 미세 조정 후에도 이러한 개선이 유효함을 보여줍니다. 특히 사전 학습 단계에서 잘 수행된 작업은 미세 조정으로 추가적인 이익을 얻지 못하지만, 잘 수행되지 않은 작업은 큰 개선을 보였습니다. 또한, 모델의 민감성 문제가 발견되었으며, 이는 추가적인 사전 학습을 통해 완화될 수 있습니다.

#### Conclusion (결론)
논문은 대형 언어 모델의 사전 학습과 미세 조정 간의 관계를 체계적으로 분석하였고, 두 단계가 상호작용하여 모델 성능에 미치는 영향을 강조했습니다. 사전 학습과 성능 최적화의 균형을 통해 더 나은 모델을 개발하는 방법을 제안하며, 미래 연구를 위한 방향성을 제시합니다.

### 2. 전체 요약
이 논문은 대형 언어 모델(LLM)의 사전 학습과 미세 조정 간의 상호작용을 분석합니다. 주된 기여는 다양한 체크포인트에서 수행된 미세 조정 실험을 통해 사전 학습이 모델 성능에 미치는 영향을 체계적으로 탐구한 것입니다. 실험 결과 사전 학습이 계속됨에 따라 모델의 잠재적인 성능 향상이 미세 조정 후에 드러나며, 사전 학습 도중 전혀 성능이 향상되지 않은 작업들도 미세 조정을 통해 큰 성능 향상을 보일 수 있음을 보여줍니다. 이 연구는 사전 학습 및 미세 조정의 실질적인 전략을 수립하는 데 중요한 통찰을 제공하며, 모델 민감성 문제를 완화하기 위한 추가적인 사전 학습의 필요성을 강조합니다.