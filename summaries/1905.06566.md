# HIBERT: Document Level Pre-training of Hierarchical Bidirectional Transformers for Document Summarization
## TL;DR
## Summary
- [https://arxiv.org/pdf/1905.06566.pdf](https://arxiv.org/pdf/1905.06566.pdf)

### 1. 섹션 요약

#### 초록 (Abstract)
초록에서는 문서 요약을 위한 계층적 양방향 변환기(Hierarchical Bidirectional Transformers, HIBERT)의 사전 학습 방법을 제안합니다. HIBERT는 라벨이 없는 데이터로 사전 학습을 통해 문서 요약 성능을 크게 향상시킵니다. CNN/Dailymail과 New York Times 데이터셋에서 최첨단 성과를 달성했습니다.

#### 서론 (Introduction)
자동 문서 요약은 중요한 내용을 유지하면서 문서를 짧게 요약하는 작업입니다. 기존의 중요한 접근 방식은 추출적 요약과 생성적 요약으로 나뉩니다. 추출적 요약은 원본 문서의 일부(주로 문장)를 추출하여 요약을 생성하는 반면, 생성적 요약은 원본 문서에 없는 새로운 단어를 생성할 수 있습니다. 본 논문에서는 변환기 모델을 사용해 계층적 문서 인코더를 사전 학습하여 추출적 요약의 성능을 개선했습니다.

#### 관련 연구 (Related Work)
본 섹션에서는 추출적 요약과 생성적 요약, 사전 학습된 자연어 처리 모델에 대한 연구를 소개합니다. 학습된 추출적 요약 모델은 문장의 점수화 문제로 다루어지며, 생성적 요약 모델은 주로 시퀀스 투 시퀀스(seq2seq) 학습에 기반합니다. 최근의 연구는 추출적 모델과 생성적 모델을 결합하여 성능 향상을 시도하고 있습니다.

#### 모델 (Model)
HIBERT 모델은 문서 내 문장을 인코딩하며, 사전 학습된 계층적 변환기 인코더를 사용합니다. 문서 내 문장들을 순차적으로 라벨링하여 중요한 문장을 추출합니다. HIBERT는 문장 레벨과 문서 레벨에서 변환기를 사전 학습하여 최종적으로 문서 요약에 적용합니다.

#### 실험 (Experiments)
HIBERT 모델의 성능을 CNN/Dailymail 및 New York Times 데이터셋에서 평가했습니다. 사전 학습과 미세 조정 과정을 통해 모델의 성능을 검증했으며, 다양한 기존 모델들과 비교하여 우수한 성능을 보였습니다. HIBERT 모델은 ROUGE 점수에서도 높은 성과를 기록했습니다.

#### 결론 (Conclusion)
HIBERT는 문서 요약을 위한 강력한 사전 학습된 계층적 양방향 변환기 인코더를 제안합니다. 사전 학습된 변환기를 추출적 요약 모델에 적용하여 요약 성능을 크게 향상시켰습니다. 향후 작업으로는 문서 질문 응답 등 다른 문서 인코딩이 필요한 작업에 모델을 적용하고, 계층적 문서 인코더의 아키텍처와 학습 목표를 개선할 계획입니다.

### 2. 전체 요약

이 논문은 HIBERT라는 문서 요약을 위한 계층적 양방향 변환기 인코더를 제안합니다. 추출적 요약 모델의 성능을 크게 향상시키기 위해 라벨이 없는 데이터를 활용하여 모델을 사전 학습합니다. HIBERT는 CNN/Dailymail과 New York Times 데이터셋에서 최첨단 성과를 기록했으며, 실험 결과 높은 ROUGE 점수를 달성했습니다. 모델은 문서 내 중요 문장을 추출함으로써 요약을 생성합니다. 향후 작업으로는 문서 질문 응답 등 다양한 작업에 HIBERT를 적용하고, 인코더의 아키텍처와 학습 방법을 지속적으로 개선할 예정입니다.

이 내용들을 바탕으로 프레젠테이션을 준비하면 AI와 머신 러닝 분야에서의 최신 혁신을 공유할 수 있을 것입니다.