# Back to Basics: Revisiting REINFORCE Style Optimization for Learning from Human Feedback in LLMs
## TL;DR
## Summary
- [https://arxiv.org/pdf/2402.14740.pdf](https://arxiv.org/pdf/2402.14740.pdf)

### 1. 논문의 주요 내용 요약

#### 1. Introduction (도입부)
이 논문에서는 인간 피드백을 활용한 강화학습(RLHF)에서 Proximal Policy Optimization (PPO) 방법론이 높은 계산 비용과 민감한 하이퍼파라미터 튜닝 문제로 인해 적합하지 않음을 지적합니다. 대신, 보다 간단한 REINFORCE 스타일 최적화가 유리할 수 있음을 주장합니다.

#### 2. Background (배경)
RLHF는 세 가지 단계를 거쳐 실행됩니다:
1. 지도 학습을 통한 모델 훈련 (SFT).
2. 보상 모델 훈련.
3. 정책 최적화를 위한 강화학습 (RL).

#### 3. Proximal Policy Optimization (PPO)
PPO는 각 토큰을 하나의 액션으로 모델링하고 부분 시퀀스를 상태로 본다. 이러한 접근 방식은 불필요하게 복잡하며 높은 계산 비용이 든다는 단점이 있습니다. 학습에서 클리핑 비율을 사용하여 큰 정책 업데이트를 방지하지만, 이는 RLHF 설정에서 필요하지 않다는 결론을 내립니다.

#### 4. REINFORCE
REINFORCE는 전체 시퀀스를 단일 액션으로 모델링하여 간단화된 방법론을 제안합니다. REINFORCE의 최종 목표는 시퀀스 단위의 보상을 최적화하는 것입니다.

#### 5. REINFORCE Leave-One-Out (RLOO)
RLOO는 여러 샘플을 활용하여 편향 없는 보상 추정값을 만들어 학습 변동성을 줄이는 방법입니다. 이 방법은 더 높은 보상을 제공하면서 계산 효율성을 유지합니다.

#### 6. 실험 및 결과
실험 결과, REINFORCE와 RLOO가 PPO보다 더 높은 성능을 보였으며, REINFORCE 스타일 최적화가 다양한 모델 및 데이터셋에서 우수한 성능을 나타냈습니다. RLOO는 노이즈에 대한 높은 내성을 보여줍니다.

#### 7. 결론
REINFORCE 스타일 최적화는 RLHF 상황에서 더 높은 성능과 효율성을 제공합니다. 또한, RLOO는 계산비용을 줄이면서도 우수한 성능을 유지합니다.

### 2. 논문의 전체 요약
이 논문은 RLHF(인간 피드백을 활용한 강화학습)에서 PPO의 복잡성을 지적하고, 이를 대체할 수 있는 보다 간단한 REINFORCE 스타일의 최적화 방법을 제안합니다. REINFORCE는 전체 시퀀스를 하나의 액션으로 모델링하며, 특히 RLOO 방식은 변동성을 줄이고 높은 보상을 제공하면서도 계산 효율성을 유지합니다. 실험 결과, REINFORCE와 RLOO가 PPO보다 더 나은 성능을 보였고, 노이즈에 대한 내성도 뛰어남을 확인할 수 있었습니다. 이 논문의 주요 기여는 RLHF 환경에서 효율적이고 성능이 우수한 REINFORCE 스타일 최적화를 입증한 것입니다.