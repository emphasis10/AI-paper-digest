# BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models
## TL;DR
## Summary
- [https://arxiv.org/pdf/2301.12597.pdf](https://arxiv.org/pdf/2301.12597.pdf)

이 논문에서는 BLIP-2라는 새로운 비전-언어 사전 학습(Vision-Language Pre-training, VLP) 프레임워크를 제안하고 있습니다. 이 프레임워크는 고정된 이미지 인코더와 대형 언어 모델(LLM)을 활용하여 효율적인 사전 학습을 가능하게 합니다.

### 각 섹션 요약:

1. **서론 (Introduction)**:
   이 논문은 비전과 언어의 다양한 작업에서 뛰어난 성능을 발휘하는 거대한 사전 학습 모델의 높은 계산 비용 문제를 해결하고자 합니다. BLIP-2는 고정된 이미지 인코더와 LLM을 사용하여 이러한 비용을 줄이고자 합니다.

2. **관련 연구 (Related Work)**:
   기존의 연구들과 다르게, BLIP-2는 고정된 이미 모델과 언어 모델 모두를 효과적이고 효율적으로 활용하여 여러 비전-언어 작업에서 더 강력한 성능을 제공합니다.

3. **방법론 (Methodology)**:
   BLIP-2의 핵심은 Querying Transformer (Q-Former)로, 두 단계의 사전 학습을 통해 모달리티 간 격차를 메웁니다. 첫 번째 단계는 고정된 이미지 인코더를 통해 비전-언어 표현 학습을 진행하고, 두 번째 단계는 고정된 LLM을 통해 비전-언어 생성 학습을 진행합니다.

4. **결과 (Experimental Results)**:
   BLIP-2는 다양한 비전-언어 과업에서 기존의 방법들보다 적은 학습 가능한 파라미터로 최첨단 성능을 달성하며, 제로 샷 이미지-텍스트 생성 능력을 보여줍니다.

5. **제한 사항 (Limitations)**:
   BLIP-2는 LLM이 적절하지 않은 추론 경로를 활성화하거나 최신 정보를 갖추고 있지 않을 경우, 이미지-텍스트 생성이 기대에 미치지 못할 수 있습니다. 또한 고정된 모델 사용으로 인해 LLM의 사회적 편향성 문제 등을 그대로 물려받습니다.

6. **결론 (Conclusion)**:
   BLIP-2는 소수의 학습 가능한 파라미터를 가지고도 여러 비전-언어 과업에서 최첨단 성능을 보여주는 효율적인 방법입니다. 우리는 BLIP-2를 다중 모달 대화형 인공지능 에이전트 구축을 향한 중요한 단계로 간주합니다.

### 전체 요약:
이 연구의 주요 기여는 고정된 이미지 인코더와 대형 언어 모델을 활용하여 적은 계산 비용으로도 효과적인 비전-언어 사전 학습을 가능하게 만든 것입니다. BLIP-2는 뛰어난 성능을 보이며, 새로운 능력을 가진 제로 샷 이미지-텍스트 생성이 가능합니다. 이 연구는 향후 다중 모달 AI 시스템의 발전에 중요한 역할을 할 것으로 기대됩니다.