# AutoJudge: Judge Decoding Without Manual Annotation
## TL;DR
## Summary
- [https://arxiv.org/pdf/2504.20039.pdf](https://arxiv.org/pdf/2504.20039.pdf)

1. **섹션별 요약**
   
   - **서론**  
     이 논문은 대규모 언어 모델(LLM) 추론을 가속화하는 AutoJudge라는 프레임워크를 소개합니다. 이는 손실이 허용되는 추정적 디코딩 방식을 통해 특정 작업에서 중요하지 않은 토큰들을 빠르게 생성하여 처리 속도를 높입니다.

   - **배경**  
     특히 추정적 디코딩은 LLM의 성능을 확보하면서도 추론을 가속화하는 데 도움을 줍니다. 이 방법은 작은 '초안' 모델에서 잠재적인 다음 토큰을 생성한 뒤, 메인 모델로 이를 검증합니다.

   - **방법론**  
     AutoJudge는 세 가지 단계로 구성됩니다. 첫째, 불일치하는 토큰 중 모델 품질에 영향을 미치는 것을 밝히기 위해 반탐욕적 검색 알고리즘을 사용합니다. 둘째, 이렇게 수집된 데이터를 활용해 중요한 토큰을 탐지하는 경량화된 분류기를 훈련시킵니다. 셋째, 이 분류기를 사용해 디코딩 알고리즘을 개선합니다.

   - **실험 및 결과**  
     AutoJudge는 GSM8K와 LiveCodeBench에서 테스트되었습니다. 결과적으로, 최대 1.5배 더 많은 토큰을 검증 주기당 허용하면서도 정확도 손실은 1% 미만으로 유지했습니다. 프로그래밍 작업에서도 유사한 성과를 거두었습니다.

   - **논의**  
     이 방법은 손실이 허용되는 추정적 디코딩을 통해 더 효율적인 추론을 가능하게 하며, 특히 지속적으로 길어지는 시퀀스를 생성하면서 발생하는 비용을 줄이는 데 기여합니다.

2. **전체 요약**
   
   이 논문은 AutoJudge라는 프레임워크를 통해 대규모 언어 모델의 추론 속도를 개선하는 방법을 제안하고 있습니다. 특히, 손실이 허용되는 추정적 디코딩을 도입하여 중요하지 않은 토큰을 빠르게 생성함으로써 처리 속도를 대폭 향상시킵니다. 이 과정에서 중요한 토큰을 식별하기 위한 반탐욕적 검색 및 경량화된 분류기 활용 등 혁신적인 접근 방식을 채택했습니다. GSM8K 및 LiveCodeBench를 통한 실험 결과는 이 방법의 효과를 뒷받침하며, 이는 특히 긴 시퀀스를 생성하는 과정을 보다 효율적으로 만들고자 하는 분야에 큰 기여를 할 수 있음을 시사합니다.