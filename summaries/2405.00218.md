# Constrained Decoding for Secure Code Generation
## TL;DR
## Summary
- [https://arxiv.org/pdf/2405.00218.pdf](https://arxiv.org/pdf/2405.00218.pdf)

### 논문 요약: 섹션별 주요 내용

#### 1. 서론
이 논문은 개발자들의 생산성을 높이기 위해 사용되는 코드 생성 대형 언어 모델(Code LLMs)이 보안 취약한 코드를 생성할 수 있다는 문제를 다룹니다. GPT와 같은 모델들이 코드 생성을 위한 다양한 방법들을 제안하지만, 상당수의 코드가 기능적인 완전성을 보장하지 못합니다.

#### 2. 배경 및 관련 연구
Code LLM 언어 모델 예시로 GitHub Copilot, Amazon CodeWhisperer 등이 있습니다. 기존 연구들은 코드의 생성 품질을 pass@k라는 지표로 측정하고 있지만, 보안 요소를 간과한 평가 방법들이 주를 이룹니다.

#### 3. 새로운 평가 지표 제안
이 논문은 보안과 완전성을 동시에 측정할 수 있는 새로운 지표인 `secure-pass@k`와 `secure@kpass`를 제안합니다. 이는 기존 지표들이 가지는 한계를 보완하고, 코드의 보안과 기능성을 동시에 평가할 수 있게 합니다.

#### 4. 제한된 디코딩 기술
제한된 디코딩을 이용하여 `Constrained Beam Sampling` 기법을 도입했습니다. 이 기술은 코드 생성 시 보안 및 기능적 완전성을 동시에 충족하는 코드를 생성할 수 있도록 합니다. 이 기법을 통해 코드의 다양성을 확보하고, 보안 및 기능성을 동시에 유지하는 코드를 생성합니다.

#### 5. 평가
논문에서는 CodeGuard+라는 새로운 평가 메트릭스를 사용하여 다양한 코드 LLMs를 테스트했습니다. 평가 결과 `Constrained Beam Sampling` 기법이 기존의 비제한 디코딩 방식보다 보안성이 우수한 코드를 생성하는데 유리함을 확인했습니다.

#### 6. 결론
이 연구는 보안과 기능성을 충족하는 코드 생성을 장려하기 위해 새로운 평가 기준과 제한된 디코딩 기법을 제안했습니다. 이를 통해 Code LLMs의 보안성을 높이고, 연구 커뮤니티의 발전을 도모하고자 합니다.

### 전체 요약
이 논문은 코드 생성 대형 언어 모델(Code LLMs)이 생성하는 코드의 보안성과 기능적 완전성을 동시에 보장하기 위한 새로운 평가 기법과 디코딩 방법을 제안합니다. 기존의 지표들이 보안만을 평가하고 기능성을 간과한 한계를 극복하기 위해, `secure-pass@k`와 `secure@kpass`라는 두 가지 새로운 지표를 도입했습니다. 또한, `Constrained Beam Sampling`이라는 새로운 디코딩 기법을 개발하여 코드의 다양성과 품질을 동시에 확보할 수 있도록 했습니다. 평가 결과 제안된 기법이 기존 방법들보다 더 나은 보안성과 완전성을 제공함을 확인했습니다.

이를 통해 개발자들이 더 안전한 코드를 생성하고, Code LLMs의 발전을 이끌 수 있을 것으로 기대됩니다.

## Similar Papers
- [Applying RLAIF for Code Generation with API-usage in Lightweight LLMs](2406.20060.md)
- [A False Sense of Safety: Unsafe Information Leakage in 'Safe' AI Responses](2407.02551.md)
- [The Good, The Bad, and The Greedy: Evaluation of LLMs Should Not Ignore Non-Determinism](2407.10457.md)
- [Yuan 2.0-M32: Mixture of Experts with Attention Router](2405.17976.md)
- [Zero-Shot Tokenizer Transfer](2405.07883.md)
- [Measuring memorization in RLHF for code completion](2406.11715.md)
- [AI Agents That Matter](2407.01502.md)
- [DevEval: A Manually-Annotated Code Generation Benchmark Aligned with Real-World Code Repositories](2405.19856.md)
- [NATURAL PLAN: Benchmarking LLMs on Natural Language Planning](2406.04520.md)
