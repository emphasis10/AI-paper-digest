# GPT or BERT: why not both?
## TL;DR
## Summary
- [https://arxiv.org/pdf/2410.24159.pdf](https://arxiv.org/pdf/2410.24159.pdf)

1. **각 섹션의 요약과 주요 공헌 및 혁신적 부분:**

- **서론**: 이 논문은 마스크 언어 모델(MLM)과 인과적 언어 모델(CLM) 간의 경계를 허물기 위해 만들어진 GPT-BERT라는 하이브리드 접근을 소개합니다. 두 모델링 패러다임의 장점을 결합하여 BERT와 GPT의 기능을 동시에 발휘할 수 있습니다.

- **방법론**: 마스크드 차기 토큰 예측(MNTP)라는 변경된 버전을 사용하여 MLM을 CLM과 통합하였습니다. 데이터셋을 복제하여 각각 다른 목적에 사용할 수 있도록 하고, 동일한 크로스 엔트로피 손실을 사용하여 두 가지 모델 모두 학습하도록 하였습니다.

- **사전 학습 및 평가**: 사전 학습 과정은 BabyLM Challenge를 바탕으로하며, 신탁 수집을 통해 작은 규모의 언어 모델링에서 성능을 개선하는 것이 목표입니다. BLiMP와 EWOK 같은 다양한 벤치마크를 사용하여 모델의 능력을 평가했습니다.

- **결론**: 하이브리드 접근 방식은 단일 목표 모델보다 더 나은 성능을 보이며, 구조적 수정 없이도 다양한 태스크를 처리할 수 있는 가능성을 열어줍니다. 특히 작은 데이터와 매개변수에서도 성능이 향상되었다는 점에서 ML 및 AI 연구 발전에 크게 기여할 수 있습니다.

2. **전반적인 요약:**

이 논문에서는 마스크 언어 모델(MLM)과 인과적 언어 모델(CLM)을 하나의 하이브리드 모델로 결합한 GPT-BERT를 제안하고, 이를 통해 성능을 향상시키는 방법을 탐구합니다. 새로운 마스크드 차기 토큰 예측(MNTP) 방법을 적용하여 두 모델링 목표를 통합하고, 이를 통해 더욱 견고하고 유연한 언어 모델을 개발할 수 있음을 보여줍니다. 이 모델은 제한된 데이터 상에서도 뛰어난 성능을 보여, 적은 자원으로 다양한 자연어 처리 태스크를 수행하는 데 있어 효과적인 접근법이라는 것을 입증했습니다.