# A Unified View of Delta Parameter Editing in Post-Trained Large-Scale Models
## TL;DR
## Summary
- [https://arxiv.org/pdf/2410.13841.pdf](https://arxiv.org/pdf/2410.13841.pdf)

이 논문은 대규모 사전 훈련된 모델을 다양한 작업에 맞추기 위한 "후처리" 기법을 다루고 있습니다. 논문은 이러한 과정에서 발생하는 델타 파라미터에 중점을 두고 있으며, 델타 파라미터는 사전 훈련 모델과 후처리 모델 간의 변수 차이를 의미합니다. 본 논문에서 제안한 주요 기여점과 혁신적인 부분을 다음과 같이 요약합니다.

### 1. 논문 요약:

#### 서론
큰 성공을 거두고 있는 대규모 사전 훈련 모델들을 다양한 작업에 적응시키기 위해 후처리가 필수적이라는 점을 강조합니다. 이러한 과정에서 델타 파라미터가 중요한 역할을 하며, 후처리 효과가 델타 파라미터에 반영된다고 설명합니다.

#### 델타 파라미터 에디팅
기존의 델타 파라미터 수정 기법은 주로 성능을 유지하거나 향상시키거나 저하시키는 쪽으로 나뉩니다. "DARE"와 같은 기술은 모델 성능을 유지하며, "BitDelta"는 성능을 다소 감소시키고, "EXPO"는 성능을 향상시킵니다. 이 논문은 이러한 방법들을 리만 합 근사 기반의 통합된 관점으로 분석하여, 왜 특정 기법이 성능에 영향을 미치는지를 설명합니다.

#### 주요 기여 및 혁신점
- **리만 합 근사**를 통한 통합 뷰: 기존의 델타 파라미터 수정 기법들이 모델 성능에 미치는 영향을 이론적으로 분석하고, 이를 보다 일반적인 형태로 확장하여 설명합니다.
- **새로운 기법의 제안**: EXPO와 같은 기법이 델타 파라미터를 더 나은 모델로 만들기 위해 확장 방법을 사용하여 성능을 향상시키는 방식을 설명하였습니다.
- **광범위한 실험**: 비전 모델(ViT)과 언어 모델(LLaMA 3, Mistral)을 포함한 다양한 모델에 대한 실험을 통해 이론적인 분석을 검증하였습니다.

### 2. 전체 요약:
이 논문은 대규모 사전 훈련 모델의 후처리에서 델타 파라미터의 중요한 역할을 강조하며, 리만 합 근사에 기반한 통합적인 뷰를 통해 기존 기법들을 분석하고 새로운 기법을 제안합니다. 이러한 통합적 분석을 통해 델타 파라미터가 어떻게 모델의 성능에 영향을 미치는지를 밝혀냈으며, 이는 모델 향상을 위한 새로운 방향을 제시하는 데에 기여합니다.