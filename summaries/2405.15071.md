# Grokked Transformers are Implicit Reasoners: A Mechanistic Journey to the Edge of Generalization
## TL;DR
## Summary
- [https://arxiv.org/pdf/2405.15071.pdf](https://arxiv.org/pdf/2405.15071.pdf)

### 1. 각 섹션의 내용 요약 및 주요 기여점과 혁신적 부분 요약

#### 서론 (Introduction)
이 논문은 트랜스포머(Transformer)가 암묵적으로 지식 기반의 추론을 학습할 수 있는지 조사합니다. 특히, 두 가지 대표적인 추론 유형인 '합성'과 '비교'에 중점을 둡니다. 트랜스포머는 'grokking(완전히 연습해서 숙달되는 현상)'을 통해 암묵적 추론을 학습할 수 있으며, 훈련이 매우 오래 진행된 후에야 이러한 능력을 발휘할 수 있다고 주장합니다. 이는 학습과 과적합의 한계를 넘어서는 것을 의미합니다. 다른 모델들이 시도해도 성과를 내지 못한 복잡한 추론 작업에서, 제안된 방법이 탁월한 정확도를 보여준다는 점에서 중요합니다  .

#### 일반 설정 (General Setup)
논문에서는 두 가지 평가 방식을 채택합니다: ID(구분 내) 일반화와 OOD(구분 밖) 일반화. 모델의 성능을 평가하기 위해 '원자적 사실'과 '추론된 사실'을 생성하여 모델이 이를 추론할 수 있는지 테스트합니다. 이를 통해 모델이 암묵적인 규칙을 학습하고 적용할 수 있는지를 확인합니다. 또한, 학습 데이터의 세부 사항과 최적화 방법을 설명합니다  .

#### 구성 - 지연된 일반화 및 체계성 결여 (Composition—Delayed Generalization without Systematicity)
구성 작업에서 트랜스포머가 체계성 있게 일반화하지 않음을 발견했습니다. 특히, 두 가지 사실을 연결하는 '합성' 작업에서 모델이 구분 외 데이터에 대해 실패하는 이유를 분석합니다. 실험 결과, 'grokking'을 통해 모델이 일반화를 수행할 수 있지만 이는 제한적이라는 점을 시사합니다  .

#### 비교 - 평행 회로를 통한 체계적 일반화 (Comparison—Systematic Generalization via Parallel Circuit)
비교 작업에서는 트랜스포머가 'grokking'을 통해 체계적 일반화를 성공적으로 수행할 수 있음을 보여줍니다. 이는 모델이 두 개의 원자적 사실을 동시에 저장하고 접근할 수 있는 '평행 회로'를 학습하기 때문입니다. 이 방식은 체계적인 일반화를 가능하게 합니다 .

#### 결론 (Conclusion)
논문은 트랜스포머가 암묵적 추론을 학습할 수 있음을 보여줍니다. 하지만 이는 과적합을 넘어서는 'grokking' 기간을 거쳐야만 가능하다는 점을 강조합니다. 이 연구는 데이터와 훈련 설정이 암묵적 추론을 유도하는 데 어떻게 도움이 되는지에 대한 통찰을 제공합니다. 또한, 트랜스포머 아키텍처의 개선점을 제안하며, 복잡한 추론 작업을 성공적으로 수행할 수 있는 잠재력을 보여줍니다 .

### 2. 전체 요약
이 논문은 트랜스포머 모델이 암묵적 추론을 학습할 수 있는지에 대해 조사합니다. 특히, '합성'과 '비교' 두 가지 추론 유형에 초점을 맞추고, 모델이 'grokking'을 통해 일반화하는 과정을 분석합니다. 실험 결과, 트랜스포머는 훈련이 오래 진행된 후에야 이러한 능력을 발휘할 수 있으며, 특히 합성 작업에서는 일반화에 어려움을 겪지만, 비교 작업에서는 성공적으로 일반화할 수 있음을 보여줍니다. 이 연구는 암묵적 추론 능력을 향상시키기 위한 데이터 및 훈련 설정의 중요성을 강조하며, 트랜스포머 아키텍처의 잠재적 개선점을 제안합니다.

이 요약과 분석을 통해 사용자들이 이 논문의 주요 기여점과 혁신적 부분을 이해하고, 프레젠테이션을 준비하는 데 도움을 줄 수 있습니다.