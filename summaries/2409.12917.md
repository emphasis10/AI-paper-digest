# Training Language Models to Self-Correct via Reinforcement Learning
## TL;DR
## Summary
- [https://arxiv.org/pdf/2409.12917.pdf](https://arxiv.org/pdf/2409.12917.pdf)

### 1. 각 섹션 요약과 주요 공헌
#### 1. 소개 (Introduction)
이 논문은 대형 언어 모델(LLM)이 학습된 데이터만을 활용해 자가 수정 능력을 키우는 방법을 제안합니다. 기존의 LLM들은 추론할 때 오류를 그대로 두거나 악화시키는 경향이 있습니다. 그러나 자가 수정 능력은 수학 문제 해결이나 코딩과 같은 복잡한 환경에서 매우 중요합니다.

#### 2. 관련 연구 (Related Work)
이전 연구들은 주로 외부 피드백을 받아서 자가 수정하는 문제에 집중해왔지만, 이 논문에서는 '내재적 자가 수정' 즉, 외부 피드백 없이 모델 자체가 오류를 수정하도록 하는 방법을 다룹니다. 이를 위해 강화 학습(REINFORCE)과 여러 단계의 모델 학습을 결합한 새로운 접근법을 제안합니다.

#### 3. 예비 연구 및 문제 설정 (Preliminaries and Problem Setup)
이 논문은 오로지 모델이 생성한 데이터로 학습을 진행하는 방법론을 설명합니다. 학습 데이터는 모델이 스스로 생성한 자가 수정 추적 데이터로 구성되며, 이 데이터로 LLM을 훈련하여 자가 수정 능력을 배양합니다.

#### 4. 방법론 (Methodology)
SCoRe(자기 수정 강화 학습)를 제안하며, 이는 두 단계로 나뉩니다. 첫 번째 단계는 모델 초기화를 최적화하여 처음 시도 때 기본 모델과 가까운 결과를 내도록 하고, 두 번째 단계는 강화 학습을 통해 보상을 최적화하여 응답의 정확도를 높입니다. 이 두 단계는 모델이 단지 처음 시도에서 최적의 응답을 내기보다, 두 번째 시도를 통해 실제로 개선된 응답을 생성하도록 유도합니다.

#### 5. 실험 (Experimental Evaluation)
논문은 수학 문제와 코딩 문제를 대상으로 한 실험을 통해 SCoRe의 성능을 평가합니다. LLM을 SCoRe 방법으로 훈련시킨 결과, 기본 모델 대비 자가 수정 능력이 약 15.6% 향상되었습니다. 이는 SCoRe가 자가 수정 능력을 크게 개선할 수 있음을 보여줍니다.

#### 6. 토론, 한계 및 결론 (Discussion, Limitations, and Conclusion)
이 논문은 SCoRe가 자가 수정 능력을 장려하는 첫 번째 기법임을 주장합니다. 그러나 더 많은 반복 학습이나 더 정교한 피드백 사용 등 개선 가능성 있는 여러 방안을 제시하고 있습니다. 더불어 SCoRe의 두 단계 구조가 모델 붕괴를 막고 효과적인 자기 수정 전략을 학습하는 데 기여함을 강조합니다.

### 2. 전체 요약
이 논문은 대형 언어 모델(LLM)이 외부 피드백 없이도 스스로 오류를 수정할 수 있는 능력을 키우는 방법(SCoRe)을 제안합니다. 이를 통해 모델이 처음 시도에서 잘못된 응답을 내더라도, 다시 시도하여 더 나은 응답을 생성할 수 있게 됩니다. SCoRe는 두 단계로 구성되며, 첫 번째 단계에서는 강화 학습을 통해 모델 초기화를 최적화하고, 두 번째 단계에서는 반복 강화 학습을 통해 보상을 최적화하여 두 번의 시도 모두에서 성능을 향상시키는 방식을 사용합니다. 이 접근법은 수학 문제와 코딩 문제를 통해 실험적으로 검증되었으며, 기존 모델 대비 자가 수정 능력이 크게 향상되었음을 보여줍니다. 이 논문은 향후 추가적인 반복 학습이나 정교한 피드백 사용 등으로 더욱 개선될 가능성이 큽니다.