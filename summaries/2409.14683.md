# Reducing the Footprint of Multi-Vector Retrieval with Minimal Performance Impact via Token Pooling
## TL;DR
## Summary
- [https://arxiv.org/pdf/2409.14683.pdf](https://arxiv.org/pdf/2409.14683.pdf)

### 주요 내용 요약

#### 1. 서론
최근 딥러닝 기법은 정보 검색(Retrieval) 영역에서 인기를 끌고 있습니다. 기존 문서 수준이 아닌 토큰 수준에서 벡터를 저장하는 다중 벡터 검색(Multi-Vector Retrieval) 방식인 ColBERT이 좋은 성능을 낸다고 알려져 있습니다. 그러나 ColBERT와 같은 시스템은 많은 저장 공간과 메모리가 필요하며, 이는 실제 적용을 어렵게 합니다.

#### 2. Token Pooling
본 연구에서는 Token Pooling이라는 방법을 소개합니다. 이 방법은 문서 인덱싱 시 벡터를 평균 풀링(Mean Pooling)하여 저장해야 하는 벡터의 수를 감소시킵니다. 이 방법은 어떤 ColBERT 모델에서도 추가 학습이나 구조 변경 없이 사용할 수 있습니다. 연구에서 세 가지 풀링 방법을 적용해본 결과, 계층적 클러스터링(Hierarchical Clustering) 방법이 가장 좋은 성능을 보였습니다. 이 방법은 인덱스 크기를 최대 50% 줄이면서도 성능 저하를 거의 초래하지 않았으며, 66% 이상 줄일 경우에도 3% 이하의 성능 저하를 보였습니다.

#### 2.1 풀링 방법
1. 순차 풀링(Sequential Pooling): 문서 내 토큰의 순서대로 풀링.
2. K-평균 클러스터링(K-Means Clustering): 코사인 거리 기반의 클러스터링으로, 풀링 팩터에 따라 클러스터 수를 조정.
3. 계층적 클러스터링(Hierarchical Clustering): Ward의 방법을 사용하여 클러스터를 형성, 풀링 팩터로 최대 클러스터 수를 제한.

#### 3. 실험 설정
영어 MS-Marco 데이터셋과 일본어 데이터셋에서 ColBERTv2와 JaColBERTv2 모델을 사용하여 Token Pooling의 효과를 평가했습니다. 평가 결과, 다수의 데이터셋에서 벡터 수를 줄이면서도 성능 저하가 거의 없음을 확인했습니다.

#### 4. 결과
- 비압축 결과: 풀링 팩터가 2인 경우 벡터 수가 50% 감소하면서도 성능이 평균적으로 약간 향상되었으며, 풀링 팩터가 3인 경우에도 성능 저하 없이 벡터 수를 66% 줄일 수 있었습니다.
- 압축 결과: ColBERTv2의 양자화(Quantization)와 결합해도 성능 저하가 거의 없음을 확인했습니다.
- 일본어 결과: 일본어 데이터셋에서도 유사한 패턴이 나타났으며, 풀링 팩터가 낮을 때 성능 저하가 거의 없었습니다.

### 논문의 주요 공헌과 혁신적 부분
1. **Token Pooling 접근법**: 추가 학습이나 모델 변경 없이 기존의 클러스터링 방법을 활용하여 다중 벡터 검색 모델에서 필요한 벡터 수를 효과적으로 줄이는 방법을 제안했습니다.
2. **저장 비용 감소**: 다양한 데이터셋에서 기능 저하 없이 저장 비용을 최대 50% 줄일 수 있으며, 더 많은 줄일 경우에도 소폭의 성능 저하만 초래했습니다.
3. **다양한 언어와 모델에 적용 가능**: 영어뿐만 아니라 일본어와 같은 다른 언어와 모델에서도 이 방법이 유효함을 입증했습니다.

### 전체 요약
- **기술적 도전 과제**: 다중 벡터 검색 모델의 높은 저장 및 메모리 요구사항.
- **해결 방안**: Token Pooling을 통한 벡터 수 감소로, 추가 학습이나 모델 변경 없이 성능 저하 없이 저장 비용을 대폭 절감.
- **실험 결과**: 다양한 데이터셋에서 성능 저하 없이 저장 비용을 50% 줄일 수 있으며, 심지어 일본어 데이터셋에서도 유사한 결과 확인.
- **미래 전망**: 이 접근법은 더 큰 데이터셋과 다양한 다중 벡터 검색 모델에 적용 가능성을 열어주며, 정보 검색 연구에 큰 기여를 할 것으로 기대됩니다.