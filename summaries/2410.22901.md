# HelloMeme: Integrating Spatial Knitting Attentions to Embed High-Level and Fidelity-Rich Conditions in Diffusion Models
## TL;DR
## Summary
- [https://arxiv.org/pdf/2410.22901.pdf](https://arxiv.org/pdf/2410.22901.pdf)

1. **섹션별 요약 및 주요 기여 요약**

- **서론**  
  이 논문은 밈 비디오 생성 과제를 제시합니다. 이 과제는 과장된 표정과 머리 자세의 묘사와 같은 특정 요구 사항을 포함하고 있으며, 반신 및 전신 컴포지션으로 확장할 가능성이 있어야 합니다. 또한 텍스트-이미지 모델의 일반화 능력을 유지하여 콘텐츠 생성의 다양성을 높이는 것이 목표입니다.

- **관련 연구**  
  이 섹션에서는 각기 다른 텍스처 조건과 비디퓨전 및 비확산 기반 메서드를 비교합니다. 이 연구는 기존 메서드와의 차이점 및 장점을 설명하고 있습니다.

- **T2I 기반 모델의 사후 학습**  
  다양한 사후 학습 방법이 복잡한 다운스트림 작업을 가능하게 한다고 설명합니다. 그러나 이러한 방법의 단점은 기초 모델의 일반화 능력을 손상시킬 수 있다는 점이 있습니다.

- **방법론: Spatial Knitting Attentions**  
  Spatial Knitting Attentions(SK Attention) 기법은 공간적 구조 정보를 유지한 채로 주목합니다. 이를 통해 모델이 자연적으로 공간적 구조를 이해하게 합니다.

- **HMReferenceNet, HMControlNet, HMDenoisingNet**  
  HMReferenceNet는 참조 이미지에서 고충실도 특징을 추출하고, HMControlNet은 표현적인 머리 자세와 얼굴 표정을 인코딩합니다. 그 후 HMDenoisingNet이 이 정보를 활용하여 영상을 생성합니다.

- **실험 및 비교**  
  SK Attentions의 성능을 다양한 기준에서 시험하여 성능의 일부 개선을 보여줍니다.

- **결론**  
  이 논문은 간단한 플러그인을 텍스트-이미지 모델에 삽입해 복잡한 작업을 수행할 수 있다고 제안합니다. 개선의 여지가 있는 부분도 남아있으며, HD 비디오 생성의 프레임 연속성을 향상시키기 위한 단계적 접근법을 제안합니다.

2. **전체 요약**

이 논문은 ‘Spatial Knitting Attentions’라는 새로운 주목 메커니즘을 활용하여, 텍스트 기반 이미지 생성 모델의 확장성과 일반화 능력을 유지하면서도 밈 비디오와 같은 복잡한 다운스트림 작업을 해결하는 방법을 제시합니다. 논문은 다양한 기술적 접근 방식을 통해, 과장된 표정과 자세의 표현 및 빅데이터 훈련에 있어 계획된 개선 방안을 제시합니다. SK Attention은 2D 구조 정보를 잘 보존하면서도 모델의 학습을 효과적으로 개선하는 가능성을 보여주었으며, 이는 디퓨전 모델 기반의 인공지능 비디오 생성 분야에서의 혁신을 제시하는 이정표가 될 수 있습니다.