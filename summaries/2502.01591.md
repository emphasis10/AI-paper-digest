# Improving Transformer World Models for Data-Efficient RL
## TL;DR
## Summary
- [https://arxiv.org/pdf/2502.01591.pdf](https://arxiv.org/pdf/2502.01591.pdf)

1. **각 섹션 요약**

   **1절: 서론**
   이 논문은 Transformer 세계 모델(TWM)을 사용하는 모델 기반 강화 학습(MBRL) 방법을 제안하며, Craftax-classic 벤치마크에서 새로운 최첨단 성능을 달성한다. 에이전트는 다양한 능력을 요구하는 오픈 월드 2D 서바이벌 게임에서 학습하며, 1M 환경 단계만으로 보상 67.42%를 달성하여 인간 성능(65.0%)을 초과한다. 혁신적 설계 선택으로 데이터 표본 효율성을 향상시켰다.

   **2절: 관련 작업**
   모델 기반 강화 학습(MBRL) 방법을 여러 축으로 나누어 설명한다. 주로 배경 계획과 결정 시간 계획의 차이나, 관찰 공간의 생성 모델과 비생산 모델에 대한 논의가 포함된다.

   **3절: 방법론**
   방법론에서는 세 가지 주요 기여를 소개한다:
   1. 현실과 상상에서의 데이터를 혼합하여 정책을 기르기 위해 Dyna-like 방식을 사용하는 것.
   2. 이미지 패치를 독립적으로 tokenize하는 최근접 이웃 토크나이저(NNT)를 사용하는 것.
   3. 블록 교사 강제를 이용하여 TWM이 미래 토큰에 대해 함께 추론하게 하는 것.

   **4절: 결과**
   다양한 실험을 통해 제안된 방법이 이전 기술의 정점(SOTA)보다 성능이 뛰어남을 보여준다. 실험 결과, 제안된 방법이 1M 스텝 내에 67.42%의 보상을 달성하고, 이전 MFRL 방법론인 DreamerV3를 초과하였다. 각 기여가 성능에 미치는 영향에 대한 분석도 포함되었다.

   **5절: 결론 및 미래 작업**
   본 연구에서 제안한 방법은 MBRL 에이전트가 상당히 높은 보상과 점수를 기록하도록 하며, 전문가의 성과를 초과하였음을 보여준다. 향후 연구에서는 다양한 환경에서의 일반성을 검증할 계획이다.

   **주요 기여 및 혁신점**
   - 정책 업데이트에 대한 현실 데이터와 상상 데이터를 혼합하여 사용하는 Dyna-like 접근 방식.
   - 이미지 패치 기반의 근접 이웃 토크나이저(NNT) 사용.
   - 블록 교사 강제(BTF) 기법으로 복잡한 미래 상태 추론을 가능하게 함.

2. **전체 요약**
   이 논문은 Craftax-classic 벤치마크에서 모델 기반 강화 학습(MBRL)에 대한 새롭고 높은 성능의 방법을 소개하며, 현실과 가상의 데이터를 모두 활용하여 정책을 훈련하는 innovative 한 접근 방식을 통해, 이전 방법보다 큰 성과를 거두었다. 세 가지 주요 기여로는 Dyna-like 정책 업데이트, 이미지 패치 독립 토크나이징, 그리고 블록 교사 강제 기법이 있다. 이러한 기법들은 성능과 데이터 효율성을 동시에 개선하여 강화 학습의 새로운 가능성을 제시하고 있다.