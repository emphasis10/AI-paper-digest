# Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality
## TL;DR
## Summary
- [https://arxiv.org/pdf/2405.21060.pdf](https://arxiv.org/pdf/2405.21060.pdf)

### 논문 요약 및 주요 내용

#### 1. 소개
이 논문은 Transformer와 구조적 상태 공간 모델(SSM) 간의 이론적 연결을 탐구하며, 두 모델의 강점을 결합하여 새로운 아키텍처인 Mamba-2를 제안합니다. Transformer는 효율성 문제로 인해 주로 작은 및 중간 규모에서 SSM에 비해 성능이 떨어집니다. 논문은 SSM과 다양한 형태의 어텐션 간의 연결을 통해 SSM의 알고리즘적 및 시스템적 최적화를 Transformer에 적용하고자 합니다.

#### 2. 배경 및 개요
구조적 상태 공간 모델(SSM)은 최근에 등장한 시퀀스 모델로, 연속적 시스템에서 영감을 받아 설계되었습니다. SSM은 시퀀스 데이터를 효율적으로 처리하기 위해 설계된 다양한 형태의 구조를 가지고 있습니다. SSM의 주요 구조로는 대각선 및 저순위(DPLR) 구조와 대각선 구조가 있습니다.

#### 3. 구조적 상태 공간 모델
SSM은 일련의 시퀀스 변환으로 볼 수 있으며, 이는 특정한 형태의 구조적 행렬, 특히 반분리 행렬과 동등하다는 것을 보여줍니다. 이 모델들은 시퀀스 데이터의 효율적 처리를 위해 설계되었습니다.

#### 4. 효율적인 알고리즘
논문은 SSM의 효율적인 연산을 위한 새로운 알고리즘을 제안합니다. 제안된 알고리즘은 블록 분해를 활용하여 효율성을 극대화합니다. 이는 SSM의 계산 비용을 줄이기 위한 중요한 기여입니다.

#### 5. 아키텍처 설계
새로운 아키텍처인 Mamba-2는 기존 Mamba의 선택적 SSM을 개선하여 설계되었습니다. Mamba-2는 계산 효율성을 극대화하고 Transformer와 경쟁할 수 있는 성능을 제공합니다. 이는 대각선 블록과 저순위 블록을 사용하여 구현됩니다.

#### 6. 시스템 최적화
SSM의 시스템 최적화를 위해 텐서 병렬 및 시퀀스 병렬 기법을 도입합니다. 이는 다양한 길이의 시퀀스를 효율적으로 처리하기 위한 방법을 포함합니다. 이러한 최적화는 SSM이 Transformer에 비해 더욱 효율적으로 작동할 수 있게 합니다.

#### 7. 결론
논문은 SSM과 Transformer의 강점을 결합한 새로운 아키텍처인 Mamba-2를 제안하며, 이 모델이 효율성과 성능 면에서 뛰어남을 보여줍니다. 향후 연구 방향으로는 SSM의 추가적인 최적화와 다양한 응용 분야에서의 활용이 제시됩니다.

### 전체 요약
이 논문은 Transformer와 구조적 상태 공간 모델(SSM) 간의 이론적 연결을 통해 새로운 아키텍처인 Mamba-2를 제안합니다. Mamba-2는 SSM의 선택적 구조를 개선하여 계산 효율성을 극대화하고 Transformer와 경쟁할 수 있는 성능을 제공합니다. 논문은 SSM의 효율적인 연산을 위한 새로운 알고리즘을 제안하고, 텐서 병렬 및 시퀀스 병렬 기법을 도입하여 시스템 최적화를 달성합니다. 이를 통해 Mamba-2는 다양한 길이의 시퀀스를 효율적으로 처리할 수 있으며, 향후 연구 방향으로 SSM의 추가적인 최적화와 다양한 응용 분야에서의 활용이 제시됩니다.

## Similar Papers
- [Mamba: Linear-Time Sequence Modeling with Selective State Spaces](2312.00752.md)
- [BitNet: Scaling 1-bit Transformers for Large Language Models](2310.11453.md)
- [Better & Faster Large Language Models via Multi-token Prediction](2404.19737.md)
- [FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning](2307.08691.md)
- [Q-Sparse: All Large Language Models can be Fully Sparsely-Activated](2407.10969.md)
- [Chimera: Effectively Modeling Multivariate Time Series with 2-Dimensional State Space Models](2406.04320.md)
- [A Primer on the Inner Workings of Transformer-based Language Models](2405.00208.md)
- [ML-Mamba: Efficient Multi-Modal Large Language Model Utilizing Mamba-2](2407.19832.md)
- [OneBit: Towards Extremely Low-bit Large Language Models](2402.11295.md)
