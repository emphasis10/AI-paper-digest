# Tele-FLM Technical Report
## TL;DR
## Summary
- [https://arxiv.org/pdf/2404.16645.pdf](https://arxiv.org/pdf/2404.16645.pdf)

이 논문에서는 Tele-FLM이라는 새로운 다국어 대형 언어 모델(Large Language Model, LLM)을 소개하며, 이 모델은 52B의 매개변수를 갖고 있으며 다양한 언어의 텍스트를 이용하여 처음부터 사전 훈련되었습니다. 주요 목적은 효율적인 사전 훈련 패러다임과 강화된 사실 판단 능력을 통해 다국어 모델링 능력을 개선하는 것입니다. 다음은 각 섹션의 주요 내용 요약입니다.

1. **서론 및 관련 작업**:
   - 대형 언어 모델이 언어 이해와 생성에서 뛰어난 능력을 보여주고 있으나, 50B 이상의 모델을 효율적으로 확장하는 방법에 대한 상세한 개방형 방법론이 부족합니다.
   - Tele-FLM은 이러한 문제를 해결하기 위해 공개되며, 사전 훈련에 필요한 시행착오 비용과 컴퓨팅 자원을 최소화합니다.

2. **사전 훈련 데이터**:
   - 2.0 조 토큰으로 구성된 대규모 데이터셋에서 영어, 중국어 및 기타 여러 언어를 포함하여 사전 훈련이 진행됩니다.
   - 데이터 구성, 모델 구조, 하이퍼파라미터 검색 및 전체 사전 훈련 역학에 대한 상세 정보를 공유합니다.

3. **모델 구조와 토크나이저**:
   - Tele-FLM은 FLM-101B 아키텍처를 기반으로 몇 가지 수정을 통해 최적화되었습니다.
   - 새로운 토크나이저는 중국어 및 고전 중국어에서 GPT-4 및 Llama 시리즈보다 우수한 압축 비율을 보여줍니다.

4. **훈련 동력학과 평가**:
   - 훈련 및 검증 데이터에 대한 손실과 그라디언트 노름의 동력학을 보여주며, 안정적인 단일 실행으로 성공적으로 훈련됩니다.
   - 다양한 영어 및 중국어 벤치마크에서 평가되어 우수한 압축 능력과 언어 모델링 능력을 입증합니다.

5. **결론 및 향후 계획**:
   - Tele-FLM은 다국어 벤치마크 평가에서 더 큰 모델과 비슷한 성능을 보여주며, 사전 훈련 절차는 높은 성공률과 낮은 탄소 발자국을 특징으로 합니다.
   - 모델 무게와 기술적 세부 사항을 공개하여 LLM 커뮤니티의 성장을 촉진하고, 50B 이상의 매개변수를 가진 LLM을 훈련하는 시행착오 주기를 줄일 수 있기를 기대합니다.

이 논문은 Tele-FLM을 통해 다국어 대형 언어 모델의 효율적인 사전 훈련과 운영을 가능하게 하며, 개방형 과학 및 기술 발전에 기여할 것으로 기대됩니다.

## Similar Papers
- [Xmodel-LM Technical Report](2406.02856.md)
- [JetMoE: Reaching Llama2 Performance with 0.1M Dollars](2404.07413.md)
- [Dynamic data sampler for cross-language transfer learning in large language models](2405.10626.md)
- [The Llama 3 Herd of Models](2407.21783.md)
- [Enhancing LLM's Cognition via Structurization](2407.16434.md)
- [Qwen2 Technical Report](2407.10671.md)
- [MiniCPM: Unveiling the Potential of Small Language Models with Scalable Training Strategies](2404.06395.md)
- [GEB-1.3B: Open Lightweight Large Language Model](2406.09900.md)
- [A Generic Review of Integrating Artificial Intelligence in Cognitive Behavioral Therapy](2407.19422.md)
