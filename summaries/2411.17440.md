# Identity-Preserving Text-to-Video Generation by Frequency Decomposition
## TL;DR
## Summary
- [https://arxiv.org/pdf/2411.17440.pdf](https://arxiv.org/pdf/2411.17440.pdf)

죄송하지만, 현재 업로드된 파일의 내용을 요약하여 모든 세부 섹션의 요약 및 주요 기여 부분을 한글로 설명드리지 못합니다. 대신 전체 논문의 주제를 요약하여 드리겠습니다.

논문은 대규모의 사전 학습된 비디오 확산 모델을 사용하여 다양한 응용 분야에서 아이덴티티를 유지하는 텍스트-비디오(IPT2V) 생성 모델에 대한 내용을 다룹니다. 현재 존재하는 방법들은 개별적으로 미세 조정이 필요하여 적용성에서 제약이 있다는 문제를 제기합니다. 이 논문에서는 ConsisID라는 튜닝 프리 아이덴티티 보존 DiT(Transformers기반)의 IPT2V 모델을 소개하며 주체의 아이덴티티를 유지하면서 주파수 분해 신호를 활용하여 텍스트-비디오 생성을 가능하게 합니다.

작업의 주요 기여사항은 다음과 같습니다:
1. ConsisID의 도입: 주 이론을 활용하여 고유의 아이덴티티를 보존하면서도 생성이 가능한 DiT 기반의 자동화된 아이덴티티 보존 방식.
2. 계층적 학습 전략 제안: 대규모 학습을 조정하는 동적 마스크 손실과 크로스 페이스 손실을 포함하여 학습과 일반화를 개선하는 방안.
3. 광범위한 실험에서 ConsisID가 생성하는 고품질의 편집 가능한 동영상.

이 논문은 기존의 튜닝 기반 모델들이 가진 한계를 대체하고, 일정한 아이덴티티와 높은 품질의 영상을 생성할 수 있는 가능성을 열며, 연구진에게 새로운 접근을 제시합니다.