# Identity-Preserving Text-to-Video Generation by Frequency Decomposition
## TL;DR
## Summary
- [https://arxiv.org/pdf/2411.17440.pdf](https://arxiv.org/pdf/2411.17440.pdf)

### 1. 섹션별 요약

- **서론**  
  본 연구에서는 대규모 사전 훈련된 비디오 확산 모델을 활용하여 얼굴의 정체성을 유지하는 텍스트-비디오 생성(IPT2V)에서 발생하는 문제점을 다룹니다. 기존 방법들은 각 개별 사례의 세밀한 조정이 필요하여 높은 비용이 발생하며, ID-Animator와 같은 해결책은 얼굴 특징 인식에도 한계를 가집니다.

- **관련 연구**  
  기존 연구는 주로 U-Net 기반의 개인화 작업을 진행했지만, 이 연구에서는 새로운 DiT(video transformer) 기반의 모델을 제시하여 고품질의 얼굴 일관성을 유지하는 비디오 생성 모델을 구성합니다.

- **방법론**  
  ConsisID라는 모델을 소개하며, 주파수 분해를 통해 얼굴의 일관성을 유지하는 비디오 생성 방법을 설명합니다. Global Facial Extractor와 Local Facial Extractor를 통해 저주파 및 고주파 요소를 각각 다룹니다.

- **실험 설정 및 결과**  
  실험은 여러 설정과 수치 분석을 통해 ConsisID가 얼굴 일관성을 잘 유지하며 높은 품질의 비디오를 생성함을 증명했습니다.

- **결론**  
  이 연구에서는 ConsisID가 다중 주파수 요소를 통해 얼굴의 일관성을 성공적으로 유지하면서, 텍스트에 기반한 비디오 생성 모델에 통합될 수 있음을 나타냅니다. 또한, 주파수 기반 제어 시스템의 한계와 향후 연구 방향에 대해 논의합니다.

### 2. 전체 요약

이 연구는 텍스트-비디오 생성에서 얼굴의 일관성을 유지하기 위한 새로운 접근법인 "ConsisID"를 소개합니다. 이 모델은 주파수 분해 방법을 사용하여 얼굴 특징을 고주파수 및 저주파수로 나누어 다루며, 이를 통해 훈련의 효율성을 높이고 ID 보존 비디오를 생성할 수 있습니다. 실험 결과는 이 모델이 기존의 방법보다 얼굴 일관성을 더 잘 유지하면서도 다양한 편집 기능을 제공한다고 증명하였습니다. ConsisID는 특히 사전 훈련된 DiT를 활용하여 추가적인 학습과정 없이도 편집 가능하고 고품질의 일관성을 유지하는 비디오를 생성하는 데 성공하였습니다.