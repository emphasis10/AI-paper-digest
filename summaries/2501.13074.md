# Autonomy-of-Experts Models
## TL;DR
## Summary
- [https://arxiv.org/pdf/2501.13074.pdf](https://arxiv.org/pdf/2501.13074.pdf)

요약 내용은 다음과 같습니다:

1. **서론**:
   - 이 논문에서는 '전문가-자치(Autonomy-of-Experts, AoE)'라는 새로운 접근 방식을 제안합니다. 현재 MoE(전문가들의 혼합)는 라우터를 사용하여 데이터를 전문가에게 할당하지만, 이는 라우터의 의사 결정과 전문가의 실행 간의 분리로 인해 비효율적입니다. AoE는 이 문제를 해결하여, 전문가들이 스스로 데이터를 처리할지 결정하도록 합니다.

2. **배경: Mixture-of-Experts (MoE)**:
   - MoE 모델은 대규모 네트워크를 작은 서브 네트워크들, 즉 전문가로 나누어 처리 효율을 높입니다. 기존의 MoE는 라우터를 통해 한정된 전문가만 활성화하여 효율적인 처리가 가능하지만, 이 과정에서 부적절한 전문가 선택이 발생할 수 있습니다.

3. **Autonomy-of-Experts (AoE) 모델**:
   - AoE는 모든 전문가들이 자치적으로 자신이 입력을 처리할 능력이 있는지를 파악한 후, 최적의 전문가를 선택합니다. 이에 따라 특정 전문가들만 계속해서 데이터를 처리하고 나머지는 멈춥니다. 전문가들의 가중치는 저차원으로 팩토링 되어 저장되며, 라우터가 제거되면서 효율성이 증가합니다.

4. **실험**:
   - 스케일이 커질수록 AoE 기반의 언어 모델이 전통적인 MoE보다 더 뛰어난 성능을 보임이 발견되었습니다. 구체적으로, AoE는 더 나은 하류 작업 성능을 제공하며, 훈련 및 전문화 측면에서 더욱 효과적입니다.

5. **결론**:
   - 이 연구는 라우터와 전문가 실행 간의 비효율 문제를 해결하기 위한 새로운 MoE 패러다임인 AoE를 소개하며, 전문성을 갖춘 전문가 선택 방법을 제안하였습니다. 이는 MoE 기반의 언어 모델이 효율적으로 개선될 수 있는 가능성을 제시합니다.

### 전체 요약:
전문가-자치(AoE)는 기존의 Mixture-of-Experts(MoE) 모델의 한계를 극복하기 위한 혁신적인 아이디어입니다. AoE는 전문가들이 스스로 데이터를 처리할지 여부를 결정하도록 하여 라우터의 필요성을 없애고, 보다 전문화된 전문가 선택으로 효율을 극대화합니다. 이를 통해 전통적인 MoE 기법보다 나은 하류 작업 성능을 발휘하며, 모델 학습 및 특화 과정에서도 탁월한 결과를 보입니다. 이러한 접근은 대규모 언어 모델의 성능을 한층 더 발전시킬 수 있는 가능성을 제안합니다.