# Block Diffusion: Interpolating Between Autoregressive and Diffusion Language Models
## TL;DR
## Summary
- [https://arxiv.org/pdf/2503.09573.pdf](https://arxiv.org/pdf/2503.09573.pdf)

1. 각 섹션 요약:
   
   **서론**: 이 논문은 블록 불연속 디퓨전 언어 모델(BD3-LMs)을 소개합니다. 이는 불연속 디퓨전 모델과 자동회귀 모델 간의 한계를 극복하여 다양한 길이의 시퀀스를 생성하고, 키-값 캐싱을 통해 추론 효율성을 향상시킵니다. 이 방법은 현재 디퓨전 모델의 한계인 고정 시퀀스 길이 생성과 확률 예측력의 한계를 해결합니다.

   **언어 모델링 배경**: 언어 모델링 패러다임은 자동회귀 모델과 불연속 디퓨전 확률 모델로 나뉩니다. 자동회귀 모델은 각 토큰의 확률 분포를 순차적으로 추정하는 반면, 디퓨전 모델은 데이터를 점차 노이즈 처리하고 이를 복원하는 역방향 과정을 모델링합니다.

   **블록 디퓨전 언어 모델링**: 블록 디퓨전 모델은 디퓨전과 자동회귀 모델을 혼합하여 사용됩니다. 이 모델은 블록 내에서 시퀀스 생성이 가능하고, 이전 블록에 조건을 두고 작동합니다. 효과적인 학습을 위해 최소한의 계산 자원으로 최적화 알고리즘이 제안됩니다. 

   **디퓨전과 AR 모델 간의 확률 차이**: BD3-LMs는 보편적인 마스킹 기법을 사용하여 디퓨전 모델과 자동회귀 모델 간의 확률 차이를 이해하고 개선하려고 합니다. 이 섹션에서는 싱글 토큰 생성과 같은 사례를 통해 이러한 차이를 설명합니다.

   **실험**: BD3-LMs는 LM1B와 OpenWebText 데이터셋에서 기존 확산 방법보다 향상된 성능을 보였으며, zero-shot 학습과 다양한 시퀀스 생성을 통해 그 우수성을 입증했습니다.

   **결론**: BD3-LMs는 디퓨전 모델의 한계를 극복하여 다양한 길이의 시퀀스를 생성할 수 있으며, 자동회귀 모델에 근접한 수준으로 향상된 확률 예측력을 보여주었습니다. 이를 통해 BD3-LMs는 더 나은 성능을 가진 새로운 상태의 기술을 제시합니다.

2. 전체 요약:

이 논문은 향상된 디퓨전 모델인 블록 불연속 디퓨전 언어 모델(BD3-LMs)을 소개합니다. BD3-LMs는 블록 단위로 토큰을 생성하는 방법으로 불연속 디퓨전과 자동회귀 모델의 장점들을 결합합니다. 이 접근법을 통해 다양한 길이의 시퀀스를 생성하고, 효율적인 추론을 가능하게 하며, 실험적으로도 자동회귀 모델에 비견되는 높은 수준의 확률 예측력을 입증했습니다. BD3-LMs는 현대 디퓨전 언어 모델의 한계를 극복하기 위한 중요한 발전을 보여줍니다.