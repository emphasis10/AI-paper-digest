# HARE: HumAn pRiors, a key to small language model Efficiency
## TL;DR
## Summary
- [https://arxiv.org/pdf/2406.11410.pdf](https://arxiv.org/pdf/2406.11410.pdf)

## 주요 기여와 혁신 부분 요약

이 논문은 소형 언어 모델(Small Language Models, SLMs)의 데이터 학습 효율을 높이기 위해 인간의 사전 지식을 활용하는 방법을 제안하고 있습니다. 이 방법은 대규모 벤치마크 데이터 유출을 방지하면서도 의미의 다양성과 데이터 품질 일관성을 유지할 수 있게 해줍니다. 이를 바탕으로, HARE-1.1B라는 이름의 SLM을 훈련하고, 다양한 벤치마크 테스트에서 최첨단 모델들과 비교했을 때 유리한 성능을 보여줍니다.

### 1. 서론
소형 언어 모델(SLM)은 계산 효율성과 실시간 응답성 면에서 주목받고 있습니다. 그러나 대부분의 기존 SLM들은 웹에서 스크랩한 대규모 데이터를 사용하여 훈련되며, 이는 일관되지 않은 데이터 품질과 의미의 다양성 부족으로 인해 훈련 효율을 저해합니다.

### 2. 데이터 구성
데이터 구성 과정은 크게 네 가지 단계로 이루어집니다:
1. **고품질 데이터 추출:** 대규모 웹 스크랩 데이터셋에서 고품질 데이터를 추출하여 의미의 다양성을 보장하고 데이터 품질을 유지.
2. **데이터 클러스터링과 합성:** 대규모 데이터를 다양한 주제로 클러스터링하고 주제별 데이터를 사용하여 고성능 LLM을 통해 합성 데이터를 생성, 의미의 다양성을 높이며 일관된 데이터 품질을 유지.
3. **NLP 작업 데이터 생성:** 자연 언어 형식으로 대규모 NLP 작업 데이터를 구축하여 의미의 다양성과 NLP 작업 해결 능력을 향상.
4. **데이터 정화:** 벤치마크 데이터 유출을 방지하기 위해 철저한 정화 절차를 시행.

### 3. 훈련
HARE-1.1B 모델은 Mistral 아키텍처를 사용해 1.1B 파라미터로 축소되고, 16개의 Nvidia-H800 GPU를 사용하여 두 단계에 걸쳐 훈련됩니다. 첫 번째 단계에서는 고품질 데이터를 사용해 520억 개의 토큰을 처리하고, 두 번째 단계에서는 최종 데이터셋을 사용해 6000억 개의 토큰을 처리합니다.

### 4. 실험
HARE-1.1B는 다양한 벤치마크 데이터셋에서 최첨단 소형 언어 모델들과 비교하여 다음과 같은 결과를 얻습니다:
- **MMLU, ARC-C, TruthfulQA** 등 벤치마크 테스트에서 평균 점수가 우수함을 보여줍니다.
- 기존 모델들에 비해 벤치마크 데이터 유출 가능성이 낮습니다.

## 종합 요약

이 논문은 소형 언어 모델의 데이터 효율을 높이기 위해 인간의 사전 지식을 활용하는 새로운 데이터 구성 원칙을 제안합니다. 이 방법은 의미의 다양성과 데이터 품질을 유지하면서 벤치마크 데이터 유출을 방지합니다. HARE-1.1B 모델은 이 새로운 방법론을 통해 훈련되었으며, 다양한 벤치마크 테스트에서 최첨단 모델들과 비교했을 때 우수한 성능을 보여주었습니다. 이 방법론은 소형 언어 모델의 학습 효율을 높이는 데 중요한 기여를 할 것으로 기대됩니다.