# Self-Consistency Preference Optimization
## TL;DR
## Summary
- [https://arxiv.org/pdf/2411.04109.pdf](https://arxiv.org/pdf/2411.04109.pdf)

1. **각 섹션의 요약**

- **소개 (Introduction):** 이 섹션에서는 대형 언어 모델(LLM)을 인간 주석 데이터에 기반하여 훈련하는 것이 성능을 향상시키는 한편, 데이터 수집 과정에서의 비용이나 시간 문제로 인해 한계가 있다는 점을 언급합니다. 이를 해결하기 위해 모델 자체가 생성한 데이터를 사용하여 반복 학습하는 방법을 소개하였습니다.

- **자기 일관성 선호 최적화 (Self-Consistency Preference Optimization, SCPO):** SCPO는 모델이 스스로 문제를 생성하고 일관되게 문제를 해결하도록 유도하는 방법입니다. 모델이 높은 일관성을 유지한다고 판단될 때 기존의 보상 기반 모델보다 더 나은 성능을 발휘하도록 합니다. 특히, 추론 시점에만 쓰이던 자기 일관성 개념을 훈련 시에도 활용하여 문제 해결 능력을 배가합니다.

- **수학적 추론 (Math Reasoning):** SCPO는 인간 라벨이 없는 상태에서도 기존의 훈련법보다 월등히 나은 성과를 보여줍니다. 주어진 문제를 스스로 해결하며, 같은 훈련 데이터 세트에서 더 높은 정확도를 기록했습니다.

- **ZebraLogic 문제 해결:** 현존하는 크고 복잡한 모델들과 비교하여 SCPO로 훈련된 LLM이 더 작은 규모임에도 불구하고 높은 정확도를 보였습니다. 특히 복잡한 ZebraLogic 문제에서 기존 최고 성능 모델을 앞섰습니다.

- **결론 (Conclusion):** SCPO는 추론 작업에서 특히 뛰어난 성과를 보여주며, 추가적인 골드 라벨 없이도 타 방법과 비견할 만한 성능을 발휘합니다. 미래 연구에서는 이러한 접근법을 요약과 같은 단일 정답이 명확하지 않은 과제에도 확장할 것을 제안합니다.

2. **전체 요약**

이 논문은 SCPO(Self-Consistency Preference Optimization)라는 새로운 훈련 방법론을 중심으로 AI와 머신러닝의 발전을 모색합니다. SCPO는 인간 주석이나 외부 보상 모델 없이도 대형 언어 모델이 스스로 향상할 수 있는 방법을 제안하였습니다. 본 연구는 수학적 추론에서부터 복잡한 논리 퍼즐 해결까지 다양한 영역에서 높은 성과를 발휘하며, 특히 ZebraLogic과 같은 문제에서 기존의 대형 모델을 능가하는 성과를 기록하였습니다. 이를 통해 SCPO가 자가 일관성을 활용하여 실제 문제 해결에 있어 탁월한 능력을 발휘할 수 있음을 증명하고 있습니다.