# Are large language models superhuman chemists?
## TL;DR
## Summary
- [https://arxiv.org/pdf/2404.01475.pdf](https://arxiv.org/pdf/2404.01475.pdf)

이 문서는 대규모 언어 모델(LLMs)이 화학 과학에서 지식과 추론 능력을 평가하는 새로운 프레임워크인 "ChemBench"의 개발과 평가에 대해 다룹니다. 이 프레임워크는 LLMs가 인간 화학자의 전문성에 대비하여 어떤 분야에서는 우수한 성능을 보이지만, 인간에게는 쉬운 일부 화학 추론 작업에서는 어려움을 겪으며, 화학물질의 안전성 프로파일과 같은 오해의 소지가 있고 과신된 예측을 하는 경우도 있다고 보고합니다. 또한, LLM을 화학 과학에서 더 안전하고 유용하게 활용하기 위한 추가 연구의 필요성을 강조합니다.

문서 전체를 통해 다룬 주요 내용은 다음과 같습니다:

1. **도구 보강 모델 (Tool augmented models)**: 직접적인 LLM 프롬프팅 외에도, 도구 보강 시스템의 성능을 조사했습니다. 여기에는 다양한 온라인 모델과 GPT-3.5-turbo, Claude 2 등이 WolframAlpha, ArXiv API, Python 인터프리터, 웹 검색 (DuckDuckGo 사용)에 대한 접근 권한을 가진 채로 사용되었습니다. 이러한 시스템은 디폴트 프롬프트를 사용해 Langchain을 통해 구현되었으며, 최대 열 번의 LLM 호출로 제한되었습니다.

2. **신뢰도 추정 (Confidence estimate)**: 모델의 신뢰도를 추정하기 위해 문제(과 답변 옵션)을 투입하고 올바른 답변에 대한 신뢰도를 1부터 5까지의 척도로 평가하도록 요청했습니다. 실질적인 사례에 더 가깝다고 판단된 언어로 된 신뢰도 추정치를 사용하기로 결정했습니다.

3. **인간 기준선 (Human baseline)**: 충분한 응답 수집이 어려울 것으로 예상되어, 인간 평가자에게 모든 질문의 관련 부분집합을 제시하기로 결정했습니다. 이를 통해 모델과 인간의 성능 차이가 큰 영역을 탐색하고자 했습니다. 참가자들의 다양한 화학 관련 경험 수준이 반영되었습니다.

4. **문제 분류 (Classification of questions into topics)**: 데이터셋을 큐레이션하는 과정에서 키워드와 출처를 체계적으로 기록했습니다. 모델 성능을 주제별로 분석할 수 있도록 이 정보 및 시퀀스 분류 모델의 출력을 활용했습니다. 남은 문제들, 예를 들어 화학 올림피아드 질문과 같은 것들은 BART 모델을 사용한 제로샷 시퀀스 분류를 통해 주제에 할당되었습니다.

5. **데이터 및 코드 접 근성**: ChemBench의 코드와 데이터는 공식 GitHub 저장소에서 제공되며, 인간 기준선 스터디의 앱 코드도 별도의 GitHub 페이지에서 확인할 수 있습니다. 이 연구의 재현성을 보장하기 위해 'show your work!' 프레임워크를 이용해 작성되었으며, 관련 코드 및 중간 분석 결과 등이 공개되어있습니다.

6. **감사의 글**: 이 연구는 칼 자이스 재단, 프리드리히 실러 대학의 "Life" 프로필 라인의 "Talent Fund", 그리고 FAIRmat 컨소시엄의 지원을 받았습니다. 또한 코드와 데이터 공개에 기여한 여러 기관 및 단체에 감사를 표했습니다.

7. **이해상충 (Conflicts of interest)**: 연구진 중 일부는 OpenAI와 Stability.AI와의 금전적 관계를 보유했습니다.

8. **저자 기여도**: 연구의 개념화부터 자료 수집, 형식 분석, 자금 조달, 조사, 방법론, 프로젝트 관리, 자원 제공, 소프트웨어 개발, 감독, 검증, 시각화, 초안 작성 및 편집 등 연구의 모든 단계에 대한 저자들의 기여도가 설명되었습니다.

### 전반적인 요약:

이 문서는 대규모 언어 모델(LLM)을 화학 과학에 적용하는 새로운 방법론인 ChemBench의 개발과 그 성능 평가에 중점을 둡니다. ChemBench는 LLM이 인간의 화학 지식과 추론 능력에 비교해 어떻게 성능을 내는지 평가하는 프레임워크입니다. 이 프레임워크는 특히 도구 보강 모델, 신뢰도 추정, 인간 기준선 설정 및 문제 분류 방법을 포함하여 다양한 방법론을 통해 LLM의 화학 관련 지식과 추론 능력을 평가합니다. 이 연구에서 공개된 데이터와 코드를 통해 연구의 재현성을 높이고, 연구 과정에서 발생할 수 있는 이해상충에 대해서도 투명하게 공개하였습니다. 이를 통해 LLM을 화학 과학 분야에 보다 안전하고 유용하게 적용할 수 있는 방법론 개발에 대한 토대를 마련합니다.