# GRIN: GRadient-INformed MoE
## TL;DR
## Summary
- [https://arxiv.org/pdf/2409.12136.pdf](https://arxiv.org/pdf/2409.12136.pdf)

### 주요 섹션 요약

#### 1. 소개 (Introduction)
이 섹션에서는 혼합 전문가(Mixtures-of-Experts; MoE) 모델의 현재 상태에 대해 설명하고, MoE 모델의 확장성 문제를 강조하며, 이를 해결하기 위한 새로운 GRIN(GRadient-INformed MoE) 훈련 기법을 소개합니다. 주요 기여로는 GRIN을 통한 전문가 라우팅의 희소 그레이디언트 추정 및 토큰 드랍을 회피하는 모델 병렬 구성 등이 있습니다.

#### 2. 모델 아키텍처 (Model Architecture)
이 섹션에서는 트랜스포머 아키텍처를 기반으로 한 GRIN MoE 모델에 대해 설명합니다. GRIN MoE는 일반적인 트랜스포머 블록을 사용하며, 각 블록은 주의(attention) 레이어와 피드포워드 레이어로 구성되어 있습니다. Residual 연결과 레이어 정규화를 포함하여 Pre-LN 방식으로 적용됩니다.

#### 3. GRIN MoE
이 섹션에서는 GRIN MoE의 두 가지 핵심 기술인 SparseMixer-v2를 사용한 전문가 라우팅 그레이디언트 추정과 토큰 드랍 없이 스케일링하는 방법에 대해 상세히 설명합니다. SparseMixer-v2는 기존의 전문가 게이팅 대신 사용되며 보다 정확한 그레이디언트 추정을 가능하게 합니다.

#### 4. 실험 (Experiment)
이 섹션에서는 GRIN MoE의 성능 평가 결과를 제시합니다. GRIN MoE는 다양한 벤치마크에서 높은 성능을 보이며, 특히 수학과 코딩 작업에서 우수한 성과를 보였습니다. GRIN MoE는 활성화된 파라미터 수 6.6억 개로 7억 개 밀집 모델 대비 성능이 뛰어나며, 14억 개 밀집 모델과 비슷한 성능을 보여줍니다.

#### 부록 (Appendix)
부록에서는 SparseMixer-v2와 GShard, SparseMixer의 차이점 등을 포함한 추가적인 기술적 세부사항을 다룹니다. 또한, SparseMixer-v2의 효과를 입증하기 위한 추가 실험 결과가 포함되어 있습니다.

### 논문의 주 기여 및 혁신
GRIN MoE는 혼합 전문가 모델의 스케일링 잠재력을 극대화하기 위해 새로운 그레이디언트 추정 방법(SparseMixer-v2)과 모델 병렬성을 제안합니다. 이 기법을 통해 토큰 드랍을 피하고, 전문가 라우팅의 정확한 그레이디언트를 추정하여 훈련 효율을 크게 향상시켰습니다. GRIN MoE는 다양한 작업에서 밀집 모델 대비 뛰어난 성능을 보여주어 혼합 전문가 모델의 실용성과 확장 가능성을 크게 증명했습니다.

### 전체 요약
이 논문은 혼합 전문가(MoE) 모델의 훈련 효율성을 극대화하기 위해 GRIN(GRadient-INformed) 방법을 제안합니다. GRIN MoE는 새로운 SparseMixer-v2 기법을 통해 전문가 라우팅의 정확한 그레이디언트를 추정하고, 토큰 드랍을 피하는 모델 병렬성을 제공합니다. 실험 결과, GRIN MoE는 다양한 벤치마크에서 고성능을 보여주었으며, 특히 수학 및 코딩 작업에서 뛰어난 성과를 기록했습니다. 이러한 결과는 GRIN MoE의 스케일링 잠재력 및 실용성을 입증하는 중요한 증거가 됩니다.