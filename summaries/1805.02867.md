# Online normalizer calculation for softmax
## TL;DR
## Summary
- [https://arxiv.org/pdf/1805.02867.pdf](https://arxiv.org/pdf/1805.02867.pdf)

## 요약

### I. 서론
소프트맥스(Softmax) 함수는 기계 학습에서 매우 중요한 역할을 하며, 특히 언어 모델링과 같은 다양한 작업에서 사용됩니다. 이 논문은 소프트맥스 함수의 메모리 접근을 줄여 성능을 향상시키는 방법을 제안합니다. 벤치마크 결과, 소프트맥스 함수의 성능이 최대 1.3배 향상되고, 소프트맥스와 TopK의 결합된 성능은 최대 5배 향상됨을 확인했습니다.

### II. 기존 소프트맥스
기존 소프트맥스 함수는 입력 벡터를 두 번 스캔하여 정규화 항(dV)을 계산하고 출력을 생성합니다. 이 과정에서 벡터 요소당 세 번의 메모리 접근이 필요합니다. 이 방법은 오버플로우 또는 언더플로우 문제를 피하기 위해 안전한 형태로 변형됩니다.

### III. 온라인 정규화 계산
온라인 정규화 알고리즘은 입력 벡터를 한 번만 스캔하여 최대 값(m)과 정규화 항(d)을 동시에 계산합니다. 이 방법은 벡터 요소당 메모리 접근을 4번에서 3번으로 줄입니다. 이 알고리즘은 안정적이며, 수학적으로 증명되었습니다.

### III-A. 병렬 온라인 정규화 계산
현대 컴퓨팅 장치는 여러 스레드를 동시에 실행할 수 있으므로, 병렬 처리를 통해 정규화 항을 계산할 수 있습니다. 이 방법은 연산을 병렬로 수행하여 효율성을 높입니다.

### IV. 소프트맥스와 Top-K 결합
온라인 소프트맥스는 TopK 함수와 결합되어 더욱 큰 성능 향상을 제공합니다. TopK 함수는 입력 벡터에서 가장 큰 값들을 찾아내므로, 소프트맥스와 결합하여 메모리 접근을 최소화할 수 있습니다. 이 결합은 메모리 접근을 줄이고 성능을 극대화합니다.

### V. 벤치마킹
CUDA C를 사용하여 GPU에서 벤치마크를 수행했습니다. 벤치마크 결과, 온라인 소프트맥스는 기존의 안전한 소프트맥스에 비해 메모리 접근이 감소하여 성능이 향상됨을 확인했습니다.

### V-A. 소프트맥스 벤치마킹
대규모 배치 처리에서 온라인 소프트맥스는 벡터 크기가 1000 이상일 때 성능이 향상되었습니다. 소규모 배치 처리에서도 성능 향상이 관찰되었으나, GPU가 완전히 활용되지 않아 성능 향상 폭이 상대적으로 적었습니다.

### V-B. 소프트맥스와 Top-K 벤치마킹
소프트맥스와 TopK를 결합한 온라인 소프트맥스는 큰 배치 처리에서 최대 5배의 성능 향상을 보였습니다. 작은 배치 처리에서는 1.5배에서 2.5배의 성능 향상을 확인했습니다.

### VI. 결과
온라인 정규화 계산을 도입함으로써 소프트맥스 함수의 메모리 접근을 줄여 성능을 향상시켰습니다. 벤치마크 결과, 소프트맥스 함수는 최대 1.3배, 소프트맥스와 TopK 결합은 최대 5배의 성능 향상을 보였습니다.

### VII. 논의
온라인 소프트맥스는 최신 GPU에서 최대 1.3배 더 빠르게 실행되며, TopK와의 결합으로 최대 5배의 성능 향상을 제공합니다. 향후 연구에서는 CPU와 같은 다른 컴퓨팅 장치에서도 유사한 성능 향상을 기대할 수 있습니다. 추가적으로, 소프트맥스와 이전 레이어를 결합하여 메모리 접근을 더 줄일 수 있는 방법도 연구할 가치가 있습니다.

---

### 전체 요약
이 논문은 소프트맥스 함수의 성능을 향상시키기 위해 온라인 정규화 계산 방법을 제안합니다. 이 방법은 메모리 접근을 줄여 성능을 향상시키며, 소프트맥스와 TopK 함수를 결합하여 더욱 큰 성능 향상을 제공합니다. 벤치마크 결과, 대규모 및 소규모 배치 처리 모두에서 성능이 향상됨을 확인했습니다. 이 연구는 기계 학습 모델의 성능을 최적화하는 데 기여할 수 있습니다.