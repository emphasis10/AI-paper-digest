# Learning Temporally Consistent Video Depth from Video Diffusion Priors
## TL;DR
## Summary
- [https://arxiv.org/pdf/2406.01493.pdf](https://arxiv.org/pdf/2406.01493.pdf)

### 섹션별 요약

**초록 (Abstract)**:
이 연구는 비디오 심도 추정의 과제를 다루며, 여기에는 프레임별 정확도뿐만 아니라 프레임 간 일관성이 중요합니다. 기존의 비디오 생성 모델에 포함된 사전 지식을 활용하여 학습 난이도를 줄이고 일반화를 개선하는 접근 방식을 제안합니다. 이 연구는 공공 Stable Video Diffusion (SVD) 모델을 사용하여 입력 비디오로부터 신뢰할 수 있는 깊이를 예측하는 방법을 탐구합니다. 절차적 학습 전략을 통해 최적의 결과를 달성하며, 실험 결과는 우리의 접근 방식이 기존 대안들보다 우수함을 보여줍니다. 우리의 접근 방식은 비디오 심도 추정의 일관성을 높이는 두 가지 실제 응용 프로그램에서도 이점을 강조합니다.

**1. 서론 (Introduction)**:
단일 카메라 비디오에서 심도를 추정하는 것은 컴퓨터 비전의 기본 과제로, 다양한 응용 분야에서 중요합니다. 최근 단일 이미지 심도 모델의 발전으로 공간적 정확도는 크게 향상되었지만, 비디오에서의 시간적 일관성 확보는 여전히 어려운 과제입니다. 기존 방법들은 시간적 일관성을 강화하려고 노력했지만, 일반적으로 성능이 부족했습니다. 비디오 생성 모델의 발전을 통해 공간적 품질과 시간적 일관성을 모두 달성하려는 노력이 이루어지고 있으며, 우리는 이러한 모델을 심도 추정에 활용하는 것을 목표로 합니다.

**2. 관련 연구 (Related Work)**:
- **단일 이미지 심도 추정 (Monocular Depth Estimation)**: 심도 추정은 판별적 방법과 생성적 방법으로 나눌 수 있습니다. 판별적 방법은 깊이를 직접 회귀하는 방식으로, 다양한 환경에서 일반화할 수 있는 모델을 학습하는 것이 최근의 트렌드입니다. 생성적 방법은 사전 학습된 생성 모델을 활용하여 심도 지도를 추정하는 방식으로, 복잡한 시나리오에서 높은 성능을 보입니다.
- **비디오 심도 추정 (Video Depth Estimation)**: 시간적 일관성을 확보하는 것이 주요 목표로, 이는 연속 프레임 간의 깜박임 효과를 제거하는 것을 의미합니다. 기존 방법들은 카메라 포즈 추정에 크게 의존하거나 시간적 일관성을 확보하기 위해 다양한 네트워크를 활용했지만, 대부분 공간적 정확도가 떨어졌습니다.
- **확산 모델 (Diffusion Models)**: 이미지 생성 작업에서 주로 사용되는 확산 모델은 최근 비디오 생성에서도 사용되며, 시간적 일관성을 통합하는 데 중점을 둡니다. 이러한 모델은 시간 축을 따라 자체 주의 메커니즘과 합성곱 연산을 통합하여 일관된 비디오 콘텐츠를 생성합니다.

**3. 방법론 (Method)**:
- **확산 공식화 (Diffusion Formulation)**: 이 연구는 SVD를 활용하여 단일 카메라 비디오 심도 추정을 연속적인 시간 확산 생성 작업으로 재구성합니다. RGB 비디오를 조건으로 하는 확산 모델을 사용하여 입력 비디오 심도에 노이즈를 주입하고 이를 제거합니다.
- **파인 튜닝 프로토콜 (Fine-Tuning Protocol)**: SVD 모델의 공간 및 시간 레이어를 순차적으로 최적화하여 최상의 성능을 달성합니다. 단일 프레임 데이터와 다중 프레임 데이터를 결합하여 학습하고, 랜덤으로 샘플링된 클립 길이를 사용하여 성능을 향상시킵니다.
- **추론 전략 (Inference Strategy)**: 긴 비디오에 대해 효율적이고 일관된 심도 추정을 위해 슬라이딩 윈도우 전략을 사용합니다. 인접한 클립 간에 정보를 교환하여 시간적 일관성을 개선합니다.

**4. 실험 (Experiment)**:
- **구현 세부 사항 및 데이터셋 (Implementation Details and Datasets)**: 다양한 단일 프레임 및 다중 프레임 데이터셋을 사용하여 모델을 학습합니다. 예를 들어, Hypersim, Tartanair, Virtual KITTI 2, MVS-Synth 등의 데이터셋을 활용합니다.
- **평가 (Evaluation)**: 다양한 벤치마크에서 우리의 방법론을 평가하여 기존 방법론과 비교합니다. 시간적 일관성 및 공간적 정확도를 평가하기 위해 다양한 지표를 사용합니다.
- **비교 실험 (Comparison Experiments)**: ChronoDepth가 기존의 판별적 및 생성적 방법론과 비교하여 우수한 성능을 보임을 입증합니다.
- **어블레이션 연구 (Ablation Studies)**: 모델 학습에 사용된 단일 프레임 데이터의 영향, 랜덤 클립 길이 샘플링의 효과, 공간-시간적 순차 학습 전략 등의 영향을 분석합니다.

**5. 결론 (Conclusion)**:
ChronoDepth는 비디오 생성 모델의 사전 지식을 활용하여 비디오 심도 추정에서 시간적 일관성을 우선시하는 모델입니다. 다양한 학습 프로토콜과 평가 방법을 통해 최상의 접근 방식을 도출했으며, 시간적 일관성에서 기존 방법보다 우수한 성능을 보입니다. 우리의 접근 방식은 심도 조건 비디오 생성 및 새로운 뷰 합성 등의 응용 프로그램에서도 이점을 제공합니다.

### 전체 요약

이 논문은 단일 카메라 비디오에서의 심도 추정을 개선하기 위해, 기존 비디오 생성 모델의 사전 지식을 활용하는 ChronoDepth를 제안합니다. ChronoDepth는 공간적 정확도와 시간적 일관성을 동시에 달성하기 위해 SVD 모델을 최적화하고, 슬라이딩 윈도우 전략을 통해 긴 비디오에서도 일관된 심도 추정을 수행합니다. 실험 결과, ChronoDepth는 기존의 판별적 및 생성적 방법보다 우수한 시간적 일관성과 유사한 수준의 공간적 정확도를 보였습니다. 이는 심도 조건 비디오 생성 및 새로운 뷰 합성 등의 실제 응용에서도 유리하게 작용합니다.

## Similar Papers
- [SpatialTracker: Tracking Any 2D Pixels in 3D Space](2404.04319.md)
- [4K4DGen: Panoramic 4D Generation at 4K Resolution](2406.13527.md)
- [Depth Anything V2](2406.09414.md)
- [Zero-shot Image Editing with Reference Imitation](2406.07547.md)
- [SV4D: Dynamic 3D Content Generation with Multi-Frame and Multi-View Consistency](2407.17470.md)
- [4Diffusion: Multi-view Video Diffusion Model for 4D Generation](2405.20674.md)
- [Noise Calibration: Plug-and-play Content-Preserving Video Enhancement using Pre-trained Video Diffusion Models](2407.10285.md)
- [Vivid-ZOO: Multi-View Video Generation with Diffusion Model](2406.08659.md)
- [Depth Anywhere: Enhancing 360 Monocular Depth Estimation via Perspective Distillation and Unlabeled Data Augmentation](2406.12849.md)
