# Lookback Lens: Detecting and Mitigating Contextual Hallucinations in Large Language Models Using Only Attention Maps
## TL;DR
## Summary
- [https://arxiv.org/pdf/2407.07071.pdf](https://arxiv.org/pdf/2407.07071.pdf)

### 요약

#### 1. 요약 - 각 섹션

**Introduction (서론)**

이 논문은 대형 언어 모델(LLM)이 생성하는 허구적 정보(즉, 잘못된 정보) 문제를 다루고 있습니다. 특히, 제공된 문맥 데이터와 관련 없는 정보를 생성하는 '문맥 허구화' 현상을 중점적으로 연구합니다. 이를 해결하기 위해 LLM이 문맥 정보를 얼마나 잘 참고하는지를 나타내는 'Lookback Ratio'라는 새로운 지표를 제안합니다. 이 지표는 각 관심주의(attention head)에서 문맥과 새로운 생성 토큰 사이의 주의 비율을 기반으로 계산됩니다.

**Related Work (관련 연구)**

허구적 정보 탐지와 억제는 주로 LLM의 내부 상태들을 분석하여 이루어졌습니다. 예를 들어, 히든 상태나 MLP 출력 등 다양한 표현을 사용하는 방법들이 있습니다. 본 논문은 이러한 기존 방법들보다 더 가벼운 'Lookback Ratio' 기반 탐지 방법을 제안합니다.

**Contextual Hallucinations Detection (문맥 허구화 탐지)**

이 섹션에서는 'Lookback Lens'라는 간단한 선형 분류기를 소개합니다. Lookback Ratio를 기반으로 문맥 허구화를 탐지하며, 이 분류기의 성능이 더 복잡한 탐지기들과 비슷하거나 그 이상임을 실험을 통해 증명합니다. 또한, 다양한 과업과 모델에서도 재훈련 없이 적용 가능함을 보였습니다.

**Contextual Hallucinations Mitigation (문맥 허구화 완화)**

문맥 허구화를 완화하기 위해 제안된 Lookback Lens Guided Decoding 전략은 LLM이 문맥상 적절한 출력을 생성할 수 있도록 합니다. 이 방법은 LLM의 다양한 텍스트 생성 시나리오에서도 효과적임을 실험을 통해 증명합니다. 실험 결과, 'XSum' 요약 과업에서 허구적 정보가 9.6% 감소하였음을 발견했습니다.

**Experimental Setup (실험 설정)**

실험은 'XSum' 요약 데이터셋과 'NQ' 질문-응답 데이터셋을 이용하여 이루어졌습니다. Lookback Lens는 제공된 문맥에 대한 모델의 주의를 기준으로 문맥 허구화를 예측합니다. 다양한 설정(같은 모델 내 이전 및 이후 전이, 교차 모델 전이)에서 Lookback Lens의 성능을 비교하여 정확성을 검증했습니다.

**Results and Discussions (결과 및 논의)**

실험 결과, Lookback Lens는 기존의 히든 상태 기반 분류기 및 텍스트 기반 모델보다 더 나은 일반화 성능을 보였습니다. 이는 Lookback Lens가 다양한 텍스트 생성 작업에서 높은 성능을 보인다는 것을 시사합니다. 또한, Lookback Lens Guided Decoding은 문맥 허구화를 효과적으로 완화하는 데 도움이 되며, 이는 텍스트 생성의 전반적인 품질을 저하하지 않음을 보였습니다.

**Conclusion (결론)**

Lookback Lens는 간단하지만 효과적인 문맥 허구화 탐지 및 완화 방법을 제공하며, LLM의 다양한 텍스트 생성 작업에 적용 가능합니다. 이 연구는 LLM의 주의 맵(Attention Map) 정보를 활용하여 허구적 정보를 억제하는 새로운 가능성을 열어줍니다.


#### 2. 전체 요약

이 논문은 대형 언어 모델(LLM)이 문맥과 무관한 허구적 정보를 생성하는 문제를 해결하기 위해 제안된 'Lookback Lens'라는 새로운 방법을 소개합니다. Lookback Lens는 LLM의 주의 맵(Attention Map)을 사용하여 문맥 정보를 얼마나 참조하는지를 나타내는 'Lookback Ratio'를 계산하고, 이를 통해 문맥 허구화를 탐지합니다. 탐지된 허구적 정보는 Lookback Lens Guided Decoding을 통해 완화됩니다. 이 방법은 다양한 모델 및 과업에 적용될 수 있으며, 적은 데이터로도 높은 성능을 보입니다. 실험 결과, Lookback Lens는 기존 방법보다 더 나은 일반화 성능을 보이며, LLM의 텍스트 생성 품질을 저하하지 않으면서도 허구적 정보를 감소시킵니다. 이러한 접근법은 LLM의 다양한 응용 분야에서 허구적 정보를 억제하는 새로운 가능성을 열어줍니다.

## Similar Papers
- [Found in the Middle: Calibrating Positional Attention Bias Improves Long Context Utilization](2406.16008.md)
- [AgentInstruct: Toward Generative Teaching with Agentic Flows](2407.03502.md)
- [LLoCO: Learning Long Contexts Offline](2404.07979.md)
- [HelpSteer2: Open-source dataset for training top-performing reward models](2406.08673.md)
- [WildBench: Benchmarking LLMs with Challenging Tasks from Real Users in the Wild](2406.04770.md)
- [RAGTruth: A Hallucination Corpus for Developing Trustworthy Retrieval-Augmented Language Models](2401.00396.md)
- [Better & Faster Large Language Models via Multi-token Prediction](2404.19737.md)
- [From Sparse to Dense: GPT-4 Summarization with Chain of Density Prompting](2309.04269.md)
- [Semantic Entropy Probes: Robust and Cheap Hallucination Detection in LLMs](2406.15927.md)
