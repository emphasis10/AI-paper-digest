# Crowd Comparative Reasoning: Unlocking Comprehensive Evaluations for LLM-as-a-Judge
## TL;DR
## Summary
- [https://arxiv.org/pdf/2502.12501.pdf](https://arxiv.org/pdf/2502.12501.pdf)

1. 각 섹션의 중요한 내용 요약:

   - **서론**: LLM-as-a-Judge라는 새로운 자동 평가 프레임워크의 필요성을 강조했습니다. 이는 인간 평가의 높은 비용과 제한된 확장성 문제를 해결하기 위한 것입니다.
   
   - **관련 연구**: 인간 평가의 한계로 인해 자동 평가가 각광받고 있음을 언급하며, 기존 방법들이 갖고 있는 한계를 설명했습니다.
   
   - **방법론**: Crowd-based Comparative Evaluation(CCE)의 개념을 도입하고 있습니다. 이는 관객 피드백을 이용해 평가의 심층적이고 포괄적인 이해를 가능하게 하는 방법입니다.
   
   - **실험 설정 및 결과**: CCE가 다섯 가지 평가 기준에서 평균 6.7%의 성능 향상을 보임을 입증했으며, 이를 통해 작은 평가 모델의 훈련 효율성을 높이는 방법을 제안했습니다.
   
   - **결론**: 이러한 접근법이 평가의 신뢰성을 높이고, 다양한 응용분야에서 적용될 수 있는 가능성을 제시했습니다.

2. 전체 요약:

   이 연구는 LLM-as-a-Judge의 제한적인 평가 능력을 극복하기 위해 Crowd-based Comparative Evaluation(CCE)을 제안했습니다. 인간의 평가 행위에서 영감을 받아 개발된 CCE는 관중의 피드백을 통해 평가의 세밀함과 포괄성을 높이는데 주안점을 두고 있습니다. 실험 결과는 CCE가 기존 평가 기준들보다 평균적으로 성능을 향상시키며, 평가를 위한 심층적 사고방식을 개발하는데 효과적임을 보여주었습니다. 이를 통해 다양한 기술적 과제에서 보다 효율적인 자동 평가 방법을 도출할 수 있는 가능성을 제시했습니다.