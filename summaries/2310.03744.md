# Improved Baselines with Visual Instruction Tuning
## TL;DR
## Summary
- [https://arxiv.org/pdf/2310.03744.pdf](https://arxiv.org/pdf/2310.03744.pdf)

#### 1. 소개
대형 멀티모달 모델(LMM)은 시각적 지시 튜닝을 통해 많은 진전을 보이고 있습니다. 이 논문은 LLaVA 프레임워크 하에서 LMM의 설계 선택을 체계적으로 조사한 첫 번째 연구입니다. 간단한 수정으로 LLaVA를 사용하여 11개 벤치마크에서 최첨단 성과를 달성하는 강력한 베이스라인을 설정했습니다.

#### 2. 관련 연구
기존 연구들은 대규모 이미지-텍스트 데이터 쌍을 사용하여 LMM의 성능을 향상시키는 데 중점을 두었습니다. 그러나 최적의 LMM 훈련 방법에 대한 명확한 지침은 없었습니다. 이 연구는 LLaVA의 간단한 아키텍처와 적은 양의 훈련 데이터로도 우수한 성능을 달성할 수 있음을 보여줍니다.

#### 3. 방법론
- **MLP Vision-Language Connector**: LLaVA의 선형 투사 층을 2층 MLP로 대체하여 멀티모달 이해 능력을 향상시켰습니다.
- **학술 과제 지향 데이터**: VQA, OCR, 지역 수준 인식을 위한 데이터를 추가하여 모델의 다양한 능력을 강화했습니다.
- **고해상도 이미지 입력**: 이미지를 그리드로 나누어 인코딩하고, 이를 결합하여 고해상도 입력을 처리할 수 있게 하였습니다.

#### 4. 실험 설정
LLaVA-1.5는 13B 모델을 사용하여 1.2M 공개 데이터로 1일 내에 훈련을 완료했습니다. 다양한 벤치마크에서 최고 성능을 달성했으며, 고해상도 입력 및 다양한 학술 데이터를 통해 모델의 세부 인식 능력을 개선했습니다.

#### 5. 주요 발견
1. **MLP 커넥터의 성능**: 간단한 2층 MLP를 사용하여 LLaVA의 멀티모달 이해 능력을 크게 향상시켰습니다.
2. **학술 데이터의 중요성**: VQA, OCR 등 학술 과제 지향 데이터를 추가함으로써 모델의 성능을 크게 향상시켰습니다.
3. **고해상도 입력의 이점**: 고해상도 입력을 처리하여 세부 인식 능력을 향상시키고, 환각 현상을 줄였습니다.
4. **데이터 효율성**: 전체 훈련 데이터의 75%를 랜덤으로 다운샘플링해도 성능 저하가 거의 발생하지 않았습니다.

#### 6. 결론
LLaVA-1.5는 간단하면서도 효과적인 접근 방식을 통해 멀티모달 모델의 성능을 크게 향상시켰습니다. 공개 데이터만을 사용하여 최고 성능을 달성함으로써, 더 많은 연구자들이 접근 가능하고 재현 가능한 베이스라인을 제공했습니다. 이는 향후 LMM 연구에 중요한 참고자료가 될 것입니다.

### 전체 요약
이 논문은 시각적 지시 튜닝을 통해 멀티모달 모델의 성능을 향상시키는 방법을 제안합니다. 간단한 아키텍처 수정과 학술 과제 지향 데이터를 추가하여 LLaVA-1.5 모델을 개발했으며, 이는 다양한 벤치마크에서 최고 성능을 달성했습니다. 고해상도 입력을 처리할 수 있는 능력을 추가하여 모델의 세부 인식 능력을 개선했으며, 데이터 효율성을 높여 적은 양의 데이터로도 높은 성능을 달성할 수 있음을 보여줍니다. 이 연구는 멀티모달 모델의 성능 향상을 위한 새로운 방향을 제시합니다.

## Similar Papers
- [Visual Instruction Tuning](2304.08485.md)
- [Ferret-v2: An Improved Baseline for Referring and Grounding with Large Language Models](2404.07973.md)
- [Matryoshka Multimodal Models](2405.17430.md)
- [SEED-Bench-2-Plus: Benchmarking Multimodal Large Language Models with Text-Rich Visual Comprehension](2404.16790.md)
- [How Far Are We to GPT-4V? Closing the Gap to Commercial Multimodal Models with Open-Source Suites](2404.16821.md)
- [LLaVA-OneVision: Easy Visual Task Transfer](2408.03326.md)
- [TextHawk: Exploring Efficient Fine-Grained Perception of Multimodal Large Language Models](2404.09204.md)
- [LLaVA-NeXT-Interleave: Tackling Multi-image, Video, and 3D in Large Multimodal Models](2407.07895.md)
- [AlignGPT: Multi-modal Large Language Models with Adaptive Alignment Capability](2405.14129.md)
