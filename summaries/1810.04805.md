# BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding
## TL;DR
## Summary
- [https://arxiv.org/pdf/1810.04805.pdf](https://arxiv.org/pdf/1810.04805.pdf)

### 1. 논문 각 섹션 요약

#### 1.1 서론 (Introduction)
BERT 모델은 고급 언어 표현을 학습하기 위해 개발된 비지도 학습 모델입니다. 이 모델은 왼쪽과 오른쪽 문맥을 모두 고려하여 깊이 있는 양방향 표현을 생성할 수 있습니다. 이로 인해 질문 응답, 언어 추론 등 여러 NLP 작업에서 뛰어난 성능을 보입니다.

#### 1.2 관련 연구 (Related Work)
기본적인 언어 표현 학습부터 최신의 비지도 학습 기법까지 다양한 접근법이 있습니다. 특히 ELMo와 OpenAI GPT가 주목받고 있으며, BERT는 이들보다 더 나은 성능을 보입니다. ELMo는 특징 기반 접근법을 사용하고 OpenAI GPT는 최소한의 작업별 파라미터만 추가하는 미세 조정 접근법을 사용합니다.

#### 1.3 BERT 모델 (BERT)
BERT는 Transformer 아키텍처를 사용하여 양방향으로 언어 모델을 학습합니다. 이를 통해 이전의 단방향 모델들의 한계를 극복했으며, '마스크드 언어 모델' (MLM)과 '문장 예측' (NSP)이라는 두 가지 사전 학습 목표를 사용합니다. 이로 인해 문맥의 양쪽을 모두 고려할 수 있어 더욱 정확한 언어 표현이 가능합니다.

#### 1.4 실험 (Experiments)
BERT는 11개의 NLP 작업에서 실험되었으며, 모든 작업에서 기존의 모든 시스템을 능가하는 성능을 보였습니다. 특히 GLUE 벤치마크에서는 평균 정확도가 4.5%에서 7%까지 향상되었습니다. 또한, Stanford Question Answering Dataset (SQuAD)에서도 높은 성능을 보였습니다.

#### 1.5 결론 (Conclusion)
BERT는 비지도 학습을 통해 다양한 NLP 작업에서 뛰어난 성능을 보이는 모델입니다. 특히 양방향 표현을 사용하여 기존 모델들이 보여주지 못한 성능 개선을 이루었으며, 다양한 작업에서 활용 가능함을 보였습니다.

### 2. 전체 요약
BERT(양방향 인코더 표현을 위한 트랜스포머)는 문장의 양쪽 문맥을 모두 포함하는 고급 언어 모델입니다. 기존의 단방향 모델들이 갖는 한계를 극복하기 위해 '마스크드 언어 모델'과 '문장 예측'을 사전 학습 목표로 사용하여 각종 NLP 작업에서 획기적인 성능 개선을 이루었습니다. 특히, 질문 응답, 언어 추론, 감정 분석 등에서 최첨단의 결과를 보여주며, 다양한 데이터셋에서 높은 성과를 보입니다. BERT의 주된 혁신은 사전 학습된 표현을 기반으로 한 미세 조정으로, 다양한 작업에서 뛰어난 성능을 가능하게 했다는 점입니다.

## Similar Papers
- [RoBERTa: A Robustly Optimized BERT Pretraining Approach](1907.11692.md)
- [How multilingual is Multilingual BERT?](1906.01502.md)
- [ALBERT: A Lite BERT for Self-supervised Learning of Language Representations](1909.11942.md)
- [SPAFIT: Stratified Progressive Adaptation Fine-tuning for Pre-trained Large Language Models](2405.00201.md)
- [Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning](2205.05638.md)
- [RETVec: Resilient and Efficient Text Vectorizer](2302.09207.md)
- [Poro 34B and the Blessing of Multilinguality](2404.01856.md)
- [AdaMix: Mixture-of-Adaptations for Parameter-efficient Model Tuning](2205.12410.md)
- [EELBERT: Tiny Models through Dynamic Embeddings](2310.20144.md)
