# A Closer Look into Mixture-of-Experts in Large Language Models
## TL;DR
## Summary
- [https://arxiv.org/pdf/2406.18219.pdf](https://arxiv.org/pdf/2406.18219.pdf)

### 1. 섹션별 요약 및 메인 기여 내용

#### 서론 (Introduction)
LLMs(대규모 언어 모델)의 발전은 자연어 처리 분야에 큰 혁신을 가져왔으며, 연구자들은 모델 크기와 학습 데이터의 양을 확장하여 모델 성능을 더욱 향상시키고 있습니다. 그러나 이러한 확장은 비용과 복잡성을 증가시키기에, Mixture-of-Experts (MoE)라는 새로운 아키텍처가 제안되었습니다. MoE는 입력을 전문 분야(전문가)별로 라우팅하여 모델의 효율성을 높이고, 비용 대비 성능을 향상시킵니다. 이 논문은 MoE 아키텍처의 적용 모델을 분석함으로써, 모델이 어떻게 작동하는지를 이해하는 첫 단계를 제시합니다.

#### 관련 연구 (Related Work)
기존 연구들은 주로 MoE의 라우터 측면을 통해 전문가 선택을 관찰하였습니다. 본 논문은 전문가의 유사성을 조사하여 효율적인 추론을 위한 중복성을 발견하고 이를 활용하는 방법을 제시합니다.

#### 방법 (Methodology)
본 연구는 세 가지 MoE 모델 (Mixtral 8x7B, DeepSeekMoE, Grok-11)의 파라미터 및 출력을 분석하여 모듈의 특성과 행동을 이해하고자 합니다. 이를 통해 각 전문가의 특성과 행동을 비교하고, MoE 모델의 내부 메커니즘을 탐구하려고 합니다.

#### 결과 (Results)
- FFN의 뉴런들은 세밀한 전문가로 작용합니다. 게이트 임베딩과 Wgate 행렬은 전문가 선택과 뉴런 활성화를 담당합니다.
- 더 깊은 층에서는 전문가의 수를 감소시키는 것이 효과적입니다.
- 라우터는 주로 출력을 큰 norm으로 가지는 전문가를 선택합니다.
- 전문가의 가중치 행렬 간의 유사성을 측정하면 평균 출력 유사성을 잘 나타낼 수 있습니다.
- 초기화 방식에 따라 전문가의 다양성이 달라질 수 있습니다.

#### 논의 (Discussion)
FFN과 Wgate의 유사성은 밀접한 관계가 있으며, 특정 초기화 방식보다 처음부터 MoE 모델을 학습시키는 것이 전문가의 다양성을 높이는 데 효과적일 수 있다고 결론지었습니다.

#### 결론 (Conclusion)
본 연구는 MoE 모델의 내부 메커니즘을 탐구하며, 특정 아키텍처 디자인과 훈련 프레임워크가 전문가의 특성화를 촉진할 수 있음을 시사합니다. 이 연구가 향후 MoE 및 모듈형 아키텍처 연구에 기초를 제공할 수 있기를 희망합니다.

### 2. 전체 요약
이 논문은 대규모 언어 모델에서의 Mixture-of-Experts (MoE) 아키텍처의 내부 작동 원리를 조사합니다. 세 가지 MoE 모델(Mixtral 8x7B, DeepSeekMoE, Grok-11)에 대한 파라미터와 출력을 분석하여, 각 전문가의 특성과 행동을 이해하고 비교합니다. 주요 발견사항으로는 전문가들이 세밀한 역할을 하여 효율성을 높이며, 초기화 방식이 전문가의 다양성에 영향을 미칠 수 있음을 밝혔습니다. 이 연구는 MoE 아키텍처의 설계와 훈련 방법에 대한 통찰을 제공하며, 앞으로의 연구에 중요한 기초를 마련합니다.

## Similar Papers
- [Multi-Head Mixture-of-Experts](2404.15045.md)
- [VCR: Visual Caption Restoration](2406.06462.md)
- [Fast Feedforward Networks](2308.14711.md)
- [Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality](2405.21060.md)
- [DiJiang: Efficient Large Language Models through Compact Kernelization](2403.19928.md)
- [Quantifying Emergence in Large Language Models](2405.12617.md)
- [Let the Expert Stick to His Last: Expert-Specialized Fine-Tuning for Sparse Architectural Large Language Models](2407.01906.md)
- [LoRA: Low-Rank Adaptation of Large Language Models](2106.09685.md)
- [Who Says Elephants Can't Run: Bringing Large Scale MoE Models into Cloud Scale Production](2211.10017.md)
