# Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback
## TL;DR
## Summary
- [https://arxiv.org/pdf/2204.05862.pdf](https://arxiv.org/pdf/2204.05862.pdf)

### 요약

#### 1. 소개 (Introduction)
이 논문은 AI 비서를 "도움이 되고 해가 없는" (Helpful and Harmless, HH) 상태로 훈련하기 위한 방법을 제안합니다. 이를 위해 인간의 피드백을 사용한 강화 학습(RLHF)과 선호 모델링(PMing)을 사용합니다. "도움이 되고 해가 없는" AI의 개념은, 각각의 개념을 별도로 다루며, 다양한 텍스트 기반 작업에 대한 도움을 제공하고 해로운 반응을 피하는 것입니다.

#### 2. 기여 (Contributions)
논문은 다음과 같은 주요 기여를 합니다:
- **대화 선호 데이터셋 수집**: 다양한 언어 모델을 사용하여 도움이 되는 대화와 해가 없는 대화 데이터셋을 수집했습니다. 이 데이터는 주로 크라우드워커와의 상호작용을 통해 얻었습니다.
- **선호 모델링 및 RLHF**: 수집된 데이터셋을 사용하여 선호 모델을 훈련하고, 이를 통해 RLHF 정책을 학습시켰습니다. 이 과정에서 온라인 학습 방식을 사용하여 매주 새로운 피드백 데이터를 추가로 반영했습니다.

#### 3. 데이터 수집 (Data Collection)
크라우드워커가 모델과 자연 언어 대화를 통해 상호작용하며, 도움이 되는 응답과 해로운 응답을 선택하도록 했습니다. 이 데이터는 선호 모델을 훈련하는 데 사용되었습니다.

#### 4. 선호 모델링 (Preference Modeling for Helpfulness and Harmlessness)
선호 모델링 과정에서는 각 대화의 마지막 응답에 점수를 매깁니다. 이를 통해 학습된 모델은 대화의 맥락을 이해하고, 더 도움이 되거나 해가 없는 응답을 생성할 수 있게 됩니다.

#### 5. RLHF를 통한 훈련 (Reinforcement Learning from Human Feedback)
RLHF는 선호 모델에서 얻은 점수를 보상으로 사용하여 모델을 훈련시키는 방식입니다. 이를 통해 모델이 사람의 피드백을 반영하여 점점 더 유용하고 해가 없는 응답을 생성하도록 합니다.

#### 6. 경쟁 목표, 전문 기술 및 OOD 감지 (Competing Objectives, Specialized Skills, and OOD Detection)
도움이 되는 것과 해가 없는 것이 종종 상충될 수 있습니다. 이 논문에서는 두 가지 목표를 균형 있게 달성하기 위해 다양한 데이터 혼합 및 가중치 조절 방법을 탐구했습니다. 또한, 모델이 훈련 데이터 분포를 벗어난 이상 행동을 감지하는 방법도 논의되었습니다.

#### 7. 온라인 학습 및 스케일링 (Iterated Online RLHF and Scaling)
매주 새롭게 업데이트된 선호 모델과 RLHF 정책을 통해 온라인 학습을 수행하였고, 이는 모델의 성능을 지속적으로 향상시켰습니다. 대규모 모델에서는 이러한 접근 방식이 특히 효과적이었으며, 작은 모델에 비해 성능 저하 없이 도움이 되는 응답과 해가 없는 응답을 더 잘 생성할 수 있었습니다.

#### 8. 결론 (Conclusion)
이 연구는 RLHF를 사용하여 AI 모델이 도움을 주면서도 해가 없는 응답을 생성할 수 있음을 보여줍니다. 제안된 방법은 AI 모델의 성능을 저하시키지 않으면서도 사용자의 요구에 더 잘 부합하도록 합니다. 이는 미래의 AI 시스템이 더 안전하고 유용하게 배포될 수 있는 가능성을 제시합니다.

### 전체 요약
이 논문은 인간의 피드백을 사용한 강화 학습(RLHF)을 통해 도움이 되고 해가 없는 AI 비서를 훈련하는 방법을 제안합니다. 데이터 수집, 선호 모델링, RLHF를 통해 모델이 점점 더 유용하고 안전한 응답을 생성하도록 학습시켰으며, 온라인 학습과 대규모 모델의 효과를 강조합니다. 연구 결과는 AI 시스템의 성능을 저하시키지 않으면서도 사용자의 요구에 잘 부합하는 AI 모델을 개발할 수 있음을 보여줍니다.

## Similar Papers
- [Language Models are Few-Shot Learners](2005.14165.md)
- [LLM-jp: A Cross-organizational Project for the Research and Development of Fully Open Japanese LLMs](2407.03963.md)
- [Learn Your Reference Model for Real Good Alignment](2404.09656.md)
- [Best Practices and Lessons Learned on Synthetic Data for Language Models](2404.07503.md)
- [Dataset Reset Policy Optimization for RLHF](2404.08495.md)
- [Efficient and Effective Vocabulary Expansion Towards Multilingual Large Language Models](2402.14714.md)
- [Concise Thoughts: Impact of Output Length on LLM Reasoning and Cost](2407.19825.md)
- [WARP: On the Benefits of Weight Averaged Rewarded Policies](2406.16768.md)
- [Tele-FLM Technical Report](2404.16645.md)
