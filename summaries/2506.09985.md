# V-JEPA 2: Self-Supervised Video Models Enable Understanding, Prediction and Planning
## TL;DR
## Summary
- [https://arxiv.org/pdf/2506.09985.pdf](https://arxiv.org/pdf/2506.09985.pdf)

**1. 각 섹션 요약**

- **서론:**
  이 논문은 AI의 핵심 문제인 세계를 이해하고 행동을 학습하는 방법에 대해 다룹니다. LeCun(2022)에 따르면, 이는 주로 관찰을 통해 배우는 것이 중요합니다. 본 논문에서는 인터넷 규모의 비디오 데이터와 소량의 상호작용 데이터(로봇 경로)를 결합한 자기 지도 학습 접근 방식을 탐구합니다.

- **V-JEPA 2:**
  V-JEPA 2는 100만 시간 이상의 인터넷 비디오 데이터로 사전 학습된 비디오 및 이미지 모델로, 동작 이해와 예측 그리고 계획을 가능하게 합니다. 이 모델은 운동 이해, 인간 행동 예측, 비디오 질문 응답 과제 등에서 최첨단 성능을 보이며, 특히 Epic-Kitchens-100 등에서 뛰어난 성과를 나타냅니다.

- **V-JEPA 2-AC:**
  사전 학습된 V-JEPA 2 모델을 기반으로, V-JEPA 2-AC라는 작업 비특화 행동 조건 모델을 훈련시킵니다. 이 모델은 프랭카 암을 사용하여 새로운 실험실 환경에서 제로샷(Zero-shot)으로 객체를 집고 놓는 작업을 가능하게 하며, 별도의 데이터 수집 없이 계획을 통해 이미지 목표를 달성할 수 있습니다.

- **자체 가이드 학습을 활용한 로봇 제어:**
  V-JEPA 2-AC는 라벨 없는 로봇 비디오 62시간으로부터 후속 학습을 통해 로봇 제어에 사용되며, 대규모 자기 지도 비디오 학습이 어플리케이션의 계획 작업을 새로운 환경에서 제로샷으로 어떻게 해결할 수 있는지 설명합니다.

**2. 논문의 주요 기여와 혁신적인 부분의 요약**

V-JEPA 2는 자기 지도 학습을 통해 비디오 모델이 물리적 세계를 이해하고 예측하며 계획을 할 수 있게끔 하는 방법론을 제시합니다. 이 연구는 비디오 이해와 인간 행동 예측에서 뛰어난 성과를 나타내며, 특히 실질적인 로봇 제어 작업에서 혁신적인 성과를 보입니다. V-JEPA 2-AC 또한 소량의 로봇 상호작용 데이터를 기반으로 이러한 작업을 지원하며, 이 모든 것이 세계 모델을 효과적으로 만들어냈습니다.

**3. 전체적인 요약**

이 논문은 V-JEPA 2와 V-JEPA 2-AC 모델을 통해 AI의 세계 모델 학습에서 새로운 접근을 제시합니다. 비디오 및 이미지 데이터를 이용해 동작의 이해 및 예측을 더욱 효과적으로 할 수 있게 하며, 새롭고 복잡한 환경에서 계획 작업을 수행하는 능력을 보여줍니다. 이는 자기 지도 학습이 물리적 세계에서 AI 시스템의 계획 역량을 획기적으로 향상시킬 수 있음을 증명합니다.