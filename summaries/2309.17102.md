# Guiding Instruction-based Image Editing via Multimodal Large Language Models
## TL;DR
## Summary
- [https://arxiv.org/pdf/2309.17102.pdf](https://arxiv.org/pdf/2309.17102.pdf)

이 문서는 **"MLLM(Multimodal Large Language Models)을 활용한 지시 기반 이미지 편집 가이드"**에 관한 연구를 담고 있습니다.

### Abstract 요약

이 논문은 자연 언어 명령을 사용하여 이미지 조작의 제어성과 유연성을 향상시키는 지시 기반 이미지 편집을 개선합니다. 그러나 사람의 지시는 때때로 현재 방법으로 분석하기에는 너무 간략합니다. MLLM은 크로스 모달 이해와 시각적으로 인식 가능한 응답 생성에서 유망한 능력을 보여줍니다. 우리는 MLLM이 편집 지시에 어떻게 도움이 되는지 조사하고 MLLM으로 가이드되는 이미지 편집(MGIE)을 제시합니다. MGIE는 표현력 있는 지시를 도출하고 명확한 가이드를 제공합니다. 이미지 편집 모델은 이 시각적 상상과 함께 조작을 수행하며, 엔드투엔드 훈련을 통해 이루어집니다. 우리는 포토샵 스타일 수정, 전체적인 사진 최적화, 그리고 부분 편집의 여러 측면을 평가합니다. 실험 결과는 MGIE가 자동 메트릭과 인간 평가 모두에서 주목할만한 개선을 이루었음을 보여줍니다.

### 1장 Introduction 요약

시각 디자인 도구는 다양한 멀티미디어 분야에서 널리 사용되고 있으나, 이를 활용하기 위해서는 사전 지식이 필요합니다. 이를 개선하기 위해 텍스트 가이드 이미지 편집이 인기를 끌고 있으며, 다양한 연구에서 활용되고 있습니다. 기존의 CLIP 텍스트 인코더와 같은 시스템은 정적 설명을 위해 훈련되었으나, 이미지 편집에서 필수적인 시각적 변환을 포착하는 데 한계가 있습니다. 대규모 언어 모델(LLM)은 다양한 언어 작업에서 상당한 발전을 보여주었고, MLLM은 자연스럽게 이미지를 입력으로 취급하고 시각적으로 인식 가능한 응답을 제공할 수 있습니다. 우리는 MLLM을 사용하여 이미지 편집에 필요한 명확하고 상세한 지시를 제공합니다.

### 2장 Related Work 요약

지시 기반 이미지 편집은 인간의 명령을 따르는 시각적 조작의 제어성과 접근성을 크게 향상시킵니다. 이전 연구들은 특정 도메인에 국한되거나 비현실적인 합성에 제한되었습니다. 대규모 훈련의 장점을 가진 확산 모델은 글로벌 캡션에 대한 크로스 모달 주의 맵을 컨트롤함으로써 이미지 변환을 달성할 수 있습니다. 지시 기반 이미지 편집은 "하늘에 불꽃놀이 추가"와 같이 구체적인 설명이나 지역 마스크에 제한되지 않는 직접적인 명령을 수용합니다. 이 논문은 MLLM을 사용하여 이미지와 주어진 프롬프트를 인식하고 명확하고 상세한 지시를 제공하여 우수한 편집 성능을 이끕니다.

이 연구는 시각 지시의 명확성과 정확성을 향상시키기 위해 MLLM을 활용합니다. 특히, MLLM을 사용하여 제공되는 표현력 있는 지시는 이미지 편집에서 중요한 역할을 하며, 이는 자동 메트릭 및 인간 평가에서의 성능 향상으로 입증됩니다. 패러다임을 변경하는 이 연구는 향후 지시 기반 이미지 편집의 발전을 위한 기반이 될 것입니다. 

### 전체 요약

이 논문은 **MLLM(Multimodal Large Language Models)을 이용한 지시 기반 이미지 편집(MGIE)에 초점**을 맞추고 있습니다. 인간의 지시가 때때로 현재의 방법으로는 충분히 이해되기 어려운 경우가 많은데, 이를 극복하기 위해 제안된 MGIE는 인간의 지시를 더 명확하고 표현력 있게 도출함으로써 이미지 편집을 개선하고자 합니다. MGIE 시스템은 MLLM과 확산 모델을 결합하여, 시각적 상상력을 기반으로 이미지를 편집하는 엔드투엔드 훈련 접근 방식을 사용합니다. 이 연구에서는 포토샵 스타일의 수정, 전체 사진 최적화, 그리고 지역 객체 변경과 같은 다양한 편집 측면을 다루며, 실험 결과는 MGIE가 자동 평가 지표와 인간 평가 모두에서 뛰어난 성능을 보임을 시사합니다.

다양한 편집 측면에서의 종합적인 연구를 진행하였으며, 시각적으로 인식 가능한 표현력 있는 지시의 중요성을 강조합니다. 이를 통해 MGIE는 지시 기반 이미지 편집의 효과성을 크게 향상시킬 뿐만 아니라, 인간의 직관에 더 부합하는 가이드를 제공함으로써 실용성을 증대시킵니다. 실제 예시에서 제시된 바와 같이, "건강한"이라는 맥락 없는 명령을 시각적으로 구체적으로 해석하여 피자에 "채소 토핑을 추가하는" 등의 편집을 이끌어내는 능력을 보여줍니다.

또한, 이 작업에서는 "IPr2Pr"이라는 사전 훈련 데이터셋을 사용하여 지시 기반 이미지 편집을 배우며, 이를 통해 인간의 지시를 따르는 이미지 편집에 있어 MGIE가 얼마나 효과적인지를 보여줍니다. MGIE의 세 가지 주요 기여는 다음과 같습니다: 1) 시각적으로 인식 가능한 표현력 있는 지시를 제공하여 명확한 가이드를 제공하는 MLLM 가이드 이미지 편집을 소개, 2) 다양한 편집 측면에서의 포괄적인 연구 수행, 3) 시각적으로 인식 가능한 표현력 있는 지시의 중요성을 입증하며 효과적으로 편집 성능을 향상시킴. 

이러한 접근 방식은 AI와 머신러닝 분야에서 주목할 만한 발전을 나타내며, 특히 이미지 편집과 인간의 지시를 이해하는 방식에서 새로운 가능성을 열어줍니다. 

## Similar Papers
- [Compressing LLMs: The Truth is Rarely Pure and Never Simple](2310.01382.md)
- [Invertible Consistency Distillation for Text-Guided Image Editing in Around 7 Steps](2406.14539.md)
- [Diffree: Text-Guided Shape Free Object Inpainting with Diffusion Model](2407.16982.md)
- [Ferret-v2: An Improved Baseline for Referring and Grounding with Large Language Models](2404.07973.md)
- [MaPa: Text-driven Photorealistic Material Painting for 3D Shapes](2404.17569.md)
- [Paint by Inpaint: Learning to Add Image Objects by Removing Them First](2404.18212.md)
- [RL for Consistency Models: Faster Reward Guided Text-to-Image Generation](2404.03673.md)
- [Editable Image Elements for Controllable Synthesis](2404.16029.md)
- [Kaleido Diffusion: Improving Conditional Diffusion Models with Autoregressive Latent Modeling](2405.21048.md)
