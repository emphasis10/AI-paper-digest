# Meta Large Language Model Compiler: Foundation Models of Compiler Optimization
## TL;DR
## Summary
- [https://scontent-gmp1-1.xx.fbcdn.net/v/t39.2365-6/448997590_1496256481254967_2304975057370160015_n.pdf?_nc_cat=106&ccb=1-7&_nc_sid=3c67a6&_nc_ohc=h8ckc-jIGyMQ7kNvgGZkNzt&_nc_ht=scontent-gmp1-1.xx&oh=00_AYDT8EmSp0QQ6bRC5JjjEd3UFsiMVRkXrTIHrCL6-IbL-w&oe=66881F4D](https://scontent-gmp1-1.xx.fbcdn.net/v/t39.2365-6/448997590_1496256481254967_2304975057370160015_n.pdf?_nc_cat=106&ccb=1-7&_nc_sid=3c67a6&_nc_ohc=h8ckc-jIGyMQ7kNvgGZkNzt&_nc_ht=scontent-gmp1-1.xx&oh=00_AYDT8EmSp0QQ6bRC5JjjEd3UFsiMVRkXrTIHrCL6-IbL-w&oe=66881F4D)

### 섹션별 요약

#### 1. 서론
이 논문은 대형 언어 모델(LLMs)이 코드 및 컴파일러 최적화 분야에서 활용될 수 있는 가능성에 대해 탐구합니다. Meta Large Language Model Compiler(LLM Compiler)는 코드 최적화 작업을 위해 특별히 설계된 사전 훈련된 모델들을 소개하며, 이러한 모델들이 컴파일러 중간 표현(IR) 및 어셈블리 언어를 이해하고 최적화 기법을 적용하는 능력을 향상시키는 것을 목표로 합니다.

#### 2. LLM Compiler: 컴파일러 최적화를 위한 Code Llama의 특화
LLM Compiler 모델은 주로 컴파일러 중간 표현 및 어셈블리 코드로 구성된 5460억 개의 토큰 데이터로 훈련되었습니다. 이 모델들은 Code Llama 모델의 가중치를 초기화하고 추가적인 훈련을 통해 컴파일러 에뮬레이션 데이터셋에서 명령어를 미세 조정했습니다. 이 훈련 단계에서는 컴파일러 최적화를 예측하고 코드 크기를 줄이는 작업에 중점을 두었습니다.

#### 3. LLM Compiler FTD: 후속 컴파일러 작업을 위한 확장
LLM Compiler FTD 모델은 컴파일러 플래그 튜닝 및 디스어셈블링 작업을 위한 추가 미세 조정을 거쳤습니다. 이 모델들은 x86_64 및 ARM 어셈블리에서 LLVM-IR로 다시 변환하는 작업에서 탁월한 성능을 보였으며, 최적화 잠재력의 77%를 달성하고, 디스어셈블링 정확도에서 45%의 회전율(14%의 정확한 일치)을 기록했습니다.

#### 4. 훈련 매개변수
데이터는 byte pair encoding(BPE)을 사용하여 토큰화되었으며, AdamW 옵티마이저를 사용하여 훈련되었습니다. 훈련 중 개별 시퀀스의 컨텍스트 길이는 4,096에서 16,384로 증가되었습니다.

#### 5. 평가
LLM Compiler 모델은 플래그 튜닝 및 디스어셈블링, 컴파일러 에뮬레이션, 다음 토큰 예측 및 소프트웨어 엔지니어링 작업에서 평가되었습니다. 플래그 튜닝 작업에서는 13B 파라미터 모델이 -Oz 대비 5.26%의 개선을 보였고, 디스어셈블링 작업에서는 13.8%의 정확한 일치를 달성했습니다.

#### 6. 관련 연구
이 논문은 다양한 소스 코드 생성 및 최적화 모델들을 검토하며, 특히 컴파일러 IR을 포함하는 모델들에 주목합니다. LLM Compiler는 컴파일러 중간 표현 및 어셈블리 코드에 대한 이해를 크게 향상시켰습니다.

#### 7. 결론
LLM Compiler는 코드 및 컴파일러 최적화 작업을 위해 설계된 대형 언어 모델로, 학계 및 산업계 연구자들이 이 모델을 사용하여 컴파일러 최적화를 더욱 발전시킬 수 있도록 지원합니다.

### 전체 요약
이 논문은 대형 언어 모델(LLMs)의 코드 및 컴파일러 최적화 분야에서의 활용 가능성을 탐구합니다. Meta AI의 LLM Compiler는 코드 최적화 작업을 위해 설계된 사전 훈련된 모델로, 컴파일러 중간 표현 및 어셈블리 언어를 이해하고 최적화 기법을 적용하는 능력을 크게 향상시켰습니다. LLM Compiler 모델은 Code Llama의 가중치를 기반으로 하여 추가적인 훈련을 통해 컴파일러 에뮬레이션 데이터셋에서 명령어를 미세 조정하였고, 후속 작업인 플래그 튜닝 및 디스어셈블링 작업에서도 탁월한 성능을 보였습니다. 이 모델들은 학계 및 산업계 연구자들이 컴파일러 최적화를 더욱 발전시킬 수 있도록 지원합니다.