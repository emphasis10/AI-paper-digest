# OctoThinker: Mid-training Incentivizes Reinforcement Learning Scaling
## TL;DR
## Summary
- [https://arxiv.org/pdf/2506.20512.pdf](https://arxiv.org/pdf/2506.20512.pdf)

### 1. 논문 섹션 요약

#### 도입부
이 연구는 대규모 강화 학습을 통해 대형 언어 모델이 논리적 사고 과정을 통해 최종 답을 얻도록 유도하는 데 중점을 둡니다. 구체적으로, 최근 모델들이 대회 수준의 수학 문제를 해결하는 데 있어 발전을 이루고 있다는 것을 설명합니다.

#### 연구 배경 및 주요 질문
강화 학습이 언어 모델의 성능을 향상시키는 데 있어 주요한 역할을 하고 있으며, 특히 수학적 추론에서 주요 모델들인 Qwen과 Llama 사이의 차이를 탐구합니다. Qwen 모델은 강화 학습에 더 적합한 반면, Llama 모델은 최종 답을 예측하는 데 있어 문제를 일으키는 경향이 있습니다.

#### 중간 훈련 전략
이 논문은 '안정-그다음-감소'라는 두 단계의 중간 훈련 전략을 통해 Llama 모델에 대해 체계적인 조사를 수행합니다. 첫 번째 단계에서는 200B 토큰으로 고품질 데이터를 사용하여 훈련하고, 두 번째 단계에서는 학습률을 조정해 다양한 데이터 조합을 통해 세 가지 가지를 훈련합니다.

#### 실험 결과
모든 모델 크기와 14개의 수학적 추론 벤치마크를 통해 두 단계 훈련의 효과성을 확인합니다. 특히 첫 번째 단계는 10-20%의 성능 향상을 일관되게 가져왔고, 강화 학습 후에는 강화된 성능을 보여줍니다.

### 2. 전체 요약
이 논문은 기존의 Llama 모델이 강화 학습을 통해 더 나은 성능을 보일 수 있도록 중간 훈련 전략을 사용하여 모델의 추론 능력을 강화하는 방법을 제안합니다. 두 단계로 구성된 '안정-그다음-감소' 전략을 통해 모델을 강화 학습에 더 적합하게 만듦으로써, Qwen 같은 강화 학습에 친화적인 모델과의 성능 차이를 줄일 수 있음을 증명합니다. 이번 연구는 차세대 추론 중심 AI 시스템의 기초를 마련하는 데 기여하고자 합니다.