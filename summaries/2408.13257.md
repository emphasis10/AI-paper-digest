# MME-RealWorld: Could Your Multimodal LLM Challenge High-Resolution Real-World Scenarios that are Difficult for Humans?
## TL;DR
## Summary
- [https://arxiv.org/pdf/2408.13257.pdf](https://arxiv.org/pdf/2408.13257.pdf)

### 1. 각 섹션 요약

#### 서론 (Introduction)
최근 다중 모드 대형 언어 모델(MLLMs)은 인간의 질문과 환경 상황을 포괄적으로 인식하기 위한 자료로 떠올랐습니다. 그러나 기존 평가 기준은 데이터 규모, 주석 품질, 과제 난이도 등의 문제로 실제 세계에서 모델이 직면하는 주요 과제를 평가하기 어렵게 만들었습니다. 이를 해결하기 위해 MME-RealWorld를 소개하며, MME-RealWorld는 고해상도 이미지를 중점으로 엄선된 29,429개의 질문-답변 쌍을 포함한 대규모 벤치마크입니다.

#### 관련 연구 (Related Work)
다양한 MLLM 벤치마크가 존재합니다. MME는 14개의 감지 및 인지 과제를 포함하며, MMBench는 3,000개 이상의 질문으로 다양한 능력 차원을 평가합니다. Seed-Bench는 19,000개의 질문을 포함하고 있으며, SEED-Bench-2는 27개의 평가 차원을 포함합니다. 이러한 벤치마크들은 주로 데이터 규모와 주석 품질 문제를 가지고 있습니다.

#### MME-RealWorld
MME-RealWorld는 다양한 공공 데이터 세트에서 300K 이상의 이미지를 수집, 전문가 팀의 주석을 통해 고해상도 이미지를 포함한 29,429개의 질문-답변 쌍을 제공합니다. 이 벤치마크는 실제 세계의 다섯 가지 시나리오에서 43개의 서브태스크를 포함합니다. MME-RealWorld는 인간조차 해결하기 어려운 난이도를 갖추고 있으며, 고해상도 이미지와 실제 세계 적용에 중점을 둡니다.

#### 실험 및 결과 (Experiments and Results)
총 28개의 MLLM을 평가한 결과, InternVL-2가 가장 우수한 성능을 나타냈으나, 모든 모델이 60% 정확도를 초과하지 못했습니다. 또한, GPT-4o와 Gemini-1.5 프로와 같은 닫힌 소스 모델은 OCR 태스크에서 뛰어난 성능을 보였지만, 더 어려운 태스크에서는 유의미한 성능 저하가 나타났습니다. 이는 모델이 고해상도 이미지 인식과 복잡한 실제 세계 시나리오 이해에 여전히 큰 과제를 가지고 있음을 시사합니다.

### 2. 전체 요약

MME-RealWorld는 기존 다중 모드 대형 언어 모델(MLLMs)의 평가 기준이 가지는 데이터 규모, 주석 품질, 과제 난이도 문제를 해결하기 위해 고안되었습니다. 총 300,000개 이상의 이미지에서 엄선된 29,429개의 고해상도 질문-답변 쌍을 통해 43개의 서브태스크를 실제 세계의 다섯 가지 시나리오에서 평가합니다. 이 연구는 인간조차 해결하기 어려운 난이도를 갖추고 있으며, 현재까지 가장 큰 규모의 완전 인간 주석 벤치마크를 제공합니다. 총 28개의 MLLM을 평가한 결과, 모든 모델은 60% 정확도를 초과하지 못하며 모델들이 고해상도 이미지와 복잡한 실제 시나리오를 이해하는 데 여전히 큰 과제를 가지고 있음을 시사합니다. 이로 인해 더 높은 수준의 모델 발전과 개선이 필요함을 강조합니다.