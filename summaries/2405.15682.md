# The Road Less Scheduled
## TL;DR
## Summary
- [https://arxiv.org/pdf/2405.15682.pdf](https://arxiv.org/pdf/2405.15682.pdf)

#### 주요 기여 및 혁신
이 논문은 대규모 언어 모델(LLM)의 효율적인 사전 학습을 위해 모델 성장 방법을 체계적으로 평가합니다. 특히, Gstack이라는 깊이별 스택킹 연산자가 학습 가속화에 뛰어난 성능을 보인다는 것을 입증했습니다. 이 논문은 LLM 사전 학습의 효율성을 높이기 위해 기존 모델 성장 방법을 네 가지 기본 연산자로 요약하고, 이를 표준화된 테스트 환경에서 평가했습니다.

---

### 섹션별 요약

#### 1. 서론 (Introduction)
LLM의 뛰어난 능력은 모델 크기의 확장에 의존하지만, 이 과정은 막대한 에너지와 환경 비용을 초래합니다. 작은 모델을 활용하여 더 큰 모델의 학습을 가속화하는 모델 성장 기술이 주목받고 있습니다. 이 논문은 LLM 사전 학습에서 모델 성장 방법의 잠재력을 체계적으로 평가합니다.

#### 2. 관련 연구 (Related Work)
모델 성장의 아이디어는 1990년대에 시작되었으며, Net2Net과 같은 초기 연구가 있습니다. 기존 연구들은 주로 BERT 모델에 집중되어 있으며, 이 논문은 LLM 시대의 모델 성장 기술을 종합적으로 검토한 첫 번째 연구입니다.

#### 3. 제안된 방법 (Proposed Method)
이 논문은 네 가지 기본 모델 성장 연산자(Gdirect, Glearn, Gzero, Grandom)를 요약하고, 이를 표준화된 LLM 사전 학습 환경에서 평가합니다. 특히, 깊이별 스택킹 연산자(Gstack)가 다른 연산자보다 학습 가속화에 탁월한 성능을 보였습니다.

#### 4. 실험 (Experiments)
깊이별 성장(G↑)이 너비별 성장(G→)보다 더 높은 학습 가속화를 제공하며, 특히 Gstack이 가장 우수한 성능을 보였습니다. Gstack을 활용한 모델은 기본 모델보다 49.1% 더 빠르게 학습을 완료했습니다. 다양한 성장 인자(g)에 대한 실험을 통해 최적의 성장 인자를 제안합니다.

#### 5. 논의 및 한계 (Discussion and Limitations)
모델 성장 기법의 채택을 위한 실질적인 가이드라인을 제공하며, 다양한 스택킹 변형 방법을 비교합니다. 또한, 함수 보존 실패의 원인을 분석하고, Gstack의 우수성을 입증했습니다.

---

### 전체 요약
이 논문은 대규모 언어 모델의 효율적인 사전 학습을 위한 모델 성장 방법을 체계적으로 검토하고 평가합니다. 특히, 깊이별 스택킹 연산자(Gstack)가 다른 성장 연산자보다 학습 가속화에 뛰어난 성능을 보이며, 이는 LLM 사전 학습의 새로운 가능성을 제시합니다. 실험 결과, Gstack을 활용한 모델은 기존 방법보다 더 빠르고 효율적으로 학습을 완료할 수 있음을 입증했습니다. 이 연구는 LLM의 사전 학습에 있어 모델 성장 기술의 실질적인 가이드라인을 제공하며, 더 나은 성능을 위한 새로운 방향을 제시합니다.

## Similar Papers
- [Thermodynamic Natural Gradient Descent](2405.13817.md)
- [4-bit Shampoo for Memory-Efficient Network Training](2405.18144.md)
- [Adam-mini: Use Fewer Learning Rates To Gain More](2406.16793.md)
- [$μ$LO: Compute-Efficient Meta-Generalization of Learned Optimizers](2406.00153.md)
- [AI Agents That Matter](2407.01502.md)
- [BitNet: Scaling 1-bit Transformers for Large Language Models](2310.11453.md)
- [Symbolic Learning Enables Self-Evolving Agents](2406.18532.md)
- [Federated Learning with Differential Privacy for End-to-End Speech Recognition](2310.00098.md)
- [Careful with that Scalpel: Improving Gradient Surgery with an EMA](2402.02998.md)
