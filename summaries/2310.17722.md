# Large Language Models as Generalizable Policies for Embodied Tasks
## TL;DR
## Summary
- [https://arxiv.org/pdf/2310.17722.pdf](https://arxiv.org/pdf/2310.17722.pdf)

이 연구 논문에서는 대규모 언어 모델을 사용하여 시각적 작업에 대한 일반화 가능한 정책을 적용하는 새로운 접근법인 LLaRP(Large LAnguage model Reinforcement Learning Policy)를 소개합니다. 이 접근법은 예측된 LLM을 활용하여 텍스트 지시와 시각적 관측을 입력으로 받고, 환경 내에서 직접적인 행동을 출력하는 방식으로, 강화 학습을 통해 정책을 훈련합니다.

### 주요 내용 요약

1. **서론 및 배경**:
   - LLMs는 텍스트 데이터로 훈련된 억억 개의 파라미터를 가진 모델로, 언어 이해 능력을 넘어 다양한 문제에서 강력한 능력을 보여줍니다. 본 연구는 이러한 LLM의 일반화 능력을 실제 환경 작업에 적용할 수 있는지 탐구합니다.

2. **LLaRP의 구조 및 기능**:
   - LLaRP는 강화 학습을 통해 시각-언어 정책을 학습하며, 이는 시각적 관측과 텍스트 지시를 입력으로 받아 환경에 적합한 행동을 출력합니다. 이 모델은 다양한 재배치 작업에서 고도의 일반화 능력을 보여줍니다.

3. **성능 평가 및 응용**:
   - LLaRP는 기존의 LSTM 기반 정책이나 제로샷 LLM 응용 프로그램보다 높은 성공률을 보여줍니다. 특히, 본 연구에서 제안한 '언어 재배치' 벤치마크에서 1,000개의 테스트 작업에서 42%의 성공률을 달성합니다.

### 혁신적인 부분
LLaRP의 혁신성은 LLM을 사용하여 시각적 및 언어적 입력을 통합해 일반화 가능한 정책을 학습하는 데 있습니다. 이는 LLM의 언어 이해 능력과 결합된 강화 학습 접근 방식을 통해, 복잡한 시각적 작업에 대한 이해와 행동 생성 능력을 크게 향상시킵니다.

이 연구는 LLM을 기반으로 한 새로운 시각-언어 정책 학습 방법을 제시함으로써, 인공 지능과 로봇공학 분야에서 실제적인 응용을 가능하게 하여 중요한 기여를 합니다.