# How to Get Your LLM to Generate Challenging Problems for Evaluation
## TL;DR
## Summary
- [https://arxiv.org/pdf/2502.14678.pdf](https://arxiv.org/pdf/2502.14678.pdf)

1. 각 섹션의 요약:

   - **초록**: 이 논문은 대규모 언어 모델(LLM)의 빠른 발전에 맞춰, 평가에 필요한 새로운 문제를 생성할 수 있는 한 가지 프레임워크 'CHASE'를 소개합니다. 이 방법은 인간의 개입 없이 LLM을 활용하여 도전적인 문제를 생성하며, 이를 통해 기존의 평가 수준을 높이고자 합니다.

   - **서론**: 최근 LLM의 급속한 발전으로 인해 기존의 평가 자료가 이를 따라가지 못하고 있습니다. 본 논문에서는 합성 데이터를 통해 복잡한 문제를 자율적으로 생성하는 방법을 제안하며, 이는 기존 데이터에 비해 더 도전적이고 평가 기준을 강화할 수 있습니다.

   - **CHASE 프레임워크와 벤치마크**: 이 프레임워크의 주요 아이디어는 보다 간단한 문제 및 해결책부터 시작하여 점진적으로 복잡한 문제를 구축하는 것입니다. 이를 통해 LLM이 직접 생성한 문제도 해결하기 어려운 형태로 만듭니다.

   - **실험 및 결과**: CHASE는 문서 기반의 질문 응답, 저장소 수준의 코드 완성, 수학적 추론을 포함한 세 가지 도메인에서 실험적으로 확인되었습니다. CHASE가 생성한 예제는 최첨단 AI 솔루션조차 해결하기 어려운 경우가 많았으며, 이는 새로운 평가 패러다임을 제시하는 데 기여할 수 있습니다.

   - **결론**: CHASE는 대규모 데이터를 효율적으로 생성할 수 있는 확장 가능하고 지속 가능한 프레임워크입니다. 또한, 인간이 평가하기 어려운 작업(예: 긴 문맥 추론)에 대한 높은 질의 평가를 가능하게 합니다. 이러한 프레임워크는 AI 모델의 성능 평가를 더욱 발전시키는 데 도움을 줍니다.

2. 전체 요약:

   전체적으로, 이 논문은 CHASE라는 새로운 프레임워크를 통해 대규모 언어 모델의 평가 방법을 혁신적으로 개선하고자 합니다. 이 프레임워크는 인공지능 모델 자체를 사용하여 도전적인 문제를 생성하고, 이를 통해 기존 평가 체계를 보강하면서도 확장성을 제공합니다. 문서 기반 질문 응답, 코드 완성, 수학적 추론 등 다양한 도메인에서 사용 가능하며, 현대 AI 모델의 한계를 극복하는 데 기여합니다.