# SARATHI: Efficient LLM Inference by Piggybacking Decodes with Chunked Prefills
## TL;DR
## Summary
- [https://arxiv.org/pdf/2308.16369.pdf](https://arxiv.org/pdf/2308.16369.pdf)

### Section-wise Summary

#### 1. Introduction

SARATHI는 대규모 언어 모델(LLM) 추론의 효율성을 개선하기 위해 설계된 새로운 방법론입니다. LLM 추론에는 입력 프롬프트를 처리하는 Prefill 단계와 출력 토큰을 생성하는 Decode 단계가 포함됩니다. 이 두 단계는 각각 다른 GPU 활용도를 가지며, 특히 Decode 단계는 매우 비효율적입니다. SARATHI는 이러한 문제를 해결하기 위해 Prefill 요청을 동일한 크기의 청크로 분할하는 Chunked-prefills와 Prefill 청크와 디코드를 배치하여 GPU 활용도를 극대화하는 Decode-maximal batching을 도입합니다.

#### 2. Background

이 섹션에서는 Transformer 아키텍처와 LLM 추론의 두 단계인 Prefill과 Decode에 대해 설명합니다. 또한 GPU 다중 사용을 위한 파이프라인 병렬 처리 방법도 다룹니다.

#### 3. Motivation

LLM 추론의 주요 비효율성은 메모리 바운드된 Decode 단계와 파이프라인 병렬 처리 중 발생하는 파이프라인 버블에 있습니다. 이를 통해 GPU 활용도가 떨어지고 전체 추론 성능이 저하됩니다.

#### 4. SARATHI: Design and Implementation

SARATHI의 두 가지 핵심 기술, 즉 Chunked-prefills와 Decode-maximal batching이 어떻게 설계되고 구현되었는지 설명합니다. Chunked-prefills는 Prefill 요청을 작은 청크로 분할하여 균일한 단위의 작업을 생성합니다. 이러한 청크는 GPU 계산을 포화시키고 파이프라인 버블을 줄이는 데 도움을 줍니다.

#### 5. Evaluation

SARATHI의 성능을 다양한 모델과 하드웨어 구성에서 평가합니다. SARATHI는 LLaMA-13B 모델에서 디코드 처리량을 최대 10배, 전체 추론 처리량을 최대 1.33배까지 개선하고, 파이프라인 병렬 처리 사용 시 파이프라인 버블을 최대 6.29배 줄여줍니다.

#### 6. Discussion

SARATHI의 성능 개선 효과를 종합적으로 입증하지만 몇 가지 도전 과제도 남아 있습니다. 예를 들어, 최적 청크 크기 선택 등 여러 요인이 성능에 미치는 영향을 추가 연구해야 합니다.

#### 7. Related Work

LLM 추론 최적화를 위한 시스템 최적화 및 모델 혁신에 관한 기존 연구를 간략히 소개합니다. FlexGen, Orca 등 관련 연구들이 언급됩니다.

#### 8. Conclusion

SARATHI는 GPU 활용을 극대화하고 파이프라인 버블을 줄여 LLM 추론 성능을 크게 개선하는 새로운 방법론입니다. 이를 통해 다양한 모델과 하드웨어 구성을 통해 성능 향상을 입증하였습니다.

### Overall Summary

SARATHI는 대규모 언어 모델(LLM) 추론의 비효율성을 해결하기 위해 개발된 혁신적인 방법론입니다. GPU의 최적 활용과 파이프라인 버블 감소를 통해 LLM 추론 성능을 대폭 개선합니다. 이를 위해 도입된 Chunked-prefills와 Decode-maximal batching은 균일한 단위의 작업을 생성하고 디코드 요청을 Prefill 청크와 함께 배치하여 GPU 활용도를 최대화합니다. 다양한 모델과 하드웨어 구성에서 SARATHI의 성능 개선 효과를 입증하며, 향후 연구를 통해 최적화된 청크 크기 선택 등 여러 요인을 추가로 탐구할 예정입니다.

## Similar Papers
- [Efficient Memory Management for Large Language Model Serving with PagedAttention](2309.06180.md)
- [MemServe: Context Caching for Disaggregated LLM Serving with Elastic Memory Pool](2406.17565.md)
- [You Only Cache Once: Decoder-Decoder Architectures for Language Models](2405.05254.md)
- [Grass: Compute Efficient Low-Memory LLM Training with Structured Sparse Gradients](2406.17660.md)
- [PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU](2312.12456.md)
- [Parrot: Efficient Serving of LLM-based Applications with Semantic Variable](2405.19888.md)
- [S-LoRA: Serving Thousands of Concurrent LoRA Adapters](2311.03285.md)
- [QMoE: Practical Sub-1-Bit Compression of Trillion-Parameter Models](2310.16795.md)
- [SpecInfer: Accelerating Generative Large Language Model Serving with Tree-based Speculative Inference and Verification](2305.09781.md)
