# SPAFIT: Stratified Progressive Adaptation Fine-tuning for Pre-trained Large Language Models
## TL;DR
## Summary
- [https://arxiv.org/pdf/2405.00201.pdf](https://arxiv.org/pdf/2405.00201.pdf)

### 섹션 요약

#### 1. Introduction (소개)
Transformer 기반의 대형 언어 모델(PLM)들은 다양한 주요 과제를 해결하는 데 있어 최고의 성능을 나타냈다. 그러나 이 모델들을 특정 작업에 맞게 완전 미세 조정하려면 많은 계산 자원과 메모리가 필요하다. 완전 미세 조정 방법은 연속적인 학습 과정에서 기존에 학습한 내용을 잊어버리는 '카타스트로픽 포게팅' 문제와 초매개변수화 문제로 인해 비효율적이다. 이 논문은 이 문제를 해결하기 위해 특정 레이어별로 전문 지식을 미세 조정하는 'Stratified Progressive Adaptation Fine-tuning(SPAFIT)' 방법을 제안한다.

#### 2. Literature Review (문헌 검토)
기존의 PLM과 미세 조정에 대한 이해가 부족한 상황에서, 효율적인 미세 조정(Parameter Efficient Fine-Tuning, PEFT) 방법들이 연구되고 있다. 주요 PEFT 방법으로 Adapter 기반 방법과 LoRA(저랭크 적응)가 있으며, 이 둘은 미세 조정 시 일부 파라미터만 수정하게 한다. BitFit는 바이어스 항만 조정하는 방법이다.

#### 3. SPAFIT
기존의 PEFT 방법들이 모든 레이어에 동일한 방식으로 적용되는 반면, SPAFIT는 각 레이어에 적절한 미세 조정 강도를 적용한다. 초기 레이어는 동결하고, 중간 레이어는 바이어스 항만 조정하며, 후반 레이어는 LoRA와 BitFit 방법을 사용하여 복잡한 조정이 이루어진다. 이를 통해 PLM의 파라미터 수를 줄이면서도 성능을 유지할 수 있다.

#### 4. 실험 및 결과
BERT-large-cased 모델을 사용하여 GLUE 벤치마크에서 다양한 PEFT 방법과 SPAFIT의 성능을 비교한 결과, 대부분의 과제에서 SPAFIT가 더 적은 파라미터로 유사하거나 더 나은 성능을 보여줬다. 특히, MRPC, STS-B, QQP와 같은 문장 유사성 과제에서 뛰어난 성능을 나타냈다.

#### 5. 토의
대부분의 PEFT 방법들이 완전 미세 조정과 비슷한 성능을 가지지만, 추가 파라미터 조정이 성능 향상에 기여하지 않는다는 사실을 확인했다. 특히, 특정 과제에서는 일부 레이어의 바이어스 항만 조정하는 것이 전체 파라미터를 조정하는 것보다 더 효과적이다.

#### 6. 미래 연구 방향
SPAFIT의 성능을 다른 복잡한 downstream 작업, 예를 들어 요약 작업에서 평가하는 것이 앞으로의 연구 방향이다. 또한, 인코더와 디코더를 모두 포함한 모델에서도 SPAFIT 방법의 성능을 연구할 예정이다.

### 전체 요약
논문은 대형 언어 모델의 효율적인 미세 조정을 위해 Stratified Progressive Adaptation Fine-tuning(SPAFIT) 방법을 제안한다. 초기 레이어는 동결하고, 중간 레이어는 바이어스 항만 조정하며, 후반 레이어는 LoRA와 BitFit 방법을 적용하여 파라미터 수를 줄이면서 성능을 개선한다. 실험 결과, 대부분의 과제에서 SPAFIT는 다른 PEFT 방법보다 적은 파라미터로 유사하거나 더 나은 성능을 보였다. 앞으로 SPAFIT를 복잡한 downstream 작업에서 평가하고, 다양한 모델 구조에 적용하는 연구가 필요하다.



## Similar Papers
- [PiSSA: Principal Singular Values and Singular Vectors Adaptation of Large Language Models](2404.02948.md)
- [VeRA: Vector-based Random Matrix Adaptation](2310.11454.md)
- [Pushing Mixture of Experts to the Limit: Extremely Parameter Efficient MoE for Instruction Tuning](2309.05444.md)
- [TIES-Merging: Resolving Interference When Merging Models](2306.01708.md)
- [Transfer Learning for Structured Pruning under Limited Task Data](2311.06382.md)
- [LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models](2310.08659.md)
- [Tied-Lora: Enhancing parameter efficiency of LoRA with weight tying](2311.09578.md)
- [Mamba: Linear-Time Sequence Modeling with Selective State Spaces](2312.00752.md)
- [Megalodon: Efficient LLM Pretraining and Inference with Unlimited Context Length](2404.08801.md)
