# Face Adapter for Pre-Trained Diffusion Models with Fine-Grained ID and Attribute Control
## TL;DR
## Summary
- [https://arxiv.org/pdf/2405.12970.pdf](https://arxiv.org/pdf/2405.12970.pdf)

### 섹션 요약

#### 1. 서론
이 논문은 Face-Adapter라는 새로운 어댑터를 소개하며, 이는 사전 훈련된 확산 모델을 활용하여 고정밀, 고품질의 얼굴 편집을 수행합니다. 기존 얼굴 재현 및 교체 기술은 주로 GAN 기반이지만, Face-Adapter는 더 나은 생성 능력을 가진 확산 모델을 활용합니다. 주요 구성 요소로는 1) 정밀한 랜드마크와 배경을 제공하는 공간 조건 생성기, 2) 얼굴 임베딩을 텍스트 공간으로 전송하는 플러그 앤 플레이 아이덴티티 인코더, 3) 공간 조건과 세부 속성을 통합하는 속성 제어기가 있습니다. 이 시스템은 다양한 StableDiffusion 모델과 원활하게 통합됩니다.

#### 2. 관련 연구
얼굴 재현은 한 얼굴의 움직임을 다른 얼굴에 전이시키는 기술로, 워핑 기반과 3DMM 기반 방법으로 나눌 수 있습니다. 워핑 기반 방법은 큰 움직임 변화를 다루기 어려워 흐릿한 결과를 초래할 수 있으며, 3DMM 기반 방법은 더 견고한 생성 결과를 제공하지만 얼굴의 세부 요소를 제공하지 못합니다. 최근 확산 모델을 이용한 방법들이 제안되었지만, 여전히 학습 비용이 높고 만족스러운 성능을 제공하지 못합니다.

#### 3. 방법론
Face-Adapter는 세 가지 주요 구성 요소로 이루어져 있습니다:
1. **공간 조건 생성기 (SCG)**: 자동으로 3D 랜드마크와 변하는 전경 영역의 마스크를 예측하여, 보다 정확한 생성 가이드를 제공합니다.
2. **아이덴티티 인코더 (IE)**: 사전 훈련된 인식 모델을 사용하여 얼굴 임베딩을 추출하고, 이를 변환기 디코더의 학습 가능한 쿼리를 통해 텍스트 공간으로 전송합니다.
3. **속성 제어기 (AC)**: 공간 제어는 대상의 움직임 랜드마크와 변하지 않는 배경을 결합하고, 속성 템플릿은 조명, 배경 일부, 머리카락 등의 부족한 속성을 보충합니다.

#### 4. 실험
VoxCeleb1 및 VoxCeleb2 데이터셋을 활용하여 Face-Adapter의 성능을 평가했습니다. 평가 메트릭으로는 PSNR, LPIPS, FID, 코사인 유사도, 포즈, 표현, 시선 오차 등이 사용되었습니다. Face-Adapter는 훈련 중 목표 배경 영역을 포함하여 배경 움직임의 간섭을 피하고, 추론 중 소스 배경을 공간 조건에 추가하여 배경 일관성을 향상시켰습니다.

#### 5. 결론
Face-Adapter는 사전 훈련된 확산 모델을 활용한 얼굴 재현 및 교체 작업에서 우수한 성능을 보이며, 이전의 최첨단 GAN 및 확산 기반 방법들을 능가합니다. 공간 조건 생성기, 아이덴티티 인코더, 속성 제어기를 통해 충분한 공간 가이던스, 아이덴티티, 필수 속성을 갖춘 조건부 인페인팅으로 재현 및 교체 작업을 수행합니다. 그러나 비디오 얼굴 재현/교체에서는 시간적 안정성을 제공하지 못하며, 추가적인 시간적 미세 조정이 필요합니다.

### 전체 요약
Face-Adapter는 사전 훈련된 확산 모델을 이용해 얼굴 재현 및 교체 작업에서 고정밀, 고품질의 결과를 제공하는 새로운 어댑터입니다. 공간 조건 생성기, 아이덴티티 인코더, 속성 제어기를 통해 얼굴 재현 및 교체 작업을 조건부 인페인팅으로 수행하며, 높은 성능과 효율성을 보입니다. 이 시스템은 다양한 StableDiffusion 모델과의 통합을 통해 모델 학습 비용을 줄이고, 과적합을 방지하면서도 고품질의 결과를 제공합니다. 향후 연구에서는 비디오 얼굴 재현/교체에서의 시간적 안정성을 개선할 필요가 있습니다.