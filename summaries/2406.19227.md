# Aligning Teacher with Student Preferences for Tailored Training Data Generation
## TL;DR
## Summary
- [https://arxiv.org/pdf/2406.19227.pdf](https://arxiv.org/pdf/2406.19227.pdf)

### 논문 요약

#### Introduction
최근 대형 언어 모델(LLMs)은 다양한 작업에서 대단한 가능성을 보여주고 있습니다. 하지만 일부 응용 프로그램에서는 프라이버시 민감 데이터나 낮은 지연 시간이 필요한 경우가 있어, 이러한 모델을 엣지 디바이스에서 로컬로 배포해야 할 필요성이 있습니다. 이런 상황에서는 능력이 제한된 엘지 디바이스에서 대규모 모델을 직접 사용할 수 없기 때문에, 대형 모델에서 경량 모델로 지식을 증류하는 방법이 필요합니다.

#### ARTE Framework
이 논문에서는 학생 모델의 선호도에 맞춰 교사 모델을 정렬하여 지식을 증류하는 ARTE(Aligning Teacher with Student Preferences)라 불리는 새로운 프레임워크를 제안합니다. 주요 단계는 다음과 같습니다:

1. **지식 도출 (Knowledge Elicitation)**: 기초 질문을 사용하여 교사 모델로부터 초안 질문 및 이유를 생성합니다.
2. **선호도 수집 (Preference Collection)**: 학생 모델의 성능을 바탕으로 질문 및 이유에 대한 선호도를 수집합니다.
3. **선호도 정렬 (Preference Alignment)**: Direct Preference Optimization (DPO)를 통해 교사 모델의 지식을 학생 모델의 선호도에 맞게 정렬합니다.

실험을 통해 이 방법이 기존의 다양한 벤치마크에서 우수한 성능을 발휘함을 보였습니다.

#### 지식 도출 (Knowledge Elicitation) 섹션 요약
먼저 교사 모델에 시드 질문을 사용하여 초안 예제를 생성하는 작업이 진행됩니다. 이러한 초안 질문과 이유는 학생 모델의 성능을 나타내는 지표로 선호도가 매겨집니다. 이후 Direct Preference Optimization (DPO)을 사용하여 교사 모델을 학생 모델의 선호도에 맞추어 정렬합니다.

#### 선호도 수집 (Preference Collection) 섹션 요약
학생 모델의 성능을 바탕으로 초안 질문과 이유에 대한 선호도를 수집합니다. 이를 통해 학생 모델이 어떤 유형의 질문과 이유를 선호하는지 파악할 수 있습니다. 긴 이유는 학생 모델이 제대로 이해하지 못할 수 있으므로, 적절한 길이의 이유가 더욱 효과적입니다.

#### 선호도 정렬 (Preference Alignment)
DPO를 사용하여 교사 모델을 학생 모델의 선호도에 맞추어 정렬합니다. 이는 교사 모델이 학생 모델의 능력과 선호도에 맞는 훈련 데이터를 생성하도록 합니다.

#### Generalization
ARTE는 다양한 태스크와 학생 모델에서 일반화 성능이 뛰어남을 보여주었습니다. 새로운 태스크와 모델에서의 실험 결과는, 정렬된 교사 모델이 원본 교사 모델보다 뛰어난 성능을 보임을 보여줍니다.

### 논문의 주요 기여 및 혁신 부분
이 논문은 학생 모델의 선호도를 반영하여 교사 모델을 정렬하여 지식을 증류하는 새로운 방법을 제시합니다. 이는 전통적인 데이터 생성 방법과는 달리, 학생 모델의 학습 능력을 최대화하는 데 중점을 둡니다. 또한, 다양한 실험을 통해 제안된 방법이 기존 방법보다 우수한 성능을 발휘함을 입증했습니다.

### 전체 요약
이 논문은 학생 모델의 선호도에 맞춰 교사 모델을 정렬하여 지식을 증류하는 새로운 방법인 ARTE를 제안합니다. 세 가지 주요 단계로 구성된 이 프레임워크는 학생 모델의 학습 능력을 최대화하는 데 중점을 두고 있습니다. 제안된 방법의 우수성은 다양한 벤치마크와 실험을 통해 입증되었습니다. ARTE는 다른 태스크와 학생 모델에도 일반화할 수 있는 강력한 도구임을 보여주었습니다.