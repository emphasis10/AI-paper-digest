# Thinking Preference Optimization
## TL;DR
## Summary
- [https://arxiv.org/pdf/2502.13173.pdf](https://arxiv.org/pdf/2502.13173.pdf)

(1) 각 섹션 요약 및 논문의 주요 기여와 혁신성:

- **서론**: 해당 논문은 대형 언어 모델의 추론 능력을 향상시키고자 하는 연구로, 다양한 오픈 소스 모델을 분석합니다. 특히, 기존의 장 단위 추론 데이터셋의 효과적인 활용을 통해 성능을 향상시키고자 합니다.

- **Thinking Preference Optimization**: 이 섹션에서는 ThinkPO(Thinking Preference Optimization)라는 새로운 방법론을 제안합니다. 이 방법은 기존 장 단위 추론 데이터를 최대한 활용하여 모델의 추론 성능을 강화하도록 설계되었습니다. 데이터의 효율적인 사용을 위해 짧은 추론과 긴 추론을 각각 거부된 샘플과 선택된 샘플로 사용하며, 이는 모델이 더 상세한 응답을 생성하도록 유도합니다.

- **훈련 파이프라인**: 모델은 초기에는 Bespoke-Strato-데이터셋을 사용하여 기초적인 모델을 훈련하고, 이후 ThinkPO를 적용하여 성능을 지속적으로 향상시킵니다. 특히, MATH500과 같은 수학적 추론 능력을 측정하는 데이터셋에서 성능 향상을 이끌어 냅니다.

- **결과 분석**: 실험 결과, ThinkPO가 기존의 SFT(Supervised Fine-Tuning) 상대적으로 8.6%의 정확도 향상 및 25.9%의 출력 길이 증가를 보여주는 등 놀라운 성능 향상을 이끌어 냈습니다. 이는 ThinkPO가 기존의 성능 병목 문제를 해결하는데 효과적이라는 것을 입증합니다.

(2) 전체 요약:

이 논문은 대형 언어 모델의 추론 능력을 최대한 끌어올리기 위해 ThinkPO라는 혁신적인 방법을 제안합니다. 이는 주어진 데이터로부터 최대한의 정보를 이끌어내기 위해 짧은 추론 응답을 거부되고 긴 응답을 선택된 샘플로 사용하여 모델이 더 긴 추론을 하도록 유도합니다. ThinkPO는 SFT와 비교하여 상당한 성능 향상을 보이며, 이는 복잡한 수학적 추론 과제에서도 활용할 수 있습니다. ThinkPO의 도입은 고품질 장 단위 데이터를 추가 수집하지 않고도 기존 데이터의 효용성을 극대화할 수 있는 경량 솔루션을 제공합니다.