# SlimGPT: Layer-wise Structured Pruning for Large Language Models
## TL;DR
## Summary
- [https://arxiv.org/pdf/2412.18110.pdf](https://arxiv.org/pdf/2412.18110.pdf)

1. 각 섹션의 요약

- **서론**: 대규모 언어 모델(LLM)은 다양한 자연어 처리 작업에서 주목할 만한 진전을 이루었으나, 방대한 매개변수 규모로 인해 실행 및 배포 시 높은 비용이 발생합니다. 시중의 LLM의 성능 유지를 위한 구조적 가지치기를 통해 효율성을 개선하고자 합니다.

- **주요 기여**: SlimGPT는 대규모 언어 모델의 구조적 가지치기를 위한 레이어별 접근 방식을 제안합니다. 이는 최소한의 비용과 자원으로 모델을 가볍게 압축하고 성능을 최대한 보존할 수 있는 방법입니다. 다양한 Transformer 기반 모델에 보편적으로 적용 가능하며, 기존 최첨단 방법보다 성능이 뛰어납니다.

- **혁신적 기법**: SlimGPT는 최적의 머리를 선택하는 배치된 탐욕적 가지치기 방식을 도입하여 다양한 구조적 가지치기를 수행합니다. 그룹화 된 콜레스키 분해를 사용하여 레이어별로 최적의 머리를 선택함으로써 성능 손실을 최소화합니다. 이는 또한 증가하는 가지치기 비율을 사용하여 레이어별 에러 누적 문제를 해결합니다.

- **결과 및 분석**: SlimGPT는 LLaMA 모델을 통해 실험하여, 이전 방법들보다 높은 성능을 보임을 확인했습니다. 또한, 이를 통해 모델의 압축 효율성을 극대화할 수 있으며, 메모리 사용량과 추론 속도를 크게 줄일 수 있습니다.

2. 전체 요약

이 논문은 대규모 언어 모델에서 구조적 가지치기를 통한 압축 방법론인 SlimGPT를 제안합니다. SlimGPT는 최적의 성능 손실 대비 최소한의 자원으로 모델을 압축할 수 있는 효과적인 방법을 제시합니다. 성능 손실을 줄이기 위해 오차 누적 문제를 해결하는 레이어별 접근 방식과 비선형 가지치기 비율을 채택하였습니다. 이러한 기법은 다양한 Transformer 기반 모델에 적용 가능하여 기존의 방법들보다 뛰어난 성능을 발휘했습니다.