# Show Less, Instruct More: Enriching Prompts with Definitions and Guidelines for Zero-Shot NER
## TL;DR
## Summary
- [https://arxiv.org/pdf/2407.01272.pdf](https://arxiv.org/pdf/2407.01272.pdf)

### 1. 섹션 요약 (Korean)

#### 1. 소개
논문에서는 Named Entity Recognition (NER)이 중요한 문제로 제시됩니다. 전통적인 NER 모델들은 특정 도메인에 특화되어 일반화 능력이 떨어지는 반면, 대형 언어 모델(LLMs)은 잘 설계된 프롬프트를 통해 제로샷 능력을 보여줍니다. 그러나, 기존의 LLMs는 테스트 세트와 많은 중첩을 가지는 엔터티 클래스들을 광범위하게 학습하는 것을 목표로 합니다.

#### 2. 관련 연구
일반적으로 NER 문제는 "시퀀스 레이블링 작업"으로 처리됩니다. 기계학습을 사용하는 방법은 시퀀스의 각 요소에 BIO 레이블을 할당합니다. BERT 모델을 미세조정한 NER 모델들은 특정 도메인에 특화되어 있지만 일반화 측면에서 한계가 있습니다.

#### 2.1 문맥 학습(In-Context Learning)
Radford 등의 연구에서 LLMs는 다중 작업 학습 능력을 처음으로 탐구했습니다. Brown 등의 연구에서는 다중 작업 능력을 강화하여 제로샷 및 퓨샷 학습을 가능하게 했습니다.

#### 2.2 제로샷 NER를 위한 미세조정
프롬프트 엔지니어링 외에도, 여러 연구에서는 작은 모델을 특정 작업에 맞추어 인스트럭션으로 튜닝하는 방법을 탐구했습니다. 대표적인 작업으로는 InstructUIE, UniNER, GoLLIE, GNER가 있습니다.

#### 3. SLIMER
SLIMER는 적은 예제와 프롬프트 내 정의와 지침을 활용하여 보지 못한 엔터티 태그를 처리하는 접근법입니다. 이 방법은 빠르고 안정적인 학습을 가능하게 하며, 특히 보지 못한 엔터티들을 이름표시하는데 효과적입니다. 주요 아이디어는 더 적은 학습 데이터와 지침이 포함된 프롬프트를 사용하는 것입니다.

##### 3.1 Show Less
기존 모델들은 광범위한 엔터티 태그와 예제로 학습되며, 이는 제로샷 성능을 강화하지만 일반화 능력에 대한 영향은 불분명합니다. SLIMER는 이러한 모델들보다 적은 데이터로 학습됩니다.

##### 3.2 Instruct More
우리는 데이터 양을 줄이는 대신 정의와 지침을 포함한 프롬프트를 사용합니다. 정의는 각 엔터티 태그를 간결하게 설명하며, 지침은 특정 엔터티를 태그하는 방법을 안내합니다.

#### 4. 실험
SLIMER는 몇 가지 표준 NER 벤치마크인 MIT와 CrossNER에서 테스트되었으며, BUSTER 데이터셋에서 전혀 보지 못한 엔터티에 대한 성능을 평가했습니다.

#### 5. 결론
SLIMER는 보지 못한 엔터티 태그를 더 잘 처리할 수 있도록 설계된 인스트럭션 튜닝된 LLM입니다. 지침이 포함된 프롬프트와 제한된 세트의 엔터티 태그로 미세조정되어, 이는 예측 정확성과 안정성을 높이는 데 효과적입니다. 추가적으로, 적은 데이터 세트로도 유사한 성능을 유지합니다.

### 2. 전체 요약 (Korean)

이 논문은 Named Entity Recognition (NER)에 초점을 맞춘 연구로, 기존의 대형 언어 모델(LLMs)이 특정 엔터티 클래스들을 학습하여 제로샷 성능을 발휘하는 방법을 개선하고자 합니다. SLIMER는 적은 예제와 정의 및 지침을 포함한 프롬프트를 사용하여 보지 못한 엔터티 태그를 효과적으로 처리하는 접근법입니다. 이 방법은 빠르고 안정적인 학습을 가능하게 하며, 특히 보지 못한 엔터티들을 정확하게 이름표시하는 데 효과적입니다. 전체적으로, SLIMER는 기존 모델들과 비교해서 적은 데이터로도 뛰어난 성능을 발휘함을 실험을 통해 입증하였습니다. SLIMER의 주요 기여는 적은 데이터와 지침이 포함된 프롬프트를 사용하여 모델의 일반화 능력을 크게 향상시킨 점입니다.

## Similar Papers
- [GLiNER multi-task: Generalist Lightweight Model for Various Information Extraction Tasks](2406.12925.md)
- [SpeechVerse: A Large-scale Generalizable Audio Language Model](2405.08295.md)
- [AutoCrawler: A Progressive Understanding Web Agent for Web Crawler Generation](2404.12753.md)
- [Large Language Models as Generalizable Policies for Embodied Tasks](2310.17722.md)
- [MMM: Multilingual Mutual Reinforcement Effect Mix Datasets & Test with Open-domain Information Extraction Large Language Models](2407.10953.md)
- [rLLM: Relational Table Learning with LLMs](2407.20157.md)
- [CodecLM: Aligning Language Models with Tailored Synthetic Data](2404.05875.md)
- [VCR: Visual Caption Restoration](2406.06462.md)
- [RE-AdaptIR: Improving Information Retrieval through Reverse Engineered Adaptation](2406.14764.md)
