# SWE-RL: Advancing LLM Reasoning via Reinforcement Learning on Open Software Evolution
## TL;DR
## Summary
- [https://arxiv.org/pdf/2502.18449.pdf](https://arxiv.org/pdf/2502.18449.pdf)

1. 본 논문의 각 섹션 요약:

- 서론: 대형 언어 모델(LLM)이 소프트웨어 엔지니어링(SE) 작업에서 점점 더 많이 사용되고 있으며, SWE-bench 같은 실무적인 문제 해결을 위한 새로운 기법이 필요하다는 배경을 설명합니다.

- 제안 방법: SWE-RL이라는 새로운 강화 학습(RL) 방법을 소개하여, 소프트웨어 개발 데이터와 규칙에 기반한 보상을 사용하여 LLM의 성능을 향상시킵니다. 이 방법은 GitHub 풀 리퀘스트 데이터를 구축하고 이를 강화 학습의 시작 데이터로 변환하여 문제를 해결할 수 있는 패치를 학습합니다.

- 평가 및 결과: Llama3-SWE-RL-70B 모델을 적용하여 SWE-bench Verified에서 41.0%의 문제 해결율을 달성했으며, 이는 중형 사이즈의 모형 중 최고 수준의 성능입니다. 이 모델은 소프트웨어 외의 다른 분야에서도 우수한 성과를 보여, 일반화된 추론 능력을 입증합니다.

- 결론: SWE-RL은 LLM의 소프트웨어 엔지니어링 역량을 강화하는 새로운 방향성을 제시하며, 강화 학습을 통해 실질적인 문제 해결 능력을 향상시킬 수 있음을 보여줍니다.

2. 전체 요약:
본 논문은 소프트웨어 엔지니어링 작업을 위한 LLM의 성능을 강화 학습 방법 SWE-RL을 통해 향상시킴으로써, 실제 문제 해결 능력을 크게 증대시켰습니다. 주로 GitHub 풀 리퀘스트 데이터를 사용하여 모델 학습을 강화하고, 문제 해결 능력을 검증하기 위한 SWE-bench에서 뛰어난 성과를 거둔 이 논문은, 소프트웨어 외에도 다양한 과제에서 일반화된 추론 능력을 획득하도록 하였습니다. 이로써 LLM의 실용성을 높이는 데 중요한 기여를 하였습니다.