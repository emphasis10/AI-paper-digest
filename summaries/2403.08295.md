# Gemma: Open Models Based on Gemini Research and Technology
## TL;DR
## Summary
- [https://arxiv.org/pdf/2403.08295.pdf](https://arxiv.org/pdf/2403.08295.pdf)

이 문서는 Google DeepMind의 Gemma 팀이 작성한 "Gemma: Gemini 연구 및 기술을 기반으로 한 공개 모델"에 대한 기술 보고서입니다. Gemma 모델은 Gemini 모델을 기반으로 하여 언어 이해, 추론, 안전성 등에서 우수한 성능을 보이는 경량화된 최첨단 오픈 모델의 가족입니다. 모델은 2개의 크기(20억 및 70억 매개변수)로 제공되며, 미세 조정된 체크포인트와 사전 훈련된 체크포인트 모두를 제공합니다.

### 도입

Gemma 모델은 최대 6조 토큰의 텍스트를 사용하여 훈련되었으며, 이는 Gemini 모델과 유사한 아키텍처, 데이터, 훈련 레시피를 사용했습니다. 이 모델들은 텍스트 분야에서 강력한 일반적 능력과 대규모에서의 이해 및 추론 기술을 달성합니다. 2억 및 70억 매개변수 모델 모두 다양한 계산 제약 조건과 개발자 요구 사항을 충족하도록 설계되었습니다.

### 모델 아키텍처

Gemma의 모델 아키텍처는 트랜스포머 디코더에 기반을 두고 있으며, 몇 가지 주요 매개변수는 다음과 같습니다: 모델 크기, 레이어 수, 피드포워드 숨겨진 차원 수, 헤드 수 등입니다. 이 모델들은 8192 토큰의 컨텍스트 길이로 훈련됩니다. 또한, 절대적 위치 임베딩 대신 회전 위치 임베딩을 사용하고, 모델 크기를 줄이기 위해 입력과 출력에서 임베딩을 공유합니다.

### 훈련 인프라 및 데이터

Gemma 모델은 TPUv5e를 사용하여 훈련되었으며, 7B 모델의 경우 16개의 포드에서 모델을 훈련시켜 총 4096개의 TPUv5e를 사용합니다. 2B 모델은 2개의 포드에서 총 512개의 TPUv5e로 훈련됩니다. 훈련 데이터는 주로 영어 웹 문서, 수학, 코드로 구성된 2조 및 6조 토큰으로 구성됩니다.

### 평가

Gemma 모델은 다양한 도메인에서 자동 벤치마크 및 인간 평가를 통해 평가되었습니다. 특히, 7B 모델은 동일하거나 더 작은 규모의 모든 오픈 소스 대안보다 뛰어난 성능을 보였으며, 몇 가지 더 큰 모델들도 능가했습니다. 인간 평가에서는 Gemma 7B IT가 Mistral v0.2 7B Instruct 모델에 대해 51.7%의 긍정적인 승률을, Gemma 2B IT는 41.6%의 승률을 기록했습니다.

### 결론 및 논의

Gemma 모델은 기존의 유사한 규모의 오픈 모델보다 뛰어난 성능을 보여줌으로써, 언어 이해, 추론, 안전성 등에서 새로운 기준을 설정했습니다. 이러한 모델들의 공개는 다음 세대의 LLM 혁신을 가능하게 하고, 모델의 안전성 개선에 기여할 것으로 기대됩니다.

## Similar Papers
- [ShieldGemma: Generative AI Content Moderation Based on Gemma](2407.21772.md)
- [RecurrentGemma: Moving Past Transformers for Efficient Open Language Models](2404.07839.md)
- [HelpSteer2: Open-source dataset for training top-performing reward models](2406.08673.md)
- [The Art of Saying No: Contextual Noncompliance in Language Models](2407.12043.md)
- [Gemma 2: Improving Open Language Models at a Practical Size](2408.00118.md)
- [ChuXin: 1.6B Technical Report](2405.04828.md)
- [Phi-3 Safety Post-Training: Aligning Language Models with a "Break-Fix" Cycle](2407.13833.md)
- [The Impact of Hyperparameters on Large Language Model Inference Performance: An Evaluation of vLLM and HuggingFace Pipelines](2408.01050.md)
- [ClashEval: Quantifying the tug-of-war between an LLM's internal prior and external evidence](2404.10198.md)
