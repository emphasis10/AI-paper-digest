# Selective Aggregation for Low-Rank Adaptation in Federated Learning
## TL;DR
## Summary
- [https://arxiv.org/pdf/2410.01463.pdf](https://arxiv.org/pdf/2410.01463.pdf)

[중요한 내용 요약]

1. **도입부**:
   - 이 논문은 AI 및 머신러닝에서 대규모 언어 모델(PLM)의 중요성과 이를 특정 작업에 적응시키기 위한 연합 학습(FL) 기법을 탐구합니다. 개인 정보 보호와 데이터 규제로 인해 분산된 데이터를 활용하는 FL이 주목받고 있습니다.

2. **연합 학습의 LoRA**:
   - 논문은 LoRA(저차 적응) 방식을 사용하여 대규모 모델의 효율적인 학습을 제안합니다. A 행렬은 일반 지식을, B 행렬은 클라이언트 특유의 지식을 학습하도록 설계되었습니다. 이러한 방식으로 데이터 공유 없이도 최적의 모델 업데이트를 이룰 수 있습니다.

3. **동기 부여 예제 및 방법**:
   - FedSA-LoRA 방식을 도입하여 A 행렬만 서버와 공유하고, B 행렬은 로컬에서 유지하여 클라이언트 특유의 지식 학습을 극대화합니다. 이는 LoRA의 학습 능력을 강화하고 데이터 이질성 하에서도 성능을 향상시킬 수 있음을 보여줍니다.

4. **수렴 분석**:
   - 제시된 방식은 기존 FedAvg 방식과 유사한 수렴 속도를 보이며, 이는 비평형(non-convex) 상황에서도 효과적입니다.

5. **실험 및 분석**:
   - 다양한 자연어 이해와 생성 과제에 대해 FedSA-LoRA 방식이 다른 방법들보다 우수한 성능을 보입니다. 특히, 데이터의 이질성 및 클라이언트 수의 변화에 따른 FedSA-LoRA의 변화를 분석하여 이 방법의 유연성과 강력함을 입증합니다.

[전체 요약]
이 논문은 대규모 언어 모델을 연합 학습 방식으로 최적화하는 방법을 제안합니다. A와 B 행렬을 사용하여 일반 지식과 클라이언트 특유 지식을 각각 학습하는 기법을 통해 데이터 공유 없이도 효율적인 연합 학습이 가능함을 보여줍니다. 제시된 FedSA-LoRA 방법은 데이터 이질성 하에서 특히 강력하며, 다양한 자연어 처리 과제에서 우수한 성능을 입증했습니다. 이러한 연구는 AI와 머신러닝의 발전에 크게 기여할 것으로 기대됩니다.