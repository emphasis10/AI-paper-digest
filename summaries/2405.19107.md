# Offline Regularised Reinforcement Learning for Large Language Models Alignment
## TL;DR
## Summary
- [https://arxiv.org/pdf/2405.19107.pdf](https://arxiv.org/pdf/2405.19107.pdf)

#### 1. 개요
이 논문에서는 대규모 언어 모델(LLM)의 정렬을 위한 새로운 접근 방식인 DRO(Direct Reward Optimisation)을 제안합니다. 기존의 인간 피드백을 통한 강화 학습(RLHF) 방법과 달리, DRO는 선호 데이터 쌍(pairwise preference data) 대신 단일 경로 데이터(single-trajectory data)를 사용합니다. 이 접근 방식은 데이터 수집의 비용을 절감하고, 더 많은 사용자 피드백을 활용할 수 있도록 합니다.

#### 2. 서론
AI 에이전트를 인간의 선호에 맞추는 것은 품질, 유용성 및 안전성을 향상시키는 데 매우 중요합니다. 현재 가장 널리 사용되는 방법론은 RLHF로, 인간 피드백을 통해 에이전트를 미세 조정합니다. 그러나, 이러한 방법은 쌍별 선호 데이터 수집에 높은 비용이 들며, 데이터가 부족한 경우가 많습니다.

#### 3. 배경
기존의 RLHF 방법론은 주로 쌍별 선호 데이터셋을 사용합니다. 이 데이터셋은 각 프롬프트와 두 개의 독립적인 응답, 그리고 그 사이의 인간 선호를 포함합니다. DRO는 단일 경로 데이터, 즉 프롬프트와 이에 대한 응답, 그리고 이에 대한 피드백을 사용하는 방법을 제안합니다.

#### 4. Direct Reward Optimisation (DRO)
DRO는 쌍별 선호 데이터를 필요로 하지 않고, 단일 경로 데이터만을 사용합니다. DRO는 KL 정규화된 정책 최적화 문제를 해결하기 위해 설계되었습니다. 이 접근 방식은 정책과 가치 함수를 함께 학습하여 최적의 정책을 도출합니다. DRO의 주요 목적은 정책과 가치 함수를 사용하여 단일 경로 데이터에서 직접 보상을 최적화하는 것입니다.

#### 5. 실험 및 결과
DRO는 T5 인코더-디코더 언어 모델을 사용하여 실험적으로 검증되었습니다. 실험 결과, DRO는 기존의 KTO(Kahneman-Tversky Optimization)와 같은 방법보다 우수한 성능을 보였습니다. UltraFeedback 데이터셋을 사용한 실험에서 DRO는 더 나은 정책 최적화 결과를 보여주었습니다.

#### 6. 결론 및 한계
이 논문에서는 DRO를 통해 대규모 언어 모델의 정렬을 위한 새로운 방법을 제시하였습니다. DRO는 쌍별 선호 데이터 수집의 어려움을 해결하고, 더 많은 사용자 피드백을 활용할 수 있게 합니다. 그러나, 본 연구는 제한된 실험과 규모로 진행되었으며, 더 큰 언어 모델에 대한 추가 연구가 필요합니다.

### 전체 요약
이 논문은 기존의 인간 피드백을 통한 강화 학습 방법론의 한계를 극복하고자 하는 새로운 접근 방식인 DRO를 제안합니다. DRO는 단일 경로 데이터만을 사용하여 대규모 언어 모델을 최적화하며, 실험 결과 기존 방법보다 우수한 성능을 보였습니다. 이 방법은 데이터 수집 비용을 줄이고 더 많은 사용자 피드백을 활용할 수 있는 가능성을 제시합니다. 그러나, 본 연구의 실험은 제한적이었으며, 더 큰 언어 모델에 대한 추가 연구가 필요합니다.