# LLM In-Context Recall is Prompt Dependent
## TL;DR
## Summary
- [https://arxiv.org/pdf/2404.08865.pdf](https://arxiv.org/pdf/2404.08865.pdf)

#### 1. 서론
이 연구는 대형 언어 모델(Large Language Models, LLMs)의 컨텍스트 회상 능력이 프롬프트에 의존적임을 탐구합니다. 특히, 'needle-in-a-haystack' 방법을 사용하여 다양한 LLM의 정보 검색 능력을 평가하고, 모델 아키텍처, 훈련 전략, 미세 조정이 회상 성능에 미치는 영향을 분석합니다.

#### 2. 관련 연구
다양한 LLM의 능력을 평가하는 것은 사용할 모델을 선택하는 데 필수적입니다. 최근에는 벤치마크 리더보드, 평가 소프트웨어, 새로운 평가 방법론 등 여러 도구와 접근 방식이 소개되었습니다.

#### 3. 방법론
'needle-in-a-haystack' 테스트는 LLM이 프롬프트에 포함된 정보를 얼마나 잘 회상하는지 평가하는 데 사용됩니다. 이 방법은 텍스트 블록(건초더미) 속에 정보(바늘)를 삽입하고 모델에게 이를 찾아내도록 요청합니다. 실험은 다양한 건초더미 길이와 바늘의 위치를 변화시켜 진행됩니다.

#### 4. 논의
연구 결과, LLM의 회상 성능은 프롬프트에 따라 달라집니다. 특히 훈련 데이터와 상충하는 프롬프트가 포함된 경우 성능이 저하될 수 있습니다. 또한, 모델의 아키텍처나 훈련 전략을 조정하거나 미세 조정을 통해 성능을 개선할 수 있음을 발견했습니다.

### 종합적인 요약
이 논문은 LLM의 컨텍스트 내 회상 능력이 프롬프트에 의존적이라는 점을 강조합니다. 이는 LLM의 실제 적용 시 고려해야 할 중요한 요소입니다. 또한, 다양한 테스트를 통해 모델의 회상 능력이 프롬프트의 내용, 모델 아키텍처, 훈련 전략에 따라 달라질 수 있음을 시사합니다. 이 연구는 LLM의 선택과 적용에 있어 신중한 평가가 필요함을 보여주며, 앞으로의 모델 개발에 있어서 회상 능력 향상을 위한 다양한 접근 방법이 모색될 필요가 있음을 제시합니다.