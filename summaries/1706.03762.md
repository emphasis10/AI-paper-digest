# Attention Is All You Need
## TL;DR
## Summary
- [https://arxiv.org/pdf/1706.03762.pdf](https://arxiv.org/pdf/1706.03762.pdf)

### 요약

#### 1. 서론
기존의 시퀀스 변환 모델은 주로 복잡한 순환 신경망(RNN) 또는 합성곱 신경망(CNN)을 기반으로 하며, 인코더와 디코더를 포함합니다. 이 논문에서는 이러한 순환 및 합성곱 구조를 완전히 배제하고, 오로지 주의 메커니즘에만 의존하는 새로운 간단한 네트워크 구조인 트랜스포머(Transformer)를 제안합니다. 이 모델은 더 높은 병렬성을 제공하고, 훈련 시간이 훨씬 적게 소요됩니다.

#### 2. 배경
트랜스포머 모델은 순차적인 계산을 줄이기 위해 설계되었습니다. 기존의 모델들은 순차적인 계산의 제약으로 인해 긴 시퀀스 처리 시 병렬화가 어려웠습니다. 하지만 트랜스포머는 이러한 문제를 해결하고자 하였으며, 여러 작업에서 자가 주의 메커니즘이 성공적으로 적용된 예가 있습니다.

#### 3. 모델 구조
##### 3.1 인코더와 디코더 스택
트랜스포머의 인코더는 6개의 동일한 레이어로 구성되며, 각 레이어는 멀티 헤드 자가 주의 메커니즘과 포지션별 완전 연결 피드포워드 네트워크로 구성됩니다. 디코더 또한 6개의 동일한 레이어로 구성되며, 인코더 출력에 대한 멀티 헤드 주의 메커니즘을 추가로 포함합니다.

##### 3.2 주의 메커니즘
트랜스포머는 스케일드 닷-프로덕트 주의(Scaled Dot-Product Attention)와 멀티 헤드 주의(Multi-Head Attention)를 사용합니다. 스케일드 닷-프로덕트 주의는 쿼리와 키, 밸류 간의 점곱을 계산하고, 멀티 헤드 주의는 이를 병렬로 여러 번 수행하여 다양한 표현 공간에서 정보를 동시에 처리할 수 있게 합니다.

##### 3.3 포지션 인코딩
트랜스포머는 순환 및 합성곱 구조를 사용하지 않기 때문에, 시퀀스의 순서를 모델에 제공하기 위해 포지션 인코딩을 사용합니다. 이를 통해 모델이 시퀀스의 상대적 또는 절대적 위치 정보를 활용할 수 있습니다.

#### 4. 훈련
트랜스포머 모델은 대규모 병렬화를 통해 빠르게 훈련될 수 있습니다. 예를 들어, WMT 2014 영어-독일어 번역 작업에서 트랜스포머는 8개의 P100 GPU를 사용하여 12시간 만에 새로운 최고 성능을 달성했습니다. 또한, WMT 2014 영어-프랑스어 번역 작업에서도 뛰어난 성능을 보였습니다.

#### 5. 결과
트랜스포머 모델은 영어-독일어와 영어-프랑스어 번역 작업에서 기존 최고 성능을 능가하는 BLEU 점수를 기록했습니다. 특히, 반자동 및 전자동 모델을 포함한 모든 이전 모델들을 능가하는 결과를 보여주었습니다.

#### 6. 결론
이 연구에서는 순환 레이어를 완전히 대체하고, 멀티 헤드 자가 주의 메커니즘을 사용하는 첫 번째 시퀀스 변환 모델인 트랜스포머를 제안합니다. 트랜스포머는 번역 작업에서 기존 모델들보다 더 빠르게 훈련될 수 있으며, 더 높은 번역 품질을 제공합니다. 향후 연구에서는 트랜스포머를 텍스트 외의 입력 및 출력 모달리티에도 적용하고, 큰 입력 및 출력을 효율적으로 처리하기 위한 제한된 주의 메커니즘을 탐구할 계획입니다.

### 전체 요약
이 논문은 트랜스포머(Transformer)라는 새로운 네트워크 구조를 제안합니다. 트랜스포머는 순환 신경망(RNN) 및 합성곱 신경망(CNN)을 사용하지 않고, 오로지 주의 메커니즘에만 의존하여 시퀀스 변환 작업을 수행합니다. 이를 통해 더 높은 병렬성을 제공하며, 훈련 시간이 크게 단축됩니다. 실험 결과, 트랜스포머는 영어-독일어 및 영어-프랑스어 번역 작업에서 새로운 최고 성능을 달성했습니다. 이 연구는 트랜스포머 모델이 다양한 작업에 효과적으로 적용될 수 있음을 보여주며, 향후 다양한 모달리티와 입력 및 출력 크기를 효율적으로 처리하기 위한 연구 방향을 제시합니다.

## Similar Papers
- [Transformer++](2003.04974.md)
- [Griffin: Mixing Gated Linear Recurrences with Local Attention for Efficient Language Models](2402.19427.md)
- [Complexity of Symbolic Representation in Working Memory of Transformer Correlates with the Complexity of a Task](2406.14213.md)
- [Depth-Adaptive Transformer](1910.10073.md)
- [Accurate Knowledge Distillation with n-best Reranking](2305.12057.md)
- [Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity](2101.03961.md)
- [Scalable MatMul-free Language Modeling](2406.02528.md)
- [Localizing Paragraph Memorization in Language Models](2403.19851.md)
- [Pointer Networks](1506.03134.md)
