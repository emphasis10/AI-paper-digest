# Mixture-of-Transformers: A Sparse and Scalable Architecture for Multi-Modal Foundation Models
## TL;DR
## Summary
- [https://arxiv.org/pdf/2411.04996.pdf](https://arxiv.org/pdf/2411.04996.pdf)

### 1. 각 섹션 요약 및 주요 기여점

#### 소개
이 논문에서는 대규모 다중 모달 모형의 사전학습에 드는 계산 비용 문제를 해결하기 위해 'Mixture-of-Transformers (MoT)'라는 희소하고 확장 가능한 아키텍처를 제안합니다. MoT는 비임베딩 매개변수를 모달리티 별로 분리하면서, 전체 입력 시퀀스에 대한 글로벌 자기 주의를 유지하여 모달리티 특화 처리를 최적화하고, 기능 간의 상호작용을 보장합니다.

#### 방법론: Mixture-of-Transformers 아키텍처
MoT는 기존 'Mixture of Experts (MoE)'와 다르게 모든 비임베딩 파라미터를 분리함으로써 다중 모달 사전학습에서 일관된 우수한 성능을 보입니다. MoT는 텍스트, 이미지, 음성의 각 모달리티에 맞게 설계된 비임베딩 파라미터 사용으로 다양한 모달리티에 맞는 처리 방식을 제공합니다.

#### 실험 및 결과
다양한 설정에서 MoT는 기존 덴스 모델이나 MoE-4x보다 적은 FLOP 수로 유사하거나 더 나은 성능을 보여주었습니다. MoT는 다중 모달 실험 설정에서 복합적인 학습 목표를 효과적으로 처리할 수 있음을 입증했습니다.

#### 결론
MoT는 다중 모달 LLM(대규모 언어 모델)의 효율성을 높이는 효율적인 프레임워크로 작용할 수 있습니다. MoT는 동시 모달리티 학습에서 필요한 계산 요구를 줄이면서 경쟁력 있는 성과를 유지할 수 있습니다. 또한, MoT와 MoE-4x의 혼합 모델은 성능을 더 향상시켰음을 보여줍니다.

### 2. 전체 요약
이 논문은 'Mixture-of-Transformers (MoT)'라는 새로운 희소 다중 모달 변형기를 소개하며, 모달리티 별 매개변수 분리를 통해 계산 비용을 줄이고 성능은 유지 혹은 향상시키는 기술을 제시합니다. 이것은 다중 모달 입력을 고려하여 모달리티 특화 처리를 유지하면서도 전체 범위에 대한 글로벌 주의를 가능하게 하여 높은 효율성을 자랑합니다. 이로 인해 MoT는 차세대 다중 모달 대규모 언어 모델의 기반이 될 수 있으며, 최소한의 계산으로 다양한 모달리티에서 우수한 성과를 보일 수 있습니다.