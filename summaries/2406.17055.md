# Large Language Models Assume People are More Rational than We Really are
## TL;DR
## Summary
- [https://arxiv.org/pdf/2406.17055.pdf](https://arxiv.org/pdf/2406.17055.pdf)

### 1. 논문의 각 섹션 요약

#### 1.1. 서론 (Introduction)
AI 시스템이 사람들과 효과적으로 소통하기 위해서는 인간의 의사결정 방식을 이해해야 합니다. 그러나 사람의 결정은 항상 합리적인 것은 아닙니다. 연구는 최신의 대형 언어 모델(LLM)이 실제 인간의 행동과 어떻게 다른지를 조사하고, LLM들이 사람들이 합리적으로 행동한다고 가정하는 경향이 있음을 발견했습니다.

#### 1.2. 관련 연구 (Related Work)
이 부분에서는 LLM과 인간 정렬(alignment), 그리고 인지 과학에서의 의사결정 모델에 대한 기존 연구들을 소개합니다. LLM의 정렬은 주로 인간 피드백을 통한 강화 학습(RLHF)으로 이루어지며, 이는 모델의 추론 능력을 향상시켰다고 합니다.

#### 1.3. 순방향 모델링 (Forward Modeling)
심리학의 전통적인 위험 선택 과제에서, LLM들이 인간의 결정을 예측하는 방법을 실험했습니다. LLM들은 사람보다 더 합리적으로 행동하는 경향이 있으며, 이는 사람들이 실제로는 합리적으로 행동하지 않는다는 점에서 차이가 있습니다.

#### 1.4. 역방향 모델링 (Inverse Modeling)
사람의 선택으로부터 선호를 추론하는 실험을 통해, LLM들이 인간의 내부 유틸리티를 추론하는 데 있어 여전히 합리적이라고 가정하는 것을 발견했습니다. 이는 사람들이 다른 사람들의 선택을 해석할 때 합리적으로 행동할 것이라고 가정하는 것과 일치합니다.

#### 1.5. 결과 (Results)
LLM들은 강력한 합리적 모델링 능력을 갖추고 있으며, 이는 모델의 능력이 향상될수록 더욱 뚜렷해집니다. 그러나 LLM의 추론은 사람들과 높은 상관관계를 가지며, 특히 어려운 상황에서 인간과의 일관성이 더 높습니다. 이는 LLM이 인간과 비슷한 휴리스틱 전략을 공유하기 때문일 수 있음을 시사합니다.

#### 1.6. 논의 (Discussion)
LLM은 사람이 더 합리적으로 행동한다고 가정하는 경향이 있으며, 이는 사람들이 실제로는 그렇게 행동하지 않는다는 점에서 문제가 될 수 있습니다. 이는 AI 모델이 인간 기대와 행동에 맞춰져야 한다고 제안하며, 사회과학적 통찰을 통한 정렬 연구의 필요성을 강조합니다.

### 2. 전체 요약
이 논문은 대형 언어 모델(LLM)이 사람의 의사결정을 예측하고 시뮬레이션하는 데 있어 사람들이 실제로 행동하는 방식보다 더 합리적으로 행동한다고 가정하는 경향이 있음을 발견했습니다. 이는 사람들이 다른 사람들을 해석할 때 사용하는 가정과 유사하지만, 실제 인간 행동을 완벽히 모사하는 데는 한계가 있습니다. 연구 결과는 인지 과학의 방법론을 통해 LLM의 정렬을 개선하고, 인간 기대와 실제 행동에 맞춘 다른 정렬 목표를 개발해야 할 필요성을 강조합니다.