# CURLoRA: Stable LLM Continual Fine-Tuning and Catastrophic Forgetting Mitigation
## TL;DR
## Summary
- [https://arxiv.org/pdf/2408.14572.pdf](https://arxiv.org/pdf/2408.14572.pdf)

1. 소장본 주요 내용 요약:

### 1. 서론
- **소개**: 최근 대형 언어 모델(LLMs)은 수많은 과제에서 놀라운 성과를 보여주었으나, 이러한 모델을 특정 과제에 맞게 미세 조정하려면 많은 자원이 필요합니다. 이를 해결하기 위해 매개변수 효율적인 미세 조정(PEFT) 방법이 주목받고 있습니다.
- **LoRA**: 기존 LoRA는 사전 학습된 가중치 행렬을 낮은 계수 행렬로 분해하여 미세 조정을 수행하지만, 새로운 과제에 대해 미세 조정을 할 때 '카타스트로픽 포겟팅' 문제를 여전히 겪습니다.
- **CURLoRA**: 이 논문에서는 "CURLoRA"라는 새로운 접근 방식을 제안합니다. 이는 CUR 행렬 분해를 사용하여 LoRA의 문제를 완화시키고, 매개변수 효율성을 높입니다.

### 2. 관련 연구
- **카타스트로픽 포겟팅**: 기존 연구는 매개변수 중요성을 고려한 Elastic Weight Consolidation, 이전 과제 네트워크를 동결하고 새로운 과제에 대한 연결만 추가하는 Progressive Neural Networks, 이전 과제의 예제를 저장하고 재사용하는 메모리 기반 접근 방식 등으로 문제를 해결하려 했습니다.
- **효율적 미세 조정**: Adapter Layer, Prefix-tuning과 같은 다양한 접근 방식들이 있습니다.
- **CUR 행렬 분해**: 여러 분야에서 효율성과 해석력을 위해 사용된 방법입니다.

### 3. CURLoRA 접근법
- **소개**: CURLoRA는 CUR 행렬 분해를 사용하여 사전 학습된 가중치 행렬을 분해하고 미세 조정하는 U 행렬만 조정합니다.
- **수학적 공식화**: 원래 가중치 행렬 \(W\)를 컬럼과 로우 기반의 확률을 사용해 샘플링한 후, 이를 통해 C, U, R 행렬로 분해하여 조정합니다.
- **적용**: 이 접근법은 모델의 원래 지식 보존을 도와주며, 과제 간 간섭을 줄이고, 매개변수 공간의 여유를 줄이는 등 다양한 이점을 제공합니다.

### 4. 실험 설정
- **데이터셋**: GLUE-MRPC, GLUE-SST-2, Sentiment140, WikiText-2 등 다양한 데이터셋을 사용하여 실험을 진행합니다.
- **모델 및 하이퍼파라미터**: Mistral 7B와 GPT-2 Large 모델을 사용하며, 각 모델에 맞는 하이퍼파라미터를 조정합니다.
- **평가 기준**: 분류 과제에서는 정확도, 언어모델링 능력에서는 Perplexity를 사용합니다.

### 5. 결과 및 논의
- **과제별 성능**: CURLoRA는 다양한 과제에서도 높은 정확도를 유지하며, 카타스트로픽 포겟팅 문제를 효과적으로 완화했습니다.
- **이론적 통찰**: CURLoRA는 모델의 원래 지식을 보존하고, 과제 간 간섭을 줄이는 이점을 제공합니다.
- **제한 사항 및 향후 연구**: 더 큰 모델과 다양한 과제에 대한 검증, 시간 및 공간 복잡도 분석 등의 추가 연구가 필요합니다.

### 6. 결론
- **공헌**: CURLoRA는 CUR 행렬 분해를 사용하여 대형 언어 모델을 효율적으로 미세 조정하는 새로운 접근 방식을 제안합니다. 이를 통해 기존 LoRA보다 더 나은 안정성과 성능을 제공합니다.

---

2. 전체 요약:
- 이 논문은 새로운 접근 방식인 CURLoRA를 제안하여 대형 언어 모델의 미세 조정을 효율적으로 수행합니다. CURLoRA는 CUR 행렬 분해를 통해 기존 LoRA가 겪는 '카타스트로픽 포겟팅' 문제를 완화시키며, 실험을 통해 다양한 과제에서도 높은 성능과 안정성을 유지함을 입증했습니다. 이를 통해 모델의 원래 지식을 보존하고, 적은 자원으로도 높은 성과를 낼 수 있는 가능성을 보여주었습니다.

---

이 요약은 AI와 머신러닝 분야의 발전에 기여할 수 있을 것입니다. 도움이 되셨기를 바랍니다!