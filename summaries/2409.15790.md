# Small Language Models: Survey, Measurements, and Insights
## TL;DR
## Summary
- [https://arxiv.org/pdf/2409.15790.pdf](https://arxiv.org/pdf/2409.15790.pdf)

### 1. 각 섹션 요약 및 주요 기여 요약

#### 소개 (Introduction)
이 논문은 소형 언어 모델(SLM, Small Language Models)의 중요성과 연구 현황을 소개합니다. SLM은 자원 효율적인 배포를 위해 설계되어 아이폰, 안드로이드 등 스마트 기기에서 포괄적으로 사용되고 있으며, 큰 언어 모델(LLM)에 비해 학문적인 주목을 덜 받고 있습니다. 이 논문의 주요 기여는 다양한 SLM 아키텍처, 데이터셋, 학습 알고리즘, 성능, 런타임 비용을 시스템적으로 조사하고 분석하는 것입니다.

#### SLM 아키텍처, 데이터셋, 학습 (SLM Architecture, Datasets, and Training)
- **SLM 아키텍처**: 다양한 SLM의 구조를 설명하며, 주요 혁신 사항과 변화를 제시.
- **데이터셋**: 고품질의 데이터셋이 SLM의 성능에 미치는 영향을 강조. 모델 기반 필터링을 사용하여 고성능을 달성한 최신 데이터셋 두 가지 FineWeb-Edu와 DCLM을 소개.
- **학습 알고리즘**: 새로운 학습 기법의 적용을 통해 SLM의 안정성과 성능을 향상.
  - Maximal Update Parameterization(µP)
  - Knowledge Distillation
  - Two Stage Pre-training Strategy

#### SLM 성능 (SLM Capabilities)
SLM의 성능을 평가하기 위해 사용된 다양한 데이터셋과 지표를 소개합니다. 주로 상식 추론, 맥락 내 학습, 수학 및 코딩 능력 등을 평가합니다. 주요 성능 지표로 HellaSwag, PIQA, OpenBookQA 등의 데이터셋을 사용하여 여러 모델의 성능을 비교합니다.

#### SLM 런타임 비용 (SLM Runtime Cost)
SLM의 디바이스 내 런타임 시간을 줄이기 위한 다양한 방법을 논의합니다. SLM의 메모리 사용량과 처리 시간을 줄이기 위해 양자화 및 하드웨어 최적화의 효과를 측정합니다. 예를 들어, context length가 모델의 메모리 사용량에 미치는 영향을 분석합니다.

#### 결론 및 향후 방향 (Conclusions and Future Directions)
SLM 연구의 주요 성과를 요약하고, 향후 연구 방향을 제시합니다. SLM 아키텍처와 디바이스 프로세서의 공동 설계와 최적화, 고품질 합성 데이터셋 구축, 디바이스와 클라우드의 협업, 그리고 효율적인 SLM 벤치마킹 등이 주요 연구 주제로 떠오릅니다.

### 2. 전체 요약

이 논문은 소형 언어 모델(SLM)의 구조, 데이터셋, 학습 알고리즘, 성능 평가, 런타임 비용 등을 포괄적으로 조사한 연구입니다. SLM은 스마트폰과 같은 자원 제약이 있는 장치에서 효율적으로 작동하도록 설계되었으며, 최근 상용화된 많은 기기에서 사용되고 있습니다. 주요 결과로 고품질의 데이터셋과 새로운 학습 기법이 SLM의 성능을 크게 향상시킬 수 있음을 보여주었습니다. 또한, 디바이스 내 SLM의 메모리 사용량과 처리 시간을 줄이기 위한 여러 방안들도 제시되었습니다. 이를 통해 SLM 연구가 더욱 효율적이고 효과적으로 발전할 수 있는 방안을 제시하며, 이는 AI와 머신러닝의 실질적인 발전에 기여할 것으로 기대됩니다.