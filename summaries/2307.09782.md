# ZeroQuant-FP: A Leap Forward in LLMs Post-Training W4A8 Quantization Using Floating-Point Formats
## TL;DR
## Summary
- [https://arxiv.org/pdf/2307.09782.pdf](https://arxiv.org/pdf/2307.09782.pdf)

### 논문 요약: ZeroQuant-FP: A Leap Forward in LLMs Post-Training W4A8 Quantization Using Floating-Point Formats

#### 1. 서론
대형 언어 모델(LLM)의 복잡성과 계산 집약성은 배포에 어려움을 줍니다. 이를 해결하기 위해, 이 논문은 부동소수점(FP) 양자화를 제안합니다. 특히 FP8 및 FP4 형식에 중점을 두어 NVIDIA H100 하드웨어와의 호환성을 고려하였습니다.

#### 2. 주요 기여 및 혁신
- **FP8 활성화의 우수성**: FP8 활성화는 INT8 활성화보다 일관되게 우수한 성능을 보입니다. 특히 매개변수가 10억 개 이상인 모델에서 그 성능 차이가 두드러집니다.
- **FP4 가중치의 잠재력**: FP4 가중치는 INT4와 비교해도 동등하거나 더 나은 성능을 보이며, FP 지원 하드웨어에서의 배포를 단순화합니다.
- **정밀도 조정을 위한 두 가지 스케일링 제약 제안**: 가중치 양자화의 정밀도 정렬 오버헤드를 줄이기 위해, 두 가지 스케일링 제약을 제안합니다. 이는 성능에 거의 영향을 미치지 않습니다.
- **Low Rank Compensation (LoRC) 통합**: LoRC 전략을 통합하여 특히 작은 모델에서 양자화 오류를 줄이고 성능을 향상시킵니다.

#### 3. 방법론
- **정밀한 양자화 적용**: GPTQ 기반의 세밀한 양자화(FGQ)를 사용하여 가중치와 활성화 양자화를 수행합니다. 또한, 가중치의 양자화 오류를 줄이기 위해 저랭크 행렬 분해(SVD)를 사용한 LoRC 방법을 조사합니다.
- **FP4에서 FP8로의 변환**: 서로 다른 정밀도 수준을 사용하는 문제를 해결하기 위해 비트 시프팅 방법을 제안합니다. 이는 디지털 시스템에서의 계산 효율성을 최적화하고 모델 성능을 유지하는 데 중요합니다.

#### 4. 실험 결과
- **FP8 활성화의 우수성 확인**: LLaMA와 OPT 모델에서 FP8 활성화가 INT8 활성화보다 더 나은 성능을 보입니다. 특히 큰 모델에서 FP8의 장점이 더 두드러집니다.
- **FP4 가중치의 우수성**: FP4 가중치 양자화는 INT4와 비교하여 더 나은 성능을 보입니다. 이는 특히 FP8을 이미 지원하는 H100 하드웨어에서 유리합니다.
- **LoRC의 효과**: LoRC 방법은 W4A8 양자화 스킴에서 성능을 향상시킵니다. 특히 작은 모델에서 그 효과가 두드러집니다.

#### 5. 결론
이 연구는 LLM의 후처리 양자화에서 부동소수점 양자화가 정수 양자화보다 훨씬 우수하다는 것을 입증합니다. 특히 FP8 활성화와 FP4 가중치는 성능 저하 없이 원래의 FP16 모델과 동등한 성능을 보입니다. LoRC 접근 방식은 W4A8 양자화 스킴을 크게 향상시킵니다. 결과적으로, FP 양자화는 모델 성능을 향상시키고, 자원이 제한된 환경에서의 효율적인 배포를 가능하게 합니다.