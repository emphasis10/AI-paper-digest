# Sigmoid Loss for Language Image Pre-Training
## TL;DR
## Summary
- [https://arxiv.org/pdf/2303.15343.pdf](https://arxiv.org/pdf/2303.15343.pdf)

#### 1. 소개
- **언어-이미지 사전 학습**은 이미지와 텍스트 쌍을 사용하여 일반적인 컴퓨터 비전 백본을 얻는 방법으로, 레이블이 붙은 대규모 데이터셋을 사용하는 대신 널리 채택되고 있습니다.
- **대조 학습**은 이미지와 텍스트 임베딩을 정렬하는 데 사용됩니다. 그러나 이 접근법은 softmax 정규화를 사용하는데, 이는 계산 비용이 많이 들고 불안정할 수 있습니다.
- **SigLIP**(Sigmoid Loss for Language-Image Pre-Training) 모델은 이러한 문제를 해결하기 위해 제안되었습니다. SigLIP는 softmax 대신 sigmoid 손실을 사용하여 효율성과 성능을 높입니다.

#### 2. 관련 연구
- 기존 연구들은 대부분 softmax 기반 대조 학습을 사용했으며, 이는 대규모 이미지-텍스트 데이터셋에서 좋은 성능을 보였습니다.
- 몇몇 연구들은 효율성을 높이기 위해 노력했지만, SigLIP는 단순한 sigmoid 손실을 통해 더 나은 성능과 효율성을 달성합니다.

#### 3. 방법론
- **Softmax 손실**은 이미지와 텍스트 모델을 훈련하여 일치하는 쌍을 정렬하고, 일치하지 않는 쌍을 분리합니다. 그러나 이는 전역 정규화가 필요하여 계산 비용이 높습니다.
- **Sigmoid 손실**은 이러한 문제를 해결합니다. 이미지-텍스트 쌍을 개별적으로 처리하여 이진 분류 문제로 변환합니다. 이 접근 방식은 전역 정규화가 필요 없고 메모리 효율적입니다.
- **효율적인 구현**: Sigmoid 손실은 메모리 효율적이고 안정적인 구현이 가능합니다. 대규모 배치 크기를 처리하는 데 유리합니다.

#### 4. 결과
- **SigLiT**(Sigmoid LiT) 모델은 32k 배치 크기에서 최적의 성능을 보이며, 1백만 배치 크기까지 성공적으로 훈련되었습니다.
- **SigLIP** 모델은 16 TPUv4 칩을 사용하여 3일 만에 71%의 ImageNet zero-shot 정확도를 달성했습니다.
- 다국어 설정에서도 SigLIP는 32k 배치 크기에서 최고의 성능을 보였습니다.

#### 5. 결론
- Sigmoid 손실은 작은 배치 크기에서 softmax 손실보다 성능이 우수하고, 메모리 효율성이 높아 대규모 배치 크기를 처리하는 데 유리합니다.
- 적절한 배치 크기는 32k로, 이는 더 큰 배치 크기에서도 성능이 포화 상태에 이르는 것으로 나타났습니다.
- SigLIP와 SigLiT 모델은 제한된 자원으로도 높은 성능을 달성할 수 있음을 보여주었습니다.

### 전체 요약
이 논문은 언어-이미지 사전 학습을 위한 Sigmoid 손실(SigLIP)을 제안합니다. 기존 softmax 기반 대조 학습의 계산 비용과 불안정성을 해결하기 위해 SigLIP는 sigmoid 손실을 사용하여 효율성과 성능을 향상시킵니다. SigLIP는 작은 배치 크기에서 뛰어난 성능을 보이며, 메모리 효율적이어서 대규모 배치 크기를 처리하는 데 유리합니다. 제한된 자원으로도 높은 성능을 달성할 수 있는 가능성을 보여주며, 연구의 결과는 언어-이미지 사전 학습의 품질과 효율성을 개선하는 데 기여할 수 있습니다.