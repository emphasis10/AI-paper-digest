# RAG-RewardBench: Benchmarking Reward Models in Retrieval Augmented Generation for Preference Alignment
## TL;DR
## Summary
- [https://arxiv.org/pdf/2412.13746.pdf](https://arxiv.org/pdf/2412.13746.pdf)

1. 각 섹션의 요약:
   - **서론**: 이 논문은 RAG (Retrieval-Augmented Generation) 상황 내에서 보상 모델을 평가하는 최초의 기준인 RAG-RewardBench를 제안합니다. 이는 1,485개의 고품질 선호 쌍을 포함하여 RALMs를 조정하는데 도움을 줍니다. 이 벤치마크는 다단계 추론, 세분화된 인용, 적절한 포기, 및 갈등 견고성 등 네 가지 RAG 특정 시나리오를 소개합니다.
   
   - **기술적 기여**: 이 논문의 주요 기여는 RAG-RewardBench라는 새로운 벤치마크를 통해 기존 보상 모델의 한계를 드러내고, 기존 RALM들이 선호 조정에 거의 개선을 보이지 않는다는 점을 발견한 것입니다. 이로 인해 선호 조정 훈련으로의 전환이 필요함을 강조합니다.
   
   - **결론**: 이 논문에서는 RAG-RewardBench를 통해 기존 보상 모델의 RAG-specific 시나리오의 한계를 분석하였고, 미래 연구에서는 긴 문맥 입력을 더 잘 이해하고 인간의 선호와 일치하는 차세대 생성 보상 모델 개발을 계획하고 있습니다.

2. 전체 요약:
   이 논문은 보상 모델을 RAG 상황에서 평가하기 위해 RAG-RewardBench라는 벤치마크를 제안합니다. RAG-RewardBench는 다단계 추론에서의 논리적 일관성, 세분화된 정확한 인용, 정보 부족 시의 적절한 포기, 그리고 문서 갈등이 있을 때의 진실성 유지 등의 RAG 특유의 과제를 다루고 있습니다. 또한 다양한 데이터 소스를 사용하여 보상 모델의 편향성 문제를 최소화하고자 했습니다. 이 연구는 현재의 보상 모델들이 RAG에서의 유용성과 해가 없는 정도에는 초점을 맞추고 있지만, 사용자의 선호도에 맞추기 위한 훈련 필요성을 지적하고 있습니다.