# Concise Thoughts: Impact of Output Length on LLM Reasoning and Cost
## TL;DR
## Summary
- [https://arxiv.org/pdf/2407.19825.pdf](https://arxiv.org/pdf/2407.19825.pdf)

### 섹션 요약

---

#### 1. Introduction

이 논문은 최근의 대규모 언어 모델(LLMs)이 복잡한 질문-응답 작업을 실행하는 능력을 탐구합니다. LLMs의 성능을 향상시키기 위해 체인 오브 띵크(Chain-of-Thought, CoT) 방식을 사용하지만, 이 방법은 응답 시간이 길어지는 문제를 유발합니다. 이에 대한 해결책으로 출력 길이를 제한하는 새로운 프롬프트 엔지니어링 전략인 Constrained Chain-of-Thought (CCoT)를 제안합니다.

---

#### 2. Related Work

최근 연구들은 주로 LLMs의 정확성 향상에 초점을 맞춰왔습니다. 그러나 모델이 크기 및 반응 시간 문제를 유발할 수 있으며, 이는 사용자 경험에 부정적인 영향을 미칩니다. 따라서 이 논문은 출력의 간결성과 정확성을 평가하는 새로운 지표를 제안합니다.

---

#### 3. Motivational Considerations

LLM의 출력 생성 시간은 다양한 요인에 의해 좌우됩니다. 모델 구조, 전처리 및 후처리 단계, 답변 디코딩 과정 등이 이에 해당합니다. 특히 프롬프트 엔지니어링 접근 방식을 사용할 때 이러한 시간이 어떻게 변하는지 이해하기 위해, 새로운 출력 간결성 평가 지표를 개발하게 되었습니다.

---

#### 4. Concept of Constrained Chain-of-Thought

새로운 CCoT 접근 방식은 모델이 출력 길이를 관리할 수 있도록 프롬프트를 더욱 정밀하게 조정합니다. 이를 통해 모델이 더 짧고 효율적인 응답을 생성하는 한편, 출력 길이가 제어되는지를 확인할 수 있습니다.

---

#### 5. Experimentation Setup

다양한 크기의 사전 학습된 LLM들(예: Falcon-7b, Llama2-70b)을 사용해 실험을 수행하였습니다. 몇 가지 다른 프롬프트 설정(CCTO-15, CCTO-30, CCTO-60, CCTO-100)을 사용해 각 모델의 시간 및 정확성을 평가했습니다.

---

#### 6. Experimental Results

- **Generation Time and Accuracy**: CCoT를 사용한 결과, 대부분의 대형 및 중형 모델이 더 짧은 응답 시간을 보였습니다. 예를 들어, Llama2-70b의 경우, 평균 생성 시간은 30.09초에서 CCoT-15를 사용하면 23.86초로 거의 절반으로 줄었습니다.
- **Output Conciseness**: 더 작은 모델(Falcon-7b, Llama2-7b)은 이 제약 조건을 제대로 처리하지 못해 생성 시간이 증가하거나 응답의 정확성이 낮아졌습니다.
- **Trade-offs in Accuracy and Efficiency**: LLM들은 출력 길이를 제한하면 정확성이 향상되는 경향이 있었지만, 모델 크기와 학습 전략에 많이 의존했습니다.

---

#### 7. Discussion and Conclusion

CCoT 접근 방식은 LLM의 출력 길이와 정확성을 동시에 향상시킬 수 있는 유망한 방법입니다. 그러나 이 방법은 모델의 크기와 학습 데이터에 따라 그 성과가 크게 달라집니다. 미래 연구는 이러한 차이를 더 깊이 이해하고 개선하는 데 초점을 맞출 수 있을 것입니다.

### 전체 요약

이 논문은 대규모 언어 모델(LLMs)에서 질문-응답 작업의 효율성을 높이기 위한 새로운 방법, 즉 Constrained Chain-of-Thought (CCoT) 프롬프트 전략을 제안합니다. CoT 방식은 응답의 설명성과 정확성을 높이지만, 생성 시간이 길어지는 문제를 야기합니다. CCoT는 이 문제를 해결하기 위해 출력 길이를 제한하도록 프롬프트를 정밀하게 조정합니다. 다양한 실험을 통해, CCoT를 사용하면 특히 대형 모델에서 응답 시간이 감소하고 정확성이 향상됨을 확인했습니다. 그러나 작은 모델에서는 이 제약 조건을 관리하는 데 어려움이 있었습니다. 이 연구는 LLM의 출력 간결성 및 정확성을 동시에 고려한 새로운 평가 지표도 제안합니다.