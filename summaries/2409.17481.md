# MaskLLM: Learnable Semi-Structured Sparsity for Large Language Models
## TL;DR
## Summary
- [https://arxiv.org/pdf/2409.17481.pdf](https://arxiv.org/pdf/2409.17481.pdf)

## 섹션 요약

### 1. 소개 (Introduction)
이 논문은 대형 언어 모델(LLMs)의 계산 비용을 줄이기 위해 MaskLLM이라는 학습 가능한 가지치기 방법을 소개합니다. MaskLLM은 Gumbel Softmax 샘플링을 통해 N:M 패턴을 학습 분포로 명확히 모델링하여 세미-구조화된 스파시티를 구현합니다. 이 방법은 고품질의 마스크 학습과 도메인 간 스파시티 이전 학습의 이점을 제공합니다.

### 2. 관련 연구 (Related Works)
여기서는 네트워크 가지치기에 관한 세 가지 주요 접근방식, 즉 구조화 가지치기, 비구조화 가지치기, 세미-구조화 가지치기에 대해 논의합니다. 기존의 방법들은 대형 언어 모델에서 선택적인 파라미터를 제거함으로써 효율성을 달성하려 노력했습니다. MaskLLM은 대규모 데이터셋을 활용하여 고품질의 마스크를 학습하는 데 중점을 둔 첫 시도입니다.

### 3. 방법론 (Methods)
#### 3.1 N:M 스파시티 (N:M Sparsity)
N:M 스파시티는 연속된 M개의 파라미터 그룹 중 N개의 비영(非零) 파라미터가 존재하는 것을 의미합니다. 본 논문에서는 2:4 스파시티를 중점으로 다루며, 이는 차후 다른 패턴으로 확장 가능할 것입니다.
#### 3.2 MaskLLM
MaskLLM은 마스크 선택 문제를 확률적 관점에서 프레임화하여 샘플링 프로세스로 모델링합니다. Gumbel Softmax를 사용하여 샘플링의 무작위성을 독립 랜덤 변수로 재매개변수화함으로써, 각 마스크 후보의 확률을 최적화할 수 있습니다. 이를 통해 원래의 고밀도 LLM 품질을 유지하면서 최적 마스크 분포를 학습합니다.

### 4. 실험 (Experiments)
LLaMA-2, GPT-3 등 대형 언어 모델들에 대해 MaskLLM을 평가하였으며, 우리의 방법은 동일한 데이터셋에서 SparseGPT보다 낮은 퍼플렉서티(PPL)을 가지는 고품질의 마스크를 학습하는 데 성공했습니다. 또한, MaskLLM은 도메인별 마스크 학습을 통해 각각의 다운스트림 작업에서 손실 없는 압축을 달성할 수 있습니다.

### 5. 결론 (Conclusion)
MaskLLM은 LLM의 학습 가능한 가지치기 방법으로, 대규모 데이터셋에서 정확한 N:M 스파시티 마스크를 학습하여 추론 중 계산 오버헤드를 줄입니다. 저희의 실험은 MaskLLM의 확장성과 실질적인 응용 가능성을 입증합니다.

## 전체 요약
이 논문은 MaskLLM이라는 대형 언어 모델(LLMs)의 계산 효율성을 높이기 위한 학습 가능한 가지치기 방법을 제안합니다. MaskLLM은 Gumbel Softmax를 통해 N:M 스파시티 패턴을 학습하고 이를 통해 효율적인 마스크를 생성하여 원래 LLM의 품질을 유지하며 계산 오버헤드를 줄입니다. 실험 결과 MaskLLM은 기존의 SparseGPT보다 낮은 퍼플렉서티를 보이며, 도메인별 맞춤 마스크를 통해 손실 없는 압축을 이룰 수 있음을 보여줍니다.