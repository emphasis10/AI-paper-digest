# FlashDecoding++: Faster Large Langauge Model Inference on GPUs
## TL;DR
## Summary
- [https://arxiv.org/pdf/2311.01282.pdf](https://arxiv.org/pdf/2311.01282.pdf)

이 논문은 대규모 언어 모델(Large Language Model, LLM) 추론을 GPU에서 빠르게 수행하는 새로운 엔진, FlashDecoding++에 대해 소개합니다. FlashDecoding++는 주요 도전 과제들을 극복하기 위한 세 가지 창의적인 방법을 제시합니다: (1) 동기화되지 않은 소프트맥스 연산과 통일된 최대값 기법, (2) 이중 버퍼링을 사용한 평평한 GEMM 최적화, 그리고 (3) 하드웨어 자원 적응형 휴리스틱 데이터 플로우입니다. 이 최적화들은 NVIDIA와 AMD GPU 모두에서 기존 Hugging Face 구현과 비교하여 최대 4.86배 및 3.93배의 속도 향상을 달성합니다. 평균적으로 FlashDecoding++는 다양한 LLM에서 기존의 최신 LLM 추론 엔진보다 1.37배 빠릅니다.

### 요약

1. **서론**:
    - 대규모 언어 모델(LLM)의 추론 성능은 다양한 응용 분야에서 중요하며, 효율적인 실행을 위한 최적화가 필수적입니다.
    - FlashDecoding++는 LLM 추론을 가속화하기 위한 새로운 엔진으로, 동기화되지 않은 부분 소프트맥스 연산, 평평한 GEMM 최적화, 하드웨어 자원 적응형 휴리스틱 데이터 플로우 등 세 가지 주요 기법을 제안합니다.

2. **배경**:
    - LLM 추론의 데이터 플로우 개요, LLM 추론에서의 주요 연산, 주의 최적화 방법에 대한 배경 정보를 제공합니다.

3. **동기화되지 않은 소프트맥스와 통일된 최대값**:
    - 기존의 동기화된 부분 소프트맥스 연산을 개선하여 연산 오버헤드를 줄이고, 각 부분 소프트맥스 결과를 개별적으로 처리할 수 있도록 통일된 최대값 기법을 도입합니다.

4. **이중 버퍼링을 사용한 평평한 GEMM 최적화**:
    - 평평한 GEMM 연산의 컴퓨팅 리소스 활용도를 높이기 위해, 이중 버퍼링 기법을 사용하여 메모리 접근 지연을 최소화하고 계산 효율을 개선합니다.

5. **하드웨어 자원 적응형 휴리스틱 데이터 플로우**:
    - 입력 동적 특성과 하드웨어 구성을 고려하여, LLM 추론 데이터 플로우를 동적으로 최적화하는 휴리스틱 접근 방식을 제안합니다.

6. **평가**:
    - 다양한 하드웨어 플랫폼과 LLM 모델에서 FlashDecoding++의 성능을 기존 상태 최신 기술과 비교하여 평가합니다.

### 종합 요약

FlashDecoding++는 대규모 언어 모델의 추론을 GPU에서 효율적으로 가속화하기 위해 개발된 새로운 엔진입니다. 이 엔진은 기존의 도전 과제를 해결하기 위한 세 가지 혁신적인 기술을 제안하며, 테스트 결과 NVIDIA와 AMD GPU에서 기존 방법들보다 우수한 성능을 보여줍니다. 이러한 최적화는 LLM을 활용한 응용 프로그램의 실행 비용과 시간을 크게 줄일 수 있으며, 실제 환경에서의 LLM 사용을 보다 효율적으로 만들 것으로 기대됩니다.

## Similar Papers
- [ByteTransformer: A High-Performance Transformer Boosted for Variable-Length Inputs](2210.03052.md)
- [A Survey on Efficient Inference for Large Language Models](2404.14294.md)
- [FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning](2307.08691.md)
- [Boosting Large-scale Parallel Training Efficiency with C4: A Communication-Driven Approach](2406.04594.md)
- [SampleAttention: Near-Lossless Acceleration of Long Context LLM Inference with Adaptive Structured Sparse Attention](2406.15486.md)
- [2BP: 2-Stage Backpropagation](2405.18047.md)
- [AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration](2306.00978.md)
- [PowerInfer-2: Fast Large Language Model Inference on a Smartphone](2406.06282.md)
- [A Survey of Multi-Tenant Deep Learning Inference on GPU](2203.09040.md)
