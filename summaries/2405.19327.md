# MAP-Neo: Highly Capable and Transparent Bilingual Large Language Model Series
## TL;DR
## Summary
- [https://arxiv.org/pdf/2405.19327.pdf](https://arxiv.org/pdf/2405.19327.pdf)

#### 1. 소개

- **문제**: 대형 언어 모델(LLM)들은 다양한 작업에서 뛰어난 성능을 보이지만, 상업적 이익으로 인해 훈련 세부 사항이 공개되지 않음.
- **목표**: 완전한 투명성을 가진 고성능의 이중언어 모델 MAP-Neo를 공개하여 연구 공동체의 발전에 기여.
- **방법**: 7B 매개변수를 가진 모델로 4.5T 고품질 토큰으로 훈련된 MAP-Neo를 공개. 훈련 데이터, 체크포인트, 데이터 클리닝 파이프라인, 훈련 코드 모두 공개.

#### 2. 관련 연구

- **기존 연구들**: 많은 기관들이 강력한 오픈소스 LLM을 발표했으나, 훈련 데이터나 과정이 불투명한 경우가 많음.
- **기여**: MAP-Neo는 첫 번째 완전 오픈소스 이중언어 LLM으로, 훈련 데이터와 전체 과정이 공개됨.

#### 3. 토크나이저

- **BPE 알고리즘**: SentencePiece를 이용해 50B 샘플로부터 토크나이저 훈련.
- **특징**: 숫자를 개별 숫자로 분리하고, 알려지지 않은 UTF-8 문자를 바이트 단위로 처리.

#### 4. 데이터 수집

- **Matrix Data Pile**: 4.5T 토큰으로 구성된 이중언어 훈련 데이터 집합.
- **데이터 처리**: 기존 오픈 데이터셋을 재처리하고, 웹에서 직접 수집한 데이터를 정리하여 고품질 데이터 구축.

#### 5. 모델 아키텍처

- **Transformer 구조**: Multi-Query Attention, RoPE 임베딩, RMSNorm, SwiGLU 활성화 함수 등을 사용.
- **모델 규모**: 2B 및 7B 매개변수 모델.

#### 6. 사전 훈련

- **2단계 전략**: 일반 능력 습득 단계와 개선 및 교정 단계로 나뉨.
- **데이터 비율**: 각 단계에서 다양한 데이터 비율 사용.

#### 7. 정렬

- **지도형 미세 조정**: 인간 행동에 맞춘 데이터로 모델의 기본 능력과 채팅 능력을 향상.
- **Iterative DPO**: 인간 피드백을 반영한 선호도 최적화를 통해 모델의 정렬 성능 개선.

#### 8. 스케일링 법칙

- **NEO 스케일링 법칙**: 다양한 고품질 데이터 셋을 포함한 모델의 손실 예측을 최적화하여 훈련 효율성을 높임.

#### 9. 인프라

- **분산 컴퓨팅**: 대규모 모델 훈련을 위한 고성능 인프라 구축. 512 GPU를 사용하여 7B 모델 훈련.

#### 10. 평가

- **벤치마크**: MAP-Neo가 다양한 벤치마크에서 기존 모델들을 능가하는 성능을 보임.
- **투명성 및 성능**: 코드, 수집 데이터, 훈련 과정 모두 공개하여 투명성을 유지하며 고성능 달성.

### 주요 기여 및 혁신 부분

- **주요 기여**:
  1. 완전 오픈소스 이중언어 LLM으로서의 투명성 제공.
  2. 다양한 고품질 데이터 셋을 포함한 훈련 데이터와 과정 공개.
  3. 다양한 벤치마크에서의 뛰어난 성능 입증.

- **혁신 부분**:
  1. 완전한 데이터 수집 및 클리닝 파이프라인 제공.
  2. NEO 스케일링 법칙을 통한 최적화된 훈련.
  3. 지도형 미세 조정 및 Iterative DPO를 통한 정렬 성능 개선.

### 전체 요약

이 논문에서는 완전한 투명성과 높은 성능을 갖춘 이중언어 대형 언어 모델인 MAP-Neo를 소개합니다. MAP-Neo는 7B 매개변수 모델로, 4.5T의 고품질 토큰으로 훈련되었습니다. 이 모델은 데이터 수집, 클리닝, 훈련 과정이 모두 공개되어 있으며, 다양한 벤치마크에서 뛰어난 성능을 보입니다. 이를 통해 연구 공동체의 발전에 기여하고, 투명하고 고성능의 언어 모델 개발에 대한 새로운 기준을 제시합니다.

## Similar Papers
- [CoverBench: A Challenging Benchmark for Complex Claim Verification](2408.03325.md)
- [OpenELM: An Efficient Language Model Family with Open Training and Inference Framework](2404.14619.md)
- [ChuXin: 1.6B Technical Report](2405.04828.md)
- [RALL-E: Robust Codec Language Modeling with Chain-of-Thought Prompting for Text-to-Speech Synthesis](2404.03204.md)
- [LLaMAX: Scaling Linguistic Horizons of LLM by Enhancing Translation Capabilities Beyond 100 Languages](2407.05975.md)
- [MobiLlama: Towards Accurate and Lightweight Fully Transparent GPT](2402.16840.md)
- [EXAONE 3.0 7.8B Instruction Tuned Language Model](2408.03541.md)
- [Internal Consistency and Self-Feedback in Large Language Models: A Survey](2407.14507.md)
- [RLHF Workflow: From Reward Modeling to Online RLHF](2405.07863.md)
