# Jailbreaking Large Language Models with Symbolic Mathematics
## TL;DR
## Summary
- [https://arxiv.org/pdf/2409.11445.pdf](https://arxiv.org/pdf/2409.11445.pdf)

### 섹션별 요약

---

#### 1. 서론
이 연구에서는 'MathPrompt'라는 새로운 공격 기법을 제안하며, 이는 큰 언어 모델(LLM)이 수학 문제를 이해하고 처리하는 능력을 악용하여 기존의 안전 메커니즘을 우회할 수 있음을 보여준다. MathPrompt는 자연어로 작성된 유해한 요청을 수학 문제로 변환하여 LLM의 안전 장치를 우회하는 접근 방식을 제시한다. 실험 결과, 13개의 최첨단 LLM에서 평균 73.6%의 성공률로 공격이 성공했음을 보여주며, 현재 AI 안전 메커니즘의 한계를 드러낸다.

#### 2. 방법론
2.1 자연어 명령어의 수학적 표현  
자연어로 된 지침과 질문을 집합론, 추상대수학, 기호 논리를 사용하여 수학적 표현으로 변환한다. 구체적인 수학적 개념을 활용하여 자연어로 표현된 의미와 구조를 포착한다.

#### 3. 실험
3.1 실험 설정  
120개의 자연어 유해 질문을 수학 문제로 변환하여 13개의 다양한 LLM에 적용했으며, 안전 설정에 따라 효과를 평가했다.

3.2 안전 훈련과 정합성  
MathPrompt는 다양한 LLM에서 평균 73.6%의 공격 성공률을 기록했으며, 이는 기존의 안전 훈련이 수학으로 인코딩된 입력값에 대해 일반화되지 않음을 시사한다. 이러한 높은 성공률은 더욱 포괄적인 안전 조치가 필요함을 강조한다.

#### 4. 결론
MathPrompt는 현재 LLM 안전 메커니즘의 중요한 취약점을 밝혔으며, 수학적 표현으로 변환된 유해 콘텐츠가 감지되지 않을 수 있음을 보여준다. 이에 따라 모든 입력 유형과 관련된 위험을 포괄하는 홀론적 접근법이 필요함을 강조한다.

#### 5. 한계
연구에서 사용된 120개의 프롬프트 세트가 모든 유형의 유해 콘텐츠를 포괄하지 못할 수 있으며, 더 넓은 범위의 LLM를 테스트함으로써 연구를 확장할 필요가 있다.

#### 6. 사회적 영향
이 연구는 AI 안전 향상을 목표로 하지만, 수학적 인코딩을 활용한 취약점이 악용될 가능성도 있어, 오히려 유해 콘텐츠의 생성과 확산을 유발할 수 있음을 지적한다. 이러한 취약점을 밝힘으로써 보다 엄격한 안전 조치의 필요성을 촉구한다.

### 주요 기여 및 혁신

**주요 기여 및 혁신 부분**  
MathPrompt는 수학적 인코딩을 활용하여 LLM의 안전 메커니즘을 우회하는 새로운 공격 기법을 제안했다. 이는 LLM이 갖춘 수학적 문제 해결 능력을 악용한 최초의 접근법으로, 기존의 AI 안전 훈련이 이와 같은 수학적 표현을 포괄하지 못함을 보여준다. 이를 통해 더 포괄적이고 개선된 AI 안전 메커니즘의 필요성을 강조한다.

### 전체 요약
이 연구는 LLM의 안전 메커니즘에 대한 중요한 취약점을 밝혀냈으며, 자연어로 된 유해한 요청을 수학 문제로 변환하여 우회할 수 있는 새로운 공격 기법 'MathPrompt'를 제안했다. 실험 결과, 평균 73.6%의 성공률로 공격이 성공했음을 보여주었으며, 이는 기존의 AI 안전 훈련이 수학적 표현에 대해 일반화되지 않음을 시사한다. 이러한 연구는 AI 안전 메커니즘을 더욱 강화하고 포괄적인 접근법이 필요함을 강조하며 중요한 학술적 및 실질적인 기여를 한다.