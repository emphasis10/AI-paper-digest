# Exploring Scaling Trends in LLM Robustness
## TL;DR
## Summary
- [https://arxiv.org/pdf/2407.18213.pdf](https://arxiv.org/pdf/2407.18213.pdf)

### 1. 논문 섹션 요약

#### 서론
이 논문은 대규모 언어 모델(LLM)의 적대적 공격에 대한 확장성을 분석합니다. 과거에는 컴퓨터 비전 모델들이 확장이 되면서 더 많은 견고성을 보인다는 결과가 있었으나, 언어 모델에서도 동일한 특징이 나타나는지를 연구합니다. 특히, 대규모 언어 모델이 어떻게 적대적 학습에 대해 반응하는지를 실험적으로 조사합니다. 이를 통해 미래의 더 강력한 모델들이 더 자연스럽게 견고해질 수 있는지, 아니면 추가적인 안전 조치가 필요한지를 논의합니다.

#### 관련 연구
적대적 예시는 이미지 분류, 음성 인식, 강화 학습 등에서 먼저 나타났으며, 이 논문에서는 언어 모델에 맞는 적대적 공격을 탐구합니다. 이전 연구들은 모델 확장이 이들 공격에서 더 강한 방어를 제공할 수 있음을 시사했지만, 언어 모델에서는 일관된 결과를 제공하지 못했습니다. 따라서 본 논문에서 이를 실험적으로 확인하고자 했습니다.

#### 실험 방법론
실험은 바이너리 분류 과제에서 LLM의 견고성을 측정하는 방법으로 설계되었습니다. 이를 위해 Pythia 모델 패밀리를 사용하며, 모델 크기(14M ~ 12B 파라미터)와 공격 방법론(무작위 토큰 및 GCG 공격)을 다양하게 조합하여 실험을 수행합니다. 각 모델은 기본 데이터를 통해 사전 학습되었으며, 그 후 각기 다른 과제를 통해 성능을 평가합니다.

#### 파인 튜닝 및 검증
모든 모델은 기본 데이터로 한 차례 파인 튜닝되어 각 과제에서 83% 이상의 정확도를 보였습니다. 그러나 모델의 웅변성과 공격 성공률 간에 많은 변동이 나타났습니다. 이는 모델의 사전 학습 체크포인트와 무작위 시드의 영향을 받은 것으로 나타났습니다. 이 결과는 단순한 확장만으로는 견고성이 확보되지 않으며, 추가적인 방어 조치가 필요함을 시사합니다.

#### 적대적 학습
더 큰 모델이 적대적 학습에서 얻는 장점이 더 작은 모델보다 크다는 결과를 보였습니다. 특히, 더 큰 모델은 적대적 예시를 통해 매우 빠르게 견고성을 확보하며, 이는 다른 유형의 공격에서도 방어력을 유지하는 경향을 보였습니다.

#### 결론
더 큰 모델은 적대적 학습을 통해 더 많은 이점을 얻지만, 견고성을 확보하기 위해서는 모델 크기뿐만 아니라 효율적인 방어 기법도 필요합니다. 향후 연구에서는 다양한 과제와 모델 규모를 대상으로 검증할 필요가 있으며, 특히 생성적 과제에서 견고성이 어떻게 변하는지를 추가로 연구할 계획입니다.

### 2. 전체 요약
이 논문은 대규모 언어 모델(LLM)이 적대적 공격에 어떻게 반응하는지를 연구합니다. 모델 크기와 적대적 학습의 조합을 통해 대규모 모델이 더 견고해질 수 있다는 것을 입증하였고, 특히 더 큰 모델이 적대적 학습에서 더 많은 이점을 얻음으로써 다양한 공격에 대한 방어력을 확보함을 보여줍니다. 그러나 견고성을 확보하기 위해서는 모델의 크기뿐만 아니라 효율적인 방어 기법의 개발이 필요합니다. 향후 연구에서는 생성적 과제와 더 큰 모델을 대상으로 검증할 계획입니다.