# DELLA-Merging: Reducing Interference in Model Merging through Magnitude-Based Sampling
## TL;DR
## Summary
- [https://arxiv.org/pdf/2406.11617.pdf](https://arxiv.org/pdf/2406.11617.pdf)

### 1. 각 섹션 요약

---

#### 1. Introduction (소개)
이 논문은 일반 대규모 언어 모델(LLM) 기반의 대화형 시스템이 광범위하게 사용되며, 이러한 모델을 특정 작업에 맞게 조정하는 것이 중요하다고 설명합니다. 이를 통해 여러 작업을 수행하는 전문 모델로 변할 수 있습니다. 그러나 각각의 작업을 위한 개별 모델을 유지하는 것은 메모리 용량과 작업 성능 측면에서 한계를 가지고 있습니다. 논문은 이러한 문제를 해결하기 위해 상동 모델(homologous models)을 병합하는 새로운 접근법, 'Drop and rEscaLe via sampLing with mAgnitude (DELLA)'를 제안합니다.

---

#### 2. Methodology (방법론)
DELLA는 세 단계로 구성되어 있습니다:
 1. **Drop:** 델타 파라미터를 떨어뜨리는 방법으로 MAGPRUNE이라는 새로운 가지치기(pruning) 방법을 사용합니다. 이 방법은 델타 파라미터를 크기에 따라 샘플링합니다.
 2. **Elect:** 델타 파라미터의 부호를 기반으로 교란을 줄입니다.
 3. **Fuse:** 선택된 델타 파라미터를 융합하여 병합된 모델을 만듭니다.

---

#### 3. Experimental Setup and Results (실험 설정 및 결과)
논문은 Wizard 모델 기반의 세 가지 전문가 모델(WizardLM, WizardMath, WizardCoder-Python)을 사용하여 DELLA의 성능을 비교합니다. DELLA 방법은 다른 방법들(DARE, TIES, TA)과 비교하여 평균적으로 2.4 포인트 더 높은 성능을 보였으며, 무작위 파라미터 삭제 방법(NODROP)보다 11.1 포인트 더 나은 성능을 보였습니다.

---

#### 4. Conclusion (결론)
결론에서는 DELLA의 혁신적인 부분인 MAGPRUNE 가지치기 기법의 중요성을 강조하며, DELLA가 다른 기존 방법들보다 효과적이라는 점을 다시 확인합니다. 또한, DELLA 방법이 다루기 어렵고 여러 하이퍼파라미터가 필요하지만, 각 모델 병합에 맞춘 최적의 성능을 제공한다고 언급합니다.

---

### 2. 전체 요약

이 논문은 "Drop and rEscaLe via sampLing with mAgnitude (DELLA)"라는 새로운 모델 병합 방법을 제안합니다. 여러 작업을 수행하는 전문가 모델을 하나의 모델로 병합하는 과정에서 발생하는 한계를 극복하기 위한 방법론으로, 델타 파라미터를 크기에 따라 샘플링하는 MAGPRUNE 가지치기 기법을 도입합니다. 실험 결과, DELLA는 기존 방식들(DARE, TIES, TA)을 평균 2.4~11.1 포인트 우수하게 능가하며, 특히 MAGPRUNE 가지치기 기법의 유효성을 증명하였습니다. 이 논문의 주요 기여는 모델 병합 과정에서 발생하는 교란을 줄이고, 병합된 모델의 성능을 대폭 향상시키는 새로운 가지치기 및 스케일링 방법을 제안한 것입니다. 연구 결과는 AI 모델 병합에 있어 중요한 참고 자료가 될 수 있습니다.