# MEGABYTE: Predicting Million-byte Sequences with Multiscale Transformers
## TL;DR
## Summary
- [https://arxiv.org/pdf/2305.07185.pdf](https://arxiv.org/pdf/2305.07185.pdf)

### 요약

#### 1. 소개 (Introduction)
이 논문은 백만 바이트 이상의 긴 시퀀스를 효과적으로 모델링하기 위해 MEGABYTE라는 새로운 멀티스케일 트랜스포머 모델을 소개합니다. MEGABYTE는 시퀀스를 고정된 크기의 패치로 분할하고, 각 패치 내에서는 작은 모델을, 패치 간에는 큰 모델을 사용하여 효율적으로 시퀀스를 예측합니다. 이를 통해 연산 비용을 줄이면서도 긴 시퀀스를 효과적으로 처리할 수 있습니다.

#### 2. 배경 (Background)
기존의 트랜스포머 모델은 짧은 시퀀스에서는 뛰어난 성능을 보이지만, 긴 시퀀스에서는 연산 비용이 급격히 증가하여 효율이 떨어집니다. MEGABYTE는 이러한 문제를 해결하기 위해 개발되었습니다.

#### 3. 모델 구조 (Model Architecture)
MEGABYTE는 세 가지 주요 구성 요소로 이루어져 있습니다:
1. **패치 임베더 (Patch Embedder)**: 시퀀스를 고정된 크기의 패치로 분할하고 각 바이트를 임베딩합니다.
2. **글로벌 모델 (Global Model)**: 각 패치의 임베딩을 입력으로 받아 패치 간의 문맥을 학습합니다.
3. **로컬 모델 (Local Model)**: 글로벌 모델의 출력을 받아 패치 내의 바이트를 예측합니다.

이러한 구조는 긴 시퀀스를 두 개의 짧은 시퀀스로 분할하여 처리하므로, 전통적인 트랜스포머 모델보다 연산 비용이 낮습니다.

#### 4. 실험 (Experiments)
MEGABYTE는 다양한 데이터셋에서 실험을 통해 기존 모델 대비 우수한 성능을 보였습니다. 특히 ImageNet 데이터셋에서의 밀도 추정과 긴 문맥 언어 모델링에서 뛰어난 성능을 보였습니다.

#### 5. 결론 (Conclusion)
MEGABYTE는 긴 시퀀스를 효율적으로 모델링할 수 있는 새로운 아키텍처를 제안합니다. 이 모델은 기존의 바이트 기반 모델들보다 뛰어난 성능을 보이며, 서브워드 모델과 경쟁할 수 있는 수준의 성능을 보입니다. 향후 연구에서는 MEGABYTE를 더 큰 모델과 데이터셋으로 확장하는 것을 제안합니다.

### 전체 요약
MEGABYTE는 백만 바이트 이상의 긴 시퀀스를 모델링하기 위해 개발된 새로운 멀티스케일 트랜스포머 아키텍처입니다. 이 모델은 시퀀스를 패치로 분할하여 패치 내에서는 작은 모델을, 패치 간에는 큰 모델을 사용함으로써 연산 비용을 줄이면서도 높은 성능을 유지합니다. 다양한 실험을 통해 MEGABYTE의 우수성을 입증하였으며, 향후 연구에서는 더 큰 스케일로의 확장을 제안합니다. 이 모델은 기존의 서브워드 모델을 대체할 가능성을 보여줍니다.

## Similar Papers
- [Griffin: Mixing Gated Linear Recurrences with Local Attention for Efficient Language Models](2402.19427.md)
- [Vision Transformers Need Registers](2309.16588.md)
- [LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale](2208.07339.md)
- [Muse: Text-To-Image Generation via Masked Generative Transformers](2301.00704.md)
- [Lory: Fully Differentiable Mixture-of-Experts for Autoregressive Language Model Pre-training](2405.03133.md)
- [DiM: Diffusion Mamba for Efficient High-Resolution Image Synthesis](2405.14224.md)
- [Better & Faster Large Language Models via Multi-token Prediction](2404.19737.md)
- [Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention](2404.07143.md)
- [You Only Cache Once: Decoder-Decoder Architectures for Language Models](2405.05254.md)
