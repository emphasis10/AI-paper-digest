# Strategist: Learning Strategic Skills by LLMs via Bi-Level Tree Search
## TL;DR
## Summary
- [https://arxiv.org/pdf/2408.10635.pdf](https://arxiv.org/pdf/2408.10635.pdf)

### 요약

#### 요약하기 전 자료
**논문 제목: STRATEGIST**

이 논문은 다중 에이전트 적대적 환경에서의 의사결정 문제를 해결하기 위한 학습 방법으로서 "STRATEGIST"를 제안합니다. 이 방법은 LLM(Self-Improvement Large Language Models)과 탐색 기법을 결합하여 효율적인 정책을 찾고, 이를 다양한 상대에 대해 평가하고 개선하는 과정을 자동화합니다.

#### 주요 공헌과 혁신
1. **이론적 기여**: STRATEGIST는 LLM을 통한 자동화된 정책 학습과 평가 프레임워크를 제시합니다.
2. **탐색 방법 개선**: 모듈식 탐색 방법을 도입하여, 아이디어 구현과 밴딧 탐색을 병합하여 샘플 효율성을 높였습니다.
3. **실험을 통한 검증**: 두 개의 게임, GOPS과 Resistance: Avalon에 이 방법을 적용하여 그 효과성을 입증했습니다. LLM 기반의 방법이 기존 강화 학습 기반의 방법보다 성능이 우수함을 보여주었습니다.

### 섹션 요약
1. **서론**
   - 최근 연구들은 LLM의 상호작용 환경에서의 의사결정 능력을 개선하기 위해 다양한 방법을 시도했습니다. 그러나 다중 에이전트 적대적 환경에서는 적절한 정책을 학습하는 데에 어려움이 있었습니다. 본 논문은 이러한 문제를 해결하기 위해 STRATEGIST라는 방법을 제안합니다.

2. **방법론**
   - STRATEGIST는 고수준 전략 공간에서의 문제 해결과 저수준 행동 공간에서의 문제 해결을 위한 LLM 자기 개선 및 탐색을 결합합니다.
   - 전략 나무를 구축하고, 이전에 생성된 전략을 개선하는 방식을 사용합니다. 이를 위해 게임 상태 평가를 위한 휴리스틱과 대화 가이드를 학습합니다.
   - 모의 자가 플레이를 통해 정책을 시험하고, 피드백을 수집하여 개선합니다.

3. **실험 및 결과**
   - 두 가지 게임, GOPS와 Avalon에서 STRATEGIST를 테스트했습니다. 자체 개선 방법이 기존 강화 학습 기반 방법들보다 더 나은 성능을 보였습니다.
   - 다양한 피드백 수집 방법을 비교한 결과 STRATEGIST가 더 높은 질의 피드백을 제공함을 확인했습니다.

4. **논의 및 결론**
   - 다중 에이전트 환경에서 피드백 노이즈와 LLM 생성의 본질적인 노이즈에도 불구하고 STRATEGIST는 평균적으로 더 나은 성능을 보였습니다.
   - 게임 rules를 LLM에게 제공하면, 사람과 같은 자연 언어 형태로 쉽게 정책을 만들어낼 수 있습니다.

### 전체 요약
이 논문은 LLM 자기 개선을 통해 다중 에이전트 적대적 환경에서의 의사결정 문제를 해결하고자 하는 방법론을 제안합니다. STRATEGIST라는 방법은 고수준 전략과 저수준 행동 공간에서의 문제 해결을 위한 LLM 자기 개선을 결합하여, LLM이 효율적이고 효과적인 정책을 학습할 수 있게 합니다. 두 가지 게임을 통해 실험한 결과, STRATEGIST는 기존 방법들에 비해 뛰어난 성능을 보였습니다. 이는 AI 에이전트의 능력을 향상시키기 위한 실질적인 접근법을 제시하며, 다양한 환경에서 적용 가능성을 확인했습니다.