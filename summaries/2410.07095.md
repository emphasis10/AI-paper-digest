# MLE-bench: Evaluating Machine Learning Agents on Machine Learning Engineering
## TL;DR
## Summary
- [https://arxiv.org/pdf/2410.07095.pdf](https://arxiv.org/pdf/2410.07095.pdf)

**1. 각 섹션의 주요 내용 요약:**

**소개 및 주요 기여:**
이 논문은 MLE-bench라는 벤치마크를 소개하며, AI 에이전트가 어떻게 머신러닝 엔지니어링 과제를 수행할 수 있는지를 평가합니다. MLE-bench는 현실적인 ML 엔지니어링 작업을 모방하여, AI 에이전트의 자율성을 인간의 성과와 비교할 수 있는 플랫폼을 제공합니다.

**MLE-bench의 설계와 개발:**
MLE-bench는 75개의 다양한 카글 대회로 구성되어 있으며, 자연어 처리, 컴퓨터 비전, 신호 처리 등의 분야를 포함하고 있습니다. 이 벤치마크는 현대적 ML 엔지니어링 과제에 적합한 문제를 선별하여 AI 에이전트의 능력을 평가합니다.

**실험 및 결과:**
오픈소스 기반의 구조물과 함께 사용된 최첨단 언어 모델들이 MLE-bench에서 의미 있는 점수를 기록했습니다. 특히, 오픈AI의 o1-preview는 AIDE라는 구조물과 함께 평균적으로 16.9%의 메달을 획득했습니다.

**제약사항 및 한계:**
데이터셋의 오염 가능성과 부정행위 탐지, 그리고 ML 연구로서 벤치마크의 한계점이 논의됩니다. 또한, AI 에이전트의 연구 수행 능력을 어떻게 더 잘 측정할 수 있을지에 대한 연구가 필요하다는 점이 강조됩니다.

**결론 및 향후 연구 방향:**
MLE-bench는 더 높은 수준의 모델을 데이터 과학과 머신러닝 엔지니어링 과제로 활용하도록 하는 데 기여할 것입니다. AI 연구 능력을 이해하는 데 있어 중요한 역할을 할 것으로 기대됩니다.

**2. 전체 요약:**

이 논문은 AI 에이전트가 머신러닝 엔지니어링 작업을 평가하는 새로운 벤치마크인 MLE-bench를 소개합니다. MLE-bench는 카글의 75개 대회를 기반으로 하여, AI 에이전트의 자율성을 인간 수준과 비교할 수 있습니다. 이는 AI 에이전트가 실제 ML 엔지니어링 과제를 얼마나 잘 수행할 수 있는지를 평가하는 데 초점을 맞추고 있습니다. 오픈소스 구조물과 결합된 최신 언어 모델들이 MLE-bench에서 우수한 성과를 내면서, AI 연구는 이를 통해 더 깊이있는 발전을 이루게 될 것입니다. 이 논문은 MLE-bench의 데이터셋 오염 문제와 연구 수행 능력의 평가에 대한 향후 연구 방향을 제안합니다.