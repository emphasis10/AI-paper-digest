# Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention
## TL;DR
## Summary
- [https://arxiv.org/pdf/2404.07143.pdf](https://arxiv.org/pdf/2404.07143.pdf)

이 논문은 "Infini-attention"이라는 새로운 주의 기법을 도입하여 Transformer 기반 대형 언어 모델(LLMs)이 무한히 긴 입력을 처리할 수 있도록 하는 방법에 대해 소개합니다. 주요 내용은 다음과 같습니다:

1. **서론**: 현재의 Transformer 및 LLMs는 주의 메커니즘의 특성 상 제한된 컨텍스트 의존 메모리를 가지고 있습니다. 이 연구는 압축 메모리를 통합하여 기존의 주의 메커니즘을 확장하는 Infini-attention을 제안하며, 이를 통해 무한한 길이의 입력 처리를 가능하게 합니다.

2. **방법**: Infini-attention은 로컬(지역적) 주의와 글로벌(장기적) 주의 메커니즘을 단일 Transformer 블록 내에서 구현합니다. 이는 기존 Transformer-XL의 방식을 개선하여, 이전 세그먼트의 주의 상태를 버리지 않고 압축 메모리에 저장하여 전체 컨텍스트 기록을 유지합니다.

3. **실험**: 이 연구는 긴 컨텍스트 언어 모델링, 1M 길이 패스키 컨텍스트 블록 검색, 500K 길이 책 요약 작업 등 여러 벤치마크에서 Infini-Transformer 모델의 효과를 입증합니다. 결과적으로, Infini-Transformer는 기존 모델을 능가하며 메모리 크기 측면에서 114배의 압축 비율을 달성합니다.

4. **관련 작업**: 압축 메모리와 관련된 이전 연구를 검토하며, Infini-attention이 기존 방식과 어떻게 다른지를 설명합니다. 특히, 이는 인간의 뉴런 플라스티시티에서 영감을 받은 접근 방식을 사용하며, Transformer의 KV 메모리 배열과 달리 계산 효율성을 위해 일정 수의 메모리 매개변수를 유지합니다.

5. **결론**: Infini-attention을 통한 이러한 접근 방식은 LLMs가 메모리와 계산 자원에 대한 경계를 설정하면서도 무한한 컨텍스트를 처리할 수 있도록 합니다. 이는 긴 컨텍스트 언어 모델링 벤치마크 및 책 요약 작업에서 우수한 성능을 보이며, 특히 1B 모델은 5K 시퀀스 길이 패스키 인스턴스에 대해 미세 조정되었을 때 1M 길이 문제를 해결할 수 있는 가능성을 보여줍니다.

이 연구는 LLMs의 확장성과 유연성을 크게 향상시키는 새로운 주의 기법인 Infini-attention을 통해, Transformer 모델이 무한한 길이의 입력을 효과적으로 처리할 수 있는 방법을 제시합니다. 이는 LLMs가 보다 다양하고 복잡한 문제를 해결할 수 있는 기반을 마련합니다.

## References
- [효과적인-Attention-매커니즘-infini-attention-의-Code-리뷰](https://hyun941213.tistory.com/m/entry/효과적인-Attention-매커니즘-infini-attention-의-Code-리뷰)