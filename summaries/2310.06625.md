# iTransformer: Inverted Transformers Are Effective for Time Series Forecasting
## TL;DR
## Summary
- [https://arxiv.org/pdf/2310.06625.pdf](https://arxiv.org/pdf/2310.06625.pdf)

### Introduction

이 논문은 트랜스포머 기반 예측기의 구조에 대해 재고하고, 이를 개선하여 iTransformer를 제안합니다. 트랜스포머가 시계열 예측에서 적절히 사용되지 못하고 있는 점을 지적하며, 기존의 구조가 다변량 시계열 예측에 적합하지 않다고 주장합니다. 이를 해결하기 위해 시계열 데이터를 독립적인 변수 토큰으로 임베딩하고, 주의 메커니즘을 통해 다변량 상관관계를 캡처합니다.

### Related Work

기존의 트랜스포머 기반 시계열 예측기는 네 가지 카테고리로 나뉩니다: 
1. 주의 메커니즘의 최적화
2. 시계열 데이터의 내재적 처리
3. 구성 요소와 구조의 재설계
4. iTransformer와 같은 역전된 구조

### iTransformer

iTransformer는 트랜스포머의 기본 구성 요소를 수정하지 않고, 이를 역전된 차원에서 사용합니다. 이를 통해 각 시계열을 독립적인 변수 토큰으로 임베딩하고, 주의 메커니즘을 사용하여 다변량 상관관계를 캡처합니다.

#### Structure Overview

iTransformer는 인코더 전용 아키텍처를 채택하며, 임베딩, 투영 및 트랜스포머 블록을 포함합니다. 각 시계열 데이터를 독립적으로 임베딩하고, 주의 메커니즘을 통해 다변량 상관관계를 캡처한 후, 피드포워드 네트워크를 통해 시계열 표현을 학습합니다.

#### Inverted Transformer Components

iTransformer는 레이어 정규화, 피드포워드 네트워크, 주의 메커니즘을 역전된 차원에서 재구성합니다. 레이어 정규화는 각 변수의 시계열 표현에 적용되며, 피드포워드 네트워크는 각 변수 토큰의 시계열 표현을 인코딩하고 디코딩합니다. 주의 메커니즘은 다변량 상관관계를 캡처하여 각 변수 토큰의 상호작용을 촉진합니다.

### Experiments

iTransformer는 여러 실제 시계열 데이터셋에서 우수한 성능을 보였습니다. 실험 결과, iTransformer는 트랜스포머 기반 예측기와 비교하여 뛰어난 성능과 일반화 능력을 보여주었습니다. 특히, 다변량 상관관계를 명확히 해석할 수 있는 장점이 있습니다.

### Conclusion and Future Work

iTransformer는 트랜스포머의 구조를 역전시켜 다변량 시계열 예측에서의 성능을 크게 향상시킵니다. 앞으로는 대규모 사전 학습과 더 많은 시계열 분석 작업에 대해 연구할 계획입니다.

---

### 종합 요약

이 논문은 트랜스포머 기반 시계열 예측기의 구조적 한계를 지적하고, 이를 개선한 iTransformer를 제안합니다. iTransformer는 트랜스포머의 기본 구성 요소를 수정하지 않으면서, 이를 역전된 차원에서 사용하여 다변량 상관관계를 효과적으로 캡처하고, 시계열 데이터를 독립적인 변수 토큰으로 임베딩합니다. 실험 결과, iTransformer는 다양한 실제 시계열 데이터셋에서 뛰어난 성능을 보였으며, 다변량 상관관계를 명확히 해석할 수 있는 장점을 지니고 있습니다. 앞으로 대규모 사전 학습과 더 많은 시계열 분석 작업에 대한 연구가 기대됩니다.