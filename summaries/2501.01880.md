# Long Context vs. RAG for LLMs: An Evaluation and Revisits
## TL;DR
## Summary
- [https://arxiv.org/pdf/2501.01880.pdf](https://arxiv.org/pdf/2501.01880.pdf)

### 1. 각 섹션 주요 내용 요약

#### 1. 서론
대규모 언어 모델(LLM)은 개방형 질문 응답에서 강력한 성능을 보여주고 있지만, 실시간 정보 부족과 도메인 특정 지식 결여 등의 문제에 직면해 있다. 이 문제를 해결하기 위해 외부 메모리를 활용하여 신뢰할 수 있는 데이터 소스를 제공하는 두 가지 방식인 긴 맥락LC(Long Context) 모델과 검색증강생성RAG(Retrieval-Augmented Generation) 모델을 소개한다.

#### 2. 긴 맥락 LLMs
많은 연구가 입력 및 출력 윈도우를 확장하여 더 많은 컨텍스트를 수용할 수 있도록 하고 있으며, 두 가지 차원(모델 역량 및 콘텍스트 길이)에 중점을 두고 있다. 최근 모델들은 32K 이상의 긴 컨텍스트를 처리할 수 있는 능력이 증가하고 있다.

#### 3. 연구 결과 및 분석
연구 결과, 전반적으로 LC가 RAG보다 질문 응답 벤치마크에서 우수한 성능을 보였으며, 특히 위키백과 기반 질문에서 그 차이가 두드러졌다. 반면, RAG는 대화 기반 질문과 일반 질문에서 장점을 보였다. 이러한 통찰력은 향후 LLM 최적화 방향을 제시한다.

#### 4. 전략 비교
장단점 비교를 통해 LC와 RAG의 성능을 탐색한다. LC는 잘 구조화된 긴 문서에서 우수하며, RAG는 보다 일반적이고 파편화된 정보를 처리하는 데 유리하다.

### 2. 전체 요약
이 논문은 대규모 언어 모델의 긴 맥락 처리 및 검색 증강 생성을 비교 분석하여 각 접근법의 장단점을 제시하고, LLM 향상을 위한 향후 연구 방향을 모색한다. 긴 맥락 모델이 특정 컨텍스트 기반 질문에 대해 특히 우수한 성과를 거두는 반면, 검색 기반 접근은 대화형 질문과 같은 다른 질문 유형에서 이점을 가진다. 이 연구는 두 가지 전략의 최적화 가능성을 명시하며, 이를 통해 LLM의 실용성과 적용 가능성을 높이는 데 기여할 것으로 기대된다.