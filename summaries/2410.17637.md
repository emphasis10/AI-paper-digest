# MIA-DPO: Multi-Image Augmented Direct Preference Optimization For Large Vision-Language Models
## TL;DR
## Summary
- [https://arxiv.org/pdf/2410.17637.pdf](https://arxiv.org/pdf/2410.17637.pdf)

## 1. 각 섹션의 중요 내용 요약

### 1.1 서론
이 논문은 대규모 비전-언어 모델(LVLM)이 사람의 시각적 선호를 예측하는 방법에 중점을 두고 있습니다. 특히, 다중 이미지를 처리하는 능력을 개선하기 위해 기존의 단일 이미지 중심의 방법들이 가진 한계를 극복하고 있습니다. MIA-DPO라는 방식을 제안하여 다중 이미지 데이터의 다양성 부족과 고비용 주석 문제를 해결합니다.

### 1.2 관련 연구
LVLM은 시각적 및 텍스트 데이터를 통합하여 인간과의 상호작용을 더욱 직관적이고 매끄럽게 만드는 데 큰 기여를 하고 있습니다. 하지만, 다중 이미지 작업에 대한 데이터와 방법에 대한 연구는 여전히 진전이 느려 MIA-DPO 프레임워크가 이를 개선하기 위해 고안되었습니다.

### 1.3 방법론
MIA-DPO는 주의를 기반으로 한 선택 메커니즘을 통해 다중 이미지 환각을 탐지하고 해결합니다. 이는 인간의 주석 없이 다중 이미지 시나리오에 효율적으로 대처할 수 있는 방법을 제공합니다.

### 1.4 실험 결과
다중 이미지 벤치마크와 단일 이미지 벤치마크에서 MIA-DPO는 기존 방법들에 비해 성능이 크게 향상되었습니다. 평균적으로 LLaVA-v1.5에서는 3.0%, InternLM-XC2.5에서는 4.3% 성능이 향상되었으며, 단일 이미지 인식 능력을 유지합니다.

### 1.5 결론
MIA-DPO는 다중 이미지 이해 능력을 크게 개선하는 데 효과적이며, 단일 이미지 작업에서도 고유한 성능을 유지합니다. 따라서, 다양한 멀티모달 작업에 응용 가능성이 높습니다.

## 2. 전체 요약
MIA-DPO는 시각적 선호 정렬을 통해 대규모 비전-언어 모델의 다중 이미지 처리 능력을 크게 개선합니다. 이 방법론은 인간 주석이나 외부 API에 의존하지 않으면서도 효과적으로 다중 이미지 환각을 탐지 및 해결합니다. 실험 결과, MIA-DPO는 여러 LVLM 구조에서의 다중 이미지 벤치마크 성능을 향상시키며 단일 이미지 인식 능력도 유지합니다. 이는 다중 이미지 시나리오에 최적화된 효율적이고 저렴한 데이터 구축 방법을 제안한 점에서 혁신적이라고 할 수 있습니다.