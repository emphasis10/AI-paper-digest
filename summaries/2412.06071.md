# KaSA: Knowledge-Aware Singular-Value Adaptation of Large Language Models
## TL;DR
## Summary
- [https://arxiv.org/pdf/2412.06071.pdf](https://arxiv.org/pdf/2412.06071.pdf)

### 1. 논문의 각 섹션 요약

#### 소개 (Introduction)
이 논문은 대형 언어 모델(LLM)의 파라미터 효율적 튜닝(PEFT)의 한계점을 해결하기 위해 지식 인식 특이값 적응(KaSA) 방법을 제안합니다. KaSA는 특이값 분해(SVD)를 사용하여 작업에 관련된 지식만을 동적으로 활성화하는 방법으로, 기존 방법들이 노이즈나 무관한 지식을 다루지 못해 성능이 저하되는 문제를 보완합니다.

#### 배경 및 문제 정의 (Background and Problem Statement)
기존 PEFT 방법은 대형 언어 모델을 특별한 작업에 맞게 최적화할 때 모든 모델 파라미터를 갱신해야 하며, 이는 높은 계산 비용과 메모리 사용을 초래합니다. KaSA는 이러한 문제를 최소화하기 위해 최소한의 파라미터만 갱신하는 방법을 채택하여 효율성을 높입니다.

#### 지식 인식 특이값 적응 (Knowledge-Aware Singular-Value Adaptation)
KaSA는 두 단계로 구성됩니다. 첫째, SVD를 기반으로 하는 지식 기반의 소음 감소를 통해 기초 모델의 노이즈와 긴 꼬리 지식을 제거하여 더 정돈된 모델을 만듭니다. 둘째, 특이값을 동적으로 조정하여 각 작업과 관련된 지식을 강조하는 방법을 사용합니다.

#### 실험 및 결과 (Experiments and Results)
KaSA는 자연어 이해(NLU), 생성(NLG), 지시 따르기, 상식 추론 등 다양한 작업에서 기존 FFT 및 14개의 PEFT 기준선을 능가한다는 점을 보여줍니다. RoBERTa와 DeBERTaV3 모델 등을 통해 실제 작업에서의 일반성을 입증합니다.

### 2. 전반적 요약
이 논문은 PEFT의 한계를 극복하기 위해 KaSA라는 혁신적인 방법을 제안합니다. 이는 특이값 분해와 지식 인식 기능을 활용하여 노이즈를 제거하고 관련 지식을 강조함으로써 모델 성능을 최적화합니다. 다양한 실험을 통해 KaSA의 우수성과 적응력을 입증하였으며, 이는 AI 분야의 발전을 위한 중요한 기여로 평가받을 수 있습니다. KaSA의 적용은 자원 제약이 있는 환경에서도 대형 모델의 효율적 사용을 가능하게 하여, AI 발전에 기여할 수 있습니다.