# BitNet a4.8: 4-bit Activations for 1-bit LLMs
## TL;DR
## Summary
- [https://arxiv.org/pdf/2411.04965.pdf](https://arxiv.org/pdf/2411.04965.pdf)

#### 1. 서론 (Introduction)
- **문제 정의**: 1-bit LLM은 계산 효율성을 극대화하면서도 기존 고정밀 모델과 유사한 성능을 제공. 하지만, 낮은 비트 수의 가중치를 사용하는 모델에서는 **활성화 값의 계산 비용**이 새로운 병목으로 작용.
- **목표**: 활성화 값을 4비트로 양자화하여 계산 효율성을 높이고, 1.58-bit 가중치를 사용하는 모델에서 성능 저하를 최소화.

---

#### 2. 주요 기여 (Main Contributions)
- **BitNet a4.8** 개발: 
  - **하이브리드 양자화와 희소화 전략** 사용.
  - 주의(attention) 및 피드포워드 네트워크(FFN) 입력에는 4비트 양자화를, 중간 상태는 8비트로 희소화.
- **효율성 증가**:
  - 4비트 INT4/FP4 커널 사용으로 추론 속도 개선.
  - 매개변수의 55%만 활성화.
  - 3비트 KV 캐시를 지원하여 대규모 LLM 추론 비용 절감.

---

#### 3. 모델 구조 (Architecture)
- 기존 BitNet b1.58의 구조를 기반으로 함.
- **활성화 값 처리**:
  - 분포 분석을 통해 입력값은 4비트로 양자화, 분포가 뾰족한 중간 값은 8비트로 희소화.
  - FFN의 활성화 값은 `Squared ReLU`와 `GLU`로 추가 희소화.
- **양자화 공식**:
  - INT4: 활성화 값을 4비트 정수로 변환.
  - INT8: 활성화 값과 가중치를 8비트로 정밀화.
  - FP4: 부동소수점 양자화를 통해 다이나믹 범위 보장.

---

#### 4. 학습 과정 (Training)
- **2단계 학습 방법**:
  1. 초기 8비트 활성화로 학습.
  2. 하이브리드 양자화와 희소화 전략으로 전환.
- **효율성**:
  - 제한된 학습 토큰 수로도 빠르게 적응.
  - 직선 추정 기법(STE)을 활용해 양자화 함수의 미분 불가능성을 극복.

---

#### 5. 실험 결과 (Experiments)
- **모델 성능 비교**:
  - BitNet a4.8은 BitNet b1.58과 유사한 성능을 유지하면서도 더 높은 추론 효율성을 달성.
  - 7B 모델 기준으로 LLaMA LLM의 고정밀 성능에 근접.
- **희소화**:
  - 활성화 매개변수의 44.5% 희소화 (7B 모델 기준).
  - 계산량 감소 효과 확인.
- **추가 실험**:
  - 다양한 양자화 기법(INT4, FP4 등)과 활성화 함수(Swish, ReLU2)를 조합하여 최적의 성능 확인.

---

#### 6. 결론 (Conclusion)
- BitNet a4.8은 **1.58-bit LLM에서 4비트 활성화 값 사용을 가능**하게 하여 추론 비용을 대폭 절감.
- 하이브리드 양자화와 희소화 전략으로 성능 손실을 최소화하면서도 효율성을 크게 향상.
- 대규모 LLM을 위한 실용적인 솔루션 제공.

---

#### 전반적인 요약
BitNet a4.8은 1-bit LLM의 한계를 극복하기 위해 개발된 모델로, 활성화 값을 4비트로 양자화하고 중간 값을 희소화함으로써 추론 효율성을 극대화합니다. 다양한 실험을 통해 LLaMA와 같은 고정밀 모델과 유사한 성능을 유지하면서도 낮은 비용으로 대규모 추론이 가능하다는 점을 입증하였습니다.