# Stronger Models are NOT Stronger Teachers for Instruction Tuning
## TL;DR
## Summary
- [https://arxiv.org/pdf/2411.07133.pdf](https://arxiv.org/pdf/2411.07133.pdf)

**1. 각 섹션의 주요 내용 및 논문의 기여와 혁신 정리:**

- **서론 (Introduction):** 
  본 논문은 명령어 튜닝에 사용되는 응답 생성기의 역할에 대해 탐구합니다. 기존에는 대형 모델이 더 좋은 교사 역할을 한다고 알려졌으나, 이 논문은 '대모델의 역설(Larger Models' Paradox)'을 도입하며 이를 비판합니다. 실험 결과, 대형 모델이 항상 더 나은 성능을 보장하지 않음을 밝혔습니다.

- **관련 연구 (Related Work):**
  명령어 튜닝에 사용되는 합성 데이터셋에 관한 과거 연구를 소개하며, 본 연구는 기존의 접근법과는 달리 응답 생성기의 호환성에 초점을 맞췄습니다. 이러한 접근이 새로운 통찰을 제공한다고 제안합니다.

- **실험 결과 (Experimental Results):**
  여러 모델을 통해 분석한 결과, Small 및 Medium 크기의 모델들이 대형 모델보다 효과적인 경우가 많았음을 발견했습니다. 이는 응답 생성기와 튜닝 대상 모델 사이의 호환성 때문이라고 설명합니다.

- **결론 및 미래 연구 (Conclusion and Future Work):**
  본 연구는 응답 생성기의 선택이 모델의 명령어 수행 능력에 미치는 영향을 강조합니다. 향후 연구에서는 데이터셋의 호환성 개선과 다양한 응답 생성기의 영향력을 탐구할 계획입니다.

**2. 전체 요약:**

이 논문은 인공지능의 명령어 튜닝에서 대형 모델이 항상 최선의 선택은 아니라는 '대모델의 역설'을 소개합니다. 다양한 크기의 모델을 통해 실험한 결과, 대형 모델이 아닌 더 작고 호환성 높은 모델이 더 나은 명령어 수행 능력을 보일 수 있음을 발견했습니다. 이에 대한 해결책으로, 호환성을 바탕으로 한 보상 모델인 'Compatibility-Adjusted Reward (CAR)'를 제안하여 모델의 효율성을 예측합니다. 전반적으로, 이 연구는 모델 선택 시 호환성의 중요성을 새롭게 조명합니다.