# The Synergy between Data and Multi-Modal Large Language Models: A Survey from Co-Development Perspective
## TL;DR
## Summary
- [https://arxiv.org/pdf/2407.08583.pdf](https://arxiv.org/pdf/2407.08583.pdf)

### 1. 요약: 각 섹션의 주요 내용

#### 1. 서론 (Introduction)
이 논문은 큰 언어 모델(LLMs)과 다중 모드 큰 언어 모델(MLLMs)의 공통된 발전 과정을 다루며, 특히 데이터 중심 접근 방식이 모델의 성능에 미치는 중요성을 강조합니다. 다중 모드 모델은 텍스트 이외의 다양한 입력을 처리할 수 있어 다양한 응용 분야에서 주요 역할을 합니다. MLLMs의 성능은 대규모 데이터와 고품질 데이터를 활용할 때 크게 향상되며, 이는 모델 개발과 데이터 개발이 상호 연관된다는 사실을 보여줍니다.

#### 2. 배경 (Background)
데이터-모델 공동 개발은 모델 성능을 최적화하는 동시에 데이터를 활용하여 이 모델을 개선하려는 접근 방식입니다. 이 논문은 다양한 다중 모드 생성 모델의 데이터를 바탕으로, 동적 훈련 데이터와 모델 간의 상호작용을 다룬 첫 번째 연구입니다.

#### 3. 다중 모드 데이터의 기여 (Multi-Modal Data Contributions for MLLMs)
MLLMs의 성능을 향상시키기 위한 기본적인 데이터 접근 방식에는 데이터 획득, 데이터 증강, 데이터 다양화, 데이터 응축, 데이터 혼합, 데이터 포장, 그리고 교차 모드 정렬이 포함됩니다. 이러한 접근 방식은 모델을 더욱 효과적으로 만들며, 더 나은 하위 집합을 만들기 위한 기술도 포함되어 있습니다. 예를 들어, 데이터 증강은 기존 데이터를 변환하거나 합성 데이터를 추가하여 데이터셋의 크기와 다양성을 확장합니다.

#### 4. MLLMs 평가 (Evaluation of MLLMs)
MLLMs를 평가하기 위한 다양한 벤치마크가 있으며, 여기에는 다중 모드 이해, 생성, 검색, 추론을 포함한 여러 평가 지표가 있습니다. 이들 벤치마크는 모델의 성능을 종합적이고 정확하게 평가하기 위한 중요한 도구입니다.

#### 5. 모델 기여 (Model Contributions for Multi-Modal Data)
모델은 데이터를 생성, 매핑, 필터링 및 평가하는 역할을 합니다. 이러한 역할을 통해 고품질의 데이터를 빠르게 생성하고 평가할 수 있습니다. 예를 들어, 모델은 복잡하고 방대한 데이터에서 중요한 정보를 자동으로 추출할 수 있습니다. 또한 모델은 데이터 시각화를 통해 이해를 돕습니다.

#### 6. 공개 데이터셋 (Public Datasets for MLLMs)
MLLMs의 개발과 평가를 위해 사용되는 다양한 공개 데이터셋이 있으며, 모델을 데이터 소스로 사용하여 만들어진 데이터셋도 포함됩니다. 이러한 데이터셋은 연구자와 개발자가 MLLMs의 성능을 지속적으로 향상시키기 위한 중요한 자원으로 작용합니다.

#### 7. 미래의 MLLMs 위한 로드맵 (Roadmap for Future MLLMs)
미래의 MLLMs 개발을 위해서는 데이터-모델 공동 개발을 지원하는 인프라가 필요합니다. 이는 대규모 데이터의 효율적인 수집, 저장, 처리 등을 지원하는 확장 가능한 데이터 관리 시스템을 포함하며, 효율적인 하드웨어와 알고리즘도 포함됩니다. 또한, 기존 데이터 중심과 모델 중심 인프라 간의 통합이 필요합니다.

### 2. 종합 요약
이 논문은 다중 모드 큰 언어 모델(MLLMs)의 개발에 있어서 데이터와 모델의 상호작용을 강조합니다. 연구자들은 고품질 데이터를 활용하여 모델 성능을 최적화하고,반대로 성능이 뛰어난 모델을 통해 더 나은 데이터를 생성하는 방법을 탐구합니다. 주요 기여는 데이터와 모델 공동 개발을 통한 MLLMs의 발전에 대한 새로운 관점을 제공하며, 관련 연구의 체계적인 리뷰와 미래의 발전을 위한 로드맵을 제안합니다. 이 논문은 MLLMs의 성능을 최적화하고 데이터의 품질을 향상시키기 위해 데이터 중심 접근 방식이 얼마나 중요한지 보여줍니다.

## Similar Papers
- [Best Practices and Lessons Learned on Synthetic Data for Language Models](2404.07503.md)
- [Data-Juicer Sandbox: A Comprehensive Suite for Multimodal Data-Model Co-development](2407.11784.md)
- [State Space Model for New-Generation Network Alternative to Transformers: A Survey](2404.09516.md)
- [On the Transformations across Reward Model, Parameter Update, and In-Context Prompt](2406.16377.md)
- [Data Mixing Made Efficient: A Bivariate Scaling Law for Language Model Pretraining](2405.14908.md)
- [Xwin-LM: Strong and Scalable Alignment Practice for LLMs](2405.20335.md)
- [MotionMaster: Training-free Camera Motion Transfer For Video Generation](2404.15789.md)
- [Mitigating Hallucinations in Large Language Models via Self-Refinement-Enhanced Knowledge Retrieval](2405.06545.md)
- [RATT: A Thought Structure for Coherent and Correct LLM Reasoning](2406.02746.md)
