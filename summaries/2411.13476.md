# When Precision Meets Position: BFloat16 Breaks Down RoPE in Long-Context Training
## TL;DR
## Summary
- [https://arxiv.org/pdf/2411.13476.pdf](https://arxiv.org/pdf/2411.13476.pdf)

이 논문은 BFloat16 정밀도를 사용할 때 Rotary Positional Embedding (RoPE)이 장문의 문맥에서의 상대적 위치 인코딩 특성이 깨지는 문제를 분석하고 있습니다. 이 문제를 해결하기 위해 새롭게 제안된 방법인 AnchorAttention을 소개하고 있습니다.

### 각 섹션 요약:

1. **소개 (Introduction):**
   - 최근 자연어 처리 분야에서는 점점 긴 시퀀스를 처리하는 모델이 등장하고 있습니다. 이런 긴 문맥을 다루기 위해 Rotary Positional Embedding (RoPE) 방식이 널리 사용됩니다. 그러나 BFloat16 포맷을 사용할 때 RoPE의 상대적 위치 인코딩 특성에 문제가 발생합니다.

2. **문제 분석 (Problem Analysis):**
   - BFloat16의 제한된 정밀도로 인해 위치 인코딩이 잘못되며, 이는 특히 시퀀스의 첫 번째 토큰에서 가장 두드러집니다. 이 문제는 학습 윈도우 크기가 커질수록 가속화되며, 모델의 성능에 심각한 영향을 미칩니다.

3. **AnchorAttention 제안 (Proposal of AnchorAttention):**
   - AnchorAttention은 모든 문서에서 첫 번째 토큰을 일관되게 앵커로 처리하여 문서 간의 불필요한 주의를 제거하고, 문맥 내에서 일관성을 유지하며, 계산 효율성을 개선하는 방법입니다.

4. **실험 결과 (Experimental Results):**
   - 제안된 AnchorAttention 방법이 장문 문맥에서의 성능을 꾸준히 개선하며, 표준 전체 주의 메커니즘에 비해 훈련 시간을 50% 이상 단축시킵니다.

5. **결론 (Conclusion):**
   - AnchorAttention은 BFloat16에서 발생하는 수치상의 문제를 완화하며 장문 문맥 기능을 증대시키고 훈련 속도를 높이는 다양한 장점을 제공합니다. 본 연구에서, AnchorAttention은 여러 LLM에서 성능을 크게 개선했습니다.

### 전체 요약:

이번 연구는 BFloat16과 사용 시 RoPE가 상대적 위치 인코딩 특성을 잃는 문제를 해결하기 위해 AnchorAttention이라는 혁신적인 방법을 제안합니다. AnchorAttention은 모든 문서의 첫 번째 토큰을 공유된 앵커로 설정하여 문서 간의 중복 주의를 제거하고, 일관된 위치 관계를 유지하면서 계산 효율성을 높입니다. 실험 결과, 이 방법은 LLM에서 장문 문맥 기능을 개선하면서도 훈련 시간을 크게 단축시킵니다. 이는 AI 연구와 발전에 중요한 기여를 할 수 있습니다.