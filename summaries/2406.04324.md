# SF-V: Single Forward Video Generation Model
## TL;DR
## Summary
- [https://arxiv.org/pdf/2406.04324.pdf](https://arxiv.org/pdf/2406.04324.pdf)

### 1. 각 섹션 요약 (중요한 내용)

#### 소개
이 논문에서는 비디오 디퓨전 모델의 샘플링 속도를 혁신적으로 향상시킬 수 있는 새로운 방법을 제안합니다. 기존 방법들이 상대적으로 느린 샘플링 속도를 가지고 있는 반면, 본 논문에서는 새로운 단일 단계 샘플링 방법을 통해 약 23배의 샘플링 속도 향상을 달성했습니다. 또한, 공간-시간 감시자를 도입하여 비디오의 품질과 모션 일관성을 개선했습니다    .

#### 관련 연구
기존의 비디오 생성 연구는 주로 적대적 훈련을 통해 고품질 이미지 생성과 일관된 모션 합성에 초점을 맞췄습니다. 최근에는 노이즈 제거 디퓨전 확률 모델을 활용하여 모델의 성능을 크게 향상시켰지만, 이러한 모델은 연산 비용이 매우 높아 실용성이 떨어집니다. 본 논문은 이러한 문제를 해결하기 위해 디퓨전 모델의 샘플링 단계를 줄이는 방법을 탐구합니다  .

#### 방법론
논문은 두 가지 주요 네트워크, 즉 생성기(Gθ)와 감시자(Dϕ)를 소개합니다. 생성기는 SVD에서 미리 학습된 UNet 모델을 기반으로 하며, 감시자는 공간과 시간 감시자 머리를 추가하여 개선한 구조를 가지고 있습니다. 공간 감시자와 시간 감시자는 각각의 다운샘플링 블록 후에 추가되며 이들의 파라미터만 훈련 중 업데이트됩니다. 이를 통해 비디오 디퓨전 모델의 단일 샘플링을 가능하게 하고, 샘플링 속도를 크게 향상시킵니다  .

#### 실험 결과
비교 결과, 제안된 방법이 기존의 SVD와 AnimateLCM 방법 보다 더 적은 단계로도 유사한 품질의 비디오를 생성할 수 있음을 확인했습니다. 생성된 비디오는 더 높은 공간적, 시간적 일관성을 가지며, 약 23배의 속도 향상을 달성했습니다. 특히, 감시자의 공간-시간 머리가 도입된 경우 더 높은 품질의 비디오를 생성할 수 있음을 실험으로 확인했습니다  .

#### 토론 및 결론
논문은 감시자에 공간-시간 머리를 도입하여 비디오의 품질을 개선한 것과, 단일 샘플링 단계로 비디오를 생성할 수 있게 한 것이 주요 기여라고 결론짓습니다. 또한, 후속 연구로는 현재 상당한 런타임을 차지하는 이미지 조건화 인코더와 VAE 디코더의 가속화를 목표로 삼고 있습니다  .

### 2. 전체 요약
이 논문은 비디오 디퓨전 모델의 단일 단계 샘플링을 가능하게 하여 기존 방법 대비 약 23배 빠른 샘플링 속도를 제공하는 혁신적인 방법을 제안합니다. 이를 위해 공간-시간 감시자를 도입하여 생성된 비디오의 품질과 모션 일관성을 크게 향상시켰습니다. 제안된 방법은 기존의 여러 단계 샘플링 방법과 비교하여 더 높은 품질과 일관성을 보여주며, 비디오 생성의 효율성을 크게 높였습니다. 후속 연구로는 이미지 조건화 인코더와 VAE 디코더의 가속화를 목표로 하여 비디오 생성의 전체적인 런타임을 더욱 줄일 계획입니다. 

이 논문의 기여와 혁신적인 부분은 비디오 디퓨전 모델의 샘플링 속도를 크게 향상시키면서도 품질을 유지한 점, 그리고 공간-시간 감시자를 도입하여 생성품질을 개선한 점에 있습니다     .