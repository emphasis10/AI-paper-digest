# AAPO: Enhance the Reasoning Capabilities of LLMs with Advantage Momentum
## TL;DR
## Summary
- [https://arxiv.org/pdf/2505.14264.pdf](https://arxiv.org/pdf/2505.14264.pdf)

1. 각 섹션의 요약:

- **소개**: 강화 학습(RL)은 대형 언어 모델(LLM)의 추론 능력을 향상시키는 강력한 방법으로 떠오르고 있습니다. 기존의 감독 학습(SFT)에 비해, 강화 학습은 체계적인 사고 체인(CoT) 추론을 효과적으로 수행할 수 있게 해줍니다.

- **관련 작업**: 최근의 LLM 포스트 트레이닝 방법들은 주로 인류가 라벨링한 선호 데이터 및 보상 모델에 의존하여 응답을 평가합니다. 전통적인 방법들은 상당한 GPU 자원을 소비하며, 강화 학습을 통한 그룹 상대 이점 추정 방법이 이를 개선하는데 기여합니다.

- **사전 조건**: 기존 GRPO(그룹 상대 정책 최적화) 알고리즘이 제안되었습니다. 이는 그룹 내 샘플의 상대 장점을 추정하여, RL 훈련에서 요구되는 불필요한 자원 소비를 줄입니다.

- **이점-증가 정책 최적화(AAPO)**: 제안된 AAPO는 기존의 GRPO의 제약을 극복하기 위해, 모멘텀 기반 이점 추정 방식을 통합하여 추론 능력을 강화합니다.

- **실험 설정 및 결과**: 다양한 수학적 추론 벤치마크에서 AAPO는 그룹 상대 이점 추정보다 향상된 최적화 성능을 보여줍니다. AAPO는 실험 결과, 모델의 응답 질을 지속적으로 향상시킵니다.

- **결론**: 이번 연구는 강화 학습 알고리즘의 최적화 동작을 심층 분석하고 AAPO 알고리즘을 제안하여 수학적 추론 벤치마크에서 우수한 성능을 달성합니다. 그러나 높은 컴퓨팅 자원 및 긴 훈련 시간이 주된 제한점으로 남아 있습니다.

2. 전체 요약:

이 논문은 강화 학습을 통해 대형 언어 모델의 추론 능력을 강화하는 방법을 제안합니다. 기존의 그룹 상대 이점 추정 기법의 한계를 극복하고자, 이점 모멘텀을 통합한 AAPO 알고리즘을 도입합니다. 다양한 수학적 추론 벤치마크 실험 결과, AAPO는 우수한 최적화 성능을 보여주었습니다. 다만, 상당한 연산 자원과 긴 훈련 시간이 요구됩니다..