# BiLLM: Pushing the Limit of Post-Training Quantization for LLMs
## TL;DR
## Summary
- [https://arxiv.org/pdf/2402.04291.pdf](https://arxiv.org/pdf/2402.04291.pdf)

### 섹션별 요약

#### 1. 서론
최근 트랜스포머 기반의 대규모 언어 모델(LLM)이 자연어 처리에서 큰 관심을 받고 있습니다. 이러한 LLM은 우수한 성능을 보여주지만, 대규모 파라미터와 높은 계산 요구 사항 때문에 메모리 제약이 있는 장치에 배치하는 데 어려움이 있습니다. 이를 해결하기 위해 모델 양자화 기술이 사용되며, 특히 Post-Training Quantization (PTQ)은 빠르고 실용적인 양자화 방법으로 주목받고 있습니다.

#### 2. 관련 연구
LLM 양자화의 최근 연구 동향은 Quantization-Aware Training (QAT)와 Post-Training Quantization (PTQ)로 나뉩니다. QAT는 모델의 성능을 더 잘 보존하기 위해 재훈련을 포함하나 비용이 많이 듭니다. 반면 PTQ는 재훈련 없이 빠르게 양자화를 수행하여 시간과 자원을 절약합니다.

#### 3. BiLLM의 제안
BiLLM은 1비트 후처리 양자화 프레임워크로, 구조적으로 중요한 가중치를 선택하고 잔여 근사화 방법을 통해 양자화 오류를 최소화합니다. 중요한 가중치는 Hessian 기반 메트릭을 사용하여 선택하고, 나머지 비중요 가중치는 최적의 분할 전략으로 이진화됩니다. 이 방법은 LLM이 극저비율 양자화를 수행할 수 있게 하여 고성능을 유지합니다.

#### 4. 실험 결과
BiLLM은 여러 LLM 패밀리에서 최고 성능을 달성하였으며, 1.07~1.11 비트의 평균 비율로 양자화를 수행했습니다. LLaMA2-70B 모델에서는 1.08 비트로 비교적 낮은 당혹도(perplexity) 8.41을 기록, FP16 기반의 OPT-66B 모델을 능가하는 성과를 보였습니다.

#### 5. 결론
BiLLM은 LLM의 1비트 양자화 프레임워크로, 구조적으로 중요한 가중치를 선택하고 나머지 가중치를 최적의 방식으로 이진화하여 고성능을 유지합니다. 이 방법은 LLM의 배치를 용이하게 하고, 특히 메모리 제약이 있는 장치에서의 활용 가능성을 높입니다.

### 전체 요약
BiLLM은 대규모 언어 모델(LLM)의 후처리 양자화를 위한 획기적인 1비트 양자화 프레임워크입니다. 이 방법은 LLM이 극저비율 비율에서도 높은 정확도를 유지할 수 있도록 설계되었습니다. 주요 기법으로는, Hasian 메트릭을 이용한 중요한 가중치 선택과 최적의 분할 전략을 통한 비중요 가중치의 이진화가 있습니다. 실험 결과, BiLLM은 기존의 최첨단 양자화 방법을 능가하는 성능을 보였고, LLM의 실제 배치를 더욱 용이하게 만들었습니다.

## Similar Papers
- [Accurate LoRA-Finetuning Quantization of LLMs via Information Retention](2402.05445.md)
- [OneBit: Towards Extremely Low-bit Large Language Models](2402.11295.md)
- [Fast Matrix Multiplications for Lookup Table-Quantized LLMs](2407.10960.md)
- [How Good Are Low-bit Quantized LLaMA3 Models? An Empirical Study](2404.14047.md)
- [LLaMA Pro: Progressive LLaMA with Block Expansion](2401.02415.md)
- [QLLM: Accurate and Efficient Low-Bitwidth Quantization for Large Language Models](2310.08041.md)
- [FlexGen: High-Throughput Generative Inference of Large Language Models with a Single GPU](2303.06865.md)
- [Extreme Compression of Large Language Models via Additive Quantization](2401.06118.md)
- [ShiftAddLLM: Accelerating Pretrained LLMs via Post-Training Multiplication-Less Reparameterization](2406.05981.md)
