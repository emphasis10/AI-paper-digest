# Accelerating Large Language Models through Partially Linear Feed-Forward Network
## TL;DR
## Summary
- [https://arxiv.org/pdf/2501.10054.pdf](https://arxiv.org/pdf/2501.10054.pdf)

### 1. 각 섹션의 중요 내용 요약 (한국어)

**1. 서론**
이 논문은 아주 큰 언어 모델(LLM)의 배포 문제를 해결하기 위해 새로운 압축 기법인 TARDIS를 제안합니다. LLM은 거대한 파라미터 수 때문에 활용에 어려움을 겪고 있으며, 기존의 압축 기술들은 일반적으로 높은 압축 비율에서 정확도가 크게 떨어지는 문제를 안고 있습니다.

**2. 배경 및 동기**
LLM의 추론 과정을 설명하며, 파라미터의 대규모로 인해 발생하는 성능 병목 현상을 강조합니다. 기존의 압축 기법들을 검토하고 이들의 한계를 드러냅니다. 특히, FFN(Feed-Forward Network) 블록이 전체 모델에서 가장 많은 파라미터를 차지하고 있어 이곳의 최적화가 필요하다고 언급합니다.

**3. TARDIS 설계**
TARDIS는 비선형 활성화 함수를 선형 함수로 부분적으로 근사화하여 파라미터 수를 줄이는 방법을 구현하고 있습니다. 이를 통해 FFN 블록 내의 파라미터를 이론적으로 최대 87.5% 줄일 수 있음을 보여줍니다.

**4. 구현 및 평가**
TARDIS의 성능을 다양한 LLM에 적용하여 실험한 결과, FFN 파라미터를 최대 80%까지 감소시키면서 기존 pruning 방법들보다 더 높은 정확도를 기록합니다. 특히, TARDIS는 vLLM 시스템에서 1.6배의 속도 향상을 달성했습니다.

**5. 결론**
TARDIS는 FFN의 비선형 활성화 함수에 대한 새로운 압축 접근 방식으로, LLM의 파라미터를 크게 줄이는 동시에 성능을 유지합니다. 본 연구는 AI 모델의 효율성을 크게 향상시킬 수 있는 잠재력을 보여줍니다.

**주요 기여와 혁신 요소**
- TARDIS는 비선형 활성화 함수를 선형 함수로 부분 근사화하여 LLM의 파라미터 수를 효과적으로 줄이는 방법을 제안합니다.
- FFN 블록의 파라미터를 80% 줄이면서도 높은 정확도를 유지하며, 기존 방법들에 비해 상당한 속도 향상을 이룹니다.

### 2. 전체 요약 (한국어)

이 논문은 TARDIS라는 새로운 LLM 압축 기법을 제안하여 비선형 활성화 함수를 부분적으로 선형 함수로 근사화함으로써 FFN 블록의 파라미터 수를 대폭 줄이고, 모델의 성능을 유지하는 방법을 찾았습니다. TARDIS는 다양한 실험 결과에서 뛰어난 정확도가 입증되었으며, 기존의 pruning 기법보다 우수한 성능을 올리며, 1.6배의 속도 향상을 달성했습니다. 이로써 LLM의 효율성을 크게 개선할 수 있는 새로운 접근 방식을 제시하고 있습니다.