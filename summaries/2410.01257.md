# HelpSteer2-Preference: Complementing Ratings with Preferences
## TL;DR
## Summary
- [https://arxiv.org/pdf/2410.01257.pdf](https://arxiv.org/pdf/2410.01257.pdf)

1. 각 섹션 요약

   - **서론**: 이 논문은 보상 모델이 AI 및 기계 학습에서 전면적인 언어 모델을 지시를 따르기 위해 중요한 역할을 한다고 설명합니다. 기존의 두 가지 보상 모델 접근 방식인 Bradley-Terry 스타일과 회귀 스타일을 비교하여, 이들의 특징과 각 접근 방식의 강점을 탐구합니다.

   - **데이터셋**: 논문에서는 HelpSteer2 데이터셋에 대한 의견을 수집하며, 이는 Bradley-Terry 스타일 훈련을 위해 설계된 선호 주석을 포함합니다. 이런 데이터는 회귀 스타일의 기존 평점을 보완하고, 사람 작성의 정당화와 함께 제공됩니다.

   - **실험 설계 및 결과**: SteerLM 회귀와 Bradley-Terry 스타일 모델을 비교하며, 각 유형의 모델이 다소 다른 목표에 최적화되어 사용될 수 있음을 보여줍니다. Bradley-Terry 모델은 상대적 점수 필터링에 적합하고, 회귀 모델은 속성 기반 예측에 더 유리합니다.

   - **주요 기여 및 혁신**: 이 논문은 선호 방향, 강도 및 정당화를 포함하는 데이터셋을 처음으로 공개하는 데 그치지 않고, Bradley-Terry와 회귀 스타일의 보상 모델을 결합한 새로운 접근 방식을 제시합니다. 이 모델은 RewardBench에서 최고 성능을 입증했고, RLHF에서 지시를 따르는 모델을 효과적으로 맞추는 데 유용함을 보여줍니다.

2. 전체 요약

   이 논문은 AI와 기계 학습 분야 내에서 다양한 보상 모델의 효과를 탐구하며, Bradley-Terry와 회귀 스타일의 모델 각각의 장단점을 데이터 분석을 통해 비교합니다. 논문은 HelpSteer2라는 새로운 데이터셋을 통해 이 두 접근 방식을 통합한 혁신적인 방법론을 제안하며, 이로 인해 보상 모델의 성능이 크게 향상됩니다. 이러한 기여는 AI 모델이 사용자 지시를 보다 안전하고 효율적으로 따르도록 지원합니다.