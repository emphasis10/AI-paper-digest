# VITA: Towards Open-Source Interactive Omni Multimodal LLM
## TL;DR
## Summary
- [https://arxiv.org/pdf/2408.05211.pdf](https://arxiv.org/pdf/2408.05211.pdf)

### 1. 각 섹션의 주요 내용 요약

#### Introduction 
VITA는 비디오, 이미지, 텍스트, 오디오 등 다양한 모달리티를 동시에 처리하고 분석할 수 있는 최초의 오픈소스 멀티모달 대형 언어 모델입니다. GPT-4o와 같은 상업 모델이 우수한 성능을 보여 주었지만, 오픈소스 커뮤니티에서는 그러한 수준에 도달하지 못했습니다. VITA는 이러한 간극을 메우기 위해 설계되었으며 자연스러운 인간-컴퓨터 상호작용을 가능하게 합니다.

#### Related Work 
기존의 GPT, LLaMA, Vicuna와 같은 언어 모델을 기반으로 구축된 다양한 멀티모달 모델들이 존재합니다. 하지만 대다수는 이미지-텍스트 모달리티에 집중하며 사용자 상호작용 기능은 상대적으로 부족합니다. VITA는 비디오, 이미지, 텍스트, 오디오 모달리티를 모두 처리할 수 있는 성능을 자랑합니다.

#### VITA
VITA는 Mixtral 8×7B라는 언어 기반 모델을 확장하여 다국어 및 다능력 이해를 향상시켰습니다. 3단계 훈련 프로세스를 통해 개발되었으며, 첫 번째 단계는 언어 모델 지침 튜닝, 두 번째 단계는 멀티모달 정렬, 세 번째 단계는 멀티모달 지침 튜닝입니다.

- **LLM Instruction Tuning**:
  다국어를 이해할 수 있도록 언어 모델의 단어 집합을 확장했습니다.
- **Multimodal Alignment**:
  텍스트와 다른 모달리티 간의 표현 차이를 극복하기 위해 멀티모달 데이터를 대량 수집하여 텍스트 특성 공간을 시각, 이미지, 오디오와 정렬합니다.
- **Multimodal Instruction Tuning**:
  모델이 텍스트나 오디오 지침을 따르고 이미지나 비디오를 이해할 수 있게 합니다.

#### Development with Duplex Pipeline
VITA는 중복 배포 스키마를 도입하여 하나의 모델이 사용자 쿼리에 응답하는 동안 다른 모델이 환경 소리를 실시간으로 추적합니다. 이와 같은 중복 배포는 정보의 연속성을 유지하므로, VITA는 대화가 중단되었을 때 빠르게 새로운 쿼리에 대응할 수 있습니다.

#### Evaluation
미세조정된 VITA 모델은 여러 모달리티에서 우수한 성능을 보이며, 비디오 이해, 이미지 이해 및 오디오 처리에서 상위 모델들과 경쟁합니다. 또한 VITA의 비음성 상호작용과 오디오 인터럽트 상호작용 기능은 새로운 기준을 제시합니다.

#### Conclusion and Future Work
VITA는 비디오, 이미지, 텍스트 및 오디오 이해를 통합하는 강력한 오픈소스 MLLM이며, 향상된 상호작용 경험을 제공합니다. 기초 기능을 더 강화하고 실시간 상호작용을 개선해야 할 필요가 있지만, VITA는 오픈소스 커뮤니티에 중요한 기여를 하고 있습니다.

### 2. 전체 요약

VITA는 비디오, 이미지, 텍스트, 오디오 등 다양한 모달리티를 동시에 처리하고 분석할 수 있는 최초의 오픈소스 멀티모달 대형 언어 모델입니다. 주요 기여는 다음과 같습니다:

1. **강력한 멀티모달 성능**: VITA는 기존 오픈소스 모델보다 뛰어난 비디오, 이미지, 텍스트, 오디오 처리 능력을 갖추고 있습니다.
2. **향상된 상호작용**: 비음성 상호작용과 오디오 인터럽트 상호작용이 가능하여 사용자 경험을 대폭 개선합니다.
3. **오픈소스 기여**: 모델, 학습 코드를 포함한 모든 것을 오픈소스로 공개하여 연구 커뮤니티의 발전에 기여합니다.

향후 VITA는 기초 기능을 더욱 강화하고 실시간 상호작용 성능을 개선하는 방향으로 발전할 예정입니다. 이를 통해 오픈소스 멀티모달 모델의 표준을 정립하는 것을 목표로 하고 있습니다.