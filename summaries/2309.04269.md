# From Sparse to Dense: GPT-4 Summarization with Chain of Density Prompting
## TL;DR
## Summary
- [https://arxiv.org/pdf/2309.04269.pdf](https://arxiv.org/pdf/2309.04269.pdf)

### 제1장: 서론
최근 몇 년간 자동 요약 기술은 크게 발전했으며, 이는 라벨이 있는 데이터셋에서 감독받은 미세 조정 방식에서 뛰어난 언어 모델(LLM)을 사용하는 제로샷 프롬프트 방식으로의 패러다임 전환 덕분입니다. 요약에서 중요한 한 측면은 정보의 밀도입니다. 요약은 원본 텍스트의 압축본이므로 이론적으로 더 높은 정보 밀도를 가져야 합니다. 본 논문에서는 GPT-4가 생성한 점점 밀도가 높은 요약 세트에 대한 인간의 선호를 조사하여 이 한계를 규명하고자 합니다.

### 제2장: 밀도 사슬 프롬프트
`Chain of Density (CoD)` 프롬프트는 요약을 점진적으로 정보 밀도가 높게 만들어줍니다. 초기 요약은 엔티티가 적게 포함된 상태로 생성되고, 매 반복마다 길이를 유지하면서 누락된 중요한 엔티티를 통합합니다. 이를 위해 요약은 추상화, 융합, 압축을 통해 기존 내용을 재구성합니다. 이를 통해 요약은 점점 추상적이고 융합된 형태가 됩니다.

### 제3장: 통계 및 인간 평가
CoD 요약본은 엔티티 밀도, 추상성, 융합률, 콘텐츠 분포와 같은 통계적 수치를 통해 그 특징을 분석하였습니다. CoD 요약은 이름 그대로 많은 엔티티를 포함하며, 중간 단계의 요약본이 가장 인기를 끌었습니다. 인간 평가에서는 엔티티 밀도가 높은 요약이 좀 더 선호되었습니다. GPT-4 기반 평가에서는 더 높은 밀도의 요약이 정보 전달력 면에서 더 나은 것으로 평가되었습니다.

### 제4장: 결과와 논의
요약의 밀도를 높이는 것이 정보 전달의 효율성을 높이는 데 효과적이지만, 너무 많은 엔티티를 포함하면 가독성과 일관성이 떨어질 수 있습니다. 중간 단계의 CoD 요약이 정보 밀도와 가독성의 균형을 가장 잘 이루는 것으로 나타났습니다.

### 제5장: 관련 연구
GPT를 활용한 요약 연구는 주로 뉴스 기사 요약에서 인간 작성 요약과 비교해 높은 평가를 받아왔습니다. 또한, 엔티티 기반의 요약 방식은 요약의 충실도와 평가 단위로 활용되었습니다.

### 제6장: 결론
본 연구는 요약의 정보 밀도가 인간의 요약 선호도에 미치는 영향을 탐구했습니다. 인간 평가와 자동 평가 결과, 중간 정도의 밀도가 유지된 요약본이 가장 높은 평가를 받았습니다. 이 연구를 통해 앞으로 요약의 고정 길이와 가변 밀도에 대한 연구가 더욱 진행될 수 있을 것입니다.

---

### 전체 요약
이 논문은 자동 요약에서 정보 밀도의 중요성을 탐구합니다. `Chain of Density (CoD)` 프롬프트를 사용해 GPT-4가 생성한 요약본을 분석하였으며, 요약의 정보 밀도를 높이는 것이 정보 전달 면에서 유리하지만, 너무 많은 엔티티를 포함할 경우 가독성과 일관성이 떨어질 수 있음을 발견했습니다. 중간 정도의 밀도가 유지된 요약본이 정보 전달력과 가독성의 균형을 가장 잘 이루는 것으로 나타났으며, 이는 인간 평가와 GPT-4의 자동 평가 모두에서 입증되었습니다. 이 연구 결과는 향후 요약 기술 발전에 중요한 기초 자료가 될 것입니다.

## Similar Papers
- [LLM See, LLM Do: Guiding Data Generation to Target Non-Differentiable Objectives](2407.01490.md)
- [Towards Robust Evaluation: A Comprehensive Taxonomy of Datasets and Metrics for Open Domain Question Answering in the Era of Large Language Models](2406.13232.md)
- [Summary of a Haystack: A Challenge to Long-Context LLMs and RAG Systems](2407.01370.md)
- [AgentInstruct: Toward Generative Teaching with Agentic Flows](2407.03502.md)
- [ClashEval: Quantifying the tug-of-war between an LLM's internal prior and external evidence](2404.10198.md)
- [Show, Don't Tell: Aligning Language Models with Demonstrated Feedback](2406.00888.md)
- [HelpSteer2: Open-source dataset for training top-performing reward models](2406.08673.md)
- [Quiet-STaR: Language Models Can Teach Themselves to Think Before Speaking](2403.09629.md)
- [Reinforcement Retrieval Leveraging Fine-grained Feedback for Fact Checking News Claims with Black-Box LLM](2404.17283.md)
