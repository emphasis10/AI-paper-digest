# Bootstrapping Language Models with DPO Implicit Rewards
## TL;DR
## Summary
- [https://arxiv.org/pdf/2406.09760.pdf](https://arxiv.org/pdf/2406.09760.pdf)

### 1. 섹션별 내용 요약

#### 1.1 서론
DPO(Direct Preference Optimization)는 인간 피드백을 통한 강화 학습을 대체할 수 있는 방법으로, 보상 모델 학습의 복잡성을 줄이고 쉽게 구현할 수 있습니다. 본 논문에서는 DPO 학습 후에 제공되는 암묵적 보상 모델을 사용하여 언어 모델을 추가로 향상시킬 수 있는지 탐구합니다.

#### 2 관련 연구
- 자기 개선 미세 조정: 소량의 인간 주석으로 언어 모델을 미세 조정하는 방법을 탐구한 연구들.
- 선호 조정 중 정책 샘플링: DPO와 그 변형이 오프라인 데이터 집합에서 빠르게 과적합되므로, 온라인 피드백이 성능 향상에 도움이 됨을 제시.
- DPO 암묵적 보상: DPO 모델의 암묵적 보상 파라미터화를 통해 빔 검색을 사용하여 추론 품질을 향상시키는 연구.

#### 3 기초
- 인간 피드백을 통한 강화 학습에서는 언어 모델을 정책으로 다루고, 파라미터화된 보상 모델을 사용하여 이를 최적화합니다.
- DPO는 인간 피드백 데이터에서 직접 최적화된 모델을 학습하는 목적식을 통해 이 정책 모델을 학습합니다.

#### 4 DPO 암묵적인 보상을 통한 부트스트랩핑
- 부트스트랩핑을 통해 자기 정렬을 추가로 개선할 수 있는 기회를 제공합니다.
- 길이 조정 보상 형성 및 경험 재생 적용.
- 온라인 샘플링과 오프라인 샘플링의 효과 분석 및 이론적 분석 제시.

#### 5 실험 결과
- Zephyr와 Llama3 기반 모델로 실험을 수행하여 DICE의 성능을 평가합니다.
- 경험 재사용 비율이 가장 좋은 성능을 제공하며, Zephyr 설정에서 γ=0.5가 적절함을 발견.

#### 6 한계 및 미래 연구
- DPO 암묵적 보상 모델이 잘 학습되지 않을 경우 훈련 파이프라인 붕괴 가능성 등 한계 제시.
- 모델 정책의 지속적 개선을 위한 방법론 연구 필요성 강조.

#### 7 결론
DICE는 DPO 암묵적 보상 모델을 활용하여 인간 선호도에 맞게 LLM을 추가로 정렬하는 새로운 접근법을 소개합니다. 이 방법은 소수의 파라미터로도 높은 성능을 발휘하며, 추가적인 인간 주석이나 외부 보상 모델 없이도 실용적이고 접근 가능함을 보여줍니다.

### 2. 전체 요약

본 논문에서는 DPO(Direct Preference Optimization)를 사용하여 언어 모델(LLM)을 효율적으로 정렬할 수 있는 방법을 제시합니다. DPO는 기존의 인간 피드백을 통한 강화 학습보다 간단히 구현 및 훈련이 가능하며, 훈련 후 암묵적 보상 모델을 제공합니다. DICE(Self-Alignment with DPO Implicit Rewards)는 이 암묵적 보상 모델을 활용하여 모델의 자기 정렬을 더욱 향상시키는 새로운 접근법입니다. 길이 조정 보상 형성 및 경험 재생을 통해, 추가적인 인간 피드백 없이 뛰어난 성능을 발휘합니다. 실험 결과, DICE는 적은 파라미터로도 높은 성과를 보여, 실용적이고 접근 가능한 방법임을 입증하였습니다.