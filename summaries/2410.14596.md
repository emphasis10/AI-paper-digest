# Teaching Models to Balance Resisting and Accepting Persuasion
## TL;DR
## Summary
- [https://arxiv.org/pdf/2410.14596.pdf](https://arxiv.org/pdf/2410.14596.pdf)

파일에 대한 요약은 다음과 같습니다.

### 1. 서론 및 문제 제기
이 논문은 대형 언어 모델(LLM)들이 설득에 취약할 수 있으며, 이것이 정보의 왜곡에 대한 위협이 될 수 있음을 강조합니다. 특히 모델이 오직 부정적인 설득에 저항하도록 만 조정될 경우 긍정적인 설득을 수용하는 역량이 부족하여 성능이 저하될 수 있음을 지적합니다. 이에 대한 해결책으로 Persuasion-Balanced Training (PBT)을 제시하며, 이를 통해 모델이 부정적인 설득에 저항하면서도 긍정적인 설득을 수용할 수 있도록 훈련합니다.

### 2. PBT 방법론
PBT는 대화의 다른 가능성을 연구함으로써 각 응답의 효과를 평가하는 멀티 에이전트, 트리 기반 패러다임을 활용하여 훈련 데이터를 생성합니다. 이를 통해 긍정적 및 부정적 설득 모두에 대한 데이터를 얻어 RLHF(선형 회귀를 통한 보상 강화 학습) 목표 아래 모델을 훈련할 수 있습니다.

### 3. 실험 설정
모델들에 대한 실험은 Mistral-7B와 Llama 3.1-8B, 70B를 대상으로 하였으며, 성능 측정은 오차율, 맞추기 정확도, 긍정적 및 부정적 설득에 대한 저항력 등을 포함합니다. 저항 훈련만 받은 모델들과 비교하여 PBT가 어떤 성능상의 이점을 가져오는지 평가합니다.

### 4. 결과
- 부정적 설득에 대한 저항: PBT는 저항 훈련을 받은 모델과 비교했을 때 왜곡된 정보의 수용률을 줄이는 데 효과적이었으며, 결과적으로 플립플라우핑의 빈도를 낮추고 정확도를 높였습니다.
- 긍정적 및 부정적 설득의 균형 잡기: PBT 훈련된 모델은 균형 잡힌 데이터에서 가장 높은 정확도를 보였습니다. 반면에 저항만 훈련된 모델은 긍정적 설득을 수용하는 데 취약한 모습을 보였습니다.
- 협업 능력: 더 강한 모델과 더 약한 모델이 함께 팀을 이루었을 때 PBT는 성능을 더 안정적으로 만들고 강한 모델이 약한 모델을 끌어올리는 데 도움을 줍니다.

### 5. 결론
LLMs는 설득에 쉽게 노출될 수 있는 반면, 그 설득이 유익할 때 이를 수용하는 것이 중요한 요소임을 발견했습니다. PBT를 통해 생성된 설득 데이터를 통해 모델을 조정함으로써, 모델이 더 안전하게 정보 왜곡에 저항하면서도 올바른 정보를 받아들일 수 있게 하였습니다.

---

### 종합 요약
이 논문에서는 대형 언어 모델의 설득 저항 능력을 향상시키는 데 있어 PBT(Persuasion-Balanced Training)의 역할을 조사하였습니다. 모델이 단순히 부정적인 설득을 피하도록 훈련될 경우 긍정적인 설득조차 허용하지 않는다는 문제점을 제기하고, 이를 해결하기 위해 긍정적 및 부정적 설득을 동시에 수용하고 저항하도록 훈련하는 방법을 제안합니다. 이 방법은 모델이 부정적 설득으로부터 안전하게 하면서도 유익한 설득을 수용함으로써 과거보다 더 안정적이고 정확하게 작동하도록 돕습니다. 이 연구는 미래의 AI 모델들이 대화형 인터페이스에서 더 나은 성능을 보여줄 수 있도록 강화하는 데 기여할 것입니다.