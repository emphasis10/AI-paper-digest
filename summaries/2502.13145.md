# Multimodal Mamba: Decoder-only Multimodal State Space Model via Quadratic to Linear Distillation
## TL;DR
## Summary
- [https://arxiv.org/pdf/2502.13145.pdf](https://arxiv.org/pdf/2502.13145.pdf)

1. **각 섹션 주요 내용 요약**

- **서론**: 이 논문은 대형 언어 모델(LLMs)의 발전과 그것이 다중모달 이해, 즉 시각과 텍스트 정보의 동시에 처리하는 능력으로 확장되는 과정에 대해 소개합니다. 특히 비전 언어 모델(VLMs)의 출현과 이들이 시각적 이해 능력을 향상시키는 방법을 설명합니다.

- **관련 연구**: 현재 LLM 기반 VLM들은 인코더, 디코더 구조로 나뉘어 있으며, 특히 디코더 방식 VLM의 도입을 통해 성능을 극대화하려는 시도를 기술합니다.

- **문제점과 목표**: 기존의 Transformer 기반 VLM이 가진 계산 복잡성과 메모리 사용의 문제점을 지적하며, 이를 해결하기 위한 선형 복잡성 VLM을 개발할 필요성을 강조합니다.

- **기술 혁신**: 이 논문에서는 Transformer의 주의를 Mamba로 변환하는 독특한 초기화와 증류 방법을 제시합니다. 이를 통해 선형 복잡성 Mamba-2 기반 VLM으로의 전환을 실현하고, 다중모달 대화 능력도 보존하도록 합니다.

- **주요 기여**: 세 가지 중요한 기여를 제시합니다: 
  1) 다중모달 상태 공간 모델 구축을 위한 혁신적인 증류 방법론 제안, 
  2) 선형 복잡성과 하이브리드 아키텍처를 가진 최초의 디코더 전용 다중모달 상태 공간 모델 제시, 
  3) 실험 결과를 통해 다양한 비전-언어 과업에서 기존 모델 대비 성능과 효율성을 대폭 개선했습니다.

2. **전체 요약**

이 논문은 선형 복잡성의 다중모달 상태 공간 모델을 개발하여 다중모달 언어 모델의 계산 효율성을 높이는 것을 목표로 합니다. 이를 위해 Transformer 기반의 주의 메커니즘을 Mamba-2 상태 공간 모델 메커니즘으로 변환하는 새로운 형태의 증류 방법을 제안하였습니다. 이러한 접근은 선행 학습된 RNN 기반의 LLM에 의존하지 않으며, 다중모달 이해 능력을 향상시키고, 적은 학술적 자원을 필요로 합니다.