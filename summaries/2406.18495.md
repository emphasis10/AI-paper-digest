# WildGuard: Open One-Stop Moderation Tools for Safety Risks, Jailbreaks, and Refusals of LLMs
## TL;DR
## Summary
- [https://arxiv.org/pdf/2406.18495.pdf](https://arxiv.org/pdf/2406.18495.pdf)

### 논문 요약

**제목:** WILDGUARD: Open One-stop Moderation Tools for Safety Risks, Jailbreaks, and Refusals of LLMs

**저자:** Allen Institute for AI, University of Washington, Seoul National University

#### Introduction 및 Abstract
- **목적:** 현대 대형 언어 모델(LLM)의 상호 작용에서 나타나는 다양한 안전성 문제를 해결하기 위해 WILDGUARD라는 다목적 중재 도구를 소개.
- **주요 기능:** 
   1. 사용자의 악의적 의도 감지.
   2. 모델 응답의 안전성 위험 탐지.
   3. 모델의 거부율 계산.
- **혁신성:** WILDGUARD는 기존 GPT-4와 유사하거나 우수한 성능을 모방하며, 오픈 소스 툴에서 가장 높은 성능을 보임.

#### Section 1: The Status Quo of Safety Moderation Tools for LLMs
- **현재 상황:** 여러 개방형 및 폐쇄형 도구를 사용하여 대형 언어 모델의 안전성 문제를 중재하지만, 많은 도구가 다양한 공격 시나리오에서 한계를 보임.
- **발견:** 
   - 악의적인 프롬프트를 탐지하는 데 기존 도구는 적절한 성능을 발휘하지 못함.
   - 모델 응답의 거부를 감지하는 데 어려움을 겪음.

#### Section 2: Building WILDGUARDMIX and WILDGUARD
- **데이터셋 구성:** WILDGUARDMIX는 다양한 출처의 92,000개 이상의 라벨링된 예제를 포함하는 다목적 안전성 중재 데이터셋.
  - **원천:** 합성 데이터, 실세계 사용자 상호 작용 데이터, 기존 주석 작성자가 작성한 데이터 등.
- **데이터 생성:** GPT-4를 이용한 고도화된 프롬프트 생성 및 오류 분석을 통해 모델 정확도 개선.
- **검증:** 인간 주석자의 검토가 포함된 엄격한 라벨링 과정.

#### Section 3: Training and Evaluation
- **모델 훈련:** WILDGUARD는 Mistral-7b-v0.3 모델을 기반으로 다목적 훈련을 진행.
- **평가 기준:** 열 가지 공공 벤치마크와 WGTEST를 이용하여 모델 성능을 평가.
- **결과:** 모든 평가 집합에서 GPT-4를 포함한 이전의 모든 개방형 모델을 능가, 특히 공격성 프롬프트의 경우 3.9% 향상된 성능을 보임.

#### Section 4: Conclusion
- **결론:** WILDGUARD는 대형 언어 모델의 다양한 안전성 문제를 해결하기 위한 통합 다목적 툴키트로, 오픈 소스 및 상용 툴과 비교하여 탁월한 성능을 입증. WILDGUARD의 공개는 앞으로 더 안전한 LLM 응용 프로그램 개발을 촉진할 것임.
- **향후 연구:** 향상된 투명성과 신뢰성을 바탕으로 한 오픈 소스 생태계 발전기여.

---

### 전반적인 요약

이 논문은 대형 언어 모델(LLM)의 안전성 문제를 해결하기 위한 WILDGUARD라는 새로운 도구를 소개합니다. WILDGUARD는 악의적인 사용자 프롬프트를 감지하고, 모델 응답의 안전성을 평가하며, 모델의 거부율을 측정하는 기능을 제공합니다. 다양한 출처의 데이터를 포함하여 훈련된 WILDGUARD는 기존의 모든 개방형 모델을 능가하는 성능을 보았습니다. 또한, GPT-4와 유사하거나 더 나은 성능을 입증하며, 앞으로 LLM의 응용과 연구에 있어 중요한 도구로 자리 잡을 것으로 예상됩니다. 

이 연구는 LLM의 안전을 강화하고, 보다 안전하고 신뢰할 수 있는 AI 응용 프로그램을 개발하는 데 기여할 것입니다.