# LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models
## TL;DR
## Summary
- [https://arxiv.org/pdf/2310.08659](https://arxiv.org/pdf/2310.08659)

### 요약

#### 1. 소개 (Introduction)
이 논문은 대형 언어 모델(LLM)의 양자화와 LoRA 미세 조정을 동시에 적용할 때 발생하는 성능 격차를 해결하기 위해 LoftQ라는 새로운 양자화 프레임워크를 제안합니다. LoftQ는 양자화된 모델과 원래 고정밀 모델 간의 불일치를 줄여 다운스트림 작업에서의 일반화를 크게 향상시킵니다.

#### 2. 배경 (Background)
사전 학습된 언어 모델은 자연어 이해(NLU)와 자연어 생성(NLG) 작업에서 뛰어난 성능을 보이지만, 수백만에서 수십억 개의 매개변수를 가지고 있어 막대한 계산 및 메모리 요구 사항이 있습니다. 양자화는 이러한 모델의 저장 요구를 줄이는 중요한 기술이며, LoRA(저랭크 어댑터)는 고정밀 모델에 효율적인 미세 조정 방법으로 사용됩니다. 그러나, 양자화와 LoRA 미세 조정을 함께 사용할 때 성능 저하가 발생할 수 있습니다.

#### 3. 방법론 (Methodology)
LoftQ는 양자화된 가중치와 저랭크 어댑터를 사용하여 원래 고정밀 가중치를 근사화합니다. 이를 위해 다음과 같은 두 가지 주요 기술을 사용합니다:
1. **LoRA-Aware Quantization**: 양자화된 가중치와 저랭크 어댑터를 함께 최적화하여 LoRA 미세 조정의 초기화를 개선합니다.
2. **Alternating Optimization**: 양자화와 특이값 분해(SVD)를 번갈아가며 적용하여 원래의 고정밀 가중치와의 불일치를 최소화합니다.

#### 4. 실험 (Experiments)
LoftQ는 자연어 이해, 질의 응답, 요약 및 자연어 생성 작업에서 기존의 양자화 방법보다 우수한 성능을 보였습니다. 특히, 2비트와 2/4비트 혼합 정밀도 환경에서 뛰어난 성능을 발휘했습니다. 다양한 모델(DeBERTaV3, BART, LLAMA-2)을 대상으로 실험을 수행했으며, 모든 설정에서 QLoRA보다 높은 성능을 기록했습니다.

#### 5. 결론 (Conclusion)
LoftQ는 LLM의 양자화와 LoRA 미세 조정에서 발생하는 불일치를 줄여 성능을 향상시키는 새로운 양자화 프레임워크입니다. 다양한 자연어 처리 작업에서 뛰어난 성능을 보였으며, 특히 저비트 양자화 환경에서 높은 성능을 발휘했습니다. 이 논문은 LoftQ가 기존의 방법보다 일관되게 우수한 성능을 제공함을 입증하였으며, 향후 연구에서는 더 큰 모델과 데이터셋에 적용하는 것을 제안합니다.

### 전체 요약
LoftQ는 대형 언어 모델의 양자화와 LoRA 미세 조정을 함께 사용할 때 발생하는 성능 저하 문제를 해결하기 위해 개발된 새로운 양자화 프레임워크입니다. 이 방법은 양자화된 가중치와 저랭크 어댑터를 최적화하여 원래 고정밀 가중치와의 불일치를 줄여 성능을 향상시킵니다. 다양한 자연어 처리 작업에서 실험을 통해 LoftQ의 우수성을 입증하였으며, 특히 저비트 양자화 환경에서 뛰어난 성능을 발휘했습니다. LoftQ는 기존의 QLoRA 방법보다 일관되게 우수한 성능을 보이며, 더 큰 모델과 데이터셋으로 확장할 가능성이 큽니다.