# Value-Incentivized Preference Optimization: A Unified Approach to Online and Offline RLHF
## TL;DR
## Summary
- [https://arxiv.org/pdf/2405.19320.pdf](https://arxiv.org/pdf/2405.19320.pdf)

#### 1. 서론
**강화 학습에서 인간 피드백(RLHF)을 통한 대형 언어 모델(LLM)의 조정**
- **RLHF의 두 가지 주요 구성 요소**:
  1. **보상 모델링**: 인간 선호도 순위를 정량적 보상 함수로 변환하여 정책 개선 안내.
  2. **RL 파인튜닝**: LLM 출력을 인간 선호도에 맞추기 위해 보상 함수를 사용하여 정책 조정.

**중요성**: 대규모 및 고품질의 선호 데이터 수집은 비용이 많이 들고 시간이 많이 소요되며, 사용 가능한 선호 데이터에 따라 오프라인과 온라인 RLHF로 나뉨.

#### 2. 기초
**브래들리-테리 모델**:
- 인간이 두 후보 응답 중 하나를 선택하는 방식으로 선호도를 모델링.
- 모델 수식: P(y1 ≻ y2|x) = exp(r*(x, y1)) / (exp(r*(x, y1)) + exp(r*(x, y2)))

#### 3. 가치 유인 선호 최적화 (VPO)
**기본 개념**:
- **VPO의 목적**: 불확실성 하에서의 낙관적/비관적 원칙을 구현하여 온라인 및 오프라인 RLHF 모두에서 LLM 정책을 직접 최적화.
- **낙관주의와 비관주의**:
  - **온라인**: 최고 가치를 내는 응답을 향한 보상 추정치 최대 가능도 추정(MLE) 정규화.
  - **오프라인**: 최소 가치를 내는 응답을 피하는 보상 추정치 MLE 정규화.
- **보상 모델링**:
  - 보상 MLE: r_MLE = argmin_r ℓ(r,D)
  - KL-정규화 값 함수: J(r, π) = E_x~ρ, y~π(·|x) [r(x, y)] - β E_x~ρ [KL(π(·|x) ∥ π_ref(·|x))]

#### 4. 실험
**VPO의 실험적 검증**:
- **텍스트 요약 및 대화 작업**: VPO의 실용성 및 효율성 확인.
- **성과**: 다양한 모델(예: Llama2-13b-chat, Flan-T5-xl)에서 VPO가 다른 기존 방법보다 성능이 뛰어남.

#### 5. 결론 및 토론
**핵심 요점**:
- VPO는 이론적으로나 실질적으로 온라인 및 오프라인 RLHF에 대한 낙관적/비관적 원칙을 성공적으로 구현.
- 향후 연구 방향: α 선택에 대한 적응 규칙 조사 및 π_cal 선택에 대한 정교한 분석.

### 종합 요약
이 논문은 인간 피드백을 통해 강화 학습을 하는 과정에서 LLM을 조정하는 혁신적인 방법인 가치 유인 선호 최적화(VPO)를 제안합니다. VPO는 불확실성 하에서 낙관적 및 비관적 원칙을 구현하여 온라인 및 오프라인 상황 모두에서 LLM 정책을 직접 최적화합니다. 이 방법은 기존의 불확실성 추정 기법의 복잡성을 피하면서도 이론적 보장과 실용성을 동시에 제공합니다. 실험 결과, VPO는 텍스트 요약 및 대화 작업에서 우수한 성능을 보였으며, 이는 이 방법의 유용성을 입증합니다. 

이 논문은 강화 학습과 인간 피드백을 통한 언어 모델의 조정에 있어 중요한 발전을 이룬 연구로, 이 방법의 이론적 배경과 실험적 검증을 통해 앞으로의 연구 및 실용적 응용 가능성을 제시합니다.

## Similar Papers
- [Understanding the performance gap between online and offline alignment algorithms](2405.08448.md)
- [Direct Preference Optimization: Your Language Model is Secretly a Reward Model](2305.18290.md)
- [Bootstrapping Language Models with DPO Implicit Rewards](2406.09760.md)
- [Self-Exploring Language Models: Active Preference Elicitation for Online Alignment](2405.19332.md)
- [From $r$ to $Q^*$: Your Language Model is Secretly a Q-Function](2404.12358.md)
- [WPO: Enhancing RLHF with Weighted Preference Optimization](2406.11827.md)
- [Multi-property Steering of Large Language Models with Dynamic Activation Composition](2406.17563.md)
- [RLHF Workflow: From Reward Modeling to Online RLHF](2405.07863.md)
- [Dataset Reset Policy Optimization for RLHF](2404.08495.md)
