# Optimizing Large Language Model Training Using FP4 Quantization
## TL;DR
## Summary
- [https://arxiv.org/pdf/2501.17116.pdf](https://arxiv.org/pdf/2501.17116.pdf)

### 1. 섹션별 요약

#### 서론
이 논문은 대형 언어 모델(LLM)의 훈련에서 요구되는 계산량을 줄이기 위한 효율적인 방법으로 4비트(FP4) 정밀도를 사용하는 프레임워크를 제안합니다. FP8 정밀도가 가능하다는 것이 입증되었으나, FP4는 수치 오류와 표현력이 떨어지는 한계가 있습니다. 이 연구는 두 가지 주요 혁신인 미분 가능한 양자화 추정기와 아웃라이어 클램핑 및 보상 전략을 도입하여 이 문제를 해결합니다.

#### 이론적 배경
IEEE 754 표준에 따라 이진 부동소수점 숫자는 부호, 지수 비트 및 가수 비트로 구성됩니다. 본 연구에서는 FP4 형식을 E2M1로 정의하여 2비트의 지수와 1비트의 가수를 사용합니다. 이는 낮은 정밀도의 수치 표현에서 올바른 양자화를 달성하기 위한 기반이 됩니다.

#### 방법론
논문에서 제안하는 방법은 크게 두 가지로, 가중치의 경우 미분 가능한 양자화 추정기를 제공하고, 활성화의 경우 아웃라이어 클램핑 및 보상 전략을 사용하여 훈련 중 발생하는 오류를 줄입니다. 이를 통해 모델의 성능을 높이면서도 훈련 효율성을 유지할 수 있습니다.

#### 실험 결과
제안된 FP4 프레임워크는 BF16과 FP8에서 훈련된 모델과 비교할 때 동등한 정확도를 보여주며, 최대 13B 파라미터와 100B 샘플로 훈련을 수행한 결과도 포함되어 있습니다. 아울러, 모델의 제로샷 평가에서 FP4로 훈련된 모델이 BF16 모델과 경쟁할 수 있는 성과를 보였습니다.

#### 한계
주요 한계는 FP4 텐서 코어 지원이 부족하여 성능 측정이 직접적으로 불가능하다는 점입니다. 현재 실험은 FP4 시뮬레이션에 의존하고 있으며, 이로 인해 계산 오버헤드가 추가되고 실행 시간이 증가합니다.

#### 결론
이 논문은 현대 대형 언어 모델을 위한 FP4 사전 훈련 프레임워크를 처음으로 제안하며, 수치 정확도를 유지하면서도 훈련의 효율성을 향상시키는 방법론을 제공합니다. 향후 연구는 극초저정밀 컴퓨팅을 위한 양자화 기법 개선과 차세대 하드웨어 설계에 좋은 기초를 마련할 수 있습니다.

### 2. 전반적인 요약
이 논문은 대형 언어 모델 훈련에서의 계산 효율성을 높이기 위해 FP4 정밀도를 활용하는 혁신적인 프레임워크를 제안합니다. 가중치와 활성화의 양자화 오류를 줄이는 두 가지 주요 방법론을 소개하며, FP4 프레임워크가 BF16 및 FP8 모델과 비교할 때 거의 동일한 정확도를 달성함을 실험적으로 입증합니다. 이러한 접근 방식은 에너지 절약과 환경 지속 가능성을 촉진함으로써 AI 발전에 기여할 수 있는 가능성을 보여줍니다.