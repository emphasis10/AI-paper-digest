# Groma: Localized Visual Tokenization for Grounding Multimodal Large Language Models
## TL;DR
## Summary
- [https://arxiv.org/pdf/2404.13013.pdf](https://arxiv.org/pdf/2404.13013.pdf)

### 논문 요약

#### 1. 서론
이 논문에서는 다중 모달 대규모 언어 모델(MLLM)의 시각적 정보에 대한 지역적 이해 능력을 강화하기 위해 Groma 모델을 소개합니다. Groma는 이미지를 관심 영역으로 분할하고 이를 통합하여 사용자의 지시사항과 모델의 응답에 연결하는 지역 토큰화 메커니즘을 사용합니다.

#### 2. 연구 배경
현존하는 MLLM은 이미지 전반에 대한 이해는 가능하지만, 구체적인 지역에 대한 정밀한 위치 지정과 이해 능력은 제한적입니다. 이러한 한계를 극복하기 위해, Groma는 이미지 내 특정 지역을 정밀하게 식별하고 그 지역의 정보를 언어 모델과 통합할 수 있는 능력을 갖추고 있습니다.

#### 3. Groma의 설계
Groma는 시각적 입력을 통해 관심 있는 지역을 식별하고 이를 지역 토큰으로 변환하는 과정을 거칩니다. 이러한 지역 토큰은 사용자의 질문이나 지시에 따라 모델이 응답을 생성할 때 참조됩니다. 또한, Groma는 지역 토큰을 통해 텍스트 출력을 이미지에 구체적으로 연결시킬 수 있어, 보다 정밀한 시각적 이해가 가능합니다.

#### 4. 실험 및 결과
Groma는 다양한 시각적 참조 및 정밀 위치 지정 벤치마크에서 우수한 성능을 보였습니다. 특히, 지역 토큰을 활용하여 생성된 답변이 시각적 맥락에 근거하여 매우 정확하게 생성됨을 입증했습니다.

#### 5. 결론
Groma는 이미지 내 특정 지역에 대한 깊은 이해를 가능하게 함으로써, MLLM의 활용 범위를 대폭 확장합니다. 이를 통해 사용자는 보다 정밀한 정보를 요구하는 다양한 시나리오에서 MLLM을 효과적으로 사용할 수 있게 됩니다.

### 종합적인 요약
Groma는 이미지의 구체적인 지역을 정밀하게 식별하고 분석할 수 있는 능력을 갖춘 새로운 형태의 다중 모달 대규모 언어 모델입니다. 이 모델은 특히 이미지 내 특정 부분에 대한 질문에 답하거나, 이미지의 구체적인 내용을 설명하는 데 특화되어 있습니다. Groma의 개발은 이미지와 언어를 연결하는 AI 기술의 발전에 중요한 기여를 하며, 다양한 실용적 응용 가능성을 제시합니다.