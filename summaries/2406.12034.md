# Self-MoE: Towards Compositional Large Language Models with Self-Specialized Experts
## TL;DR
## Summary
- [https://arxiv.org/pdf/2406.12034.pdf](https://arxiv.org/pdf/2406.12034.pdf)

### 1. 각 섹션 요약 (Korean)

#### 1. 소개 (Introduction)
이 논문은 Self-MoE라는 접근 방식을 소개하며, 이는 큰 언어 모델(LLM)을 자기-전문화 전문가들(MiXSE)로 구성된 모듈 시스템으로 변환합니다. 이 방법은 자생성 합성 데이터를 사용해 전문가 모듈을 구축하고, 자기 최적화된 라우팅을 통합하여 다양한 목표 작업을 효율적으로 처리할 수 있게 합니다. 이는 인간이 라벨링한 데이터나 추가 매개변수 없이 LLM의 전반적인 기능을 향상시킵니다.

#### 2. 문제 정의 (Problem Statement)
본 연구는 최소한의 자원과 추가 매개변수 없이 LLM의 목표 기능을 개선하는 방법을 중점적으로 다룹니다. 전통적으로 모놀리식 구조인 LLM은 전문화된 데이터를 요구하여 적응성과 확장성이 제한됩니다. Self-MoE의 모듈화, 구성 접근 방식을 통해 자생성 합성 데이터를 활용하여 자가 개선을 실현하고자 합니다.

#### 3. 방법론 (Method: Self-MoE)
Self-MoE는 특화된 전문가 모듈과 라우팅 컴포넌트를 학습하여 상호 협력하는 구성 모델을 구축하는 프레임워크입니다. 이에 따라 자가-전문화된 각각의 전문가 모듈과 이를 기반으로 하는 MiXSE를 설명합니다. 각 전문가 모듈은 LoRA를 통해 가벼우면서도 목표 기능을 위한 합성 데이터를 사용해 자가-전문화됩니다.

#### 4. 구현 세부 사항 (Implementation Details)
이 연구에서는 기본 LLM으로 Gemma-7B를 사용하여 실험을 진행했으며, Self-MoE를 다양한 모델에 적용했습니다. 각 도메인에서 5,000개의 합성 데이터를 생성하고, 실험에서는 20,000개의 데이터를 사용했습니다. 이는 전체 MiXSE 모델의 추가 매개변수는 약 1%에 불과하도록 했습니다.

#### 5. 주요 결과 및 성과 (Main Results)
Benchmark 결과에 따르면 Self-MoE는 다양한 목표 도메인에 걸쳐서 기본 LLM보다 우수한 성능을 나타냈습니다. MiXSE는 특히 인스턴스 병합 및 가중치 병합과 같은 다른 강력한 기법들을 능가하며, 더 나은 유연성과 해석 가능성을 제공했습니다.

#### 6. 결론 (Conclusion)
Self-MoE는 인간이 라벨링한 데이터에 의존하지 않고 모듈화, 자가-개선된 전문가 시스템인 MiXSE를 구축하여 목표 기능, 적응성 및 해석 가능성을 향상시킵니다. 본 연구는 모놀리식 모델의 한계를 극복하고, 효율적이고 확장 가능한 시스템을 위한 중요한 방향을 제시합니다.

### 2. 전체 요약 (Korean)

이 논문은 기존의 모놀리식 대형 언어 모델(LLM)의 한계를 극복하기 위해 Self-MoE를 제안합니다. Self-MoE는 자가-전문화된 전문가 모듈(MiXSE)을 활용하여 다양한 작업을 효율적으로 처리하는 모듈 시스템으로, 인간이 라벨링한 데이터 없이도 LLM의 전반적인 성능을 향상시킵니다. 각 모듈은 자생성 합성 데이터를 통해 가벼우면서도 자기 최적화된 라우팅을 갖추어, 목표 도메인에서의 동적이고 정확한 처리를 가능하게 합니다. 이를 통해 다양한 벤치마크에서 기본 LLM을 능가하는 성과를 보였으며, 특히 다른 병합 방법보다 유연성과 해석 가능성이 뛰어나다는 점에서 장점을 가집니다. Self-MoE는 인간 데이터 라벨링의 필요성을 줄이면서도 확장 가능한 시스템을 구축하는 데 중요한 방향성을 제시합니다.