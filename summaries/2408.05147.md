# Gemma Scope: Open Sparse Autoencoders Everywhere All At Once on Gemma 2
## TL;DR
## Summary
- [https://arxiv.org/pdf/2408.05147.pdf](https://arxiv.org/pdf/2408.05147.pdf)

### 1. 섹션별 요약

#### 1.1 서론

이 논문에서는 "드문 자동인코더(Sparse Autoencoders, SAEs)"가 언어 모델의 내부 활성화를 분석하고 해석 가능한 특징으로 분해하기 위한 유망한 비지도 학습 방법으로 소개됩니다. 높은 훈련 비용 때문에 산업 외의 연구 적용은 제한적입니다. 이를 해결하기 위해 "Gemma Scope"를 도입하였으며, 이는 모든 계층과 하위 계층에서 SAEs를 훈련시키고 공개함으로써 더 야심 찬 해석 연구를 촉진하려는 목적이 있습니다.

#### 1.2 드문 자동인코더들

드문 자동인코더(SAEs)는 언어 모델의 활성화를 분해하고 재구성하는 데 사용되며, 특정 의미를 가지는 벡터들의 조합으로 분해됩니다. 이 논문에서는 JumpReLU SAEs를 사용하여 더 효율적인 훈련 및 높은 재구성 충실도를 달성했습니다. 또한, 이들 SAEs는 훈련한 모델의 성능을 여러 메트릭을 통해 평가하여 공개합니다.

#### 1.3 JumpReLU SAEs

JumpReLU SAEs는 등장 활성화 함수와 제곱 에러 및 L0 정규화를 사용하여 훈련되었으며, 이로 인해 특정 토큰에서 활성화되는 변수가 달라집니다. 이러한 방식을 통해 다양한 활성화 조건에서 좋은 재구성 능력을 유지할 수 있습니다.

#### 1.4 훈련 절차

Gemma Scope의 훈련 과정은 복잡하고 자원이 많이 들어가는 작업이었습니다. 약 2000개의 SAE가 각기 다른 희소성 수준으로 훈련되었습니다. 이를 통해 효율적이고 스케일 가능한 SAEs를 구현하는 데 성공했습니다.

#### 1.5 성능 평가

여러 가지 메트릭을 사용하여 SAEs의 성능이 평가되었습니다. 여기에는 재구성된 활성화의 정도와 이로 인해 발생하는 언어 모델의 손실 증가 등을 포함합니다. 추가적으로, 다양한 폭과 희소성 설정에서의 SAE 성능을 분석하였습니다.

---

### 2. 논문의 기여 및 혁신적 부분

이 논문의 주요 기여는 "Gemma Scope"라는 포괄적인, 개방형 JumpReLU SAEs를 도입한 것입니다. Gemma Scope는 모든 계층과 하위 계층에서 SAEs를 훈련하고, 이를 공개함으로써 AI 커뮤니티 내에서 심도 있는 해석 연구를 촉진합니다. 이로 인해 더 야심 찬 AI 안전 및 해석 연구가 가능해졌습니다.

---

### 3. 종합 요약

이 논문은 드문 자동인코더(SAEs)라는 비지도 학습 방법론을 통해 언어 모델의 내부 활성화를 해석 가능하게 만드는 데 중점을 두었습니다. 고비용의 훈련 문제를 해결하기 위해, "Gemma Scope"라는 포괄적인 JumpReLU SAEs의 모음을 도입하고 공개하였습니다. 이를 통해 더 많은 연구자들이 SAEs를 활용한 해석 연구에 접근할 수 있게 되었으며, 다양한 계층과 희소성 설정에서의 SAE 성능을 평가하여 효율적이고 스케일 가능한 방식으로 구현하였습니다. 주요 기여는 이러한 자원을 AI 커뮤니티에 개방하여 더 깊이 있는 연구를 가능하게 만든 점에 있습니다.