# Mamba: Linear-Time Sequence Modeling with Selective State Spaces
## TL;DR
## Summary
- [https://arxiv.org/pdf/2312.00752.pdf](https://arxiv.org/pdf/2312.00752.pdf)

이 논문은 "Mamba: Linear-Time Sequence Modeling with Selective State Spaces"로서, 새로운 종류의 선택적 상태 공간 모델을 소개하고, 이를 이용한 시퀀스 모델링 방법을 제안합니다. 이 모델은 트랜스포머 아키텍처의 비효율성을 극복하고, 선형 시간 복잡도로 긴 시퀀스를 효율적으로 처리할 수 있는 새로운 구조를 제공합니다.

### 주요 내용 요약

1. **서론 및 배경**:
   - 기존의 대규모 시퀀스 모델, 특히 트랜스포머는 정보 처리 능력이 뛰어나지만, 긴 시퀀스에서 계산 비용이 매우 큽니다. 이를 해결하기 위해, 선택적 상태 공간 모델(SSM)이 제안됩니다.

2. **선택적 SSM의 구조**:
   - 선택적 SSM은 입력에 따라 모델의 동작을 조정할 수 있는 메커니즘을 갖추고 있습니다. 이는 불필요한 정보를 걸러내고 필요한 정보만을 기억하는 데 도움을 줍니다.

3. **하드웨어 인식 알고리즘**:
   - 효율적인 계산을 위해, 선택적 SSM은 하드웨어 인식 알고리즘을 사용하여 GPU 메모리 계층에서 효율적으로 상태를 처리합니다.

4. **Mamba 아키텍처**:
   - 선택적 SSM을 기반으로 하는 단순화된 신경망 아키텍처인 Mamba는 MLP 블록 없이 구현되어 있습니다. 이는 학습과 추론에서 높은 성능을 제공합니다.

5. **성능 평가**:
   - Mamba는 언어, 오디오, 유전자 등 다양한 데이터 모달리티에서 최고의 성능을 보여주며, 특히 언어 모델링에서는 같은 크기의 트랜스포머보다 우수한 결과를 보입니다.

### 혁신적인 부분
이 연구의 혁신적인 점은 선택적 SSM을 통해 시퀀스 모델의 계산 효율성과 성능을 동시에 개선하는 것입니다. 이는 특히 긴 시퀀스를 효과적으로 처리할 수 있는 능력을 제공하며, 기존 모델들이 가진 한계를 극복합니다.

이 연구는 시퀀스 모델링 분야에서 중요한 발전을 나타내며, 다양한 데이터 처리 작업에 큰 영향을 미칠 것입니다.