# UrBench: A Comprehensive Benchmark for Evaluating Large Multimodal Models in Multi-View Urban Scenarios
## TL;DR
## Summary
- [https://arxiv.org/pdf/2408.17267.pdf](https://arxiv.org/pdf/2408.17267.pdf)

### 1. 각 섹션 요약

#### 서론
이 논문은 최근 연구 커뮤니티에서 대형 멀티모달 모델(LMM)의 개발에 대한 관심이 증가하고 있음을 설명합니다. LMM은 여러 도시 과제를 해결하며 인간 중심의 인공지능 모델을 목표로 합니다. 이 논문의 주된 목표는 LMM을 평가하고 개발하기 위한 종합적인 벤치마크인 UrBench를 소개하는 것입니다.

#### UrBench 소개
UrBench는 도시 환경에서 LMM을 평가하기 위한 종합 벤치마크로, 11.6K 질문을 바탕으로 14가지 과제 유형에 대해 평가합니다. 이는 기존 벤치마크가 단일 시점에서 도시 과제만 평가하는 한계를 보완합니다. UrBench는 다각적인 도시 환경을 수렴하여 LMM의 시설, 과정, 이해 능력을 검토합니다.

#### 세부 평가
리서치 결과에 따르면 현존하는 최고의 LMM들도 인간 전문가에 비해 여전히 17.4%의 성능 격차가 있습니다. 특히 LMM이 여러 도시 관점 간 관계를 이해하는 데 어려움을 겪고 있으며, 다양한 도시 관점과 환경에서 일관성 없는 성과를 보입니다. 이는 현 LMM들이 도시 환경 이해에서 많은 개선 여지가 있음을 시사합니다.

#### 결론
UrBench의 평가 결과는 LMM이 인간 전문가에 비해 도시 환경에서 여전히 한계가 있음을 보여줍니다. 이 논문은 LMM이 도시 환경에서 더 나은 성능을 발휘하도록 개선될 수 있는 방향을 제시하고 있습니다. 또한, UrBench는 다양한 도시 과제를 위한 높은 품질의 데이터와 평가 결과를 공개하여, 후속 연구에 기여하고자 합니다.

### 2. 전체 요약
이 논문은 도시 환경에서의 큰 멀티모달 모델(LMM)을 평가하기 위해 UrBench라는 새로운 벤치마크를 소개합니다. UrBench는 11.6K 개의 질문을 통해 14가지 과제 유형에 대한 평가를 제공합니다. 주된 목표는 다각적인 도시 이해를 반영하여 LMM의 성능을 종합적으로 평가하는 데 있습니다. 연구 결과, 현재 LMM들은 인간 전문가에 비해 성능이 뒤떨어지며, 여러 도시 관점에서의 이해 및 일관성 면에서 어려움을 겪고 있습니다. UrBench는 이러한 성능 격차를 지적하며, 미래 연구 방향을 제시하는 귀중한 데이터를 제공합니다.