# Distilling an End-to-End Voice Assistant Without Instruction Training Data
## TL;DR
## Summary
- [https://arxiv.org/pdf/2410.02678.pdf](https://arxiv.org/pdf/2410.02678.pdf)

### 섹션별 요약 및 주요 기여

- **소개 (Introduction)**
  이 논문은 음성 인식 모델과 대형 언어 모델(LLM)을 통합하여 DiVA라는 새로운 음성 기반 LLM을 제시합니다. ASR(자동 음성 인식)을 통해 음성을 문자로 변환하는 기존 방법의 한계를 극복하고 음성의 풍부한 정보를 효과적으로 보존합니다.

- **관련 연구 (Related Work)**
  기존 연구와 비교하여 DiVA는 대규모 다중 작업 감독 미세 조정(SFT)을 사용하지 않으며, 자체 감독을 통해 텍스트와 음성 간의 지식 전달에 주력합니다. 특히, DiVA는 기존의 대규모 고가의 감독 데이터 없이도 효과적으로 학습할 수 있는 새로운 접근 방식을 제시합니다.

- **방법론 (Method)**
  DiVA는 사전 학습된 오디오 인코더(Whisper Encoder)와 새로운 모듈(Q-Former)을 사용하여 음성 데이터를 처리합니다. 이것은 음성 데이터를 텍스트 데이터와 함께 효율적으로 통합할 수 있게 해줍니다.

- **결론 (Conclusion)**
  DiVA는 적은 연산 자원으로도 높은 성능을 발휘하며, 사용자 선호도 조사에서 경쟁 모델보다 우수한 결과를 보였습니다. 더 적은 데이터로 신속히 적응 가능하다는 것을 보여줌으로써, 대규모 투자 없이도 LLM을 음성에 적용할 수 있는 잠재력을 강조했습니다.

### 전체 요약

이 논문은 새로운 방법론 DiVA를 통한 음성 및 텍스트 통합 LLM(대형 언어 모델)의 구현을 다룹니다. DiVA는 과거의 오디오 인코더와 텍스트 기반 LLM을 결합하여, 일반적인 SFT 데이터 없이 자체 감독 학습을 수행함으로써 음성의 풍부한 정보 보존과 효율적인 학습을 달성합니다. 이 모델은 발화 기반 질의 응답, 분류 및 번역과 같은 여러 작업에 대해 잘 일반화되며, 경쟁 모델보다 적은 연산 자원만으로도 높은 사용자 선호도를 획득했습니다. 이는 새로운 데이터에 대한 대규모 투자 없이도 LLM의 음성 처리 능력을 향상시킬 수 있는 가능성을 보여줍니다.