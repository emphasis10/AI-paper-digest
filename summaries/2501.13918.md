# Improving Video Generation with Human Feedback
## TL;DR
## Summary
- [https://arxiv.org/pdf/2501.13918.pdf](https://arxiv.org/pdf/2501.13918.pdf)

각 섹션의 내용을 요약하여 한국어로 제공하겠습니다.

1. **서론**:
   연구배경 및 필요성을 다루며, 텍스트에서 비디오로 변환하는(T2V) 모델의 성능을 높이기 위해 강화 학습을 적용합니다. 이는 T2V 모델의 시각적 품질, 모션 품질, 텍스트 정렬을 개선하기 위해 개발된 비디오 보상 모델을 포함한 다차원적 보상 모델을 제안합니다.

2. **관련 연구**:
   시각 품질과 의미적 일관성을 평가하기 위한 기존 연구들을 소개하며, 이러한 과제를 위해 대규모 비전-언어 모델(VLM)을 활용한 보상 모델링이 제안됩니다. 새로운 연구는 특히 최신 비디오 생성 모델을 다루고 있으며, 심층적인 보상 모델링 전략을 조사합니다.

3. **VideoReward 데이터셋 및 모델**:
   본 연구는 최신 비디오 생성 모델에게 적합한 대규모 인간 선호 데이터셋을 구축하고, 이 데이터셋을 기반으로 VideoReward라는 다차원 비디오 보상 모델을 개발합니다.

4. **강화 학습 기반 정렬**:
   플로우 기반 모델을 위한 세 가지 정렬 알고리즘을 제안하며, 고정된 β를 사용할 때 Flow-DPO가 다른 방법보다 뛰어난 성능을 보인다고 언급합니다. 이러한 방법들은 사용자들이 여러 정렬 목표를 조정 가능하게 합니다.

5. **실험**:
   다양한 T2V 모델을 위한 보상 모델과 강화 학습 정렬 알고리즘의 성능을 실험적으로 평가합니다. 또한, VideoGen-RewardBench라는 벤치마크를 사용하여 최신 모델을 평가하고, VideoReward가 기존 모델보다 우수한 성능을 보인다고 결론지었습니다.

6. **결론**:
   제안된 모델이 현대 비디오 생성 모델의 정확하고 공정한 평가를 가능하게 하며, 미래 연구에서는 보다 강력한 보상 모델 개발과 더 다양한 조건부 생성 작업으로의 확장을 제안합니다. 보상 해킹 문제를 극복하기 위한 방법과 높은 품질을 유지하며 정렬을 개선할 수 있는 보다 혁신적인 연구 필요성을 강조합니다.

**전체 요약**:
이 논문은 인간 피드백과 강화 학습을 통해 텍스트에서 비디오로 변환하는(T2V) 생성 모델의 품질을 개선하는 방법을 제안합니다. 대규모 데이터셋인 VideoReward를 구축하고 이를 기반으로하는 다차원 보상 모델을 개발하여, 비디오 생성 모델의 시각적, 모션 품질, 텍스트 정렬을 평가합니다. 또한, 플로우 기반 정렬 알고리즘을 도입하여 최신 비디오 생성 기술의 성능을 향상시키고, 향후 연구에서는 더욱 개선된 보상 모델 개발과 다른 조건부 생성 작업의 적용을 제안합니다.