# McEval: Massively Multilingual Code Evaluation
## TL;DR
## Summary
- [https://arxiv.org/pdf/2406.07436.pdf](https://arxiv.org/pdf/2406.07436.pdf)

### 1. 섹션별 요약

#### Introduction
이 논문에서는 다국어 코드 평가를 위한 새로운 벤치마크인 MCEVAL을 소개합니다. 기존의 벤치마크가 주로 Python에 의존하는 반면, MCEVAL은 40개의 프로그래밍 언어를 포함하고 있어 코드 LLM의 다국어 평가를 보다 포괄적으로 수행할 수 있습니다. MCEVAL은 코드를 생성, 이해, 완성하는 과제를 통해 코드 LLM의 성능을 평가합니다.

#### Multilingual Code Evaluation: MCEVAL
MCEVAL은 코드 생성, 이해, 완성이라는 세 가지 주요 평가 작업을 포함하고 있으며, 16k개의 테스트 샘플을 통해 40개의 프로그래밍 언어를 다룹니다. 이를 위해 코드 샘플을 수집하고, 인간 주석자들이 주석을 달아 데이터셋을 구축합니다.

#### Dataset Statistics
데이터셋 통계는 40개 언어에 대해 다루며, 각 언어별로 테스트 샘플 수, 코드의 복잡도 수준 등이 포함되어 있습니다.

#### Human Annotation & Quality Control
전문 소프트웨어 개발자들이 주석 작업을 수행하며, 주석 작업의 일관성과 정확성을 보장하기 위해 엄격한 가이드라인을 따릅니다. 또한, GPT-4 Turbo를 사용하여 초안 질문을 생성하고 이를 검토하여 고품질의 평가 샘플을 담보합니다.

#### Evaluation Tasks
평가 작업은 코드 생성, 이해, 완성의 세 가지로 이루어져 있으며, 각 작업은 주어진 문제 설명과 테스트 케이스를 기반으로 합니다. 다양한 프로그래밍 언어에서의 성능을 평가하기 위해 각기 다른 난이도의 문제들이 출제됩니다.

#### mCoder
mCoder는 MCEVAL-INSTRUCT로 훈련된 다국어 코드 생성 모델로, 여러 프로그래밍 언어에서 코드를 생성할 수 있습니다. mCoder는 광범위한 실험을 통해 성능이 입증되었습니다.

#### McEval-Instruct
MCEVAL-INSTRUCT는 다국어 데이터셋을 통해 훈련된 지시 튜닝 모델로, 다양한 언어에서 정확하게 코드를 생성하고 설명할 수 있는 능력을 가지고 있습니다.

#### Experiments
실험 섹션은 다양한 코드 LLM들을 MCEVAL에서 평가한 결과를 포함합니다. 이를 통해 개방형 모델과 폐쇄형 모델 간의 성능 격차를 비교 분석합니다.

#### Experiment Setup
실험 설정 섹션에서는 MCEVAL 벤치마크에서 사용된 실험 환경과 조건을 설명합니다. 평가를 위한 코드와 데이터셋을 기준으로 실제 테스트가 어떻게 이루어졌는지 상세히 다룹니다.

#### Main Results
주요 결과에서는 MCEVAL 벤치마크를 통해 평가된 다양한 코드 LLM들의 성능을 비교 분석합니다. 모든 작업에 대해 각 모델의 성능을 도표로 요약하여 보여줍니다.

#### Further Analysis
추가 분석 섹션에서는 주요 결과에서 도출된 인사이트를 심화 분석합니다. 어떤 모델이 어떤 언어 또는 특정 작업에 더 적합한지, 그리고 왜 그런지를 탐구합니다.

#### Related Work
관련 연구 섹션에서는 기존의 코드 LLM 벤치마크와 MCEVAL의 차별점을 설명합니다. 이를 통해 MCEVAL의 필요성과 기여도를 강조합니다.

#### Conclusion
결론에서는 MCEVAL 벤치마크의 주요 발견과 기여를 요약하고, 향후 연구 방향을 제안합니다. MCEVAL-INSTRUCT와 mCoder 모델들이 코드 LLM 연구의 새 기준을 제시하며, 다국어 코드 이해와 생성의 새로운 가능성을 열었습니다.

---

### 2. 전체 요약

이 논문은 코드 대형 언어 모델(LLM)의 다국어 평가를 위한 벤치마크인 MCEVAL을 제안합니다. 기존 벤치마크가 주로 Python에 집중하고 있는 반면, MCEVAL은 40개의 프로그래밍 언어를 포함하여 다국어 코드 평가의 범위를 확장했습니다. MCEVAL은 16k개의 테스트 샘플을 사용해 코드 생성, 이해, 완성 작업을 평가하며, 전문 개발자들이 주석 작업을 통해 높은 품질의 데이터셋을 보장합니다. MCEVAL-INSTRUCT와 mCoder 모델은 다국어 코드 생성 능력을 갖추고 있으며, 다양한 코드 LLM들의 성능을 체계적으로 비교 평가합니다. 이를 통해 MCEVAL은 코드 LLM 연구의 새로운 기준을 제시하며, 다국어 코드 이해와 생성의 새로운 가능성을 열었습니다.

## Similar Papers
- [CodeEditorBench: Evaluating Code Editing Capability of Large Language Models](2404.03543.md)
- [AutoCoder: Enhancing Code Large Language Model with \textsc{AIEV-Instruct}](2405.14906.md)
- [CodeV: Empowering LLMs for Verilog Generation through Multi-Level Summarization](2407.10424.md)
- [DevEval: A Manually-Annotated Code Generation Benchmark Aligned with Real-World Code Repositories](2405.19856.md)
- [InverseCoder: Unleashing the Power of Instruction-Tuned Code LLMs with Inverse-Instruct](2407.05700.md)
- [PECC: Problem Extraction and Coding Challenges](2404.18766.md)
- [Unleashing the Power of Data Tsunami: A Comprehensive Survey on Data Assessment and Selection for Instruction Tuning of Language Models](2408.02085.md)
- [Applying RLAIF for Code Generation with API-usage in Lightweight LLMs](2406.20060.md)
- [IRASim: Learning Interactive Real-Robot Action Simulators](2406.14540.md)
