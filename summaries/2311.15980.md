# Direct2.5: Diverse 3D Content Creation via Multi-view 2.5D Diffusion
## TL;DR
## Summary
- [https://arxiv.org/pdf/2311.15980.pdf](https://arxiv.org/pdf/2311.15980.pdf)

이 연구 논문에서는 Direct2.5 모델을 사용하여 다양하고 높은 품질의 3D 콘텐츠를 생성하는 새로운 접근 방식을 소개합니다. 이 모델은 멀티뷰 2.5D 확산을 통해 3D 데이터의 구조적 분포를 직접 모델링하며, 원래의 2D 확산 모델의 강력한 일반화 능력을 유지합니다.

### 주요 내용 요약

1. **서론 및 배경**:
   - 기존의 3D 콘텐츠 생성 방식은 주로 훈련된 2D 확산 모델을 사용하거나 제한된 3D 데이터에 대해 직접 3D 확산 모델을 훈련하는 방식을 사용했습니다. 그러나 이러한 방식들은 각각 시간이 많이 소요되거나 생성 다양성이 떨어지는 문제가 있습니다.

2. **Direct2.5의 구조 및 기능**:
   - Direct2.5는 멀티뷰 2.5D 확산을 통해 3D 모델을 생성합니다. 이 과정에서 멀티뷰 노멀 맵을 생성하고, 이를 이용해 일관된 3D 모델로 통합합니다. 또한, 3D 기하학이 주어진 상태에서 빠른 외관 생성을 위한 모듈도 설계되었습니다.

3. **성능 평가 및 응용**:
   - 복잡한 텍스트 프롬프트를 사용한 실험에서 Direct2.5는 SDS 기반 방식이나 직접 3D 생성 방식에 비해 우수한 성능을 보였습니다. 특히, 10초 만에 다양하고 고품질의 3D 텍스처 메쉬를 생성할 수 있는 능력을 입증했습니다.

### 혁신적인 부분
Direct2.5의 혁신성은 기존의 2D 확산 모델에서 파생된 멀티뷰 2.5D 확산을 사용하여 직접 3D 기하학 분포를 모델링하고, 이를 통해 빠르고 다양한 3D 콘텐츠 생성을 가능하게 한 점입니다. 이 방법은 3D 콘텐츠 생성의 효율성을 크게 향상시키며, 특히 실시간 응용 프로그램에서 유용할 것으로 기대됩니다.

이 논문은 3D 콘텐츠 생성 방법론에 혁신적인 접근을 제시함으로써, 이 분야에서의 발전에 크게 기여할 것으로 예상됩니다.

## Similar Papers
- [BitsFusion: 1.99 bits Weight Quantization of Diffusion Model](2406.04333.md)
- [JointNet: Extending Text-to-Image Diffusion for Dense Distribution Modeling](2310.06347.md)
- [Improved Distribution Matching Distillation for Fast Image Synthesis](2405.14867.md)
- [Dual3D: Efficient and Consistent Text-to-3D Generation with Dual-mode Multi-view Latent Diffusion](2405.09874.md)
- [Magic-Boost: Boost 3D Generation with Mutli-View Conditioned Diffusion](2404.06429.md)
- [Autoregressive Model Beats Diffusion: Llama for Scalable Image Generation](2406.06525.md)
- [Applying Guidance in a Limited Interval Improves Sample and Distribution Quality in Diffusion Models](2404.07724.md)
- [Atlas3D: Physically Constrained Self-Supporting Text-to-3D for Simulation and Fabrication](2405.18515.md)
- [Step-aware Preference Optimization: Aligning Preference with Denoising Performance at Each Step](2406.04314.md)
