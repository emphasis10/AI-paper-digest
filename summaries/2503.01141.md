# How Well do LLMs Compress Their Own Chain-of-Thought? A Token Complexity Approach
## TL;DR
## Summary
- [https://arxiv.org/pdf/2503.01141.pdf](https://arxiv.org/pdf/2503.01141.pdf)

1. 각 섹션 요약 및 주요 기여

- **도입 및 배경**: 이 논문은 대규모 언어 모델(LLM)에서 체인 오브 사고(CoT) 프롬프트를 사용할 때의 추론 길이와 성능 간의 균형을 탐구하며, 효율적인 추론 전략의 필요성을 강조합니다. CoT 프롬프트는 모델이 최종 답변에 도달하기 전에 중간 추론 단계를 생성하도록 유도하여 복잡한 문제 해결을 지원하지만, 이로 인해 응답 길이가 길어져 효율성에 대한 우려가 생깁니다. 대답 길이를 줄이면서 정확성을 유지하는 방법을 연구하여 이러한 문제를 해결하려 합니다.

- **주요 연구 방법**: 이 연구는 다양한 프롬프트 기반 압축 전략을 체계적으로 평가하여 추론 길이와 성능 간의 교환 관계를 분석합니다. 실험 결과, 각 작업에 따라 필요한 최소 토큰 수(토큰 복잡도) 아래에서는 정확한 응답을 얻기 힘들다는 사실을 발견하였습니다. 이는 정보 이론적 한계와 관련이 있으며, 기존의 프롬프트 전략들이 이론적 한계로부터 멀리 떨어져 있음을 시사합니다.

- **실험 및 평가**: 여러 LLM과 다양한 수학적 문제 데이터셋에서 프롬프트를 테스트하여 CoT의 응답 길이를 줄이는 다양한 방법의 성능을 측정했습니다. 연구 결과, 많은 프롬프트들이 응답 길이와 정확도 간의 일반적인 교환 관계 곡선 위에 있음을 발견했습니다. 이는 주어진 프롬프트들이 체인 오브 사고의 조합보다 길이에 의해 주로 영향을 받음을 의미합니다.

- **결론 및 제한 사항**: 연구는 추론 길이와 LLM의 성능 간의 상호작용을 새롭게 조명하고, 기존 프롬프트 기반 전략이 최적의 정확도-길이 한계에 도달하지 못하고 있다는 점에서 개선의 여지가 있음을 강조합니다. 한편, 이론적 상한선에 도달하기 위한 실제적인 어려움과 다른 도메인에서의 보편성의 제한을 인식해야 합니다.

2. 종합 요약

이 논문은 체인 오브 사고(CoT) 프롬프트를 사용하는 대규모 언어 모델의 효율성을 개선하기 위한 방법을 제안합니다. 연구는 추론 길이와 정확도 간의 교환 관계, 즉 토큰 복잡성을 강조하며, 최적의 효율성을 달성하기 위해서는 이론적인 한계와 현재의 방법 사이에 큰 차이가 있음을 발견했습니다. 연구 결과는 프롬프트 압축 전략의 성능 벤치마킹을 위한 도구를 제공하며, 연구자들이 LLM의 추론 효율성을 높이는 새로운 방법론을 개발하는 데 기여할 수 있습니다.