# Evaluating Open Language Models Across Task Types, Application Domains, and Reasoning Types: An In-Depth Experimental Analysis
## TL;DR
## Summary
- [https://arxiv.org/pdf/2406.11402.pdf](https://arxiv.org/pdf/2406.11402.pdf)

### 1. 각 섹션 요약

#### 1.1 서론 (Introduction)
이 논문은 2억~11억 개의 파라미터를 가진 10개의 오픈 소스 언어 모델(LM)을 다양한 작업 유형, 응용 도메인, 추론 유형에 대해 분석하여 그 성능을 평가합니다. 소형 모델들이 특정 상황에서 대형 최첨단 언어 모델(SOTA LM)들과 경쟁할 수 있음을 보여줍니다.

#### 1.2 실험 설정 (Experimental Setup)
- **데이터셋**: Super-Natural Instructions 데이터셋의 테스트 분할을 사용하여 12가지 작업 유형, 36개의 도메인, 18개의 추론 유형을 분석합니다. 총 11,810개의 작업 인스턴스를 사용합니다.
- **프롬프트 스타일**: 작업 정의를 포함하거나 포함하지 않는 식으로 0, 2, 4, 8개의 예제를 포함한 8가지 프롬프트 스타일로 실험을 수행합니다.

#### 1.3 사용된 언어 모델 (Language Models Used)
Gemma-2B, Llama-3-8B, Mistral-7B 등 5개의 사전 학습 모델과 이 모델들의 지시-튜닝(instruction-tuned) 버전들을 사용하여 성능을 평가합니다.

#### 1.4 평가지표 (Evaluation Metrics)
언어 모델의 출력물의 의미적 정확성을 평가하기 위해 BERTScore, ROUGE, METEOR와 같은 지표를 사용합니다.

#### 1.5 실험 결과 (Experiments and Results)
- **전반적인 성능**: 대부분의 사전 학습 모델들이 다양한 작업 유형에 대해 우수한 성능을 보였습니다.
- **작업 유형별 비교**: Gemma-2B는 소형 모델 중 최고 성능을 보였고, Falcon-2-11B는 대형 모델 중 최고 성능을 보였습니다.
- **도메인별 비교**: 특히 Gemma-2B는 자원 제한 상황에서 강력한 선택지가 되었습니다.
- **추론 유형별 비교**: 여러 프롬프트 스타일에 따른 성능 변동을 고려한 결과, 가장 적절한 프롬프트 스타일이 모델과 작업 유형에 따라 달라짐을 확인했습니다.

#### 1.6 결론 (Conclusion)
소형 언어 모델도 특정 조건에서 대형 모델과 경쟁할 수 있음을 밝혔다. 연구 결과는 모델과 프롬프트의 전략적 선택을 통해 사용자의 필요에 맞는 언어 모델을 선택할 수 있는 가이드라인을 제공합니다.

### 2. 전체 요약

이 논문은 2억~11억 개의 파라미터를 가진 10개의 오픈 소스 언어 모델을 다양한 작업 유형, 응용 도메인 및 추론 유형에 대해 평가합니다. Super-Natural Instructions 데이터셋을 활용하여 각 모델의 출력물의 의미적 정확성을 분석하고, BERTScore, ROUGE, METEOR와 같은 지표를 통해 성능을 비교합니다. 연구 결과, 많은 소형 모델들이 특정 조건에서 대형 최첨단 모델과 견줄 수 있음을 보여주며, 각 모델과 프롬프트 스타일을 전략적으로 선택함으로써 사용자가 자신의 필요에 맞는 모델을 효율적으로 선택할 수 있는 방법을 제시합니다. 주요 기여로는 모델 성능 평가를 위한 3층 구조의 스키마 제안과 여러 프롬프트 스타일에 따른 상세한 실험적 분석이 있습니다. 이 논문은 소형 오픈 모델도 잘 활용하면 대형 모델에 필적할 수 있음을 입증하며, 모델 선택에 유용한 가이드를 제공합니다.

## Similar Papers
- [Best Practices and Lessons Learned on Synthetic Data for Language Models](2404.07503.md)
- [Retrieval Augmented Generation or Long-Context LLMs? A Comprehensive Study and Hybrid Approach](2407.16833.md)
- [Self-MoE: Towards Compositional Large Language Models with Self-Specialized Experts](2406.12034.md)
- [Adaptive Retrieval-Augmented Generation for Conversational Systems](2407.21712.md)
- [Does Fine-Tuning LLMs on New Knowledge Encourage Hallucinations?](2405.05904.md)
- [Hierarchical Prompting Taxonomy: A Universal Evaluation Framework for Large Language Models](2406.12644.md)
- [XC-Cache: Cross-Attending to Cached Context for Efficient LLM Inference](2404.15420.md)
- [Judging the Judges: Evaluating Alignment and Vulnerabilities in LLMs-as-Judges](2406.12624.md)
- [Long-context LLMs Struggle with Long In-context Learning](2404.02060.md)
