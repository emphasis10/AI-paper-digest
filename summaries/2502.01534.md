# Preference Leakage: A Contamination Problem in LLM-as-a-judge
## TL;DR
## Summary
- [https://arxiv.org/pdf/2502.01534.pdf](https://arxiv.org/pdf/2502.01534.pdf)

1. **각 섹션의 중요 내용 요약 (한국어):**

   **1. 서론**
   - 최신 대형 언어 모델(LLM)의 발전은 모델 개발에 있어 두 가지 중요한 데이터 주석 방법인 LLM-as-a-judge와 LLM 기반 데이터 합성을 가능하게 하였습니다. 그러나 이들의 결합으로 발생하는 오염 문제인 '선호 유출(preference leakage)'에 대한 주의가 부족했습니다. 이 연구에서는 선호 유출 문제를 정의하고 이를 통해 생성된 데이터와 평가자 간의 관련성을 파악합니다.

   **2. 관련 연구**
   - LLM-as-a-judge는 LLM을 활용하여 응답을 자동으로 평가하는 방식이며, 최근 다양한 분야에서 채택되고 있습니다. 그러나 LLM-as-a-judge는 평가 과정에서의 편향 문제와 취약성이 드러났습니다. 특히, 이 연구에서는 데이터 유출 이슈를 다루고 있습니다.

   **3. 선호 유출**
   - 선호 유출은 생성자 LLM과 평가자 LLM 간의 연관성에서 발생하는 데이터 오염 문제입니다. 이는 같은 모델 사용, 상속 관계, 같은 모델 패밀리 내의 관계로 설명됩니다. 선호 유출은 모델 평가 과정에서의 체계적 바이어스를 초래하며 탐지가 어려운 문제입니다.

   **4. 주요 실험**
   - 실험에서 다양한 LLM 모델을 사용하여 데이터 생성 및 평가를 수행하였습니다. 실험 결과, 연관 모델 간의 선호 유출이 확인되었으며, 이는 LLM 기반 평가의 공정성을 해칠 수 있음을 보여주었습니다.

   **5. 추가 분석**
   - 연관성과 학습 방법, 데이터 혼합 전략을 통해 선호 유출 문제를 추가 탐색하였으며, 이는 다양한 시나리오에서의 영향을 입증하였습니다.

   **6. 결론**
   - 선호 유출 문제를 체계적으로 조망함으로써, LLM 기반 시스템의 신뢰성을 높이는 방향을 제안합니다. 이 연구는 AI 평가 방법의 개선과 데이터 소스의 다변화를 촉구합니다.

2. **전체 요약 (한국어):**

   이 논문은 대형 언어 모델(LLM) 기반 평가 시스템에서 발생하는 '선호 유출' 문제를 조명하고 있습니다. LLM-as-a-judge와 LLM 기반 데이터 생성의 결합은 모델 훈련의 효율성을 높이지만, 이로 인해 평가 과정에서의 편향이 심화될 수 있습니다. 연구를 통해 선호 유출이 생성자와 평가자의 모델 간의 연관성에서 비롯된다는 것을 발견하였으며, 이는 실험을 통해 명확하게 입증되었습니다. 선호 유출은 모델의 공정성을 떨어뜨릴 수 있는 요소로, 연구의 결과는 AI 시스템의 신뢰성을 향상시키기 위한 다양한 방향 제안을 포함하고 있습니다. 이 연구는 향후 AI의 평가는 물론 데이터 접근 방식 개선에 기여할 수 있습니다.