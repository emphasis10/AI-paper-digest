# Careful with that Scalpel: Improving Gradient Surgery with an EMA
## TL;DR
## Summary
- [https://arxiv.org/pdf/2402.02998.pdf](https://arxiv.org/pdf/2402.02998.pdf)

### 1. 섹션 별 요약

#### 서론 (Introduction)
이 논문은 인공지능(AI) 및 기계 학습(ML) 모델을 훈련할 때, 주 목적(loss)과 보조 목적(auxiliary loss)를 균형 있게 최적화하는 새로운 방법인 Bloop을 제안합니다. Bloop은 주 목적을 우선으로 하면서도 보조 목적의 달성을 돕는 방향으로 매개변수를 업데이트합니다.

#### Bloop 알고리즘 (The Bloop Algorithm)
이 섹션에서는 Bloop의 작동 원리를 설명합니다. Bloop은 주 목적의 손실이 최소가 되도록 보조 손실을 제약 조건으로 설정하는 이중 수준 최적화문제를 풉니다. 이를 통해 주 손실이 낮아지면서도 보조 손실을 효과적으로 관리할 수 있습니다.

#### 실험 (Experiments)
실험에서는 다양한 데이터셋과 모델 아키텍처에 대해 Bloop의 효과를 검증합니다. 크게 세 가지 범주로 나눠 실험을 진행했습니다:
1. 부가적인 편향을 적용한 가중치 훈련
2. 다중 작업 학습
3. 두 데이터셋을 대상으로 한 공동 훈련

결과적으로 Bloop은 다른 방법들과 비교하여 더 나은 Pareto 효율성을 보여주었으며, 이는 특정 문제 상황에서 더 우수한 테스트 성능으로 이어질 수 있음을 입증했습니다.

#### 결론 (Conclusion)
논문은 Bloop 알고리즘이 기존의 방법들에 비해 주 목적과 보조 목적 간의 가장 좋은 균형을 찾는데 효과적임을 결론지었습니다. 이는 특히 다중 작업 학습이나 여러 데이터셋에서 훈련할 때 유용합니다.

---

### 2. 전체 요약

이 논문에서는 AI 및 ML 모델의 훈련 과정에서 주 목적과 보조 목적을 균형 있게 최적화하기 위한 새로운 알고리즘인 Bloop을 제안합니다. Bloop은 주 목적의 최적화를 우선으로 하면서 보조 목적도 달성할 수 있는 이중 수준 최적화 문제를 해결합니다. 다양한 실험을 통해 Bloop이 다른 기존 방법들에 비해 더 나은 Pareto 효율성을 갖는다는 것이 입증되었으며, 이는 여러 작업과 데이터셋을 포함한 복잡한 학습 시나리오에서 특히 유용합니다. Bloop은 높은 일반화 성능을 유지하면서도 보조 지표들을 효과적으로 최적화할 수 있습니다.

이러한 특징들은 Bloop을 AI 및 ML 연구 및 실무 분야에서 중요한 도구로 만들며, AI의 발전에 중대한 기여를 할 수 있을 것으로 기대됩니다.