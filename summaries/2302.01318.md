# Accelerating Large Language Model Decoding with Speculative Sampling
## TL;DR
## Summary
- [https://arxiv.org/pdf/2302.01318.pdf](https://arxiv.org/pdf/2302.01318.pdf)

### 요약 구성 및 해석

#### 1. 각 섹션의 요약
**도입부 (Introduction)**:
이 논문은 대규모 언어 모델의 디코딩 속도를 높이기 위한 *추측 샘플링 (Speculative Sampling)* 알고리즘을 제안합니다. 추측 샘플링은 규모가 큰 모형에서 샘플링을 간소화하여 여러 토큰을 한 번에 생성하도록 합니다. 이는 초안 모델에서 생성된 짧은 연속성을 평행하게 평가함으로써 이루어집니다. 이 방법을 통해 샘플 품질을 손상시키거나 모델 자체를 수정할 필요 없이 성능 향상이 가능합니다.

**관련 연구 (Related Work)**:
다양한 연구들이 Transformer 모델의 샘플링 지연 시간을 줄이기 위해 다양한 방법들을 제안했습니다. 예를 들어, 모델 크기를 줄이기 위한 양자화와 디스틸레이션 기법이 있습니다. 또한, 병렬 처리를 통해 샘플링 성능을 향상시키는 다양한 접근법이 연구되었습니다.

**추측 샘플링 기법 (Speculative Sampling Technique)**:
이 방법에서는 초안 모델과 목표 모델을 사용합니다. 초안 모델에서 생성한 여러 토큰들을 목표 모델로 평가한 후, 수정된 거부 샘플링 스킴을 사용해 일부 토큰을 채택합니다. 이를 통해 목표 모델의 분포를 보존하면서도 샘플링 속도를 높입니다. 결국, 초안 모델을 통해 K개의 토큰을 병렬로 생성한 후 목표 모델을 통해 이를 평가하여 샘플링 속도를 향상시킵니다.

**결과 (Results)**:
추측 샘플링 기법을 Chinchilla 모델에서 평가한 결과, 성능 지표와 샘플 시간에서 상당한 향상이 있음을 확인했습니다. HumanEval, XSum 벤치마크에서 최대 2.5배의 속도 향상을 보였으며, 이는 샘플 분포의 왜곡 없이 이루어졌습니다. 이 기법은 특히 작은 배치 사이즈에서의 지연 시간을 줄이는 데 효과적입니다.

#### 2. 논문의 전체 요약
이 논문은 대규모 언어 모델의 디코딩 속도를 획기적으로 높이기 위한 추측 샘플링(Speculative Sampling) 기법을 제안합니다. 기존 모델을 수정하거나 성능을 저해하지 않고도 더욱 빠른 샘플링이 가능하도록 하는 것이 주요 목표입니다. Chinchilla 모델(70억 매개변수)에서의 평가 결과 최대 2.5배의 속도 향상과 안정성을 보여주며, 이는 AI와 머신러닝의 대규모 모델 디코딩 분야에서 중요한 진보를 의미합니다.

### 전체 요약

이 논문은 대규모 언어 모델의 디코딩 속도를 향상시키기 위한 새로운 추측 샘플링 기법을 소개합니다. 이 기술은 초안 모델과 목표 모델을 사용하여 샘플링 속도를 높이고, 대규모 데이터 처리에서 더욱 효율적인 결과를 제공합니다. 실험 결과, HumanEval 및 XSum 벤치마크에서 모델 샘플링 속도가 최대 2.5배까지 향상되어, 이 방법의 효과와 실용성을 입증했습니다. 이러한 성과는 대규모 언어 모델 디코딩의 미래 가능성을 크게 확장시키는 중요한 기여를 합니다.