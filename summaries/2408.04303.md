# Trans-Tokenization and Cross-lingual Vocabulary Transfers: Language Adaptation of LLMs for Low-Resource NLP
## TL;DR
## Summary
- [https://arxiv.org/pdf/2408.04303.pdf](https://arxiv.org/pdf/2408.04303.pdf)

### 1. 섹션별 주요 내용 요약 및 설명

#### Introduction (서론)
다어 토큰화 전략은 모든 언어에 대해 공정하지 않으며, 특히 서유럽 언어에 유리합니다. 이를 해결하기 위해 각 언어에 맞춘 별도의 토큰화를 제안합니다. 대규모 언어 모델(LLM)의 훈련에는 많이 필요한 데이터가 부족하여 전이 학습이 필수적입니다.

#### Background and Related Work (배경 및 관련 연구)
사전 학습한 언어 모델(PLM)을 새로운 언어와 도메인에 적응시키는 것은 여전히 중요한 과제입니다. 주목할 만한 접근법 중 하나는 PLM의 어휘를 목표 언어 또는 도메인과 더 일치하는 어휘로 교체하는 것입니다. 이는 모델의 적응 과정을 가속화하고 성능을 향상시킵니다.

#### Trans-Tokenization (트랜스 토큰화)
트랜스 토큰화는 새로운 언어에 적응하기 위해 높은 자원의 언어에 해당하는 토큰 임베딩을 이용하는 방법입니다. 이 방법은 각 언어별로 최적화된 토큰화를 가능하게 하여 성능 향상에 기여합니다. 사용된 전략은 초기 프리트레이닝 단계 동안 매핑을 복원하거나 지속적인 손실로 매핑을 통합하여 어휘 품질을 향상시킵니다.

#### Experiments (실험)
실험에는 저자원의 언어와 중간자원의 언어를 위한 여러 모델을 훈련하였습니다. 트랜스 토큰화를 통해 대부분의 토큰에 대한 초기화를 제공하여 전이 학습 성능을 개선하였습니다. 이 방법은 Tatar와 같은 저자원 언어에서도 효과적임을 확인하였습니다.

#### Conclusion (결론)
본 연구에서는 LLM을 저자원 언어로 적응시키기 위한 새로운 접근법을 소개하였습니다. 트랜스 토큰화와 Hydra LLM을 활용하여 다양한 다운스트림 작업에서 높은 성능을 입증하였으며, 고품질의 병렬 데이터 필요성을 완전히 해소하였습니다. 우리의 기여가 크로스-언어 어휘 전이 및 언어 적응 연구에 영감을 줄 것을 기대하며, 더 나은 연구가 지속되길 바랍니다.

### 2. 전체 요약
이 논문은 다언어 토큰화의 불공정성을 해결하고, 저자원 및 중간자원 언어에 적응할 수 있는 새로운 트랜스 토큰화 전략을 제안합니다. 트랜스 토큰화는 높은 자원의 언어에 해당하는 토큰 임베딩을 사용하여 새로운 언어에 고품질의 모노링구얼 모델을 훈련할 수 있게 합니다. 이를 통해 대규모의 병렬 데이터 없이도 언어 모델 성능을 극대화할 수 있습니다. 이번 연구는 특히 Tatar와 같은 저자원 언어에서도 효과적인 성능을 입증하여, 다양한 언어에 대한 LLM 훈련에 중요한 기여를 하고 있습니다.

본 연구는 크로스-언어 어휘 전이 분야에서의 진전을 통해 전 세계적으로 언어 활성화에 긍정적인 영향을 미칠 것으로 기대됩니다.