# S$^2$R: Teaching LLMs to Self-verify and Self-correct via Reinforcement Learning
## TL;DR
## Summary
- [https://arxiv.org/pdf/2502.12853.pdf](https://arxiv.org/pdf/2502.12853.pdf)

### 1. 각 섹션 요약

- **소개**  
  이 논문에서는 대규모 언어 모델(LLM)의 효율성을 높이기 위한 S2R 프레임워크를 소개합니다. 이는 모델이 추론 중에 스스로 검증 및 수정할 수 있도록 배우게 합니다. 주된 목표는 모델의 사고력 향상을 위해 적은 자원만을 사용하는 것입니다.

- **관련 연구**  
  LLM의 테스트 시 컴퓨팅 성능을 확장하는 여러 연구가 있으며, 특히 해답을 찾는 과정에서 모델이 자발적으로 제시하는 문제 해결 능력을 향상시키는 방법이 탐구되었습니다.

- **S2R 프레임워크**  
  S2R은 LLM이 스스로 검증 및 수정 행동을 통해 효율적으로 사고할 수 있도록 합니다. 이를 위해 교육 및 강화 학습을 통해 행동을 초기화하고, 모델의 사고력을 강화시킵니다.

- **보상 기반 학습**  
  보상 기반 학습(RL)은 LLM의 성능 향상에 유효하다고 밝혀졌습니다. 이 논문에서는 과정 수준과 결과 수준의 보상 기반 학습이 어떻게 적용되는지 설명합니다.

- **오프라인 보상 기반 학습 탐색**  
  오프라인 RL은 온라인 RL보다 효율적인 대안으로, 큰 데이터 샘플링을 통한 정확한 기준 추정으로 성능을 향상시킬 수 있습니다.

- **결론**  
  S2R은 최소한의 자원으로 LLM의 추론을 강화하는 효율적인 프레임워크입니다. 실험 결과, 세 가지 다른 베이스 모델 및 여러 수학적 추론 벤치마크에서 S2R의 효과가 입증되었습니다.

### 2. 전체 요약

이 논문은 스스로 검증 및 수정하는 능력을 배움으로써 기존의 대규모 언어 모델의 사고력을 향상시키는 S2R 프레임워크를 제시합니다. 기존의 긴 연쇄 사고(long thought reasoning) 방식과는 달리, 최소한의 데이터와 훈련 시간을 사용하여 모델의 성능을 크게 증가시켰습니다. 이 접근법은 특히 수학적 문제를 다룰 때 강력한 성능을 보여주었으며, 다양한 난이도의 문제 해결에 유연하게 대응할 수 있는 능력을 강화했습니다. S2R은 결과 수준과 과정 수준의 보상 기반 학습을 활용하여 LLM의 사고 과정을 심화시킵니다.