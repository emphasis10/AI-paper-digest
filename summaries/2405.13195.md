# CamViG: Camera Aware Image-to-Video Generation with Multimodal Transformers
## TL;DR
## Summary
- [https://arxiv.org/pdf/2405.13195.pdf](https://arxiv.org/pdf/2405.13195.pdf)

#### 1. 소개 (Introduction)
이 논문에서는 3차원 카메라 모션을 고려한 이미지-비디오 생성 기법을 다룹니다. 주로 텍스트 기반 프롬프트를 사용하는 기존 영상 생성 방식에서 벗어나 3차원 카메라 이동을 명시적으로 제어할 수 있는 새로운 방법을 제안합니다. 이는 한 장의 이미지와 카메라 경로 신호를 입력받아 비디오를 생성하며, 입력 프레임에서 보이지 않는 영역을 자동으로 보충하는 장점이 있습니다.

#### 2. 관련 연구 (Related Work)
기존의 신경 영상 복원 기술(Novel View Synthesis Methods)과 영상 연속 생성 기법(Video Continuation Methods), 객체 중심 기법(Object-Centric Methods) 등에 대해 평가합니다. 제안된 방법은 새로운 관점을 생성하는 기존 연구와 차별화되며, 다양한 장면 이동을 구현할 수 있는 새로운 접근법을 제시합니다.

#### 3. 접근 방법 (Approach)
이미지 \( I \)와 3차원 카메라 경로 \( C \)를 입력받아 영상 프레임 세트를 생성하는 문제를 다룹니다. 비디오 및 카메라 경로를 이산 토큰으로 표현하여 카메라 이동 과업을 학습하도록 설계된 비디오 변형기를 사용합니다.

##### 3.1. 데이터 생성 위한 신경방사장 (Neural Radiance Fields for Data Generation)
실제 집 장면을 렌더링하여 영상과 카메라 경로 토큰을 생성합니다. 이는 현실적인 조명 효과와 세밀한 지오메트리 데이터를 포함하기 위해 사용됩니다.

##### 3.2. 비디오 토큰화와 비디오 변형기 모델 (Video Tokenization and Video Transformer Model)
기존 연구의 방식을 따르며, 비디오를 3차원 공간-시간 벡터 양자화 오토인코더로 토큰화하고 자동회귀 변형기로 처리합니다.

##### 3.3. 토큰화된 카메라 경로의 표현 (Representation of Tokenized Camera Paths)
3차원 카메라 이동 경로를 소리 데이터를 처리하는 방식으로 변환하여 학습합니다. 이는 기존 모델의 오디오 아키텍처를 재사용하는 접근법을 채택합니다.

#### 4. 실험 (Experiments)
1억 개 매개변수 모델을 사용하여 17 프레임 길이의 128x128 해상도 영상을 생성합니다. 카메라 경로를 424개의 토큰으로 표현하며, 초기 모델은 비디오 연속성 작업에 대해 훈련된 모델에서 시작해 카메라 이동 작업에 대해 미세 조정됩니다.

##### 4.1. 평가 지표 (Evaluation Metrics)
생성된 영상과 실제 영상의 광학 흐름 차이를 평균 제곱 오차로 측정합니다. 이는 생성된 영상이 주어진 카메라 경로를 얼마나 잘 따르는지를 평가하는데 사용됩니다.

##### 4.2. 평가된 카메라 경로 (Camera Paths Evaluated)
카드 방향으로 3차원 번역을 포함한 다양한 카메라 경로를 정의하고 평가합니다.

##### 4.3. 평가 결과 (Evaluations)
혼합된 데이터셋(NeRF 장면과 대규모 비디오 데이터)을 사용해 모델을 훈련한 결과, 이전보다 더 정확한 카메라 경로 추적과 고품질의 영상을 생성할 수 있었습니다. 다만, 네트워크가 특정 장면에 과적합되면서 장면 내 동작이 줄어드는 문제도 관찰되었습니다.

#### 5. 결론 (Conclusion)
카메라 경로를 새로운 모달리티로 다루어 3차원 카메라 이동을 제어하는 비디오를 생성할 수 있는 가능성을 입증했습니다. 사전 훈련된 백본과 다양한 훈련 데이터의 혼합을 활용하여 실내 장면 외의 일반적인 이미지-비디오 작업에서도 3차원 카메라 제어가 가능합니다.

### 2. 전체 요약
이 논문은 3차원 카메라 움직임을 명시적으로 제어하여 이미지-비디오를 생성할 수 있는 새로운 방법을 제안합니다. 혼합된 데이터를 사용하여 훈련된 비디오 변형기를 활용해 고품질의 영상을 생성하고, 종합적인 평가를 통해 다양한 카메라 경로를 처리할 수 있는 능력을 입증하였습니다. 이 연구는 카메라 경로를 새로운 모달리티로 다루어 텍스트 기반 프롬프트보다 더욱 정밀한 영상 생성 제어를 가능하게 함으로써 AI와 영상 생성 분야에서 중요한 기여를 합니다.


## Similar Papers
- [VD3D: Taming Large Video Diffusion Transformers for 3D Camera Control](2407.12781.md)
- [Tora: Trajectory-oriented Diffusion Transformer for Video Generation](2407.21705.md)
- [Vivid-ZOO: Multi-View Video Generation with Diffusion Model](2406.08659.md)
- [Invisible Stitch: Generating Smooth 3D Scenes with Depth Inpainting](2404.19758.md)
- [I4VGen: Image as Stepping Stone for Text-to-Video Generation](2406.02230.md)
- [Vid3D: Synthesis of Dynamic 3D Scenes using 2D Video Diffusion](2406.11196.md)
- [What Matters in Detecting AI-Generated Videos like Sora?](2406.19568.md)
- [Controlling Space and Time with Diffusion Models](2407.07860.md)
- [Training-free Camera Control for Video Generation](2406.10126.md)
