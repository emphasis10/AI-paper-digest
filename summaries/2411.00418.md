# Self-Evolved Reward Learning for LLMs
## TL;DR
## Summary
- [https://arxiv.org/pdf/2411.00418.pdf](https://arxiv.org/pdf/2411.00418.pdf)

### 섹션별 요약

1. **소개 (Introduction) 부분**
   - RLHF(인간의 피드백을 통한 강화 학습)는 대형 언어 모델(LLM)을 인간의 선호도에 맞추는 접근 방식입니다. 인간이 주로 라벨링한 데이터를 활용해 RM(보상 모델)을 학습합니다. 하지만 인간 데이터 의존성을 줄이는 대안으로, AI 피드백을 통한 강화 학습(RLAIF)이 등장했습니다.

2. **LLM의 자기 학습 (Self-Learning in LLMs)**
   - 자기 학습 방식이 대형 언어 모델의 성능을 자체적으로 개선하는데 사용됩니다. 이 과정은 외부의 지도 없이 모델이 스스로 학습하며 성능을 향상하는 메커니즘을 제공합니다.

3. **자기 진화형 보상 학습 (Self-Evolved Reward Learning, SER)**
   - SER은 모델이 초기에는 소량의 인간 라벨 데이터를 사용하여 학습을 시작하고 이후 스스로 데이터에 라벨을 붙이고 자가 학습을 합니다. 이를 통해 대량의 인간 라벨 데이터 의존성을 크게 줄일 수 있습니다. 모델은 학습 과정에서 자신의 예측을 반복적으로 개선합니다.

4. **실험 결과 (Experiment Results)**
   - SER 프레임워크는 다양한 데이터 세트 및 모델 크기에서 일관된 성능 향상을 보여주었습니다. 실험 결과에 따르면, 전체 라벨 데이터의 15%만으로 동일한 성능을 달성하거나 때로는 더 나은 성능을 보였습니다.

5. **결론 (Conclusion)**
   - 논문의 실험을 통해 SER이 모델의 성능을 높이는 데 효과적이라는 것이 증명되었습니다. 이는 보상 모델의 성능 향상을 통해 더욱 강력한 LLM을 교육하는 결과를 가져옵니다.

### 전체 요약
이 논문은 대형 언어 모델의 자가 학습을 위한 혁신적인 접근법인 'Self-Evolved Reward Learning(SER)'을 제안합니다. 이는 제한된 인간 라벨 데이터를 사용하여 보상 모델을 강화하는 새로운 프레임워크입니다. SER은 연속적인 피드백 스트림을 통해 보상 모델을 스스로 개선하고, 이를 통해 모델이 더욱 적은 외부 데이터를 바탕으로 학습할 수 있게 합니다. 이 방식은 데이터 의존도를 줄이면서도 향상된 성능을 보여주며, 다양한 실험을 통해 그 유효성을 검증받았습니다. 이 접근법은 AI 피드백을 잘 활용함으로써 인간 자원의 사용량을 현저히 줄여줍니다.