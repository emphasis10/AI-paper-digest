# MiMo: Unlocking the Reasoning Potential of Language Model -- From Pretraining to Posttraining
## TL;DR
## Summary
- [https://arxiv.org/pdf/2505.07608.pdf](https://arxiv.org/pdf/2505.07608.pdf)

1. 논문의 각 섹션 요약:

   - **서론**: MiMo-7B는 강화 학습으로 훈련되어 수학적 추론 및 코드 생성과 같은 복잡한 작업에서 뛰어난 성과를 보이는 큰 언어 모델입니다. MiMo-7B의 목표는 고급 추론 기능을 갖춘 모델을 개발하는 것입니다.

   - **사전 훈련**: MiMo-7B는 다양한 출처에서 고품질의 추론 패턴을 포함하는 데이터를 수집하여 사전 훈련되었습니다. 이 과정에서 텍스트 추출 도구와 데이터를 정교화하여 추론 데이터의 밀도를 높였습니다. 또한, 다양한 합성 추론 데이터를 생성하고, 이를 통해 모델의 추론 능력을 강화했습니다.

   - **모델 아키텍처**: MiMo-7B는 그루핑 쿼리 주의, SwiGLU 활성화, RoPE와 같은 요소를 포함하는 디코더 전용 트랜스포머 아키텍처를 채택했습니다. 다단계 데이터 혼합 전략으로 인한 다양한 데이터 처리를 통해, 모델의 추론 능력을 효과적으로 극대화했습니다.

   - **사후 훈련**: MiMo-7B-RL 모델은 RL 데이터 집합을 통해 추가 훈련이 이루어졌습니다. 이 훈련 과정에서는 다양한 난이도 수준의 테스트 케이스를 포함하는 코드 보상을 통한 훈련 전략을 사용하여, RL 정책 최적화를 이루었습니다.

   - **평가 결과**: MiMo-7B-Base 모델은 추론 능력에서 탁월한 성과를 보였고, 32B 옵션보다 뛰어난 고급 수학 및 코드 작업에서 뛰어났습니다. MiMo-7B-RL은 수학과 학문적인 질문에서 성능이 매우 우수하며, 특히 복잡한 질문을 해결하는 데 강점을 보였습니다.

   - **결론**: MiMo-7B 시리즈는 오픈소스로 제공되며, 강력한 추론 능력을 가진 대형 언어 모델을 개발하는 데 기여할 것으로 기대됩니다. 이 모델은 더욱 발전된 대규모 강화 학습의 데이터 및 모델 설계에 유용한 통찰을 제공할 것입니다.

2. 전체 요약:

   MiMo-7B는 사전 훈련 및 사후 훈련 과정을 통해 개발된 대형 언어 모델로, 특히 수학적 추론 및 코드 생성에서 독보적인 성능을 발휘합니다. 다양한 데이터 소스와 세밀화된 데이터 필터링을 통해 고품질의 추론 데이터를 활용하고, 강화 학습을 통해 수학과 코드 작업에서의 성능을 극대화했습니다. MiMo-7B는 기존의 대형 모델과 비교하여도 우수한 성과를 보여주며, 오픈소스로 제공되어 다양한 연구 및 응용 분야에서 활용이 가능할 것입니다.