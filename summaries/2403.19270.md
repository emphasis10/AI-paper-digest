# sDPO: Don’t Use Your Data All at Once
## TL;DR
## Summary
- [https://arxiv.org/pdf/2403.19270.pdf](https://arxiv.org/pdf/2403.19270.pdf)

이 논문은 대규모 언어 모델(LLM) 개발이 진행됨에 따라, 인간 선호도와의 정렬이 점점 더 중요해지고 있음을 지적합니다. 이를 해결하기 위해 직접 선호 최적화(Direct Preference Optimization, DPO)를 확장한 단계별 DPO(sDPO)를 제안합니다. 이 방법은 사용 가능한 선호도 데이터셋을 분할하여 단계별로 활용함으로써, DPO 훈련 프레임워크 내에서 보다 정밀하게 정렬된 참조 모델을 사용할 수 있게 합니다. 또한, sDPO는 최종 모델을 더 높은 성능으로 훈련시키며, 기존의 더 많은 파라미터를 가진 인기 있는 LLM보다 우수한 결과를 달성합니다.

### 소개 및 방법론

- 대규모 언어 모델(LLM)은 NLP 분야에서 획기적인 발전을 이루었습니다. 특히, 이러한 모델의 안전성과 유용성을 보장하기 위해 정렬 튜닝이 필요합니다.
- DPO는 인간 또는 AI의 판단을 사용하여 선호하는 답변과 거부된 답변을 선택하는 선호도 데이터셋을 큐레이션하여 LLM을 훈련시키는 방법입니다.
- sDPO는 선호도 데이터셋을 한 번에 모두 사용하는 대신 단계별로 사용하여 더 정렬된 참조 모델을 사용하는 방법을 제안합니다.

### 실험

- 실험을 통해 sDPO가 다양한 규모의 언어 모델과 데이터셋에 적용할 때 기존 DPO보다 우수한 성능을 보임을 입증합니다.
- 또한, 참조 모델을 단계별로 개선하는 sDPO의 효과를 실험적으로 검증하고, 이를 통해 최종 정렬 모델의 성능을 향상시킵니다.

### 결론 및 연구의 한계

- sDPO는 선호도 데이터를 단계적으로 사용함으로써 더 높은 성능의 모델을 생성할 수 있는 새로운 방법을 제시합니다.
- 이 방법은 복잡한 DPO 데이터 콜렉션을 세분화하는 최적의 전략을 찾는 것과 같은 추가적인 연구가 필요합니다.
- 연구는 주로 SOLAR 10.7B 모델을 사용했으며, 실험 환경의 확장을 통해 다양한 LLM에 대한 더 폭넓은 이해를 얻을 수 있을 것입니다.

이 논문은 대규모 언어 모델의 정렬 튜닝을 개선하는 새로운 방법론을 제안하며, 이는 언어 모델의 성능을 향상시키는 데 기여할 수 있습니다.