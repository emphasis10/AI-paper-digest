# EVLM: An Efficient Vision-Language Model for Visual Understanding
## TL;DR
## Summary
- [https://arxiv.org/pdf/2407.14177.pdf](https://arxiv.org/pdf/2407.14177.pdf)

### 주요 내용 요약

#### 1. 서론
본 논문은 효율적이고 고성능의 멀티모달 비주얼-언어 모델(Efficient Vision-Language Model, EVLM)을 제안하고 있습니다. 이 모델은 대규모 이미지-텍스트 프리트레이닝을 통해 뛰어난 성능을 발휘합니다.

#### 2. 모델 아키텍처
**비쥬얼 인코더:** 
- EVA2-CLIP-E-Plus 모델을 이용하여 비주얼 인코딩 수행.
- 최종 트랜스포머 블록의 뉴럴 네트워크와 헤드 레이어를 제거하고, 트랜스포머의 40개의 계층에서 8개의 피처 시퀀스를 균일하게 샘플링하여 Gated Cross-Attention 레이어로 전달.

**Gated Cross-Attention Layer:**
- Flamingo 모델과 유사하게 비전과 텍스트 간의 상호작용을 위한 Gated Cross-Attention을 사용.
- 시퀀스 길이가 16인 학습 가능한 토큰 세트를 시퀀스에 추가하여 비주얼 피처를 운반하도록 설계.

**대규모 언어 모델:**
- Qwen-14B-Chat 1.0 모델을 이용하여 뛰어난 컨텐츠 이해 및 논리적 추론 능력 발휘.

#### 3. 멀티태스크 연속 프리트레이닝
프리트레이닝 단계와 인스트럭션 파인튜닝 사이에 멀티태스크 연속 프리트레이닝 단계를 도입했습니다. OCR, 이미지 해상도 증가, 텍스트 토큰의 크로스 엔트로피를 최소화하는 목표를 가지고 훈련합니다.

#### 4. 평가
EVLM은 다양한 멀티모달 과제를 평가하여 비주얼 이해 능력을 종합적으로 평가했다. 다수의 벤치마크에서 경쟁 모델을 능가하는 성능을 보였으며, 다양한 과제에서 이미지를 캡션하고 영상 데이터를 처리하는 데 있어서도 뛰어난 성과를 보입니다.

### 전체 요약
이 논문은 새로운 멀티모달 비주얼-언어 모델인 EVLM을 제안하며, 효율적인 훈련 및 성능 향상을 위해 다양한 혁신적인 방법론을 도입했습니다. 모델 아키텍처는 Flamingo를 기반으로 하며, 대규모 이미지-텍스트 데이터셋을 통해 다양한 시각적 입력을 잘 인지할 수 있도록 지속적인 훈련단계를 연동했습니다. 본 모델은 특히 이미지 캡션, 영상 캡션 등에서 경쟁 모델을 능가하는 뛰어난 성능을 발휘했습니다. 

본 논문은 EVLM의 설계, 훈련, 실험 결과를 상세히 다루며, 향후 연구 방향으로 더 강력한 언어 모델을 사용하고, 긴 시퀀스에 대한 비디오 이해 능력을 탐구하는 것을 제안합니다.

## Similar Papers
- [ConvLLaVA: Hierarchical Backbones as Visual Encoder for Large Multimodal Models](2405.15738.md)
- [Parrot: Multilingual Visual Instruction Tuning](2406.02539.md)
- [Video-to-Audio Generation with Hidden Alignment](2407.07464.md)
- [CatLIP: CLIP-level Visual Recognition Accuracy with 2.7x Faster Pre-training on Web-scale Image-Text Data](2404.15653.md)
- [A Survey on Mixture of Experts](2407.06204.md)
- [MobileCLIP: Fast Image-Text Models through Multi-Modal Reinforced Training](2311.17049.md)
- [Hunyuan-DiT: A Powerful Multi-Resolution Diffusion Transformer with Fine-Grained Chinese Understanding](2405.08748.md)
- [LLoCO: Learning Long Contexts Offline](2404.07979.md)
- [VSSD: Vision Mamba with Non-Causal State Space Duality](2407.18559.md)
