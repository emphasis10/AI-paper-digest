# SambaNova SN40L: Scaling the AI Memory Wall with Dataflow and Composition of Experts
## TL;DR
## Summary
- [https://arxiv.org/pdf/2405.07518.pdf](https://arxiv.org/pdf/2405.07518.pdf)

### 주요 내용 요약

1. **서론 및 배경**:
   - 이 논문에서는 **SambaNova SN40L**이라는 데이터플로우 가속기를 활용하여 AI 메모리 벽을 극복하는 방법을 소개합니다. SambaNova SN40L은 전문가 모델의 조합(Composition of Experts, CoE) 시스템을 사용하여 수천억 개의 파라미터를 가진 단일 LLM(대규모 언어 모델)과 경쟁할 수 있는 효율적인 솔루션을 제시합니다. 기존의 대규모 모델과 달리, CoE는 다수의 소규모 전문가 모델을 조합하여 비용과 복잡성을 줄입니다.

2. **방법론**:
   - SambaNova SN40L은 150개의 전문가 모델과 1조 개의 파라미터를 가진 CoE 시스템을 구현합니다. 이 시스템은 새로운 3단계 메모리 시스템을 도입하여 온칩 SRAM, 온패키지 HBM, 오프패키지 DDR DRAM을 결합합니다. 이를 통해 다양한 벤치마크에서 2배에서 13배의 성능 향상을 보여줍니다. 또한, 모델 전환 시간을 단축시키고 시스템의 메모리 사용 효율성을 극대화합니다.

3. **실험**:
   - 여러 벤치마크에서 SambaNova SN40L의 성능을 평가한 결과, 8개의 RDU 소켓을 사용하여 비연결 기준 대비 2배에서 13배의 속도 향상을 달성했습니다. 또한, Samba-CoE의 추론 배포에서 8-소켓 RDU 노드는 머신 풋프린트를 최대 19배 줄이고, 모델 전환 시간을 15배에서 31배 단축하며, 전체적으로 DGX H100 대비 3.7배, DGX A100 대비 6.6배의 속도 향상을 보였습니다.

### 혁신적인 부분
SambaNova SN40L의 혁신성은 3단계 메모리 시스템을 도입하여 AI 메모리 벽을 극복하고, CoE 시스템을 통해 대규모 모델의 비용과 복잡성을 줄이는 데 있습니다. 또한, 스트리밍 데이터플로우와 공격적인 연산자 결합을 통해 다양한 AI 작업에서 높은 성능을 유지하면서 메모리 사용을 최적화합니다. 이러한 접근 방식은 대규모 언어 모델을 더 효율적으로 운영할 수 있게 하며, 특히 메모리 제약이 큰 상황에서 큰 이점을 제공합니다.

## Similar Papers
- [AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration](2306.00978.md)
- [SambaLingo: Teaching Large Language Models New Languages](2404.05829.md)
- [LLaMA-NAS: Efficient Neural Architecture Search for Large Language Models](2405.18377.md)
- [Accelerating LLM Inference with Staged Speculative Decoding](2308.04623.md)
- [The Impact of Hyperparameters on Large Language Model Inference Performance: An Evaluation of vLLM and HuggingFace Pipelines](2408.01050.md)
- [Inference Performance Optimization for Large Language Models on CPUs](2407.07304.md)
- [LLMCad: Fast and Scalable On-device Large Language Model Inference](2309.04255.md)
- [Evolutionary Optimization of Model Merging Recipes](2403.13187.md)
- [A Survey of Multi-Tenant Deep Learning Inference on GPU](2203.09040.md)
