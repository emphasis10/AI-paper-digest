# $μ$LO: Compute-Efficient Meta-Generalization of Learned Optimizers
## TL;DR
## Summary
- [https://arxiv.org/pdf/2406.00153.pdf](https://arxiv.org/pdf/2406.00153.pdf)

### 주요 내용 요약

#### 1. Introduction (서론)
딥러닝의 발전은 많은 혁신을 가져왔으며, 특히 신경망의 학습 최적화에서 중요한 역할을 합니다. 기존의 수작업으로 설계된 최적화 기법들은 최적의 성능을 보장하지 못하며, 이를 개선하기 위해 데이터 기반의 학습 최적화 방법이 주목받고 있습니다. 이 논문에서는 학습된 최적화기(LO)의 메타-일반화 문제를 해결하기 위해 Maximal Update Parametrization (µP) 이론을 적용합니다.

#### 2. Related Work (관련 연구)
이 섹션에서는 학습된 최적화기와 그 일반화 문제에 대한 기존 연구들을 소개합니다. 또한, 학습 최적화기에서 µP를 사용하는 방법과 기존의 표준 파라미터화(SP)와의 비교 연구를 설명합니다.

#### 3. Method (방법론)
- **Background:** 학습 최적화기의 메타 학습 문제를 정의하고, 이를 해결하기 위한 표준 접근 방식을 설명합니다.
- **µLO:** µP를 사용하여 학습 최적화기를 구성하는 방법을 소개합니다. 여기에는 초기화 방법, 업데이트 방법, 그리고 µP가 최대 업데이트 파라미터화를 달성하는 데 필요한 이유를 설명하는 이론적 근거가 포함됩니다.

#### 4. Empirical Evaluation (실험 평가)
- **Pre-activation Stability:** µP가 학습 과정에서 신경망의 사전 활성화 안정성을 어떻게 유지하는지 실험적으로 검증합니다.
- **Meta-generalization:** µLO가 더 넓은 네트워크, 더 깊은 네트워크 및 더 긴 학습 기간에 대해 일반화할 수 있는 능력을 평가합니다. µLO는 VeLO와 비교하여 상당한 성능 향상을 보였습니다.
- **Far Out-of-distribution Tasks:** µLO가 메타 학습에 사용되지 않은 새로운 작업에 대해 얼마나 잘 일반화하는지 평가합니다.
- **Longer Training Horizons:** µLO가 훨씬 더 긴 학습 기간 동안 일반화할 수 있는 능력을 평가합니다.

#### 5. Limitations (제한 사항)
논문의 제한 사항으로는 메타 학습이 이미지 분류를 위한 MLP에만 집중되었으며, 더 넓은 모델에 대한 평가가 포함되지 않았다는 점을 언급합니다.

#### 6. Conclusion (결론)
µP를 사용한 학습 최적화기는 메타 학습 비용을 증가시키지 않으면서 더 넓은 네트워크와 더 긴 학습 기간에 대해 뛰어난 일반화 성능을 보여줍니다. 이는 저비용 메타 학습의 가능성을 제시합니다.

### 전반적인 요약
이 논문은 학습된 최적화기(LO)가 새로운 문제에 대해 일반화하는 데 겪는 어려움을 해결하기 위해 Maximal Update Parametrization (µP) 이론을 적용한 연구입니다. 기존의 표준 파라미터화(SP) 방식에 비해 µP를 사용하면 더 넓고 깊은 신경망 및 더 긴 학습 기간에 대해 뛰어난 성능을 발휘할 수 있음을 실험적으로 입증했습니다. µLO는 메타 학습 비용을 크게 증가시키지 않으면서 VeLO와 같은 대규모 학습된 최적화기보다도 더 나은 성능을 보여줍니다. 이는 비용 효율적인 메타 학습 최적화기의 발전에 기여할 수 있는 중요한 연구입니다.