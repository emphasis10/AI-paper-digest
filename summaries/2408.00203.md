# OmniParser for Pure Vision Based GUI Agent
## TL;DR
## Summary
- [https://arxiv.org/pdf/2408.00203.pdf](https://arxiv.org/pdf/2408.00203.pdf)

### 1. 섹션별 요약

#### Introduction
이 논문은 GPT-4V 등 대형 비전-언어 모델이 여러 운영체제와 애플리케이션에서 UI(screen) 상의 작업을 정확하게 수행할 수 있도록 돕기 위해 OMNIPARSER라는 새로운 방법론을 제안합니다. OMNIPARSER는 UI 스크린샷을 구조화된 요소로 파싱하여 대형 비전-언어 모델이 스크린 상의 특정 영역과 관련된 행동을 정확하게 예측하도록 도와줍니다.

#### Related Works
기존 연구에서는 UI 화면의 세부 이해에 집중한 여러 모델들을 제안했으나, 이들은 HTML 정보나 추가적인 뷰 계층 구조으
이 데이터에 의존하는 경향이 있습니다. OMNIPARSER는 이러한 제한을 극복하고자 합니다.

#### Methods
OMNIPARSER는 두 개의 정교하게 조정된 모델(인터랙터블 아이콘 감지 모델, 기능 설명 모델)과 OCR 모듈을 결합하여 UI를 파싱합니다. 이는 사용자가 수행할 작업을 예측하기 전에 화면의 정보를 구조화하여 제공함으로써 대형 비전-언어 모델의 부담을 줄이고 예측의 정확성을 높입니다.

#### Experiments and Results
OMNIPARSER는 여러 벤치마크(ScreenSpot, Mind2Web, AITW)에서 기존의 방법들에 비해 성능이 크게 향상되었습니다. 특히, SeeClick, CogAgent, Fuyu 등과 비교했을 때도 높은 정확도를 보였습니다.

#### Discussions
OMNIPARSER의 일반적인 실패 사례와 이를 개선하기 위한 잠재적 방법에 대해 논의합니다. 예를 들어, 반복되는 아이콘 및 텍스트, 예측된 박스의 부정확성, 아이콘의 오해 등이 있습니다.

#### Conclusion
OMNIPARSER는 PC 및 모바일 플랫폼 전반에서 추가적인 HTML 정보 없이 UI 화면을 파싱할 수 있는 일반적인 도구로서 GPT-4V의 성능을 크게 향상시킵니다.

### 2. 전체 요약

OMNIPARSER는 UI 스크린샷을 구조화된 요소로 파싱하여 대형 비전-언어 모델(GPT-4V 등)이 여러 플랫폼과 애플리케이션에서 사용자 인터페이스 관련 작업을 더 정확하게 예측하고 수행할 수 있도록 돕습니다. 이 방법론은 인터랙터블 아이콘 감지 모델과 기능 설명 모델을 결합하여 추가적인 HTML 정보 없이도 높은 성능을 입증합니다. 다양한 벤치마크 시험에서 OMNIPARSER는 기존의 다른 모델들보다 더 나은 성능을 보였으며, 특히 일반적인 실패 사례에 대한 분석과 개선 방법도 제시합니다. 이를 통해 OMNIPARSER는 다양한 기기 및 애플리케이션에서 AI의 효율성과 정확성을 한층 더 끌어올릴 수 있는 가능성을 제시합니다.

## Similar Papers
- [AMEX: Android Multi-annotation Expo Dataset for Mobile GUI Agents](2407.17490.md)
- [Read Anywhere Pointed: Layout-aware GUI Screen Reading with Tree-of-Lens Grounding](2406.19263.md)
- [Ferret-UI: Grounded Mobile UI Understanding with Multimodal LLMs](2404.05719.md)
- [ReXTime: A Benchmark Suite for Reasoning-Across-Time in Videos](2406.19392.md)
- [Gemini vs GPT-4V: A Preliminary Comparison and Combination of Vision-Language Models Through Qualitative Cases](2312.15011.md)
- [VideoGUI: A Benchmark for GUI Automation from Instructional Videos](2406.10227.md)
- [AgentInstruct: Toward Generative Teaching with Agentic Flows](2407.03502.md)
- [WILBUR: Adaptive In-Context Learning for Robust and Accurate Web Agents](2404.05902.md)
- [Samba: Simple Hybrid State Space Models for Efficient Unlimited Context Language Modeling](2406.07522.md)
