# Transformers to SSMs: Distilling Quadratic Knowledge to Subquadratic Models
## TL;DR
## Summary
- [https://arxiv.org/pdf/2408.10189.pdf](https://arxiv.org/pdf/2408.10189.pdf)

### 1. 섹션별 중요한 내용 요약

#### Introduction (서론)
이 논문은 Transformer 아키텍처가 자연어 처리에서 중요한 역할을 하지만, 이와 관련된 높은 연산 비용 문제 때문에 서브쿼드라틱(거듭제곱 이하) 모델의 필요성을 강조합니다. 이 논문에서는 "MOHAWK"라는 기법을 도입하여, 사전 훈련된 Transformer 모델의 지식을 서브쿼드라틱 모델인 상태 공간 모델(State Space Models, SSM)로 증류(distillation)하는 방법을 제안합니다.

#### MOHAWK Method (MOHAWK 방법론)
MOHAWK는 Transformer 모델의 지식 요소를 단계적으로 SSM으로 증류하는 세 단계 프로세스를 사용합니다. 첫 번째 단계는 시퀀스 변환 행렬을 맞추는 것이고, 두 번째 단계는 숨겨진 상태를 정렬하는 것이며, 세 번째 단계는 전체 예측을 맞추는 것입니다. 이 방법론을 통해 작은 데이터셋으로도 높은 성능을 얻을 수 있습니다.

#### Mamba-2 및 Phi-Mamba모델
Mamba-2는 구조화된 상태 공간 모델로, Transformer의 Attention 메커니즘과 유사한 시퀀스 변환을 수행합니다. Phi-Mamba는 이 Mamba-2를 기반으로 한 하이브리드 모델로, 일부 Attention 레이어를 유지하고 나머지는 Mamba-2 블록으로 대체하여 설계되었습니다.

#### Hybrid Phi-Mamba Model (하이브리드 Phi-Mamba 모델)
하이브리드 Phi-Mamba 모델은 기존의 Attention 레이어 다수를 Mamba-2 블록으로 대체하여 높은 성능을 유지하면서도 연산 비용을 줄이는 목표를 가지고 개발되었습니다. 이 모델은 Attention 블록을 줄이면서도 거의 동일한 성능을 기록하였습니다.

#### Empirical Validation (실증 검증)
실험 결과, MOHAWK와 Phi-Mamba 모델은 기존의 서브쿼드라틱 모델들보다 높은 정확도를 기록했으며, 특히 큰 데이터셋을 사용하지 않고도 강력한 성능을 보였습니다. 이 방법론은 학습 비용 대비 높은 성능을 제공하여 향후 연구에 중요한 기초가 될 것입니다.

### 2. 전체 요약
이 논문은 Transformer 모델의 높은 연산 비용 문제를 해결하기 위해 서브쿼드라틱 상태 공간 모델(SSM)로 지식을 증류하는 방법론인 "MOHAWK"를 제안합니다. 주요 내용은 다음과 같습니다:

1. **서론**: Transformer 모델의 연산비용 문제와 서브쿼드라틱 모델의 필요성을 제기합니다.
2. **MOHAWK 방법론**: 세 단계의 증류 과정(시퀀스 변환 맞춤, 숨겨진 상태 정렬, 전체 예측 맞춤)을 통해 Transformer의 지식을 SSM으로 이전합니다.
3. **Phi-Mamba 및 하이브리드 모델**: Mamba-2와 하이브리드 Phi-Mamba 모델을 사용하여 성능을 극대화하고 비용을 최소화합니다.
4. **실증 검증**: 소량의 데이터셋으로도 매우 높은 성능을 기록하며, 기존 서브쿼드라틱 모델들에 비해 우수한 성능을 보입니다.

MOHAWK는 이러한 증류 과정을 통해 SSM이 Transformer의 학습 자원을 활용할 수 있게 하여, 향후 서브쿼드라틱 모델의 개발에 큰 기여를 할 수 있습니다.