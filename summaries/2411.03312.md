# Inference Optimal VLMs Need Only One Visual Token but Larger Models
## TL;DR
## Summary
- [https://arxiv.org/pdf/2411.03312.pdf](https://arxiv.org/pdf/2411.03312.pdf)

### 1. 연구 목적
이 논문은 Vision Language Models (VLMs)의 **추론 최적화**를 위해 **시각 토큰 수와 언어 모델(LLM) 크기 간의 최적 트레이드오프**를 탐구합니다. 특히, 추론 비용을 줄이기 위해 극단적으로 적은 수의 시각 토큰을 사용하는 동시에, 가능한 한 큰 LLM을 활용하는 전략의 필요성을 강조합니다.

---

### 2. 섹션별 요약

#### **1) Introduction (소개)** 
VLM은 이미지와 텍스트를 동시에 처리하는 강력한 모델로 발전했으나, **추론 시 높은 계산 비용**과 **지연 시간**이 문제입니다. 기존 연구는 시각 토큰을 줄이려는 "토큰 압축" 방법에 집중했지만, 시각 토큰과 LLM 크기 간 최적 균형은 명확하지 않았습니다.  
**주요 발견**:
- 추론 비용 내에서 최적 성능을 내려면 가능한 큰 LLM을 사용하면서 시각 토큰 수를 최소화해야 합니다.
- 일부 작업에서는 시각 토큰을 극단적으로 줄이는 것이 필수적임을 강조합니다.  

---

#### **2) Preliminaries (기초 지식)**

1. **추론 비용(FLOPs)의 계산**:
   - FLOPs = \(N \times T\)  
     \(N\): LLM의 파라미터 수, \(T\): 처리해야 할 총 토큰 수  
     여기서 \(T\)는 텍스트 토큰(Q), 시각 토큰(V), 생성된 토큰(G)의 합입니다.
2. **시각 토큰 압축**:
   - 기존 모델(CLIP 등)은 이미지를 고정된 수의 패치(576개)로 변환합니다.
   - 압축을 통해 토큰 수를 줄이고, 이를 LLM에 전달해 계산량을 절감할 수 있습니다.  

---

#### **3) Inference Scaling Laws (추론 스케일링 법칙)**

1. **스케일링 법칙의 제안**:
   모델 성능을 LLM 크기와 시각 토큰 수의 함수로 모델링:
   \[
   Y(N, T) = \frac{A}{N^\alpha} + \frac{B}{T^\beta} + D
   \]
   \(N\): LLM 파라미터, \(T\): 시각 토큰 수, \(A, B, D, \alpha, \beta\): 학습된 파라미터  
   여기서 \( \alpha \)는 LLM의 품질, \( \beta \)는 토큰 품질을 나타냅니다.
2. **결과 분석**:
   - **LLM 크기 증가가 성능에 더 큰 영향을 미침**: \( \alpha = 0.077 \), \( \beta = 0.015 \)로 LLM 크기가 토큰 수보다 5배 더 중요한 영향을 미침.
   - **텍스트 길이에 따른 변화**:
     텍스트 입력이 길어질수록 시각 토큰 수를 더 늘리는 것이 유리할 수 있음.

---

#### **4) Query-Based Token Compression (질의 기반 토큰 압축 기법)**

1. **필요성**:
   - 극단적 토큰 감소(1~16개 토큰)에서도 성능을 유지하기 위해 사용자의 질의에 따라 가장 중요한 정보를 유지하는 방식 필요.
2. **방법**:
   - 텍스트 임베딩을 사용하여 시각 토큰과 결합한 뒤, **학습 가능한 컨볼루션 필터**를 통해 정보를 압축.

---

#### **5) 실험 결과**  
QueCC(제안된 방법)는 **극단적인 토큰 감소(1~4개 토큰)**에서도 기존 방법보다 더 나은 성능을 달성했습니다. 특히, 사용자의 질의에 따라 중요한 정보를 선택적으로 압축하는 방식이 효과적이었습니다.

---

### 3. 논문의 주요 기여
- **추론 최적화에 필요한 새로운 스케일링 법칙**을 제안: LLM 크기와 시각 토큰 수 간의 최적 균형을 이론적으로 설명.
- **극단적 토큰 압축을 위한 Query-Based Token Compression 기법**을 개발: 사용자의 입력 질의와 관련된 중요한 정보를 유지하면서 성능 저하를 최소화.
- **실제 시각 추론 작업에서의 비용 효율성을 입증**: 큰 LLM과 적은 시각 토큰이 최적 조합임을 강조.

---

### 4. 논문의 전체 요약
이 논문은 VLM 추론 성능을 최적화하기 위해 LLM 크기와 시각 토큰 수 간의 관계를 새롭게 조명했습니다. 특히, 시각 토큰을 극단적으로 줄이고, 가능한 큰 LLM을 사용하는 것이 최적의 성능을 달성하는 방법임을 발견했습니다. 또한, 질의 기반 압축 기법(QueCC)을 통해 극단적인 토큰 감소에서도 성능을 유지할 수 있음을 입증했습니다. 이는 미래 VLM 개발에서 중요한 방향성을 제공합니다. 