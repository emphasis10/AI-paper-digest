# MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression
## TL;DR
## Summary
- [https://arxiv.org/pdf/2406.14909.pdf](https://arxiv.org/pdf/2406.14909.pdf)

1. 각 섹션의 요약 및 논문 전체 요약:

### 섹션별 요약:

#### 1. 서론
이 섹션에서는 대규모 언어 모델(LLM)의 다양성과 주의 메커니즘이 특히 중요한 역할을 한다고 설명합니다. LLM의 성능을 향상시키기 위해 입력 길이를 확장하는 것이 중요하지만, 이는 계산 및 메모리 요구사항을 증가시킵니다.

#### 2. 예비 지식 및 관련 연구
이 섹션에서는 주의 메커니즘과 효율적인 주의 방법에 대해 설명합니다. 특히 다중-헤드 자가 주의(MHA) 메커니즘과 기존의 희소 주의 방법에 대해 다룹니다.

#### 3. MoA (Mixture of Attention) 제안
이 섹션에서는 MoA를 제안합니다. MoA는 LLM의 각 레이어와 주의 헤드에 대해 다양한 희소 주의 설정을 자동으로 최적화하는 방법입니다. 이를 통해 LLM의 맥락 길이를 3.9배 확장하고, 검색 정확도를 1.5배에서 7.1배 증가시킵니다.

#### 4. MoA의 자동 압축 파이프라인
MoA의 압축 파이프라인을 설명합니다. MoA는 먼저 각 주의 값의 손실 영향을 프로파일링하고, 그 다음 다양한 길이에서 최적의 규칙을 선택합니다. 이를 통해 각각의 주의 헤드에 대해 정확도 손실을 최소화합니다.

#### 5. 실험
MoA의 성능을 평가하기 위해 여러 기준과 모델을 대상으로 실험을 진행합니다. MoA는 50% 밀도에서 GPU 메모리를 1.2배에서 1.4배 줄이고, 디코딩 처리량을 5.5배에서 6.7배 증가시키는 것으로 나타났습니다.

#### 6. 결론 및 향후 연구
MoA는 LLM의 각 주의 헤드와 입력 길이에 맞춤형 희소 주의 마스크를 자동으로 선택합니다. 한계점으로는 매우 낮은 밀도 예산에서의 성능 저하가 있으며, 이는 향후 연구에서 개선될 수 있습니다.

### 논문의 주요 기여와 혁신 부분:
- MoA는 이질적인 탄성 규칙을 사용하여 LLM의 각 레이어와 주의 헤드에 맞춤형 희소 주의 설정을 제공합니다.
- MoA의 자동화된 최적화 파이프라인은 다양한 입력 길이에 대해 최적의 압축 계획을 찾도록 설계되었습니다.
- MoA는 LLM의 맥락 길이를 3.9배 확장하며, 희소 및 밀집 모델 간의 성능 차이를 크게 줄입니다.

### 전체 요약:
이 논문은 LLM의 효율성을 높이기 위해 희소 주의 방법을 자동으로 최적화하는 'Mixture of Sparse Attention (MoA)'를 제안합니다. MoA는 각 레이어와 주의 헤드에 대해 다양한 희소 주의 설정을 자동으로 최적화하며, 이를 통해 LLM의 맥락 길이를 3.9배로 확장하고 검색 정확도를 크게 향상시킵니다. 또한 MoA의 자동화된 최적화 파이프라인을 통해 다양한 입력 길이에 대해 최적의 압축 계획을 찾을 수 있습니다. 실험 결과, MoA는 GPU 메모리를 줄이고 디코딩 처리량을 크게 향상시키는 것으로 나타났으며, 이는 LLM의 성능을 유지하면서도 효율성을 극대화하는 데 기여합니다.

## Similar Papers
- [DiTFastAttn: Attention Compression for Diffusion Transformer Models](2406.08552.md)
- [Mixture-of-Depths: Dynamically allocating compute in transformer-based language models](2404.02258.md)
- [SnapKV: LLM Knows What You are Looking for Before Generation](2404.14469.md)
- [Mixture of Nested Experts: Adaptive Processing of Visual Tokens](2407.19985.md)
- [Can LLMs Learn by Teaching? A Preliminary Study](2406.14629.md)
- [Transfer Learning for Structured Pruning under Limited Task Data](2311.06382.md)
- [A Survey on Efficient Inference for Large Language Models](2404.14294.md)
- [ShiftAddLLM: Accelerating Pretrained LLMs via Post-Training Multiplication-Less Reparameterization](2406.05981.md)
- [Fast Matrix Multiplications for Lookup Table-Quantized LLMs](2407.10960.md)
