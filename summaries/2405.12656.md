# Retrieval-Augmented Language Model for Extreme Multi-Label Knowledge Graph Link Prediction
## TL;DR
## Summary
- [https://arxiv.org/pdf/2405.12656.pdf](https://arxiv.org/pdf/2405.12656.pdf)

#### 1. 서론 및 관련 연구 (Introduction and Related Works)
대형 언어 모델(LLMs)은 개방형 질문에 대한 추론에서 환각 문제와 높은 훈련 비용이라는 두 가지 주요 문제에 직면합니다. 기존 연구들은 언어 모델의 입력을 지식 그래프(KG) 정보로 보강하는 방식을 통해 이 문제를 해결하려고 시도했습니다. 그러나 이 방식은 두 가지 한계를 가지고 있습니다. 첫째, KG의 한 홉 이웃에서 관련 정보를 추출하지 못합니다. 둘째, 다양한 특성을 가진 KG에 동일한 보강 전략을 적용해 성능이 저하될 수 있습니다. 이를 해결하기 위해 우리는 새로운 과제인 극단적인 다중 라벨 KG 링크 예측 작업을 제안합니다. 이는 구조화된 실제 세계 지식을 사용하여 다중 응답을 예측하도록 모델을 강화합니다.

#### 2. 방법론 (Method)
이 연구의 주요 목표는 주어진 지식 그래프(G)에서 다중 라벨 KG 링크 예측 문제를 해결하는 것입니다. 지식 그래프는 엔티티 집합(V)과 관계 집합(E)으로 구성된 방향성 그래프입니다. 이 문제를 해결하기 위해, 우리는 다음과 같은 단계를 포함하는 프레임워크를 제안합니다:
1. **문제 정의**: 다중 라벨 KG 링크 예측 문제를 정의하고, 각 트리플(h, r, t)에 대해 여러 개의 대상 엔티티(t)를 예측하는 과제입니다.
2. **데이터셋 준비**: 두 가지 대형 지식 그래프인 WN18RR과 FB15k-237을 사용합니다. 각 그래프의 엔티티 설명 데이터셋을 준비하여 모델의 입력으로 사용합니다.
3. **프레임워크**: BERT 모델을 사용하여 입력 텍스트를 처리하고, SimCSE 모델을 사용하여 가장 관련성 높은 한 홉 이웃을 선택합니다. 선택된 한 홉 이웃의 설명을 포함한 입력 텍스트를 BERT 모델에 입력하여 예측을 수행합니다.

#### 3. 실험 연구 (Experimental Study)
우리는 제안된 방법의 성능을 XMTC(극단적인 다중 라벨 텍스트 분류) 작업의 모델들과 비교했습니다. 실험 결과, 제안된 모델이 WN18RR과 FB15k-237 데이터셋 모두에서 경쟁 모델들보다 높은 정확도를 보였습니다. 특히, 우리의 모델은 P@k (k=1, 3, 5)에서 높은 성능을 나타냈습니다.

#### 4. 제거 연구 (Ablation Study)
제안된 프레임워크의 다양한 구성 요소를 제거하거나 대체하여 성능에 미치는 영향을 평가했습니다. 그 결과, d(트리플 설명)와 SimCSE 모델이 프레임워크에 필수적이며, 필터링된 1-홉 선택 전략이 성능에 중요한 영향을 미친다는 것을 확인했습니다. 또한, 제안된 손실 함수와 훈련 전략이 극단적인 다중 라벨 KG 링크 예측 작업에 대해 높은 성능을 보이는 것으로 나타났습니다.

#### 5. 결론 (Conclusion)
우리의 실험은 지식 그래프의 특성에 따라 다른 보강 전략이 필요함을 보여줍니다. WN18RR은 많은 비연결 트리플과 낮은 1-홉 이웃 수를 가지고 있어 추론이 어려우므로, 설명 데이터와 1-홉 이웃을 추가 정보로 사용하여 입력을 보강했습니다. 반면, FB15k-237은 대부분의 엔티티가 고유 명사이며, 설명이 중요한 역할을 합니다. 우리는 BERT 모델의 입력에 가장 관련성 높은 1-홉 이웃과 그 설명을 선택하여 성능을 향상시켰습니다.

### 전체 요약
이 논문은 대형 언어 모델(LLMs)의 환각 문제와 높은 훈련 비용을 해결하기 위해 극단적인 다중 라벨 지식 그래프 링크 예측 작업을 제안합니다. 이 작업은 구조화된 실제 세계 지식을 사용하여 여러 응답을 예측하는 모델을 강화합니다. 우리는 두 가지 대형 지식 그래프(WN18RR과 FB15k-237)를 사용하여 제안된 프레임워크를 평가했으며, 제안된 모델이 경쟁 모델들보다 높은 성능을 보임을 확인했습니다. 또한, 다양한 보강 전략과 손실 함수, 훈련 전략이 성능 향상에 중요한 역할을 한다는 것을 확인했습니다. 이 연구는 다양한 특성을 가진 지식 그래프에 맞춤형 보강 전략을 적용하여 다중 라벨 예측 성능을 향상시키는 방법을 제시합니다.

## Similar Papers
- [Mitigating Hallucinations in Large Language Models via Self-Refinement-Enhanced Knowledge Retrieval](2405.06545.md)
- [BiomedRAG: A Retrieval Augmented Large Language Model for Biomedicine](2405.00465.md)
- [RATT: A Thought Structure for Coherent and Correct LLM Reasoning](2406.02746.md)
- [mDPO: Conditional Preference Optimization for Multimodal Large Language Models](2406.11839.md)
- [Depth Anywhere: Enhancing 360 Monocular Depth Estimation via Perspective Distillation and Unlabeled Data Augmentation](2406.12849.md)
- [DogeRM: Equipping Reward Models with Domain Knowledge through Model Merging](2407.01470.md)
- [LVLM-Interpret: An Interpretability Tool for Large Vision-Language Models](2404.03118.md)
- [Arcee's MergeKit: A Toolkit for Merging Large Language Models](2403.13257.md)
- [GNN-RAG: Graph Neural Retrieval for Large Language Model Reasoning](2405.20139.md)
