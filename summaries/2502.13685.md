# MoM: Linear Sequence Modeling with Mixture-of-Memories
## TL;DR
## Summary
- [https://arxiv.org/pdf/2502.13685.pdf](https://arxiv.org/pdf/2502.13685.pdf)

1. 요약

- **초록**: 본 논문은 AI 기술을 개선하기 위해 '메모리 혼합(Mixture-of-Memories, MoM)'이라는 새로운 아키텍처를 제안합니다. MoM은 독립적인 메모리 상태들을 이용하며, 입력 토큰을 각각의 메모리 상태로 라우팅하여 막대한 메모리 용량 증대를 이루고, 메모리 간섭을 최소화합니다. 이로써 기존 선형 시퀀스 모델보다 성능이 우수하며, 메모리 상태별로 컴퓨팅 복잡도가 선형으로 유지됩니다.

- **도입부**: 주의 메커니즘은 AI의 여러 분야에 기여하였지만, 전통적 Transformer는 시퀀스 길이에 비례하여 복잡도가 증가하는 단점이 있습니다. 이를 극복하기 위해 여러 선형 시퀀스 모델링 방식이 제안되었으나, 고정된 사이즈의 메모리 상태로 정보가 압축되면서 제한된 메모리 용량과 메모리 간섭 문제가 발생합니다.

- **방법론**: MoM은 입력 데이터를 여러 메모리 상태에 분산하여 저장합니다. 이는 접촉 간섭을 방지하고 다양한 입력 정보를 보존할 수 있으며, 생물학적 메커니즘에서 영감을 받아 설계되었습니다.

- **실험 결과**: MoM은 다양한 실험에서 선형 시퀀스 모델들보다 우수한 성능을 보였으며, 특히 회상 집약적 작업에서 Transformer 모델에 필적할 만한 성능을 나타냈습니다. 

- **결론**: MoM은 메모리 용량을 확장하고 메모리 간섭을 제거함으로써 선형-시간 학습 및 일정한 메모리 이용으로 효율성을 유지하며, 회상-강화 성능을 요구하는 분야에 효과적입니다.

2. 전체 요약

본 논문은 기존 선형 시퀀스 모델의 메모리 용량 문제를 해결하기 위해 '메모리 혼합' 아키텍처를 제안했습니다. 다중 메모리 상태와 입력 토큰의 라우팅을 통해 막대한 메모리 용량을 유지하고, 메모리 간섭을 최소화하였습니다. 실험 결과, MoM은 일차적으로 회상 집약적 작업에서 높은 성능을 발휘하며, 효율성과 성능을 동시에 개선할 수 있는 가능성을 제시합니다.