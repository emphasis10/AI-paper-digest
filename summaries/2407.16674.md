# KAN or MLP: A Fairer Comparison
## TL;DR
## Summary
- [https://arxiv.org/pdf/2407.16674.pdf](https://arxiv.org/pdf/2407.16674.pdf)

### 1. 각 섹션 요약

#### Abstract
이 논문은 새로운 방법을 소개하지 않고, KAN(콜모고로프-아르놀드 네트워크)과 MLP(다층 퍼셉트론) 모델을 여러 작업에 걸쳐 공정하고 포괄적으로 비교합니다. 주된 관찰 결과는, 기호 공식 표현 작업을 제외하고는 MLP가 일반적으로 KAN보다 성능이 뛰어나다는 점입니다. B-스플라인 활성화를 적용한 MLP는 기호 공식 표현에서 KAN을 추월하거나 해당 수준을 상회합니다. 그러나 MLP가 이미 KAN을 능가하는 다른 작업들에서는 B-스플라인 활성화가 성능을 크게 향상시키지 않습니다. 

#### Introduction
MLP는 현대 딥러닝 모델의 기본 구성 요소로 여러 층의 노드로 구성되어 있습니다. 반면, KAN은 기존의 MLP와 달리 가변 가중치 대신 학습 가능한 B-스플라인 함수를 사용하여 노드가 아닌 엣지에 활성 함수를 적용합니다. 이 논문에서는 동일한 파라미터나 FLOPs(초당 부동 소수점 연산수)를 사용하는 환경에서 KAN과 MLP의 성능을 여러 작업에 걸쳐 공정하게 비교합니다. 결과적으로 KAN이 기호 공식 표현에서만 성능이 우세하며, 다른 작업에서는 MLP가 뛰어납니다.

#### Methods
KAN과 MLP의 수식을 제시하며, 특히 활성 함수와 선형 변환의 순서가 두 구조의 차별화 요소임을 강조합니다. KAN은 비선형 변환을 먼저 수행하고, 그 뒤에 선형 변환을 수행합니다.

#### Experiments
다양한 도메인에서 KAN과 MLP를 비교했습니다. 기계 학습, 컴퓨터 비전, 자연어 처리, 오디오 처리, 기호 공식 표현 작업에서 실험을 했습니다. 실험 결과, KAN은 기호 공식 표현 작업에서 MLP를 초과하는 성능을 보였으나, 다른 작업에서는 MLP가 우수했습니다.

#### Performance Comparison
기호 공식 표현 작업에서 KAN은 파라미터 수를 제어하였을 때 8개의 데이터셋 중 7개에서 MLP를 능가했습니다. FLOPs를 제어했을 때는 MLP와 유사한 성능을 보였으며, 일부 데이터셋에서만 초과 성능을 보였습니다. 다음으로 ARCHITECTURE ABLATION 실험을 통해 KAN과 MLP의 구조적 차이를 분석했습니다.

#### Continual Learning
MNIST 데이터셋을 활용해 학습 모델의 지속적인 학습 능력을 평가했습니다. MLP는 KAN보다 안정적으로 학습을 유지했으며, KAN은 잊어버리는 속도가 빨랐습니다. 이는 KAN이 지속적 학습 상황에서도 MLP보다 열등함을 보여줍니다.

#### Conclusion
KAN의 기호 공식 표현에서의 우월성은 B-스플라인 활성 함수 덕분입니다. 그러나 다른 작업에서는 B-스플라인 활성 함수가 MLP 성능을 크게 향상시키지 않습니다. KAN은 지속적 학습에서 더 많은 잊어버리는 문제를 겪었습니다. 이 결과는 향후 연구에 중요한 통찰을 제공합니다.

### 2. 전체 요약

이 논문은 KAN과 MLP의 성능을 여러 작업에 걸쳐 공정하게 비교하면서, KAN이 기호 공식 표현에서 뛰어난 성능을 가지지만 다른 작업에서는 MLP가 더 우수하다는 것을 보여줍니다. 또한, KAN이 B-스플라인 활성 함수 덕분에 기호 공식 표현에서 우월성을 가지지만, 다른 작업에서는 이 활성 함수가 큰 도움이 되지 않습니다. 마지막으로, 지속적 학습 상황에서도 MLP가 KAN보다 우수한 성능을 보였습니다. 이 연구는 KAN과 다른 MLP 대안에 대한 향후 연구에 중요한 통찰을 제공할 것입니다.