# Tracking Universal Features Through Fine-Tuning and Model Merging
## TL;DR
## Summary
- [https://arxiv.org/pdf/2410.12391.pdf](https://arxiv.org/pdf/2410.12391.pdf)

### 1. 논문 각 섹션 요약

**서론**
이 논문은 두 가지 전이 학습 상황(새로운 도메인으로의 미세 조정과 모델 병합)을 통해 언어 모델의 특징이 어떻게 나타나고 사라지며 유지되는지 연구하였습니다. 특히, 영어 텍스트와 Python 프로그래밍 언어로 학습된 모델을 Lua 프로그래밍 언어와 다른 영어 텍스트로 각각 미세 조정하고, 이후 이를 병합하여 특징의 변화를 분석했습니다.

**관련 연구**
이전 연구에서는 Transformer 모델 간의 특징 수렴과 유니버설리티를 논의하였으며, 다양한 모델에서 공통적인 특징을 추출하는 방법이 제공되었습니다. 이러한 연구는 프로그램 언어가 영어와 겹치는 부분과 독특한 코드 구조를 가지고 있어 특징 진화에 대한 연구에 적합하다고 설명합니다.

**방법론**
작은 언어 모델을 훈련하여 각 모델의 MLP 활성화로부터 특징을 추출하고 비교했습니다. 이를 통해 특징의 변화를 관찰했습니다. 각 모델은 BabyLM과 The Stack 데이터셋의 샘플로 훈련되었으며, 최종적으로 LuaStories라는 통합 모델로 병합되었습니다.

**결과 및 논의**
연구 결과, 미세 조정 및 병합 모델에서 대부분의 특징이 희석되었으나, 약 20%의 특징은 BabyPython에서 LuaStories까지 유지되었으며, 이 중 일부는 가변 할당과 같은 프로그래밍 관련 특징이었습니다.

**결론**
모델 파라미터의 구형 선형 보간을 사용하여 부모 모델의 특징을 유지할 수 있었다고 결론지었습니다. 특히, 미세 조정 및 병합을 통해 특징의 희석이 발생하였으며 향후 더 깊은 언어 모델로의 확장이 필요함을 시사하였습니다.

### 2. 전체 요약

이 논문은 전이 학습 과정에서 언어 모델의 특징이 어떻게 진화하는지를 탐구하였습니다. 영어와 Python 언어로 훈련된 모델을 Lua와 추가적인 영어 텍스트로 미세 조정하여, 특징의 출현과 소멸 및 지속성을 분석했습니다. 특이하게도 모델 병합 과정을 통해 부모 모델의 특정 특징이 유지될 수 있었으며, 이를 통해 프로그래밍 언어 처리 시 발생할 수 있는 여러 문제점을 해결할 수 있는 가능성을 제시하였습니다. 현재 실험은 작은 규모의 데이터셋에 한정되어 있어, 더 깊고 다양한 도메인으로의 확장이 중요하다고 제안합니다.