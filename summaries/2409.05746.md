# LLMs Will Always Hallucinate, and We Need to Live With This
## TL;DR
## Summary
- [https://arxiv.org/pdf/2409.05746.pdf](https://arxiv.org/pdf/2409.05746.pdf)

### 논문 요약

#### 1. Introduction
이 섹션에서는 대형 언어 모델(LLM)들의 기본 원리와 한계에 대해 설명합니다. LLM은 언어 패턴을 예측하는 간단한 원리를 바탕으로 작동하며, 주어진 단어 시퀀스에서 다음에 올 단어를 예측하는 방식으로 운영됩니다. 이 모델들은 최근의 발전으로 인해 인상적인 성과를 보였지만, 모델이 진정으로 언어를 이해하는지에 대한 질문과, 인공 산출물에 의존하는 위험성에 대한 의문이 남아 있습니다.

#### 2. The Essence of Large Language Models
LLM의 핵심 원리는 언어 패턴의 예측입니다. 이 모델들은 주어진 단어 시퀀스에서 다음에 올 단어를 예측하는 방식으로 동작하며, 이는 확률적 언어 생성의 기본이 됩니다. 모델은 학습된 데이터를 기반으로 다음에 올 단어의 확률을 계산하여 문장을 생성합니다. 이 과정을 통해 모델은 기계적으로 언어를 처리하고, 학습된 데이터에 기반하여 새로운 텍스트를 창작합니다.

#### 3. Architectures of Large Language Model Generation
LLM의 아키텍처는 크게 Linear RNN, Mamba, Kolmogorov-Arnold Networks (KANs) 등의 다양한 구조를 포함합니다. Linear RNN은 순차적으로 데이터를 처리하는 반면, Mamba는 선택적 상태 공간을 사용하여 빠른 시퀀스 모델링을 가능하게 합니다. KANs는 수학적 기반을 제공하여 신경망을 대체할 수 있는 새로운 아키텍처로 연구되고 있습니다.

#### 4. Transfer Learning and Parameter-Efficient Fine-Tuning (PEFT)
전이 학습과 파라미터 효율적 미세 조정은 모델의 효율성을 높이기 위해 사용됩니다. 전이 학습은 사전 훈련된 모델을 새 작업에 재사용하는 방법으로, 추가적인 미세 조정 없이도 높은 성능을 보일 수 있습니다. PEFT는 최소한의 파라미터 수정을 통해 모델을 미세 조정하는 기술로, 이를 통해 모델의 효율성을 크게 증가시킬 수 있습니다.

#### 5. Hallucinations in LLMs: What They Are and How They Happen
LLM의 환각(hallucination)은 모델이 잘못되거나 일관되지 않은 정보를 생성하는 현상을 말합니다. 이는 훈련 데이터에 없는 정보나 논리적으로 맞지 않는 내용을 생성할 때 발생하며, 크게 사실적 오류, 해석 오류, 그리고 완전한 허구로 분류됩니다. 이러한 환각은 종종 높은 확신을 가지고 생성되기 때문에 더욱 문제가 될 수 있습니다.

#### 6. Identification of LLM Hallucination and Mitigation Strategies
LLM의 환각을 식별하고 완화하는 다양한 전략들이 제시되고 있습니다. 대표적으로 Chain-of-Thought (CoT) Prompting, Self-Consistency, Uncertainty Quantification, Faithful Explanation Generation이 있습니다. 이러한 기법들은 모델의 추론 과정을 명확히 하거나, 여러 추론 경로 중 가장 일관된 답변을 선택하는 등의 방식을 통해 환각을 줄이려고 합니다.

#### 7. All Hallucinations are Structural Hallucinations
환각은 LLM의 수학적, 논리적 구조에서 본질적으로 발생하는 현상입니다. 이는 훈련 데이터의 불완전성, 정보 검색의 불확실성, 의도 분류의 어려움 등이 주요 원인입니다. 아키텍처나 데이터셋의 개선만으로는 이러한 문제를 완전히 해결할 수 없으며, 환각은 항상 일정 확률로 발생할 수 밖에 없습니다.

#### 8. Hallucination is Inevitable: Claims and Proofs
환각이 필연적이라는 것을 수학적 증명과 이론을 통해 뒷받침합니다. 이 섹션은 훈련 데이터의 불완전성과 정보 검색의 불확실성, 의도 분류의 어려움 등을 다룹니다. 또한 LLM의 생성 단계에서 발생할 수 있는 환각의 불가피성을 논증합니다.

#### 9. Fact Checking Mechanisms are Inherently Insufficient
모든 생성된 텍스트를 사실 확인하기 위한 메커니즘은 불충분할 수 밖에 없습니다. 이는 훈련 데이터가 항상 완전하지 않으며, 모든 텍스트를 제한된 단계 내에서 확인하는 것이 불가능하기 때문입니다. 따라서 사실 확인 메커니즘 자체가 환각을 완전히 제거할 수 없다는 점을 강조합니다.

### 전체 요약
이 논문은 LLM의 환각 현상이 필연적임을 수학적 증명과 이론을 통해 설명합니다. LLM은 언어 패턴의 예측을 통해 작동하며, 전이 학습과 파라미터 효율적 미세 조정을 통해 성능을 높일 수 있습니다. 그러나 모델의 구조적 특성상 환각은 항상 일정 확률로 발생할 수 밖에 없으며, 이를 완전히 제거하는 것은 불가능합니다. 이러한 환각 문제는 훈련 데이터의 불완전성, 정보 검색의 불확실성, 의도 분류의 어려움 등에서 기인하며, 다양한 전략을 통해 완화를 시도하고 있으나 완전한 해결책은 존재하지 않습니다. 이러한 이해를 바탕으로 LLM을 사용할 때의 주의점과 미래 연구 방향을 제시합니다.