# Does Refusal Training in LLMs Generalize to the Past Tense?
## TL;DR
## Summary
- [https://arxiv.org/pdf/2407.11969.pdf](https://arxiv.org/pdf/2407.11969.pdf)

### 1. 논문 각 섹션요약

**1. 서론 (Introduction)**
이 논문에서는 대형 언어 모델(LLMs)이 강력한 기능을 제공하지만, 유해한 콘텐츠 생성, 사이버 공격 지원 등 악용의 위험이 있다는 점을 지적합니다. 이를 해결하기 위한 다양한 기술적 접근법이 있지만, 단순히 현재 시제를 과거 시제로 재구성하면 많은 최첨단 LLM의 거부 훈련을 우회할 수 있음을 보여줍니다.

**2. 관련 연구 (Related Work)**
주요 연구에서는 LLM의 일반화, 거부 훈련의 실패, 그리고 최근에 보고된 다양한 Jailbreak 접근법을 다룹니다. 특히 본 논문에서는 과거 시제로의 단순한 재구성이 효과적인 공격이 될 수 있음을 검토합니다.

**3. 방법론 (Methodology)**
Jailbreak 공격은 유해한 요청의 거부 훈련을 우회하는 프롬프트를 찾아내는 과정입니다. 본 논문에서는 사전 정의된 유해 요청 세트를 사용하여 각 요청을 과거 시제로 재구성하고 이를 평가하는 방식으로 접근합니다. 평가 대상 LLM으로는 Llama-3 8B, Claude-3.5 Sonnet, GPT-3.5 Turbo 등이 포함됩니다.

**4. 결과 (Results)**
과거 시제로의 재구성은 놀라울 정도로 많은 모델에서 성공적인 공격을 유발할 수 있음을 보여줍니다. 특히 GPT-4o 모델에서는 성공률이 88%에 달했습니다. 반면, 미래 시제의 재구성은 덜 효과적임이 밝혀졌습니다. 이는 모델이 과거의 요청을 더 무해한 것으로 간주하기 때문일 수 있습니다.

**5. 논의 (Discussion)**
과거 시제 예제는 현재의 정렬 방식(SFT, RLHF, DPO)이 일반화하는 데 한계가 있음을 보여줍니다. 특히 다른 언어로의 일반화는 가능하지만 다른 시제로의 일반화는 실패합니다. 이는 내부 표현이 시제에 따라 다르게 맵핑되기 때문으로 추정합니다. 이러한 한계는 추가 연구가 필요함을 시사합니다.

**6. 결론 (Conclusion)**
본 논문은 간단한 과거 시제 재구성만으로도 많은 최신 LLM의 거부 훈련을 효과적으로 우회할 수 있음을 보여줍니다. 이는 현재 일반적으로 사용되는 정렬 기술이 취약점을 가질 수 있음을 시사하며, 이러한 취약점을 탐구하는 것이 중요함을 강조합니다.

---

### 2. 전체 요약

이 논문은 대형 언어 모델(LLMs)이 현재 시제를 과거 시제로 재구성하는 것만으로도 거부 훈련을 우회할 수 있다는 흥미로운 발견을 다룹니다. 여러 최신 LLM에 대한 실험 결과, 과거 시제로의 단순한 재구성이 매우 효과적인 공격 방식으로 나타났습니다. 이는 LLM이 과거의 요청을 더 무해한 것으로 간주하기 때문일 가능성이 높습니다. 반대로 미래 시제의 재구성은 덜 효과적이었습니다.

이 논문은 현재 광범위하게 사용되고 있는 정렬 기술(SFT, RLHF, DPO)이 다른 언어로는 잘 일반화되지만, 시제 간의 일반화에는 실패함을 시사합니다. 이러한 연구는 AI 정렬 기술의 취약점을 드러내며, 더 신뢰할 수 있는 모델 개발을 위해 추가 연구가 필요함을 강조합니다.

이 논문의 주요 기여는 LLM의 간단한 재구성 공격이 얼마나 효과적인지, 그리고 이것이 현재의 정렬 기술에 대한 중요한 시험 수단이 될 수 있음을 보여주는 데 있습니다.