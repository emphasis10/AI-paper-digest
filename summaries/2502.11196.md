# How Do LLMs Acquire New Knowledge? A Knowledge Circuits Perspective on Continual Pre-Training
## TL;DR
## Summary
- [https://arxiv.org/pdf/2502.11196.pdf](https://arxiv.org/pdf/2502.11196.pdf)

1. 논문의 각 섹션 요약:

   - **서론**
     연구팀은 LLM(대형 언어 모델)이 새로운 지식을 얻는 방식을 이해하고, 이러한 지식을 신경망 내에서 어떻게 구조적으로 내재화하는지를 설명하고자 한다. 이를 위해, 연구팀은 지식 회로(evolution circuits)의 진화를 분석하여, 모델의 성능을 향상시키는 방법을 제시한다.

   - **성능 분석**
     연구는 각 지식 회로가 모델의 알고리즘에 맞게 행동 패턴을 재현할 수 있는지를 평가하여, LLM의 새로운 지식 습득 능력을 검증한다. 'Hit@10' 메트릭을 사용하여 이러한 성능을 측정하며, 새로운 지식의 학습 효율성은 기존 지식과의 관련성에 크게 영향을 받는다는 점을 발견하였다.

   - **토폴로지 분석**
     지식 회로의 동적 변화를 토폴로지적 관점에서 분석한다. 연구진은 회로가 어떻게 진화하면서 모델의 성능이 개선되는지를 그래프 이론적 메트릭을 통해 설명한다. 중요한 지식 회로의 중앙 집권화가 진행되며, 중앙의 주요 간선과 노드가 정보 흐름의 중심지가 된다.

   - **구성요소 분석**
     구성요소의 진화 패턴을 분석하는데 중점을 둔다. 연구팀은 훈련 중 구성 요소의 변화가 회로 성능에 어떻게 기여하는지를 자세히 밝혀냈다. 중요한 발견 중 하나는 특정 주의(attention) 헤드가 LLM의 사실 회상(factual recall)에 직접 기여한다는 점이었다.

   - **결론**
     새로운 지식 획득이 기존 지식과의 연관성에 크게 좌우되며, 지식 회로의 진화에는 형성과 최적화라는 두 가지 뚜렷한 단계가 있다는 것을 결론적으로 밝힌다. 이러한 관찰 결과는 LLM의 지속적 학습을 개선하기 위한 전략 방안을 제시한다.

2. 전체 요약:

   이 논문은 LLM의 새로운 지식 획득에 대한 이해를 심화시키기 위해 지식 회로의 진화를 분석한 연구로, 회로의 형성과 최적화를 통해 모델이 어떻게 성능을 향상시키는지를 설명한다. 특히, 관련성 높은 새로운 지식이 어떻게 더 효율적으로 통합되는지를 토폴로지와 구성요소의 변화와 연계하여 밝히고 있다. 이 논문은 LLM의 지속적 학습 성능을 향상시키기 위한 중요한 이론적 근간을 제공하며, 해당 지침을 바탕으로 다양한 도메인에 적용할 전략적 접근법을 제안한다.