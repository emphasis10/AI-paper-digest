# LLaMA Pro: Progressive LLaMA with Block Expansion
## TL;DR
## Summary
- [https://arxiv.org/pdf/2401.02415.pdf](https://arxiv.org/pdf/2401.02415.pdf)

#### 1. 서론
대규모 언어 모델(LLMs)은 자연어 처리에서 큰 혁신을 일으켰지만, 프로그래밍, 수학, 생물의학, 금융 등 특정 분야에서는 여전히 한계가 있습니다. 기존 연구들은 도메인별 전이 학습을 통해 LLMs의 다방면 능력을 향상시키려 했으나, 이는 대규모 데이터와 많은 계산 자원을 필요로 합니다. 이에 따른 해결책으로 우리는 블록 확장(block expansion) 기법을 제안합니다. 이 방법은 기존의 사전 학습된 LLM을 복사한 Transformer 블록을 추가해 확장한 후, 도메인 특화된 데이터만을 사용해 조정하는 것입니다.

#### 2. 관련 연구
최근 LLMs의 발전은 모델과 데이터 규모의 성장에 힘입어 다양한 작업에서 최첨단 성능을 이끌어냈습니다. 특히, 특정 도메인에 모델을 맞추기 위한 연구가 활발히 이루어지고 있습니다. 그러나 이러한 연구들은 종종 모델의 일반적인 능력을 저하시킵니다. 우리는 블록 확장을 통해 이러한 문제를 해결하고자 합니다.

#### 3. 방법론
블록 확장은 LLaMA 블록 구조에서 시작합니다. 각 블록 후에 동일한 블록을 추가해 모델의 깊이를 증가시킵니다. 추가된 블록은 초기 출력 선형 행렬을 0으로 초기화해 모델의 원래 출력을 유지합니다. 이를 통해 모델은 새로운 도메인 지식을 통합하면서도 기존의 일반적인 능력을 유지할 수 있습니다.

#### 4. 실험
코드와 수학에 중점을 둔 데이터셋을 구축하여 실험을 진행했습니다. LLAMA PRO는 기존 모델과 비교해 종합 벤치마크에서 우수한 성능을 보였습니다. 특히, 코드와 수학 영역에서의 성능이 두드러졌습니다.

#### 5. 결론
우리는 블록 확장 기법을 통해 LLMs의 도메인 특화 능력을 향상시키면서도 원래의 일반적인 능력을 유지할 수 있음을 보였습니다. LLAMA PRO는 다양한 작업에서 우수한 성능을 발휘하며, LLMs의 향후 발전에 중요한 통찰을 제공합니다.

---

### 전체 요약
이 논문은 대규모 언어 모델(LLMs)의 도메인 특화 능력을 향상시키기 위해 블록 확장 기법을 제안합니다. 이 기법은 기존의 사전 학습된 모델에 Transformer 블록을 추가해 모델의 깊이를 증가시키고, 도메인 특화 데이터를 사용해 추가된 블록만을 조정함으로써 새로운 지식을 통합합니다. 이를 통해 모델은 도메인 특화 작업에서 뛰어난 성능을 보이면서도 원래의 일반적인 능력을 유지할 수 있습니다. 실험 결과, LLAMA PRO는 코드와 수학 영역에서 기존 모델보다 우수한 성능을 보였습니다. 이 연구는 LLMs의 도메인 특화 학습과 관련된 중요한 통찰을 제공하며, 향후 LLMs의 발전에 기여할 수 있을 것으로 기대됩니다.

## Similar Papers
- [Bootstrapping Language Models with DPO Implicit Rewards](2406.09760.md)
- [Plot2Code: A Comprehensive Benchmark for Evaluating Multi-modal Large Language Models in Code Generation from Scientific Plots](2405.07990.md)
- [Self-Tuning: Instructing LLMs to Effectively Acquire New Knowledge through Self-Teaching](2406.06326.md)
- [MMAU: A Holistic Benchmark of Agent Capabilities Across Diverse Domains](2407.18961.md)
- [BiLLM: Pushing the Limit of Post-Training Quantization for LLMs](2402.04291.md)
- [Meta-Rewarding Language Models: Self-Improving Alignment with LLM-as-a-Meta-Judge](2407.19594.md)
- [VoCo-LLaMA: Towards Vision Compression with Large Language Models](2406.12275.md)
- [LAB: Large-Scale Alignment for ChatBots](2403.01081.md)
- [SeaLLMs 3: Open Foundation and Chat Multilingual Large Language Models for Southeast Asian Languages](2407.19672.md)
