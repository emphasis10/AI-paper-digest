# SemiEvol: Semi-supervised Fine-tuning for LLM Adaptation
## TL;DR
## Summary
- [https://arxiv.org/pdf/2410.14745.pdf](https://arxiv.org/pdf/2410.14745.pdf)

### 1. 각 섹션 요약 및 주요 기여와 혁신 부분

**서론**
이 논문은 대규모 언어 모델(LLM) 개선을 위한 반지도 강화 학습(SFT)의 중요성과 이를 위한 데이터 비용 절감 전략을 설명합니다. 기존의 지도 학습 방식은 많은 라벨링된 데이터에 의존하므로, 비용이 많이 듭니다.

**도전과제**
실제 환경에서는 소량의 라벨링된 데이터와 대량의 비라벨링된 데이터가 혼합되어 있습니다. 이러한 하이브리드 데이터 환경에서의 모델 성능을 최적화하기 위해 SEMIEVOL 프레임워크가 제안되었습니다.

**방법론**
SEMIEVOL은 라벨링된 데이터와 비라벨링된 데이터를 통합하여 모델을 개선하는 접근법입니다. '지식 전파'와 '적응적 선택'의 바이레벨로 지식을 전파하고, 다중 언어 모델의 협력적 학습을 통해 비라벨링된 데이터를 학습에 활용합니다. 이렇게 함으로써 더 고품질의 학습 데이터를 생성합니다.

**실험 및 결과**
SEMIEVOL의 성능은 MMLU, MMLU-Pro, ConvFinQA 같은 다양한 데이터셋에서 시험되었으며, SFT와 자가 진화 방법보다 일관된 효과를 보였습니다. 이 방법은 기존의 SFT와 비교하여 여러 데이터 시나리오에서 모델 성능을 향상시키는 데 효과적입니다.

**결론**
SEMIEVOL은 실제 하이브리드 데이터 시나리오에서 LLM의 성능을 개선하는 실질적인 과제를 처음으로 다룹니다. 모델이 지속적인 진화와 반복적 진화를 통해 실세계 응용에서 효과성을 높이는 데 필요한 기반을 제공합니다.

### 2. 전체 요약

SEMIEVOL 프레임워크는 기존의 LLM이 대량의 라벨링된 데이터를 필요로 하는 문제를 해결하기 위해 제안되었습니다. 이 방법론은 라벨링된 데이터와 비라벨링된 데이터를 함께 사용하여, 지식 전파와 적응적 선택을 통해 모델 성능을 크게 향상시킵니다. 실험을 통해 다양한 환경에서의 일관된 성능 향상을 확인하였고, 이는 LLM이 적은 양의 라벨링된 데이터를 사용해도 효과적으로 발전할 수 있음을 보여줍니다. 이러한 접근법은 LLM의 실세계 적용에서 지속적이고 반복적인 향상을 가능하게 하여, 미래의 AI 개발에 중요한 기여를 합니다.