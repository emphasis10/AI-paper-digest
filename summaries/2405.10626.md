# Dynamic data sampler for cross-language transfer learning in large language models
## TL;DR
## Summary
- [https://arxiv.org/pdf/2405.10626.pdf](https://arxiv.org/pdf/2405.10626.pdf)

### 요약
이 논문은 **ChatFlow**라는 대형 언어 모델(LLM)을 제안하여 크로스 언어 전이 학습을 통해 중국어 대형 언어 모델을 효율적으로 학습하는 방법을 제시하고 있습니다. 주된 기여는 **동적 데이터 샘플러**를 도입하여 영어와 중국어의 언어 모델 간 지식 전이를 원활하게 하고 모델 성능을 향상시키는 것입니다.

### 각 섹션 요약

#### 1. 서론
대형 언어 모델(LLM)은 자연어 처리(NLP) 분야에서 중요한 역할을 하고 있으며, 영어 외 다른 언어로 LLM을 학습하는 데에는 대규모 코퍼스 확보와 계산 자원이라는 도전 과제가 있습니다. 이 논문에서는 이러한 문제를 해결하고 중국어 대형 언어 모델을 효율적으로 학습하기 위해 ChatFlow를 제안합니다.

#### 2. 방법론
- **크로스 언어 전이 학습**: 영어와 중국어, 병렬 코퍼스를 사용하여 LLaMA2 모델을 지속적으로 학습시켜 크로스 언어 표현을 정렬합니다. 이를 통해 영어 모델의 지식을 중국어 모델로 전이합니다.
- **동적 데이터 샘플러**: 점진적으로 모델을 비지도 사전 학습에서 지도 미세 조정으로 전환시켜 데이터 분포의 급격한 변화를 방지하고 모델 학습을 원활하게 합니다.
- **학습 데이터**: 약 50GB의 데이터를 사용하여 LLaMA2-7B 모델을 기반으로 ChatFlow를 학습합니다. 데이터는 영어 및 중국어의 비지도 코퍼스, 병렬 코퍼스, 그리고 지시 데이터로 구성됩니다.

#### 3. 실험
- **설정**: LLaMA2-7B 가중치를 초기화하여 ChatFlow를 학습합니다. 영어 단어와 중국어 단어가 혼합된 코퍼스를 사용하여 모델을 학습하고 평가합니다.
- **주요 결과**: ChatFlow는 다양한 중국어 및 영어 벤치마크에서 우수한 성능을 보입니다. 특히 기존의 중국어 모델과 비교하여 더 나은 성능을 나타냅니다.
- **사용자 평가**: SuperCLUE 벤치마크에서 ChatFlow는 7B 모델 중 5위를 차지했으며, 이는 중국어 및 영어 모델을 동시에 학습한 다른 모델들과 비교하여도 경쟁력 있는 성능을 나타냅니다.

#### 4. 한계 및 결론
ChatFlow는 크로스 언어 전이 학습을 통해 중국어 대형 언어 모델을 효율적으로 학습하는 방법을 제시하며, 동적 데이터 샘플러를 도입하여 학습 과정의 원활한 전환을 제공합니다. 향후 연구에서는 더 많은 데이터를 통합하여 성능을 더욱 향상시키는 것을 목표로 합니다.

### 전체 요약
**ChatFlow**는 크로스 언어 전이 학습을 통해 중국어 대형 언어 모델을 효율적으로 학습하는 방법을 제안하며, 동적 데이터 샘플러를 통해 데이터 분포의 급격한 변화를 방지하고 학습을 원활하게 합니다. 실험 결과, ChatFlow는 다양한 중국어 및 영어 벤치마크에서 우수한 성능을 보였으며, 이는 다른 중국어 모델과 비교하여도 경쟁력 있는 성능을 나타냅니다. 이 연구는 크로스 언어 전이 학습의 가능성을 보여주며, 향후 연구를 통해 더 많은 데이터를 통합하여 성능을 더욱 향상시킬 수 있는 방법을 제시하고 있습니다.

## Similar Papers
- [Tele-FLM Technical Report](2404.16645.md)
- [How Far Are We to GPT-4V? Closing the Gap to Commercial Multimodal Models with Open-Source Suites](2404.16821.md)
- [Qwen2 Technical Report](2407.10671.md)
- [LLM2Vec: Large Language Models Are Secretly Powerful Text Encoders](2404.05961.md)
- [SambaLingo: Teaching Large Language Models New Languages](2404.05829.md)
- [BLADE: Enhancing Black-box Large Language Models with Small Domain-Specific Models](2403.18365.md)
- [Self-Tuning: Instructing LLMs to Effectively Acquire New Knowledge through Self-Teaching](2406.06326.md)
- [Dallah: A Dialect-Aware Multimodal Large Language Model for Arabic](2407.18129.md)
- [Efficient and Effective Vocabulary Expansion Towards Multilingual Large Language Models](2402.14714.md)
