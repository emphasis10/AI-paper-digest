# Let the Expert Stick to His Last: Expert-Specialized Fine-Tuning for Sparse Architectural Large Language Models
## TL;DR
## Summary
- [https://arxiv.org/pdf/2407.01906.pdf](https://arxiv.org/pdf/2407.01906.pdf)

### 1. 각 섹션의 요약 및 주요 기여 사항

**1. 서론**
이 논문은 거대 언어 모델(LLM)의 파라미터 효율적 미세 조정(PEFT)의 중요성을 강조합니다. 특히, PEFT는 제한된 자원으로 LLM을 맞춤화하는 데 필수적입니다.

**2. 배경**
PEFT 방법들은 일반적으로 세 가지 접근 방식을 따릅니다:
1. 새로운 파라미터 추가
2. 기존 파라미터 선택
3. 저차원 적응(LoRA) 적용

이 논문은 이러한 기존 연구들을 바탕으로 드문 아키텍처를 사용하는 모델에 초점을 맞추어 PEFT 기술을 연구합니다.

**3. 방법론**
이 논문은 문제 해결을 위한 ESFT(Expert-Specialized Fine-Tuning)방법을 제안합니다. ESFT는 특정 작업과 가장 관련 있는 전문가만을 선택하고 나머지 전문가와 모듈의 파라미터를 고정시키는 방식으로, 전문가의 특수화를 유지하면서도 자원 효율성을 극대화합니다.

**4. 결과**
ESFT는 전체 파라미터 미세 조정(FFT)과 비교하여 훈련 시간과 저장 공간에서 큰 이점을 보여줬으며, 작업에 필요한 훈련 가능한 파라미터 수를 대폭 줄였습니다. 그림 5와 표 3에 따르면 ESFT는 특히 저장 공간과 훈련 시간 면에서 우수한 성능을 발휘했습니다.

**5. 분석**
ESFT 방법론은 각 층과 작업에서 선택된 전문가 수를 분석하여 전문가 선택 과정을 이해했습니다. ESFT는 일반적으로 각 작업에 대해 2에서 15명의 전문가를 사용했으며, 이는 FFT와 비교해 75%에서 95%가 적은 파라미터를 사용하는 것을 보여줬습니다. 또한, 특정 작업에서 더 나은 성능을 유지했습니다.

**6. 결론**
ESFT는 MoE(Mixture-of-Experts) 아키텍처를 사용하는 모델에서 파라미터 효율적 미세 조정 방법을 연구했습니다. ESFT는 훈련 비용을 크게 줄이면서도 전체 파라미터 미세 조정 결과와 비슷하거나 더 나은 성능을 보여줬습니다.

### 2. 전반적인 요약

이 논문은 거대 언어 모델(LLM)의 파라미터 효율적 미세 조정(PEFT) 방법을 연구했습니다. 특히, 드문 아키텍처를 사용하는 모델에 대해 ESFT(Expert-Specialized Fine-Tuning)라는 새로운 방법을 제안했습니다. ESFT는 작업과 관련된 전문가만을 선택하고 나머지 전문가와 모듈의 파라미터를 고정하는 방식으로, 훈련 비용을 크게 줄이면서도 전체 파라미터 미세 조정과 유사하거나 더 나은 성능을 보여줬습니다. 이 논문의 주요 기여는 ESFT가 드문 아키텍처에서의 PEFT의 새로운 기준을 제시한다는 점입니다.