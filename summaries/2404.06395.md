# MiniCPM: Unveiling the Potential of Small Language Models with Scalable Training Strategies
## TL;DR
## Summary
- [https://arxiv.org/pdf/2404.06395.pdf](https://arxiv.org/pdf/2404.06395.pdf)

이번 페이퍼 요약은 큰 데이터 세트와 복잡한 모형에 대한 AI 및 머신 러닝 연구의 진보를 소개하는 것입니다. 요약은 다음 섹션 별로 나뉩니다:

### 1. 초록(Abstract)
- 개발 중인 대규모 언어 모형(Large Language Models, LLMs)에 대한 관심이 증가하고 있지만, 자원 효율성과 실용적 비용에 대한 우려가 제기되었습니다.
- 작은 언어 모형(Small Language Models, SLMs)의 잠재적 가능성을 탐색하는 것이 중요합니다.
- MiniCPM 1.2B 및 2.4B는 각각의 카테고리에서 뛰어난 성능을 보이며, 7B-13B LLMs와 비교해 볼 때에도 비슷한 수준의 능력을 보였습니다.
- 모델 규모 및 데이터 규모에서 확장 가능성을 보여주는 접근 방식으로, 효율적인 데이터-모델 스케일링 법칙 연구에 기여합니다.

### 2. 서론(Introduction)
- 대규모 언어 모형(LLMs) 개발에 대한 열정이 커졌지만, 이러한 대규모 모델 훈련은 비용적, 운영적으로 비효율적입니다.
- SLMs에 대한 관심이 다시 높아지고 있으며, 이는 실용적 배포에 효율적인 솔루션을 제공하고, 확장 가능한 전략으로 훈련시킬 경우 향후 더 큰 모델 개발을 안내할 수 있습니다.

### 3. 관련 작업(Related Work)
- SLMs에 대한 개념이 시간이 지남에 따라 크게 변화하여 현재는 7억 개의 매개변수를 초과하지 않는 규모로 이해되고 있습니다.
- 사용자 기기에서 GPU 없이도 배포할 수 있는 능력이 특징입니다.

### 4. 모델 윈드 터널 실험(Model Wind Tunnel Experiments)
- 모델 훈련의 여러 측면이 규모에 관계없이 보편적이라는 개념을 기반으로, SLM을 통해 광범위한 실험을 진행했습니다.
- 이러한 실험은 중요한 하이퍼파라미터, 최적 배치 크기의 크기 확장, 그리고 학습률의 안정성이 SLM에서 LLM으로 경험을 전달하는 데 있어 중요하다고 제시합니다.

### 5. MiniCPM 계열(MiniCPM Family)
- MiniCPM 기반 모델들을 소개하며, 이는 RLHF 모델, 긴 문맥 모델, 그리고 MoE 모델로 구성됩니다.
- 각각의 모델은 특정 기능을 강화하고, SLMs가 긴 문맥을 효과적으로 처리할 수 있음을 보여줍니다.

### 6. 결론(Conclusion)
- MiniCPM은 1.2B 및 2.4B 매개변수를 가진 두 가지 SLM을 소개하며, 이는 큰 규모의 모델과 비교하여 우수한 성능을 보였습니다.
- 제안된 WSD 스케줄러를 통해 지속적인 훈련을 촉진하고, 스케일링 법칙에 대한 효율적인 연구를 가능하게 합니다.

### 종합 요약
이 연구는 SLMs의 개발과 이들이 LLMs를 대체할 수 있는 잠재력에 초점을 맞춥니다. MiniCPM 모델은 소규모임에도 불구하고 비슷하거나 더 나은 성능을 보이는 것으로 나타났으며, 향후 연구 방향성에 더 효율적인 제안을 제공합니다.

## Similar Papers
- [Yuan 2.0-M32: Mixture of Experts with Attention Router](2405.17976.md)
- [MiniCPM-V: A GPT-4V Level MLLM on Your Phone](2408.01800.md)
- [Spectra: A Comprehensive Study of Ternary, Quantized, and FP16 Language Models](2407.12327.md)
- [Advancing LLM Reasoning Generalists with Preference Trees](2404.02078.md)
- [AutoDetect: Towards a Unified Framework for Automated Weakness Detection in Large Language Models](2406.16714.md)
- [Tele-FLM Technical Report](2404.16645.md)
- [Beyond the Turn-Based Game: Enabling Real-Time Conversations with Duplex Models](2406.15718.md)
- [Is Programming by Example solved by LLMs?](2406.08316.md)
- [On the Transformations across Reward Model, Parameter Update, and In-Context Prompt](2406.16377.md)
