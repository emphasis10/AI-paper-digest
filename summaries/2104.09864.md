# RoFormer: Enhanced Transformer with Rotary Position Embedding
## TL;DR
## Summary
- [https://arxiv.org/pdf/2104.09864.pdf](https://arxiv.org/pdf/2104.09864.pdf)

### 논문의 요약 및 주요 내용

#### 1. 서론
이 논문에서는 기존의 위치 인코딩 방법을 개선한 새로운 방식인 회전 위치 임베딩(Rotary Position Embedding, RoPE)을 소개합니다. RoPE는 절대 위치를 회전 행렬로 인코딩하고, 상대 위치 의존성을 셀프 어텐션 구조에 통합합니다. 이 방식은 시퀀스 길이의 유연성, 상대 거리 증가에 따른 토큰 간 의존성 감소, 선형 셀프 어텐션과의 호환성을 제공하여, RoFormer로 명명된 새로운 트랜스포머 모델이 기존 모델보다 성능이 뛰어남을 실험적으로 보여줍니다.

#### 2. 배경 및 관련 연구
기존의 위치 인코딩 방법은 주로 절대 위치 인코딩과 상대 위치 인코딩으로 나뉩니다. 이 연구에서는 기존의 방법들을 검토하고, 이들의 한계를 극복하기 위해 RoPE를 제안합니다. RoPE는 위치 정보를 회전 행렬을 통해 인코딩하여 셀프 어텐션 구조와 자연스럽게 통합되도록 합니다.

#### 3. 제안하는 접근 방식
RoPE는 2차원 벡터 공간에서 시작하여 고차원 공간으로 일반화됩니다. 이를 통해 각 단어의 임베딩 벡터를 회전시켜 상대 위치 정보를 효과적으로 인코딩할 수 있습니다. 이 방법은 상대 위치 정보를 회전 행렬을 사용하여 인코딩함으로써, 기존의 위치 인코딩 방식들과 비교해 더 효율적이고 직관적인 해석을 제공합니다.

#### 4. 실험 및 평가
RoFormer는 다양한 자연어 처리(NLP) 작업에서 평가되었습니다. 주요 실험 결과는 다음과 같습니다:
- **기계 번역:** RoFormer는 WMT 2014 영어-독일어 번역 작업에서 기존 트랜스포머 모델보다 더 높은 BLEU 점수를 기록했습니다.
- **사전 학습 언어 모델링:** RoFormer는 BERT와 비교하여 더 빠른 수렴 속도를 보였으며, 마스크 언어 모델링(MLM) 손실이 더 낮았습니다.
- **GLUE 벤치마크:** RoFormer는 GLUE 벤치마크에서 BERT보다 뛰어난 성능을 보여주었습니다.
- **중국어 데이터:** RoFormer는 긴 문서 처리에서 기존 모델보다 우수한 성능을 나타냈습니다.

#### 5. 결론
이 연구에서는 상대 위치 의존성을 명시적으로 통합한 새로운 위치 임베딩 방법을 제안합니다. RoPE는 회전 행렬을 통해 상대 위치 정보를 자연스럽게 인코딩하며, 이를 통해 트랜스포머 아키텍처의 성능을 향상시킵니다. 실험 결과, RoFormer는 다양한 작업에서 기존 모델보다 뛰어난 성능을 보여주었습니다.

### 전체 요약
이 논문은 새로운 위치 임베딩 방식인 회전 위치 임베딩(RoPE)을 제안합니다. RoPE는 절대 위치 정보를 회전 행렬로 인코딩하고 상대 위치 의존성을 통합하여, 트랜스포머 모델의 성능을 개선합니다. RoFormer로 명명된 이 모델은 다양한 NLP 작업에서 기존 모델보다 뛰어난 성능을 보였습니다. 주요 기여는 새로운 위치 인코딩 방법의 제안과 이를 통한 성능 향상에 있습니다. RoFormer는 특히 긴 문서 처리에서 탁월한 성능을 나타내며, 자연어 처리 분야에서 중요한 발전을 이룩했습니다.