# TÜLU 3: Pushing Frontiers in Open Language Model Post-Training
## TL;DR
## Summary
- [https://arxiv.org/pdf/2411.15124.pdf](https://arxiv.org/pdf/2411.15124.pdf)

## 섹션별 요약

- **서론 (Introduction):** 이 논문에서는 최신 AI 언어 모델인 TÜLU 3의 개발 배경과 목표를 소개합니다. TÜLU 3는 전적으로 오픈 소스 데이터와 훈련 코드를 사용하는 현대적 언어 모델로, 다양한 평가 및 개발 레시피를 제공합니다.

- **TÜLU 3 개요 (TÜLU 3 Overview):** 전통적인 사후 훈련과 비교해 TÜLU 3는 여러 훈련 단계를 아우르는 복잡한 구조를 지니며, 공개된 데이터 및 코드를 기반으로 모델을 최적화할 수 있습니다.

- **데이터 및 실험 (Data and Experiments):** 데이터의 품질과 출처를 분석하고, 핵심 스킬을 타겟으로 한 프롬프트를 필터링하여 SFT(Supervised Fine-Tuning) 데이터셋을 생성하고 실험을 통해 그 효과를 입증하였습니다.

- **선호 조정 (Preference Finetuning):** On-policy 데이터 생성 방법을 활용하여 선호 데이터셋을 대규모로 확장하였으며, 선호 조정 알고리즘을 통해 모델 성능을 극대화했습니다.

- **검증 가능한 보상을 통한 강화 학습 (Reinforcement Learning on Verifiable Rewards):** 모델의 정확한 결과에 대해 보상이 제공되는 새로운 형태의 강화 학습을 도입하여, 수학과 같은 특정 분야에서의 성능을 향상시켰습니다.

- **평가 프레임워크 (Evaluation Framework):** TÜLU 3는 오픈 소스 평가 도구를 통한 평가 체계를 운영하여 모델 개발을 지원합니다. 이 시스템은 개발 중 모호한 프롬프트나 데이터의 오염 등을 제거하는 데 사용됩니다.

## 전체 요약

TÜLU 3는 최신 오픈 언어 모델로서, 과거 사후 훈련 모델들의 한계를 뛰어넘기 위해 개발되었습니다. 오픈 소스 데이터를 활용해 데이터 평가 및 정화, 코드 개발, 그리고 선호 조정 알고리즘 등의 혁신적인 방법론을 도입하였으며, 검증 가능한 보상을 통한 강화 학습을 통해 특정 영역에서의 성능을 극대화하였습니다. 이러한 개방된 훈련 파이프라인은 다양한 과제에서 높은 성능의 모델을 생성할 수 있도록 하여 AI 분야의 발전을 위한 기초를 제공합니다.