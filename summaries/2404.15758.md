# Let's Think Dot by Dot: Hidden Computation in Transformer Language Models
## TL;DR
## Summary
- [https://arxiv.org/pdf/2404.15758.pdf](https://arxiv.org/pdf/2404.15758.pdf)

### 요약

#### 1. 서론
- **연구 목적**: 이 논문은 체인 오브 싱크(Chain-of-Thought, CoT) 응답이 언어 모델의 성능을 개선시키는 이유가 인간과 유사한 작업 분해 덕분인지, 아니면 추가 토큰으로 인한 계산 증가 때문인지를 조사합니다.
- **주요 발견**: 연구 결과, 트랜스포머 모델이 의미 없는 토큰(예: '......')을 사용해도 CoT 없이 해결할 수 없는 어려운 알고리즘 작업을 해결할 수 있음을 보였습니다. 하지만 이러한 토큰 사용은 특정한 조밀한 감독이 필요합니다.

#### 2. 관련 연구
- **트랜스포머의 표현력**: 최근 연구에 따르면, 추가적인 중간 토큰 없이 트랜스포머는 매우 병렬화된 문제만 해결할 수 있습니다. CoT를 추가하면 더 복잡한 문제도 해결할 수 있게 됩니다.
- **CoT와 필러 토큰 비교**: CoT는 문제를 사람처럼 분해하여 해결하는 반면, 필러 토큰은 단순히 계산 용량을 증가시키는 역할을 합니다.

#### 3. 합성 데이터: 3SUM과 2SUM
- **3SUM 문제**: 주어진 숫자 배열에서 세 숫자의 합이 0이 되는지를 판단합니다.
- **2SUM-Transform 문제**: 숫자 배열의 두 숫자의 합이 0이 되는지와, 입력이 변환된 상태에서 이를 계산합니다.
- **데이터 생성**: 3SUM과 2SUM 문제를 해결하기 위해 체인 오브 싱크와 필러 토큰을 사용한 데이터를 생성했습니다.

#### 4. 실험 결과
- **3SUM 문제**: 필러 토큰을 사용한 모델이 중간 토큰 없이 즉각적으로 답을 제시한 모델보다 성능이 뛰어났습니다. 특히 입력 길이가 길어질수록 필러 토큰의 효과가 더 두드러졌습니다.
- **2SUM 문제**: 필러 토큰을 사용한 모델이 즉각 답변 모델보다 더 높은 정확도를 보였습니다.

#### 5. 결론
- **연구의 함의**: 이 연구는 CoT 토큰의 추가적인 계산 용량이 실제로 모델의 성능을 향상시킬 수 있음을 보여줍니다. 자연어 처리에서 필러 토큰이 유용할 가능성이 있으며, 이는 LLM(대형 언어 모델)에서 더 큰 이점을 제공할 수 있습니다.

### 전체 요약
이 논문은 트랜스포머 언어 모델에서 체인 오브 싱크(CoT)와 필러 토큰의 효과를 비교하여, CoT가 계산 용량 증가에 따른 성능 향상을 가져올 수 있음을 실험적으로 입증합니다. 필러 토큰은 CoT처럼 의미 있는 계산 단계를 제공하지 않지만, 특정 문제에서는 계산 용량을 증가시켜 성능을 향상시킬 수 있습니다. 이 연구는 필러 토큰이 자연어 처리와 같은 실제 데이터에서도 유용할 수 있음을 시사합니다.