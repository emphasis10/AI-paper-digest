# Is Flash Attention Stable?
## TL;DR
## Summary
- [https://arxiv.org/pdf/2405.02803.pdf](https://arxiv.org/pdf/2405.02803.pdf)

### 주요 내용 요약

1. **서론 및 배경**:
   - 대규모 기계 학습 모델 훈련은 시스템의 크기와 복잡성 때문에 독특한 도전을 제시합니다. 특히, Generative AI 모델을 훈련하는 과정에서는 종종 훈련 불안정성이 발생하며, 이는 손실 급증의 형태로 나타납니다. 이 논문에서는 수치 편차가 훈련 불안정성의 원인 중 하나로 제시되며, 이를 정량화하는 새로운 접근 방법을 개발합니다.

2. **Flash Attention 분석**:
   - Flash Attention은 트랜스포머 모델에서 주목 메커니즘을 가속화하기 위해 널리 사용되는 최신 최적화 기술입니다. 이 기술은 시스템 병목 현상을 해결하고자 메모리 접근을 최소화하면서 주목 행렬을 계산합니다. 그러나 이 과정에서 추가되는 재조정 인자들이 수치적 편차를 초래할 가능성이 있습니다.

3. **실험 방법론 및 수치 편차 측정**:
   - 수치 마이크로벤치마크를 개발하여 Flash Attention의 수치 편차를 고립시켜 분석합니다. 다양한 수치 정밀도에서 Flash Attention과 기준선 주목(Attention)의 출력 행렬을 비교함으로써, Flash Attention이 기준선 주목보다 약 10배 더 많은 수치 편차를 보임을 확인합니다.

4. **모델 가중치 차이를 통한 수치 편차의 맥락화**:
   - Flash Attention이 모델 훈련 중 가중치에 미치는 영향을 정량화하기 위해, 최대 차이와 와서스타인 거리(Wasserstein Distance) 메트릭을 사용합니다. 이를 통해 Flash Attention이 훈련 중 모델 가중치에 미치는 영향을 상한선으로 제시하며, Flash Attention이 낮은 정밀도 훈련보다 약 2-5배 적은 모델 가중치 편차를 초래함을 발견합니다.

### 혁신적인 부분
이 연구의 혁신성은 Flash Attention과 같은 최적화 기술이 대규모 모델 훈련에 도입될 때 발생할 수 있는 수치적 편차를 정량화하고 이해하는 데 기여합니다. 특히, 이 연구는 수치 편차의 영향을 실험적으로 측정하고 모델 가중치 변화와의 관계를 맥락화하는 새로운 프레임워크를 제시합니다. 이러한 접근 방식은 훈련 최적화 기술이 실제 모델 훈련과 어떻게 상호 작용하는지 더 깊이 이해하는 데 도움을 줍니다.

이 연구는 훈련 최적화 기술이 기계 학습 모델의 수치적 안정성에 미치는 영향을 평가하고, 이로 인한 훈련 불안정성을 완화하는 방안을 모색하는 데 중요한 기여를 합니다.