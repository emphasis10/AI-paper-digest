# CharXiv: Charting Gaps in Realistic Chart Understanding in Multimodal LLMs
## TL;DR
## Summary
- [https://arxiv.org/pdf/2406.18521.pdf](https://arxiv.org/pdf/2406.18521.pdf)

### 1. 논문 각 섹션 요약

#### 섹션 1: 서론
본 논문의 서론에서는 멀티모달 대형 언어 모델(Multimodal Large Language Models, MLLMs)을 소개합니다. MLLMs은 텍스트뿐만 아니라 이미지, 오디오, 비디오 등 다양한 입력을 처리할 수 있으며, 과학 논문, 금융 보고서, 뉴스 기사 등의 차트를 이해하는 능력이 필요합니다. 기존의 평가 지표들은 다양한 차트 유형과 복잡성, 질문 형식에서 부족함이 있다고 지적합니다.

#### 섹션 2: 기존 벤치마크가 차트 이해 능력을 과대평가함
이 섹션에서는 기존 벤치마크들이 차트를 이해하는 MLLMs의 능력을 어떻게 과대평가하는지 설명합니다. 특히, FigureQA와 DVQA와 같은 벤치마크들은 자동 생성된 질문 템플릿을 사용하며, 실제 다양성과 복잡성이 결여되어 있습니다. 또한, 기존의 머신러닝 모델들은 텍스트 인식(OCR)이나 명령어 추종과 같은 단순한 오류보다 사실 오류를 더 많이 범하는 경향이 있습니다.

#### 섹션 3: CharXiv - 현실적이고 도전적인 차트 이해 벤치마크
CharXiv는 arXiv 논문에서 수집된 현실적이고 도전적인 차트로 구성된 벤치마크입니다. CharXiv는 사람이 작성한 질문과 답변으로 구성된 QA 쌍을 포함하며, 보다 일반적인 차트 이해 능력을 테스트합니다. 이 섹션에서는 CharXiv의 차트 큐레이션 방식과 질문 구성 방식을 설명합니다.

#### 섹션 4: 실험
이 섹션에서는 다양한 MLLMs 모델을 평가한 실험 결과를 제시합니다. 실험 결과, 대다수의 모델들은 기본적인 설명 질문에서조차 오류를 범하며, 인간과의 큰 성능 차이를 보입니다. 인간은 설명 질문에서 92.1%, 추론 질문에서 80.5%의 정확도를 보이는 반면, 모델들은 크게 뒤쳐져 있습니다.

#### 섹션 5: 결론
결론에서는 CharXiv 벤치마크의 중요성과 현재 모델들의 한계를 재차 강조합니다. CharXiv는 MLLMs의 차트 이해 성능을 더 정확하게 평가할 수 있는 도구로, 향후 MLLMs의 발전 방향을 제시하는 데 기여할 것입니다.

### 2. 전체 요약
이 논문은 MLLMs의 차트 이해 능력을 평가하기 위한 새로운 벤치마크 CharXiv를 소개합니다. CharXiv는 arXiv 논문에서 수집된 다양한 차트와 사람이 작성한 질문을 포함하여, 기존 벤치마크들이 가지는 제한점을 극복하고자 합니다. 실험 결과, 현재의 MLLMs은 인간과 비교했을 때 차트 이해 능력이 크게 부족함을 보였으며, 이는 향후 연구의 중요한 방향성을 제시합니다. CharXiv는 MLLMs의 일반화 가능성과 성능 한계를 효과적으로 측정할 수 있는 도구로 활용될 것입니다.

## Similar Papers
- [ChartMimic: Evaluating LMM's Cross-Modal Reasoning Capability via Chart-to-Code Generation](2406.09961.md)
- [BRIGHT: A Realistic and Challenging Benchmark for Reasoning-Intensive Retrieval](2407.12883.md)
- [Large Scale Transfer Learning for Tabular Data via Language Modeling](2406.12031.md)
- [ReXTime: A Benchmark Suite for Reasoning-Across-Time in Videos](2406.19392.md)
- [MMIU: Multimodal Multi-image Understanding for Evaluating Large Vision-Language Models](2408.02718.md)
- [LongVideoBench: A Benchmark for Long-context Interleaved Video-Language Understanding](2407.15754.md)
- [OlympicArena Medal Ranks: Who Is the Most Intelligent AI So Far?](2406.16772.md)
- [MINT-1T: Scaling Open-Source Multimodal Data by 10x: A Multimodal Dataset with One Trillion Tokens](2406.11271.md)
- [MMEvalPro: Calibrating Multimodal Benchmarks Towards Trustworthy and Efficient Evaluation](2407.00468.md)
