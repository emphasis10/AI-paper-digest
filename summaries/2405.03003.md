# Parameter-Efficient Fine-Tuning with Discrete Fourier Transform
## TL;DR
## Summary
- [https://arxiv.org/pdf/2405.03003.pdf](https://arxiv.org/pdf/2405.03003.pdf)

### 1. 각 섹션 요약과 주요 공헌 및 혁신 부분 요약

#### **Abstract**
이 논문은 파라미터 효율적 미세 조정을 위해 높은 성능을 발휘하면서도 적은 수의 가중치 파라미터를 학습하는 FourierFT 방법을 제안합니다. 이는 자연어 처리, 자연어 생성, 명령어 튜닝 및 이미지 분류와 같은 다양한 작업에서 기존의 LoRA 방법보다 적은 파라미터로 더 나은 성능을 보여 줍니다.

#### **Introduction**
대규모 기초 모델(LFM)은 다양한 분야에서 탁월한 성능을 보여주지만, 전체 미세 조정 과정에서 많은 메모리와 저장소를 필요로 합니다. 기존의 LoRA 방법은 이 문제를 해결하였지만, 여전히 많은 파라미터 수를 필요로 합니다. 본 논문에서는 Fourier 변환을 사용하여 적은 수의 스펙트럼 계수만 학습함으로써 더 나은 성능을 제공하는 FourierFT 방법을 제안합니다.

#### **Related Works**
기존의 파라미터 효율적 미세 조정 방법들은 크게 비가중치 기반 방법과 가중치 기반 방법으로 나눌 수 있습니다. 본 논문은 가중치 기반 방법에 중점을 두어 LoRA의 단점을 보완하고자 하며, Fourier 변환의 우수한 표현력을 활용하여 파라미터 수를 크게 줄이는 방법을 제안합니다.

#### **Methodology**
FourierFT는 가중치 변화를 공간 도메인의 매트릭스로 간주하고, 스펙트럼 도메인에서 소수의 계수만 학습합니다. 이를 통해 역 이산 Fourier 변환을 사용하여 가중치 변화를 복원합니다. 이 방식은 LoRA 방식보다 훨씬 적은 파라미터로 동등 또는 더 나은 성능을 달성할 수 있습니다.

#### **Experiments**
FourierFT는 다양한 NLP와 CV 작업에서 기존의 LoRA 방법보다 적은 파라미터로 더 나은 성능을 보여주었습니다. 예를 들어, RoBERTa 모델의 자연어 이해, GPT-2 모델의 자연어 생성, LLaMA 모델의 명령어 튜닝, 그리고 비전 트랜스포머의 이미지 분류 작업에서 기존 방법들을 능가합니다.

#### **Results**
각 미세 조정 작업에서 FourierFT는 기존의 방법들에 비해 우수한 성능을 나타냈으며, 파라미터 수를 크게 줄였습니다. 특히, FourierFT는 자연어 이해 작업에서 약 0.024M의 파라미터만으로 기존의 전체 미세 조정 방식에 근접하거나 우수한 성능을 보여줍니다.

#### **Discussion**
FourierFT의 주요 장점은 적은 파라미터 수로 높은 성능을 유지하는 것으로, 이는 모델의 스케일(깊이와 폭)이 커질수록 더 두드러집니다. 이 방법은 다양한 작업에서 LoRA와의 비교에서도 일관된 우수성을 보였습니다.

#### **Conclusion**
FourierFT는 저장 메모리 요구량을 크게 줄이면서도 다양한 NLP와 CV 작업에서 우수한 성능을 발휘하는 간단하면서도 강력한 미세 조정 방법입니다. 이를 통해 여러 도메인, 작업, 사용자 선호도에 맞춘 맞춤형 미세 조정을 가능하게 합니다.

### 2. 전체 요약
이 논문의 주된 기여는 파라미터 효율적 미세 조정 방법인 FourierFT를 제안한 것입니다. FourierFT는 Fourier 변환의 강력한 표현력을 활용하여 적은 수의 스펙트럼 계수만 학습함으로써, 기존의 LoRA 방식보다 훨씬 적은 파라미터로 동등 또는 더 나은 성능을 달성합니다. 이 방법은 특히 자연어 처리, 자연어 생성, 명령어 튜닝 및 이미지 분류와 같은 다양한 작업에서 뛰어난 성능을 발휘하며, 모델의 규모가 커질수록 그 장점이 더 두드러집니다. FourierFT는 모델 미세 조정의 저장 메모리 요구 사항을 현저히 줄이면서도 높은 성능을 유지할 수 있어, 여러 도메인과 작업에 맞춘 맞춤형 미세 조정을 가능하게 합니다.


## Similar Papers
- [DoRA: Weight-Decomposed Low-Rank Adaptation](2402.09353.md)
- [MoA: Mixture-of-Attention for Subject-Context Disentanglement in Personalized Image Generation](2404.11565.md)
- [VeRA: Vector-based Random Matrix Adaptation](2310.11454.md)
- [PlacidDreamer: Advancing Harmony in Text-to-3D Generation](2407.13976.md)
- [DiJiang: Efficient Large Language Models through Compact Kernelization](2403.19928.md)
- [OLoRA: Orthonormal Low-Rank Adaptation of Large Language Models](2406.01775.md)
- [MLCM: Multistep Consistency Distillation of Latent Diffusion Model](2406.05768.md)
- [Improving GFlowNets for Text-to-Image Diffusion Alignment](2406.00633.md)
- [Tied-Lora: Enhancing parameter efficiency of LoRA with weight tying](2311.09578.md)
