# AI Agents That Matter
## TL;DR
## Summary
- [https://arxiv.org/pdf/2407.01502.pdf](https://arxiv.org/pdf/2407.01502.pdf)

### 1. 각 섹션의 주요 내용을 요약 및 설명

#### 도입 (Introduction)

이 논문은 AI 에이전트 평가의 현재 문제점을 강조하고 있습니다. 현재 평가 지표가 주로 정확도에만 초점을 맞추고 있어 불필요하게 복잡하고 비용이 많이 드는 에이전트가 등장하고 있습니다. 논문은 비용과 정확도를 동시에 최적화함으로써 더 나은 에이전트를 설계할 수 있다고 주장합니다. 또한 모델 개발자와 실제 사용 환경에서의 평가자가 서로 다른 평가 기준을 가져야 함을 지적합니다.

#### AI 에이전트 평가의 비용 통제 (AI Agent Evaluations Must Be Cost-Controlled)

이 섹션에서는 AI 에이전트 평가의 비용을 통제할 필요성을 강조합니다. 현재 대부분의 AI 에이전트는 언어 모델을 여러 번 호출하여 정확성을 높이는데, 이는 상당한 비용을 초래합니다. 논문은 세 가지 새로운 간단한 에이전트를 소개하고, 이들이 더 복잡한 최첨단 에이전트보다 비용이 덜 들면서도 더 나은 성과를 보인다고 보고합니다.

#### 비용과 정확도의 공동 최적화 (Jointly Optimizing Cost and Accuracy Can Yield Better Agent Designs)

이 섹션은 비용과 정확도를 동시에 최적화하는 것이 더 나은 에이전트 설계를 가능하게 한다고 주장합니다. 이를 위해 논문은 DSPy 프레임워크를 수정하여 HotPotQA 데이터셋에서 비용을 줄이면서도 정확도를 유지하는 실험을 수행했습니다. 결과적으로 비용이 53% 감소한 반면, 성능은 거의 유지됨을 확인했습니다.

#### 모델 개발자와 다운스트림 개발자의 평가 필요 (Model and Downstream Developers Have Distinct Benchmarking Needs)

이 섹션에서는 모델 개발자와 다운스트림 개발자가 서로 다른 평가 요구를 가지고 있음을 강조합니다. 모델 평가에서는 달러 비용 대신, 모델 파라미터 수나 학습에 사용된 컴퓨팅 리소스를 평가 지표로 사용해야 한다고 제안합니다. 반면 다운스트림 평가에서는 실제 비용을 평가에 포함시켜야 함을 지적합니다.

#### 에이전트 벤치마크는 지름길을 허용 (Agent Benchmarks Enable Shortcuts)

많은 에이전트 벤치마크가 과적합(overfitting) 문제를 허용하고 있어, 실제 성능이 벤치마크 성능과 일치하지 않는 경우가 발생합니다. 논문은 적절한 홀드아웃 데이터셋을 사용하여 이러한 문제를 해결할 것을 제안합니다.

#### 에이전트 평가는 표준화와 재현성이 부족 (Agent Evaluations Lack Standardization and Reproducibility)

이 섹션에서는 웹 아레나(WebArena)와 휴먼 이밸(HumanEval) 평가에서 재현성 부족과 표준화의 문제를 밝힙니다. 잘못된 재현성은 정확도 추정치를 부풀리고 에이전트의 능력에 대한 과도한 낙관을 초래할 수 있습니다. 논문은 이러한 문제를 해결하기 위해 더 많은 표준화와 명확한 평가 프레임워크를 제안합니다.

#### 결론 (Conclusion)

논문은 AI 에이전트 평가가 아직 초기 단계에 있으며, 비용 통제, 모델 및 다운스트림 평가의 분리, 적합한 홀드아웃 사용, 평가 표준화 등을 통해 보다 엄격한 평가를 해야 한다고 강조합니다.

### 2. 전체 요약

이 논문은 AI 에이전트 평가에서 발생하는 주요 문제점을 다루고 있으며, 모든 평가가 주로 정확도에만 초점을 맞추는 것이 문제로 지적됩니다. 정확도와 함께 비용을 최적화함으로써 더 나은 에이전트를 개발할 수 있음을 실험을 통해 증명했습니다. 또한, 평가의 비용을 통제하고, 모델 개발자와 실제 적용 환경에서의 다운스트림 평가자가 서로 다른 기준을 가져야 한다고 주장합니다. 에이전트 벤치마크의 과적합 문제와 재현성 부족 문제를 해결하기 위해 적절한 홀드아웃 데이터셋과 표준화된 평가 프레임워크를 제안합니다. 결론적으로, 이 논문은 AI 에이전트의 평가와 벤치마크 방법론을 재고할 필요가 있음을 강조하며, 이를 통해 더 실제적인 환경에 적합한 에이전트를 개발할 수 있는 기초를 마련하고자 합니다.