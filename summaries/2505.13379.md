# Thinkless: LLM Learns When to Think
## TL;DR
## Summary
- [https://arxiv.org/pdf/2505.13379.pdf](https://arxiv.org/pdf/2505.13379.pdf)

1. 각 섹션 요약:

- 서론
  인공지능과 기계학습에서 논리적 추론이 가능한 모델들이 복잡한 문제를 해결하는 데 효과가 있음을 설명합니다. 이러한 방식이 모든 문제에 적용되면 계산 효율성이 떨어진다는 점을 지적하며, 'Thinkless'라는 프레임워크를 통해 LLM이 문제의 복잡성에 따라 적절한 추론 방식을 선택할 수 있도록 학습하는 방법을 제안합니다.

- 관련 연구
  효율적인 추론 모델의 연구 및 개선 방안을 설명합니다. 체인 오브 쏘트(Chain-of-Thought) 방식을 활용한 보다 압축된 추론 경로를 위한 연구들이 주목받고 있음을 소개합니다.

- 방법론
  'Thinkless' 프레임워크는 강화학습 방법론을 통해 문제가 주어졌을 때 짧은 형태와 긴 형태의 응답 중 선택할 수 있도록 설계되었습니다. 하이브리드 추론 모델 훈련을 위해 디스틸레이션 과정과 강화학습 과정으로 구성되어 있으며, 'Decoupled Group Relative Policy Optimization(DeGRPO)' 알고리즘을 사용하여 더욱 안정적이고 효과적인 학습이 가능하도록 돕습니다.

- 실험
  실험은 LLM과 다양한 데이터셋을 사용하여 진행됩니다. 'Thinkless' 모델을 통해 긴 체인 형식의 추론을 줄이고 효율성을 극대화하는 방법을 입증합니다. 특히 복잡한 문제에서는 긴 추론을 선호하고, 간단한 문제에서는 짧은 응답을 선택하는 능력을 보여줍니다.

- 결론
  'Thinkless' 프레임워크는 강화학습을 통해 모델이 문제의 복잡성에 따라 응답 형태를 자율적으로 결정할 수 있도록 하며, 불필요한 긴 형태의 추론을 줄여 전체 시스템의 비용을 낮추고 사용자 대기 시간을 개선합니다.

2. 전체 요약:

이 논문은 강화학습을 통해 'Reasoning Language Models'이 문제의 복잡성 및 모델의 역량에 따라 상황에 맞는 추론 방식을 자율적으로 선택할 수 있는 'Thinkless' 프레임워크를 제안합니다. DeGRPO 알고리즘을 도입하여 학습 과정을 정교하게 분리하고, 이는 안정적으로 추론 모드를 선택하고 응답 정확성을 개선하는데 기여합니다. 실험 결과 여러 벤치마크 데이터셋에서 이 방법이 긴 형태의 추론 사용을 50%에서 90%까지 줄이며 효율성을 크게 향상시킨다는 것을 보여줍니다.