# Megalodon: Efficient LLM Pretraining and Inference with Unlimited Context Length
## TL;DR
## Summary
- [https://arxiv.org/pdf/2404.08801.pdf](https://arxiv.org/pdf/2404.08801.pdf)

**[MEGALODON: 효율적인 LLM 프리트레이닝과 무제한 문맥 길이로의 추론]**

**1. 서론 및 배경**
이 연구는 기존의 Transformer 모델이 긴 시퀀스 데이터 처리에 있어 계산 복잡도가 증가하는 문제를 해결하고자 합니다. MEGALODON은 MEGA(지수 이동 평균을 활용한 게이트 어텐션) 아키텍처를 기반으로, 더욱 복잡한 기술적 요소를 추가하여 무제한 문맥 길이를 모델링할 수 있는 신경 아키텍처를 제안합니다.

**2. MEGA 아키텍처**
MEGA는 입력 시퀀스 각 차원을 확장하여 지수 이동 평균(EMA)를 계산하고, 게이트 어텐션 메커니즘을 통해 문맥 정보를 인코딩합니다. 이는 시퀀스 내 각 시점에서의 문맥적 의존성을 포착하는 데 도움을 주어 시퀀스 모델링의 효율성을 높입니다.

**3. MEGALODON의 기술적 진보**
- **CEMA:** 복소 지수 이동 평균(Complex EMA)은 EMA를 복소수 영역으로 확장하여, 각 시점의 은닉 상태를 보다 효과적으로 계산합니다.
- **시간 단계 정규화:** 자동 회귀 시퀀스 모델링을 위해 그룹 정규화를 확장하여 시간 단계별로 평균과 분산을 계산합니다. 이는 모델이 시간에 따라 변화하는 데이터의 내부 변동을 효과적으로 처리할 수 있게 합니다.
- **정규화 어텐션:** 기존의 어텐션 메커니즘을 개선하여, 입력 시퀀스의 각 부분에 대해 보다 안정적이고 효율적인 학습이 가능하도록 합니다.

**4. 실험 결과**
MEGALODON은 LLAMA2 모델과 비교하여 더 낮은 트레이닝 손실을 달성하고, 다양한 벤치마크에서 뛰어난 성능을 보여줍니다. 특히 긴 문맥을 가진 언어 모델링 태스크에서 그 성능이 입증되었으며, 이미지넷, 위키텍스트 등 다양한 데이터셋에서도 일관된 성능 향상을 보여줍니다.

**전체 요약:**
MEGALODON은 기존 Transformer의 한계를 극복하고, 긴 시퀀스 데이터를 효과적으로 처리할 수 있는 신경 아키텍처를 제안합니다. 이는 특히 복잡한 언어 이해 및 생성 작업에서의 효율성과 정확성을 크게 향상시킵니다. 다양한 기술적 개선을 통해, MEGALODON은 무제한 문맥 길이에 대한 모델링 능력을 갖추며, 이를 통해 보다 깊고 정확한 언어 처리가 가능해집니다.