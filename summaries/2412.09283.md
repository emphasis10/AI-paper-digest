# InstanceCap: Improving Text-to-Video Generation via Instance-aware Structured Caption
## TL;DR
## Summary
- [https://arxiv.org/pdf/2412.09283.pdf](https://arxiv.org/pdf/2412.09283.pdf)

### 1. 각 섹션의 요약

**서론**
이 논문은 텍스트-비디오 생성에서 인스턴스 수준의 정밀한 비디오 캡션을 위한 새로운 프레임워크인 `InstanceCap`을 제안합니다. 기존의 비디오 캡션은 종종 세부 정보가 부족하고 환각 현상이나 부정확한 모션 표현으로 인해 생성된 영상의 몰입감을 해치고 있었습니다.

**관련 연구**
비디오 재캡셔닝 분야에서는 두 가지 주요 방식인 수동 주석과 멀티모달 대형 언어 모델을 사용한 종단 간 재캡셔닝이 존재합니다. 수동 주석은 정확도가 높지만, 대규모 데이터셋을 구축하는 데 한계가 있습니다.

**방법론**
`InstanceCap` 파이프라인을 도입하여 비디오를 인스턴스 단위로 분해하고, 구조화된 구문으로 재구성합니다. 이를 통해 밀도가 높은 프롬프트를 정밀하고 간결한 묘사로 변화시킬 수 있습니다. 이 방법론에서는 AMC와 같은 보조 모델을 사용하여 전역 비디오를 인스턴스로 변환하며, 인스턴스 충실도를 높입니다.

**실험 결과**
실험 결과, `InstanceCap`은 기존 모델에 비해 비디오와 캡션 사이의 충실도를 크게 향상시켰으며, 환각 및 반복을 줄이는 데 매우 효과적임을 보여줍니다.

**논의 및 결론**
이 논문의 주요 기여는 `InstanceCap`을 활용하여 캡션과 비디오 사이의 높은 충실도를 보장하면서도 환각 현상 및 반복성을 줄이는 것입니다. 향후 연구에서는 더 큰 비디오 데이터셋에 `InstanceCap`을 적용하고 강력한 T2V 모델을 훈련할 계획입니다.

### 2. 전체 요약
이 논문은 기존의 비디오 캡션이 만연한 정보 부족과 환각 문제를 해결하고자 `InstanceCap` 프레임워크를 제안합니다. 이 방법은 비디오를 인스턴스 단위로 변환하여 충실도가 높은 구조화된 캡션을 제공하며, 영상과 캡션 간의 일관성을 높입니다. 실험 결과 `InstanceCap`은 기존 기법들보다 더 나은 성과를 보여주며, 향후 대규모 데이터셋에 적용할 가능성을 엿볼 수 있습니다.