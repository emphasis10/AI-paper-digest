# Continuous Speech Synthesis using per-token Latent Diffusion
## TL;DR
## Summary
- [https://arxiv.org/pdf/2410.16048.pdf](https://arxiv.org/pdf/2410.16048.pdf)

1. 각 섹션의 요약:
   - **소개**: 이 논문은 SALAD라는 새로운 지속적 모달리티에 대한 제로샷 음성 합성 모델을 제안합니다. SALAD는 텍스트와 오디오 토큰을 활용하여 텍스트를 특정 화자의 음성으로 변환합니다.
   - **관련 연구**: 기존의 제로샷 TTS(음성 합성) 시스템들은 텍스트와 오디오 토큰을 사용하며, SALAD는 이를 확장하여 더 많은 유연성과 품질을 제공합니다.
   - **SALAD 방법론**: SALAD는 연속적인 표현을 사용하여 불연속적인 음성 합성 기법을 연장했습니다. 이 방법은 텍스트-음향(T2A) 모델과 의미-음향(S2A) 모델을 포함하고 있습니다.
   - **실험**: 다양한 모델을 실험하여 SALAD의 성능을 테스트했습니다. 그 결과, SALAD의 T2A 모델은 가장 높은 명확성 점수를 얻었으며, 음질과 유사성 면에서 사실과 유사한 수준을 유지했습니다.
   - **토론 및 한계**: SALAD 모델은 데이터 정렬을 필요로 하지 않기 때문에 대규모 데이터 소스를 쉽게 활용할 수 있습니다. 다만, 불연속 표상에 대한 의존을 피하기 위한 지속적인 연구 개발이 필요합니다.

2. 전체 요약:
   SALAD는 제로샷 텍스트-음성 합성에 혁신적인 접근 방식을 제안합니다. 이 모델은 연속 표현을 기반으로 하여 사실에 가까운 음성을 생성하고, 텍스트 데이터와 오디오 데이터의 매칭이 필요 없고 유연성을 높였습니다. 이로 인해 대규모의 다양한 데이터 소스를 활용할 수 있으며, 음성 합성의 명확성과 품질을 개선하였습니다. 이 연구는 음성 합성 분야에서 새로운 길을 제시하며, 향후 연구에서 응용할 수 있는 기반을 제공합니다.
