# How multilingual is Multilingual BERT?
## TL;DR
## Summary
- [https://arxiv.org/pdf/1906.01502.pdf](https://arxiv.org/pdf/1906.01502.pdf)

### 논문 요약

#### 1. 각 섹션 요약
1. **서론**
    - **설명:** 다국어 BERT(M-BERT)는 104개의 언어로 구성된 위키피디아 코퍼스를 사용해 미리 훈련된 언어 모델입니다. 이 논문은 M-BERT가 다국어 간 모델 전이에서 얼마나 잘 일반화할 수 있는지를 조사합니다. 특히, 하나의 언어에서 특정 작업을 훈련하고 다른 언어에서 평가하는 "제로샷 전이"에 주목합니다.
    - **핵심 기여:** M-BERT의 다국어 표현 능력을 검증하고, 이 모델이 언어 간 전이를 수행하는 매커니즘을 다양한 실험을 통해 분석합니다.

2. **모델 및 데이터**
    - **설명:** M-BERT는 12개의 레이어로 구성된 트랜스포머 모델이며, 모든 언어의 위키피디아 페이지를 사용해 학습되었습니다. Named Entity Recognition(NER)과 Part-of-Speech(POS) 태깅 실험을 수행하며, CoNLL-2002 및 2003 데이터셋과 Universal Dependencies(UD) 데이터셋을 사용했습니다.
 
3. **어휘 암기**
    - **설명:** 다국어 단어 조각을 사용하는 M-BERT는 어휘 겹침이 존재하는 경우 전이를 잘 수행하지만, 겹침이 없는 경우에도 성능을 유지합니다. 이는 단순한 어휘 암기 이상의 다국어 표현을 학습했음을 시사합니다.

4. **언어 구조 인코딩**
    - **설명:** M-BERT는  언어 간 전이를 위해 특정 언어 간 술어 구조의 유사성에 따른 일반화 능력을 분석합니다. 그 결과, 단일 언어 입력을 사용해 코드 스위칭 텍스트나 전사된 텍스트로 일반화할 수 있음을 보여줍니다.

5. **다국어 특성 공간의 특성화**
    - **설명:** M-BERT의 피처 공간이 다국어인지 검증하기 위해, 동일한 문장을 두 언어로 번역한 쌍을 사용해 실험을 설계했습니다. 그 결과, 언어 쌍에 관계없이 동일한 문장에 대한 변환이 가능함을 확인했습니다.

6. **결론**
    - **설명:** M-BERT는 명시적으로 훈련되지 않았음에도 불구하고 다국어 표현을 잘 학습하며, 스크립트 간 전이 및 코드 스위칭에서도 성능을 발휘합니다. 다만, 언어학적 특징이 다른 언어 간 전이에는 한계가 존재합니다.

#### 2. 전체 요약
이 논문은 다국어 BERT(M-BERT)가 104개의 언어 위키피디아 코퍼스를 기반으로 사전 훈련된 모델로서 언어 간 일반화를 잘 수행하는지를 분석합니다. 연구 결과, M-BERT는 하나의 언어로 작업을 학습하고 다른 언어로 평가하는 제로샷 전이를 놀라운 수준으로 성능을 발휘합니다. 특히, 언어 간 겹침이 없는 경우에도 다국어 표현을 사용해 전이 성능을 유지하며, 언어 구조의 유사성에 따른 일반화 능력을 입증했습니다. 논문은 각기 다른 스크립트 간 전이, 코드 스위칭 텍스트 및 전사된 텍스트로의 일반화를 통해 M-BERT의 다국어 표현 학습 능력을 평가하고 있습니다. М-BERT는 명시적으로 다국어 훈련이 되지 않았음에도, 다국어 표현 능력을 자연스럽게 학습했고, 이는 연구 및 실무에서 매우 의미 있는 기여입니다.