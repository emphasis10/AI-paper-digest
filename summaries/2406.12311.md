# Mixture of Scales: Memory-Efficient Token-Adaptive Binarization for Large Language Models
## TL;DR
## Summary
- [https://arxiv.org/pdf/2406.12311.pdf](https://arxiv.org/pdf/2406.12311.pdf)

### 1. 각 섹션 요약

#### Introduction (소개)
이 논문은 **Mixture of Scales (BinaryMoS)**라는 새로운 이진화 기법을 소개합니다. 전통적인 이진화 방법은 모델 크기를 줄일 수 있지만 자연어 처리(NLP) 성능이 크게 저하되는 문제가 있습니다. BinaryMoS는 토큰에 따라 이진 가중치를 동적으로 조정하여 이러한 문제를 해결합니다.

#### Background (배경)
기존의 이진화 방법은 높은 압축률을 제공하지만, 자연어 처리 모델의 표현 능력을 크게 제한합니다. 이를 해결하기 위해 몇 가지 이진화 기술이 개발되었지만, 대부분은 메모리 오버헤드가 높고 정확도가 충분하지 않습니다.

#### Proposed BinaryMoS (제안된 BinaryMoS)
1. **Binarization with Mixture of Scale (모듈형 이진화)**:
    - BinaryMoS는 여러 개의 스케일링 팩터를 사용하여 이진화된 모델의 표현력을 높입니다. 기존의 이진화 방법은 단일 스케일링 팩터를 사용하지만, BinaryMoS는 토큰마다 동적으로 조정되는 여러 개의 스케일링 전문가를 사용합니다.
  
2. **Router Design (라우터 설계)**:
    - 토큰별로 적응형 스케일링 팩터를 생성하기 위해 라우터를 설계하였습니다. 라우터는 입력 토큰을 기반으로 스케일링 전문가들을 결합하여 토큰별 스케일링 팩터를 생성합니다.
  
3. **Impact on LLM Compression (LLM 압축에 미치는 영향)**:
    - BinaryMoS는 약간의 메모리 오버헤드는 있지만 이진화 모델의 압축률을 크게 향상시킵니다. 예를 들어, LLaMA-1/2-7B 모델을 11.24배 압축할 수 있습니다.

4. **Quantization-Aware Knowledge Distillation (양자화 인식 지식 증류)**:
    - 교사 모델의 지식을 학생 모델에 전달하는 지식 증류 (Knowledge Distillation) 방법을 채택하여 이진화된 모델의 성능을 극대화합니다.

#### Experiments (실험)
1. **Experimental Settings (실험 설정)**:
    - 다양한 모델 및 데이터셋(LLaMA, OPT)을 사용하여 BinaryMoS의 성능을 평가하였습니다.

2. **Analysis on Scaling Experts (스케일링 전문가에 대한 분석)**:
    - 스케일링 전문가의 수에 따른 성능을 분석한 결과, 4명의 전문가를 사용하는 것이 최적임을 발견하였습니다.

3. **Token-Adaptive Scaling Factors (토큰별 적응형 스케일링 팩터 분석)**:
    - BinaryMoS은 토큰마다 다른 스케일링 팩터를 생성함으로써 모델 성능을 향상시킵니다.

4. **Perplexity and Accuracy Results (혼란도 및 정확도 결과)**:
    - BinaryMoS는 기존의 이진화 방법들에 비해 모든 성능 지표에서 더 우수한 성능을 보였습니다.

5. **Comparison with 2-bit Quantization (2비트 양자화 비교)**:
    - BinaryMoS는 2비트 양자화 방법보다도 더 낮은 메모리 요구 조건에서 더 나은 성능을 보여주었습니다.


### 2. 논문의 전체 요약

이 논문은 대용량 언어 모델(LLM)의 이진화 효율성을 높이기 위해 **Mixture of Scales (BinaryMoS)**라는 혁신적인 이진화 기법을 제안합니다. BinaryMoS는 기존의 정적인 스케일링 팩터 대신, 토큰별로 적응형 스케일링 팩터를 생성하여 모델의 표현력을 유지합니다. 실험 결과, BinaryMoS는 기존 이진화 기법 및 2비트 양자화 방법에 비해 혼란도와 정확도에서 우수한 성능을 보였으며, 메모리 사용량도 더 적습니다. 이 연구는 LLM 압축 분야에서 새로운 가능성을 제시하며, 향후 연구의 방향성을 제시합니다.

## Similar Papers
- [BiQGEMM: Matrix Multiplication with Lookup Table For Binary-Coding-based Quantized DNNs](2005.09904.md)
- [OneBit: Towards Extremely Low-bit Large Language Models](2402.11295.md)
- [4-bit Shampoo for Memory-Efficient Network Training](2405.18144.md)
- [Finch: Prompt-guided Key-Value Cache Compression](2408.00167.md)
- [How Good Are Low-bit Quantized LLaMA3 Models? An Empirical Study](2404.14047.md)
- [ShiftAddLLM: Accelerating Pretrained LLMs via Post-Training Multiplication-Less Reparameterization](2406.05981.md)
- [ZeroQuant-FP: A Leap Forward in LLMs Post-Training W4A8 Quantization Using Floating-Point Formats](2307.09782.md)
- [BiLLM: Pushing the Limit of Post-Training Quantization for LLMs](2402.04291.md)
- [LLaMA-NAS: Efficient Neural Architecture Search for Large Language Models](2405.18377.md)
