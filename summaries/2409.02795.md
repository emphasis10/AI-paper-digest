# Towards a Unified View of Preference Learning for Large Language Models: A Survey
## TL;DR
## Summary
- [https://arxiv.org/pdf/2409.02795.pdf](https://arxiv.org/pdf/2409.02795.pdf)

### 전체 요약

논문은 대규모 언어 모델(LLMs)의 선호도 학습(Preference Learning)에 대한 통합된 관점을 제시하고 있습니다. 선호도 학습은 LLM의 출력 결과를 인간의 선호와 일치하게 만드는 과정으로, 주로 네 가지 구성 요소로 분해됩니다: 모델, 데이터, 피드백, 알고리즘. 이러한 통합된 프레임워크를 통해 기존의 다양한 알고리즘 간의 관계를 명확히 하고, 이들의 강점을 시너지를 내기 위해 적용방안을 탐구합니다. 또한, 이 논문은 주요 알고리즘의 작동 예제를 제공하여 독자들이 명확히 이해할 수 있도록 도와줍니다.

### 섹션별 요약

#### 1. 서론

LLMs의 발전과 이를 인간 선호도에 맞추기 위한 다양한 시도가 존재합니다. pre-training과 supervised finetuning이 기본적인 언어 능력을 개발하는 데 중요한 역할을 하지만, 선호도 조정은 LLM이 독성을 띠거나 오해를 일으킬 수 있는 내용을 생성하는 것을 방지하기 위한 중요한 단계입니다.

#### 2. 정의와 공식화

선호도 학습의 개념을 정리하고, 이를 구현하기 위한 기본 프레임워크를 제시합니다. 선호도 학습은 인간이 선호하는 데이터에 따라 모델을 조정하는 과정으로, 이는 강화학습(Reinforcement Learning)과 감독학습(Supervised Learning)을 통해 이루어질 수 있습니다.

#### 3. LLM 선호도 학습의 통합된 관점

기존 연구를 통합하여 LLM 선호도 학습의 최적화 목표를 통합된 프레임워크 내에서 설명합니다. 이 장에서는 RL과 SFT 기반 방법론의 최적화 목표를 동일한 프레임워크 내에서 설명하고, 온라인 및 오프라인 설정에서 알고리즘을 어떻게 독립적으로 사용할 수 있는지 소개합니다.

#### 4. 선호도 데이터

선호도 데이터는 주로 on-policy와 off-policy의 두 가지 데이터 수집 방식으로 나뉩니다. On-policy 데이터는 실시간으로 생성되는 데이터이고, off-policy 데이터는 사전에 수집된 데이터입니다. 다양한 데이터 소스를 통해 선호도를 학습할 수 있습니다.

#### 5. 피드백

피드백은 모델의 성능을 최적화하는 데 있어 중요한 역할을 합니다. 피드백은 인간 또는 모델에서 직접 제공될 수 있으며, 모델 기반 보상 모델, pair-wise scoring 모델, LLM-as-a-judge 방식으로 제공될 수 있습니다.

#### 6. 알고리즘

선호도 학습 알고리즘은 크게 네 가지로 분류됩니다: Point-wise 방법, pair-wise 대조, list-wise 대조, 그리고 학습이 불필요한 정렬 방법. 각 방법론은 샘플을 평가하여 모델의 경사 계수를 계산하는 방식이 다릅니다.

#### 7. 평가

평가 방법은 rule-based와 LLM-based 평가로 나뉘며, 각 방법의 한계와 장점을 논의합니다. LLM 기반 평가 모델은 다양한 편향을 보일 수 있으며, 이를 개선하기 위한 여러 벤치마크와 연구가 진행 중입니다.

#### 8. 미래 방향

미래 연구는 더 나은 품질의 다채로운 선호 데이터, 신뢰할 수 있는 피드백 및 확장 가능한 감독 기술, 더 나은 선호도 학습 알고리즘, 그리고 LLM의 보다 포괄적인 평가 방법을 개발하는 데 초점을 맞추어야 합니다.

#### 9. 결론

통합된 프레임워크를 통해 다양한 선호도 학습 방법론을 체계적으로 정리하고, 향후 연구 방향을 제시합니다. 이 논문은 연구자들이 LLM을 인간의 선호도에 맞추는 데 도움을 줄 수 있는 이해를 제공하는 데 목적이 있습니다.