# SliceGPT: Compress Large Language Models by Deleting Rows and Columns
## TL;DR
## Summary
- [https://arxiv.org/pdf/2401.15024.pdf](https://arxiv.org/pdf/2401.15024.pdf)

이 연구 논문은 "SLICEGPT: Compress Large Language Models by Deleting Rows and Columns"이라는 제목으로, 대규모 언어 모델의 압축을 위한 새로운 방법을 소개합니다. 이 방법은 모델의 가중치 행렬에서 행과 열을 제거함으로써 파라미터의 수를 줄이고, 계산 요구 사항을 감소시키는 동시에 성능을 유지하는 것을 목표로 합니다.

### 주요 내용 요약

1. **서론 및 배경**:
   - 대규모 언어 모델은 많은 파라미터와 계산 비용을 요구하며, 이를 감소시키기 위한 다양한 기술이 소개되어 왔습니다. 이 논문에서는 특히 가중치 행렬의 행과 열을 제거하는 새로운 방법을 제안합니다.

2. **방법론 (SliceGPT)**:
   - SliceGPT는 트랜스포머 네트워크의 각 블록에서 정규화 연산과 상호작용하는 직교 변환을 이용합니다. 이 변환은 네트워크의 출력에 변화를 주지 않으면서 행렬의 크기를 줄이는 데 사용됩니다. 이 방법을 통해 모델의 크기를 최대 30%까지 줄이면서도 원래 모델의 90% 이상의 성능을 유지할 수 있습니다.

3. **실험 결과**:
   - 다양한 대규모 모델(예: LLAMA-2 70B, OPT 66B)에 대한 실험에서 SliceGPT는 기존의 2:4 스파스 패턴보다 뛰어난 성능을 보였습니다. 특히, 모델의 성능을 거의 그대로 유지하면서 계산 비용을 크게 줄일 수 있었습니다.

4. **토론 및 결론**:
   - 이 기술은 특히 GPU 자원이 제한적인 환경에서 대규모 모델을 효율적으로 배치할 수 있게 합니다. 또한, 복구 미세조정 없이도 좋은 성능을 유지할 수 있어, 실용적인 적용 가능성이 매우 높습니다.

### 혁신적인 부분
SliceGPT의 혁신성은 대규모 언어 모델의 가중치 행렬을 직교 변환을 통해 변형하고, 주요 성분만을 유지하는 방식으로 효율적으로 모델 크기를 줄이는 점에 있습니다. 이는 계산 효율성을 크게 향상시키면서도 모델의 성능을 유지할 수 있는 방법을 제공하며, 대규모 모델의 실용적인 활용을 가능하게 합니다.

이 연구는 대규모 언어 모델의 효율적인 배치와 운용에 중요한 기여를 하며, 향후 더욱 발전될 새로운 연구 방향을 제시합니다.

## Similar Papers
- [ReLU Strikes Back: Exploiting Activation Sparsity in Large Language Models](2310.04564.md)
- [SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression](2306.03078.md)
- [SparseGPT: Massive Language Models Can Be Accurately Pruned in One-Shot](2301.00774.md)
- [Hierarchical Context Merging: Better Long Context Understanding for Pre-trained LLMs](2404.10308.md)
- [ShiftAddLLM: Accelerating Pretrained LLMs via Post-Training Multiplication-Less Reparameterization](2406.05981.md)
- [Training Neural Networks from Scratch with Parallel Low-Rank Adapters](2402.16828.md)
- [Q-Sparse: All Large Language Models can be Fully Sparsely-Activated](2407.10969.md)
- [ZeroQuant-FP: A Leap Forward in LLMs Post-Training W4A8 Quantization Using Floating-Point Formats](2307.09782.md)
- [Prompt Sketching for Large Language Models](2311.04954.md)
