# What Matters in Transformers? Not All Attention is Needed
## TL;DR
## Summary
- [https://arxiv.org/pdf/2406.15786.pdf](https://arxiv.org/pdf/2406.15786.pdf)

### 1. 섹션별 요약

#### Abstract
이 논문은 대규모 언어 모델(LLM)의 효율성을 높이기 위해 블록, MLP 레이어, 어텐션 레이어의 구조적 중복성을 탐구합니다. 특히, 어텐션 레이어의 높은 중복성을 발견하고 이를 안전하게 제거할 수 있음을 보여줍니다. 또한, 어텐션과 MLP 레이어를 함께 제거하는 Joint Layer Drop 방법을 제안하여 효율성을 향상시켰습니다. 다양한 실험을 통해 제안된 방법의 효율성을 입증했습니다.

#### Introduction
트랜스포머 기반 대규모 언어 모델(LLM)은 자연어 이해와 생성에서 뛰어난 성능을 보이지만, 효율성 문제가 있습니다. GPT-3와 같은 대규모 모델은 많은 메모리와 자원이 필요하며, 이는 실제 응용에 장애가 됩니다. 본 논문은 LLM의 중복적인 구조를 제거하여 효율성을 높이는 방안을 제안합니다. 트랜스포머 아키텍처는 MLP 레이어와 어텐션 레이어로 구성되어 있으며, 이들의 중복성을 분석하고 제거의 효과를 실험적으로 검증합니다.

#### Related Works
LLM의 효율성을 높이기 위한 다양한 연구가 있었지만, 주로 트랜스포머 아키텍처의 특성을 충분히 고려하지 않았습니다. 트랜스포머 모델은 여러 블록으로 구성되며 각 블록은 어텐션 레이어와 MLP 레이어로 이루어져 있습니다. 이 연구는 이러한 다양한 모듈의 중복성을 조사하고 모델 압축 기술인 양자화와 가지치기를 통해 효율적인 아키텍처 설계를 제안합니다.

#### Methodology
이 섹션에서는 유사성 기반 메트릭을 사용하여 LLM 내 모듈의 중요도를 평가한 후, 중복적인 모듈을 제거하는 방법을 설명합니다. 입력과 출력의 코사인 유사도를 계산하여 모듈의 중요도를 결정하고, 중요도가 낮은 모듈을 제거합니다. 단층 제거와 Joint Layer Drop 방식을 통해 어텐션 레이어와 MLP 레이어를 함께 제거하여 효율성을 최적화합니다.

#### Experiments
다양한 실험을 통해 어텐션 레이어와 MLP 레이어를 제거하는 방법의 효과를 검증했습니다. 예를 들어, Llama-2-13B와 Mistral-7B 모델에서 8개의 블록을 제거하면 성능이 크게 저하되지만, 어텐션 레이어는 제거해도 성능에 큰 영향을 미치지 않는 것으로 나타났습니다. 또한, Joint Layer Drop 방법을 통해 더 높은 제거 비율과 성능을 달성할 수 있음을 확인했습니다.

#### Results
어텐션 레이어를 제거하면 메모리 사용량과 계산 비용을 감소시키면서도 성능을 유지할 수 있습니다. 반면, MLP 레이어를 제거하면 성능 저하가 더 큽니다. Joint Layer Drop은 MLP 레이어와 어텐션 레이어를 함께 제거하여 더 나은 성능과 효율성을 제공합니다.

#### Conclusion
이 연구는 트랜스포머 아키텍처에서 블록, MLP 레이어, 어텐션 레이어의 구조적 중복성을 재검토하고, 효율성을 높이기 위한 모듈 제거 방법을 제안합니다. 제안된 방법은 모델을 더 간결하고 효율적으로 만들어주며, 향후 네트워크 아키텍처 설계에 유용한 통찰을 제공합니다.

### 2. 논문의 전반적인 요약

이 논문은 대규모 언어 모델(LLM)의 구조적 중복성을 분석하고, 이를 제거하여 효율성을 높이는 방법을 제안합니다. 유사성 기반 메트릭을 사용하여 중복적인 모듈을 식별하고, 이러한 모듈을 제거하는 실험을 통해 성능 저하 없이 메모리와 계산 비용을 줄일 수 있음을 입증했습니다. 특히, 어텐션 레이어의 높은 중복성을 발견하고, 이를 안전하게 제거할 수 있음을 보여주었으며, 추가적으로 어텐션과 MLP 레이어를 함께 제거하는 Joint Layer Drop 방법을 제안하여 효율성을 한층 더 향상시켰습니다. 이 연구는 향후 AI 네트워크 아키텍처 설계에 중요한 통찰을 제공합니다.