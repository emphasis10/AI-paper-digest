# SWE-PolyBench: A multi-language benchmark for repository level evaluation of coding agents
## TL;DR
## Summary
- [https://arxiv.org/pdf/2504.08703.pdf](https://arxiv.org/pdf/2504.08703.pdf)

### 1. 각 섹션의 요약

#### 서론
- **지능형 코딩 에이전트의 발전**: AI 기반 코딩 에이전트는 최근 몇 년간 큰 주목을 받았습니다. 이러한 에이전트는 코드 완성, 번역, 문서화, 디버깅 등 다양한 소프트웨어 엔지니어링 과제를 수행할 수 있습니다. 그러나 다양한 환경에서의 성능과 효율성은 아직 해결되지 않은 과제로 남아있습니다.

#### 관련 연구
- **코드 생성의 역사와 현재**: 자동 프로그램 생성에 관련된 연구는 오랜 역사를 지니고 있으며, 최근에는 거대 언어 모델을 활용한 코드 생성이 주목받고 있습니다. SWE-Bench 등 여러 벤치마크가 존재하며, 최근에는 다중 언어를 지원하는 SWE-PolyBench가 개발되었습니다.

#### SWE-PolyBench 개발
- **벤치마크 구축**: SWE-PolyBench는 21개의 저장소에서 가져온 2110개의 데이터로 구성되어 있으며, Java, JavaScript, TypeScript, Python을 포함합니다. 코드 수정, 기능 추가, 코드 리팩토링을 포함한 다양한 작업을 평가할 수 있는 새로운 메트릭이 도입되었습니다.

#### SWE-PolyBench 특징
- **초기 벤치마크**: SWE-Bench와 비교해 SWE-PolyBench는 다양한 언어와 작업 유형을 반영하며, 코드의 구조적인 복잡성 평가에 중점을 둡니다.

#### LLM 기반의 작업 분류
- **태스크 분류**: Bug Fix, Feature Request, Refactoring, Security, Testing 다섯 가지 카테고리로 작업을 분류하며, 이를 통해 에이전트의 강점과 약점을 파악할 수 있습니다.

#### 오픈 소스 코딩 에이전트 평가
- **평가 방법 및 도전 과제**: Aider, Agentless, SWE-agent 등의 오픈 소스 코딩 에이전트를 SWE-PolyBench에 적응시켜 평가했습니다. 각 에이전트는 특정 언어에 따라 성능이 상이하며, Python에서 상대적으로 높은 성능을 보였습니다.

#### 한계점 및 결론
- **평가의 한계**: 현재 벤치마크는 일부 코드 품질 측면을 충분히 반영하지 못하며, 향후 연구에서 보다 포괄적인 평가가 필요합니다.

### 2. 전반적인 요약

SWE-PolyBench은 코드 생성 에이전트를 다양한 환경에서 평가할 수 있는 새로운 벤치마크로, Java, JavaScript, TypeScript, Python 등의 다양한 프로그래밍 언어를 포함합니다. 이 벤치마크는 실험을 통해 코드 에이전트의 강점과 약점을 파악하고, 특히 복잡한 문제를 처리하는 데 있어서의 한계를 극복하기 위한 방향을 제시합니다. 에이전트는 언어 구조적 복잡성에 따른 성능 변화를 보였으며, 특히 Python에서 가장 높은 성과를 나타냈습니다. 이를 통해 보다 나은 AI 코딩 도우미의 개발과 적용 가능성을 제고하려는 노력을 반영합니다.