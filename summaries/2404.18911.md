# Kangaroo: Lossless Self-Speculative Decoding via Double Early Exiting
## TL;DR
## Summary
- [https://arxiv.org/pdf/2404.18911.pdf](https://arxiv.org/pdf/2404.18911.pdf)

이 논문은 "Kangaroo"라는 새로운 자기 추론적 디코딩 프레임워크를 제안합니다. 이 프레임워크는 큰 언어 모델들의 추론 속도를 높이기 위해, 이중 조기 종료 메커니즘을 사용하는 자체 초안 작성 모델을 기반으로 합니다. 주요 내용은 다음과 같습니다:

1. **서론 및 배경**:
   - 큰 언어 모델들의 병목 현상은 주로 메모리 대역폭에 의해 제한되며, 이는 자동 회귀 디코딩의 주요 지연 시간을 초래합니다.
   - 기존의 적대적 디코딩 방법들은 빠른 추론을 목표로 하지만, 다양한 문제점을 지니고 있습니다.

2. **Kangaroo 프레임워크**:
   - 이 프레임워크는 고정된 얕은 부분 네트워크를 사용하여 자체 초안 작성 모델을 구현하고, 나머지 레이어는 큰 타겟 모델로 사용됩니다.
   - 추가적인 조기 종료 메커니즘을 도입하여, 더 어려운 토큰에 대한 불필요한 계산 비용을 방지합니다.

3. **주요 기여**:
   - 간단하면서도 효율적인 어댑터 네트워크를 훈련하여 자체 초안 모델과 큰 타겟 모델 간의 격차를 해소합니다.
   - Kangaroo는 Spec-Bench에서 Medusa-1보다 88.7% 적은 추가 파라미터를 사용하면서도, 1.7배의 속도 향상을 달성합니다.

4. **실험 및 결과**:
   - 여러 자체 초안 추론 방법과의 비교를 통해 Kangaroo의 효과를 검증하고, 각각의 핵심 구성 요소를 식별하기 위한 소거 연구(ablation studies)를 수행합니다.

5. **결론**:
   - Kangaroo는 큰 언어 모델의 추론을 가속화하는 새로운 접근 방식을 제공하며, 특히 어려운 토큰에 대한 추론 시간을 줄이는 데 중점을 둡니다.

이 논문은 큰 언어 모델의 추론 속도를 향상시키기 위한 새로운 방법론을 제안하며, 이는 실제 적용에서 효율적인 디코딩 속도 향상을 가능하게 할 것입니다.

## Similar Papers
- [TriForce: Lossless Acceleration of Long Sequence Generation with Hierarchical Speculative Decoding](2404.11912.md)
- [Beyond the Speculative Game: A Survey of Speculative Execution in Large Language Models](2404.14897.md)
- [Medusa: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads](2401.10774.md)
- [SLAB: Efficient Transformers with Simplified Linear Attention and Progressive Re-parameterized Batch Normalization](2405.11582.md)
- [LazyLLM: Dynamic Token Pruning for Efficient Long Context LLM Inference](2407.14057.md)
- [Draft & Verify: Lossless Large Language Model Acceleration via Self-Speculative Decoding](2309.08168.md)
- [LLM in a flash: Efficient Large Language Model Inference with Limited Memory](2312.11514.md)
- [DiJiang: Efficient Large Language Models through Compact Kernelization](2403.19928.md)
- [MemServe: Context Caching for Disaggregated LLM Serving with Elastic Memory Pool](2406.17565.md)
