# Aya 23: Open Weight Releases to Further Multilingual Progress
## TL;DR
## Summary
- [https://arxiv.org/pdf/2405.15032.pdf](https://arxiv.org/pdf/2405.15032.pdf)

### 종합 요약

이 논문은 Aya 23이라고 불리는 다국어 지침을 조정한 언어 모델 가족을 소개합니다. Aya 23 모델은 23개 언어를 지원하면서 기존의 광범위한 다국어 모델 Aya 101보다 적은 언어에 더 많은 용량을 할당하여 성능을 높였습니다. 이 모델은 8B와 35B 두 가지 크기로 제공되며, 둘 다 평가된 모든 언어에서 탁월한 성능을 나타냈습니다. 주요 혁신은 효율적인 훈련 설정과 새로운 평가 방법입니다. 전반적으로, 이 모델은 연구자 및 실무자들이 다국어 모델과 응용 프로그램을 발전시키는 데 기여할 것입니다.

### 섹션별 요약

#### 1. 도입부
이 논문은 Aya 23이라고 하는 다국어 지침을 조정한 언어 모델을 소개합니다. Aya 23은 Cohere의 Command 모델을 기반으로 하며, 23개 언어를 지원하고 있습니다. 이 모델의 주된 목표는 영어에 치중된 기존 LLM의 한계를 극복하고, 다국어 지원을 확장하는 것입니다.

#### 2. 사전 훈련 모델
Aya 23 모델은 두 가지 크기(8B, 35B)로 제공됩니다. 이 모델은 병렬 블록 아키텍처, SwiGLU 활성화 기능, 편향 없는 밀도 레이어, 회전 위치 임베딩 등 최신 기술을 활용하여 성능을 극대화했습니다. 토크나이저는 256k 크기로, 효율적인 데이터 표현을 보장합니다.

#### 3. 지침 조정
Aya 23 모델의 지침 조정은 명령 기반 데이터와 함께 특별한 토큰 형식을 사용하여 이루어졌습니다. 이 형식은 명령 튜닝 및 추론 시 일관된 결과를 제공합니다.

#### 4. 다국어 평가
Aya 23 모델은 여러 다국어 평가 기준을 통해 성능을 측정했습니다. 평가 항목은 새로운 판별 작업, 일반 목적 언어 이해, 다국어 수학적 추론, 생성 작업 등으로 나뉩니다. Aya 23은 타 모델들과 비교했을 때 대부분의 평가 항목에서 우위를 차지했습니다.

### 주요 기여와 혁신 부분
1. **다국어 성능 개선**: Aya 23은 기존의 광범위한 다국어 모델보다 적은 언어에 더 많은 용량을 할당하여 성능을 극대화했습니다. 다국어 수학적 추론 성능은 Aya 101에 비해 6.6배 증가하였습니다.
2. **효율적인 훈련 아키텍처**: 병렬 블록 아키텍처와 SwiGLU 활성화 기능 등을 사용하여 훈련 효율성을 높이고, 훈련 안정성을 개선하였습니다.
3. **포괄적 평가 체계**: 여러 평가 기준을 통해 모델 성능을 포괄적으로 측정하였으며, 이로 인해 Aya 23은 다양한 언어에서 우수한 성능을 발휘할 수 있음을 입증했습니다.

### 전체 요약
이 논문은 Aya 23이라는 새로운 다국어 언어 모델을 통해 LLM의 다국어 지원을 크게 향상시켰습니다. Aya 23은 23개 언어에 더 많은 용량을 투자하여, 기존의 광범위한 다국어 모델보다 뛰어난 성능을 보여주었습니다. 또한, 효율적인 훈련 아키텍처와 포괄적인 평가 체계를 통해 다양한 언어에서의 실제 응용 가능성을 높였습니다. 이를 통해 다국어 모델과 응용 프로그램 발전에 크게 기여할 것입니다.