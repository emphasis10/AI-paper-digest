# Look Every Frame All at Once: Video-Ma$^2$mba for Efficient Long-form Video Understanding with Multi-Axis Gradient Checkpointing
## TL;DR
## Summary
- [https://arxiv.org/pdf/2411.19460.pdf](https://arxiv.org/pdf/2411.19460.pdf)

### 1. 주요 섹션 요약 및 논문의 핵심 기여

#### 소개
논문은 기존의 Transformer 기반 모델이 긴 비디오 시퀀스를 처리할 때 발생하는 메모리 및 계산 요구 증가의 문제를 해결하고자 한다. 이를 위해 Mamba-2 구조를 사용하여 Transformer의 주의 메커니즘을 대체하고, 시간과 메모리 요구사항을 선형적으로 개선하여 장시간 비디오 콘텐츠를 처리할 수 있는 Video-Ma2mba를 소개한다.

#### 관련 연구
비디오 이해를 위한 기존의 방법들은 주로 짧은 시퀀스를 일반화하거나 구조화된 상태 공간 모델(SSM)을 활용하는 것 등이 포함된다. 이러한 방법들은 긴 비디오에 대한 메모리 효율성을 향상시키는데 기여하지만, 이 논문에서는 이러한 한계를 넘어서고자 한다.

#### 모델 구조 및 방법론
Video-Ma2mba는 Transformer의 복잡성을 줄이기 위해, attention 메커니즘을 Mamba-2 아키텍처로 대체하고, Multi-axis Gradient Checkpointing(MA-GC)을 도입한다. 이를 통해 메모리 사용을 줄이고 시간 복잡도를 선형으로 만든다.

#### 실험적 검증
제안된 Video-Ma2mba는 1초 간격으로 전체 비디오 시퀀스를 처리하여 기존의 샘플링 방식보다 더 효과적으로 긴 시퀀스를 이해할 수 있다. 실험은 Video-Ma2mba가 단일 GPU에서 1 FPS에서 2시간 이상의 연속 비디오 시퀀스를 처리할 수 있음을 보인다.

#### 논의 및 결론
Video-Ma2mba는 기존 더 큰 모델의 성능을 능가하거나 맞먹는 성과를 달성하여 비디오 시퀀스 모델링의 경계를 확장한다. 이 논문은 메모리 효율성을 달성함과 동시에 0.8M 컨텍스트 길이까지의 비디오 시퀀스를 처리할 수 있는 메커니즘을 제안한다.

### 2. 전체 요약
Video-Ma2mba는 긴 시퀀스 비디오 처리를 위해 기존의 Transformer 기반 LMM의 문제점을 해결하고자 한다. 논문은 매우 긴 비디오 시퀀스를 효과적으로 처리하기 위해 Mamba-2 아키텍처와 Multi-axis Gradient Checkpointing을 결합하여 시간과 메모리 요구사항을 줄여준다. 실험 결과는 이 모델이 장시간 비디오 이해에 있어서 메모리 효율성을 개선하는 것을 입증한다. 이러한 접근은 장기적으로 AI 분야에서 높은 잠재력을 지니고 있으며, 비디오 데이터의 복잡성과 규모가 증가함에 따라 더욱 중요해질 것으로 예상된다.