# Outcome-Refining Process Supervision for Code Generation
## TL;DR
## Summary
- [https://arxiv.org/pdf/2412.15118.pdf](https://arxiv.org/pdf/2412.15118.pdf)

1. 각 섹션의 요약:

- **소개 (Introduction)**: 대규모 언어 모델(LLMs)은 코드 생성에서 뛰어난 능력을 보여주지만, 복잡한 알고리즘적 사고가 필요한 작업에서는 여전히 어려움을 겪습니다. 이를 해결하기 위해, 우리는 결과 자체의 개선 과정을 감독하는 새로운 패러다임인 결과 개선 프로세스 감독(Outcome-Refining Process Supervision, ORPS)을 제안합니다.

- **초록 (Abstract)**: ORPS는 구체적인 실행 신호를 활용하여 모델의 판단을 지지하고, 다수의 솔루션 경로를 동시에 유지하는 트리 구조 탐색을 사용합니다. 이를 통해 다양한 솔루션 전략을 탐구하고 개선할 수 있습니다.

- **방법론 (Methodology)**: ORPS는 결과의 개선을 감독할 과정으로 다룹니다. 트리 구조를 통한 빔 서치는 각 상태가 이론적 이해와 실제 구현을 동시에 포괄하도록 하여 보다 깊은 사고 탐구를 가능하게 합니다. 이로 인해 다양한 전략을 탐구할 수 있습니다.

- **실험 (Experiments)**: ORPS는 다양한 벤치마크에서 Pass@1 정확도가 평균 26.9% 증가하고 런타임이 42.2% 감소하는 등 기존 방법을 능가하는 성능을 보였습니다. 이는 충분한 논리적 공간과 구체적인 피드백 신호가 복잡한 프로그래밍 문제를 해결하는데 중요함을 나타냅니다.

- **결론 (Conclusion)**: 본 연구는 구조화된 추론 공간과 구체적인 실행 피드백이 복잡한 프로그래밍 작업 해결에 필수적임을 보여줍니다. ORPS는 더 작은 모델에서도 효과적임을 증명하며, 비싼 주석 데이터 없이도 신뢰할 수 있는 검증을 제공합니다.

2. 전체 요약:

이 논문은 인공지능과 기계 학습에서 코드 생성을 개선하기 위한 새로운 접근법으로 결과 개선 프로세스 감독(ORPS)을 제안합니다. 전통적인 방법이 단순히 최종 결과에 초점을 맞추는 것과 달리, ORPS는 과정 전반의 사고를 감독의 대상으로 삼아, 다양한 솔루션 경로와 알고리즘적 개선을 탐구합니다. 이를 통해 더 작은 모델이 높은 정확도와 성능을 달성할 수 있게 하며, 기존의 비싼 주석 데이터 없이도 신뢰성 있는 검증을 가능하게 합니다.