# MLKV: Multi-Layer Key-Value Heads for Memory Efficient Transformer Decoding
## TL;DR
## Summary
- [https://arxiv.org/pdf/2406.09297.pdf](https://arxiv.org/pdf/2406.09297.pdf)

### 섹션별 요약

#### 1. 소개
- **요약:** 이 논문에서는 트랜스포머 모델의 메모리 사용량을 줄이기 위한 새로운 방법으로 다중 레이어 키-값(Multi-Layer Key-Value, MLKV) 공유 방식을 소개합니다. 이는 현재 널리 사용되는 다중 쿼리 주의(Multi-Query Attention, MQA) 및 그룹 쿼리 주의(Grouped-Query Attention, GQA)의 한계를 극복하고 메모리 효율성을 크게 개선할 수 있게 합니다.

#### 2. 배경
- **요약:** MQA와 GQA는 각각 키-값 헤드를 공유하여 메모리 사용량을 줄이는 방식입니다. 그러나 이 방식들은 하나의 레이어 내에서만 키-값 헤드를 공유하기 때문에 여전히 메모리 사용량의 한계가 있습니다. 이를 극복하기 위해 여러 레이어 간 키-값 헤드를 공유하는 MLKV가 제안되었습니다.

#### 3. 다중 레이어 키-값 공유(MLKV)
- **요약:** MLKV는 여러 레이어 간 키-값 헤드를 공유하여 메모리 사용을 최적화합니다. 이는 기존의 MQA나 GQA보다 작은 메모리를 사용하면서도 성능 저하를 최소화합니다. MLKV는 특히 큰 스케일의 트랜스포머 모델에서도 적용 가능성을 보여줍니다.

#### 4. 실험
- **요약:** 실험에서는 Pythia-160M 모델을 기준으로 MLKV의 메모리 절감 효과와 성능을 평가했습니다. 총 9개의 모델 변형을 사용하여 벤치마크 테스트와 메모리 사용량, 추론 속도 등을 측정하였습니다. 이때 사용된 데이터는 'The Pile' 데이터셋의 5%입니다.

#### 5. 결과
- **요약:** MLKV는 MQA와 GQA 대비 메모리 사용량을 크게 줄일 수 있으며, 특정 설정에서는 아주 미미한 성능 저하만을 보입니다. 특히 MLKV-6가 메모리와 성능의 최적의 균형점으로 평가되었습니다.

#### 6. 논의
- **요약:** MLKV는 메모리와 성능 사이에서 유연한 선택지를 제공합니다. 이 논문에서는 메모리 제약이 크지 않은 상황에서는 여전히 GQA/MQA가 더 효과적임을 밝혔다. 그러나 메모리 사용량이 더 중요한 경우 MLKV가 유용할 수 있습니다.

#### 7. 관련 연구
- **요약:** MLKV와 관련된 다양한 연구들은 자원 최적화와 성능 제고를 목적으로 다양한 KV 캐시 최적화 방법들을 다루고 있습니다. 이 논문에서는 특히 최근 제안된 Cross-Layer Attention(CLA)과의 차별점을 설명했습니다.

#### 8. 결론
- **요약:** MLKV는 트랜스포머 모델의 메모리 사용을 현저히 줄이면서도 성능 저하를 최소화할 수 있는 방법을 제시합니다. 이는 대규모 모델에서의 실용적인 적용 가능성을 보여주며, 특히 메모리 제약이 큰 환경에서 유리합니다.

기여 및 혁신:
- **주요 기여:** MLKV는 기존의 키-값 캐시 최적화 방식인 MQA 및 GQA의 한계를 극복하는 새로운 접근법을 제안합니다.
- **혁신성:** 여러 레이어 간 키-값 헤드를 공유함으로써 메모리 사용을 크게 줄이면서도 성능 저하를 방지하는 점에서 혁신적입니다.

### 전체 요약
이 논문은 트랜스포머 모델의 키-값 캐시 사용을 최적화하기 위해 다중 레이어 키-값(Multi-Layer Key-Value, MLKV) 공유 방식을 제안합니다. MLKV는 기존의 다중 쿼리 주의(Multi-Query Attention, MQA) 및 그룹 쿼리 주의(Grouped-Query Attention, GQA)의 한계를 극복하고, 메모리 사용을 현저히 줄이면서도 성능 저하를 최소화할 수 있습니다. 또한, 다양한 벤치마크 테스트를 통해 MLKV의 효능을 입증하며, 특히 메모리 제약이 큰 환경에서 더욱 유용하게 적용될 수 있음을 보여줍니다. 

이 논문은 AI 모델의 메모리 사용량을 줄이는 실질적인 방안을 제시하며, 이를 통해 대규모 트랜스포머 모델의 효율적인 운영을 가능하게 합니다.