# PrefixQuant: Static Quantization Beats Dynamic through Prefixed Outliers in LLMs
## TL;DR
## Summary
- [https://arxiv.org/pdf/2410.05265.pdf](https://arxiv.org/pdf/2410.05265.pdf)

### 논문의 주요 내용 요약 (한국어)

#### 1. 서론
이 논문은 대용량 언어 모델(LLM)의 메모리 효율성과 추론 속도를 향상시키기 위한 '양자화'라는 기술을 소개합니다. 특히, 활성화 양자화를 기존의 채널 중심에서 토큰 중심으로 이동하여 문제가 되는 행동이나 오작동을 방지하려는 목표를 가지고 있습니다. 이를 위해서 새로운 기술인 PrefixQuant를 제안하고 있으며, 이는 추가적인 재훈련 없이 오프라인 단계에서 문제 원인을 격리합니다.

#### 2. 기존 연구와 다른 점
기존의 많은 방식들은 동적 양자화에 의존해 토큰 수준에서 적응하지만, PrefixQuant는 정적 양자화를 통해 높은 성능을 유지하면서도 비용을 줄이는 방식입니다. 이는 특히 Llama-3-8B 모델의 경우 기존 방법보다 접근성과 성능이 높고, 추론 속도가 FP16을 사용하는 모델보다 1.60배에서 2.81배까지 빠릅니다.

#### 3. 제안하는 기술 (PrefixQuant)
PrefixQuant의 주요 혁신은 KV 캐시 안에 있는 이상값을 사전에 설정하면서 발생합니다. 이렇게 함으로써 학습 안정성을 높이고, 문제 토큰에서 발생하는 정보 손실을 줄여줍니다. 특히, 이 방법은 다른 최적화 기반 양자화 방법의 성능을 쉽게 향상시킬 수 있는 플러그-앤-플레이 모듈로서의 가능성을 가지고 있습니다.

#### 4. 실험 결과
논문은 다양한 모델에 걸쳐 PrefixQuant의 성능을 검증한 결과, Fine-tuning을 거치지 않은 상태에서 기존의 동적 양자화 방법들과 비교할 때 동등하거나 더 나은 성능을 보였습니다. 이후 Fine-tuning을 진행하면 성능이 더욱 강화되어, W4A4KV4로 서버되는 Llama-3-8B 모델에서 일반 지식 추론 작업의 평균 정확도가 기존 방법보다 월등히 높았습니다.

#### 5. 결론
PrefixQuant는 토큰별 이상값을 효과적으로 처리해 정적 양자화를 가능하게 하고, 동적 양자화를 능가하는 성과를 보여주며, 모델 학습을 안정화해 다양한 최적화 기반 방법들의 성능을 보강합니다. 이러한 간단하고 적용 가능한 접근법이 LLM 압축 및 최적화 연구의 유망한 방향이 될 수 있음을 보여줍니다.

### 전체 요약
이 논문은 대용량 언어 모델의 효율성 문제를 해결하기 위해 새로운 기술인 PrefixQuant를 제안합니다. 이는 기존의 복잡하고 비용이 많이 드는 동적 양자화를 대신해 정적 양자화를 가능하게 하며, 학습 안정성을 높이는 한편, 추론 속도와 모델의 정확성을 크게 향상시킵니다. 이를 통해 다양한 인공지능 연구와 응용에 도움이 될 수 있는 중요한 방법론적 기여를 합니다.