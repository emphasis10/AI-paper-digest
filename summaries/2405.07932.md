# PARDEN, Can You Repeat That? Defending against Jailbreaks via Repetition
## TL;DR
## Summary
- [https://arxiv.org/pdf/2405.07932.pdf](https://arxiv.org/pdf/2405.07932.pdf)

### 논문 요약

#### 1. 초록
대형 언어 모델(LLMs)은 자연어 처리 작업에서 성공을 거두었지만, 엄격한 안전 조정 과정에도 불구하고 Llama 2와 Claude 2 같은 안전 조정된 LLM들도 여전히 탈옥(jailbreak) 공격에 취약합니다. 이 논문에서는 LLM의 출력을 반복하도록 요청하는 방법인 PARDEN을 제안합니다. 이 방법은 모델의 미세 조정이나 화이트 박스 접근을 필요로 하지 않으며, Llama-2와 Claude-2에 대한 기존 탈옥 탐지 방법보다 훨씬 뛰어납니다.

#### 2. 서론
최근 대형 언어 모델의 급속한 발전으로 인해 챗봇, 검색 엔진 등 다양한 응용 프로그램에서 큰 변혁을 경험하고 있습니다. 그러나 이러한 모델들은 여전히 악의적인 입력을 통해 생성되는 유해한 출력을 막기 위해 추가적인 방어 방법이 필요합니다. PARDEN은 모델 자체를 이용해 탈옥을 방지하는 방법으로, 모델이 개선됨에 따라 방어력도 함께 향상될 수 있습니다.

#### 3. 관련 연구
많은 연구들이 LLM의 탈옥 프롬프트를 수동으로 찾거나 자동으로 탐색하는 방법을 제안했습니다. PARDEN은 이러한 기존 방법들과 비교하여 반복 출력을 통한 방어 메커니즘을 제공합니다. 이는 탈옥 프롬프트를 탐지하고 거부 응답을 유도함으로써 유해한 출력을 방지합니다.

#### 4. 배경
LLM은 일반적인 인터넷 텍스트 코퍼스를 사용하여 사전 학습을 거친 후, 조정 및 안전을 위해 미세 조정됩니다. 이러한 조정 과정은 LLM이 악의적인 프롬프트에 대해 거부 응답을 생성하도록 학습시킵니다.

#### 5. PARDEN 설계 목표
PARDEN은 LLM의 출력을 반복하도록 하여 유해한 출력을 탐지합니다. 이는 원래의 출력과 반복된 출력 간의 BLEU 점수를 비교하여 이루어집니다. 높은 BLEU 점수는 반복된 출력이 원래 출력과 거의 동일함을 나타내며, 낮은 BLEU 점수는 유해한 출력을 거부하는 것을 나타냅니다.

#### 6. 실험 설정
PARDEN의 성능을 평가하기 위해 다양한 벤치마크 데이터셋을 사용했습니다. 이 데이터셋에는 성공적인 공격, 실패한 공격 및 무해한 프롬프트가 포함되어 있습니다. PARDEN은 기존 방법들에 비해 훨씬 높은 AUC 점수와 낮은 거짓 긍정률(FPR)을 달성했습니다.

#### 7. 결과
PARDEN은 기존 방법보다 훨씬 높은 성능을 보여주었으며, 특히 높은 진양성률(TPR)과 낮은 거짓 긍정률(FPR)에서 탁월한 성과를 보였습니다. 예를 들어, Llama2-7B에서 TPR이 90%일 때 FPR을 약 11배 감소시켰습니다.

#### 8. 결론 및 미래 작업
PARDEN은 기존 방법들에 비해 현저한 개선을 이루었지만, 안전하게 조정된 기본 모델에 의존합니다. 향후 연구는 안전한 기본 모델의 개발과 PARDEN을 통한 추가적인 미세 조정을 강조합니다.

### 전체 요약
이 논문에서는 대형 언어 모델의 탈옥 공격에 대한 방어 메커니즘으로 PARDEN을 제안합니다. PARDEN은 모델의 출력을 반복하도록 하여 유해한 출력을 탐지하고 거부하도록 합니다. 실험 결과, PARDEN은 기존 방법들보다 훨씬 높은 성능을 보였으며, 특히 높은 진양성률과 낮은 거짓 긍정률에서 탁월한 성과를 보였습니다. 이 연구는 안전하게 조정된 기본 모델의 중요성을 강조하며, 향후 PARDEN을 통해 추가적인 안전성을 확보할 수 있는 가능성을 제시합니다.

## Similar Papers
- [Does Refusal Training in LLMs Generalize to the Past Tense?](2407.11969.md)
- [WildGuard: Open One-Stop Moderation Tools for Safety Risks, Jailbreaks, and Refusals of LLMs](2406.18495.md)
- [WalledEval: A Comprehensive Safety Evaluation Toolkit for Large Language Models](2408.03837.md)
- [LLoCO: Learning Long Contexts Offline](2404.07979.md)
- [SpeechGuard: Exploring the Adversarial Robustness of Multimodal Large Language Models](2405.08317.md)
- [The Instruction Hierarchy: Training LLMs to Prioritize Privileged Instructions](2404.13208.md)
- [A False Sense of Safety: Unsafe Information Leakage in 'Safe' AI Responses](2407.02551.md)
- [What matters when building vision-language models?](2405.02246.md)
- [Mission Impossible: A Statistical Perspective on Jailbreaking LLMs](2408.01420.md)
