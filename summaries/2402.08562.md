# Higher Layers Need More LoRA Experts
## TL;DR
## Summary
- [https://arxiv.org/pdf/2402.08562.pdf](https://arxiv.org/pdf/2402.08562.pdf)

### 주요 섹션 요약

1. **서론**:
   - 이 논문은 대형 언어 모델(LLM)의 매개변수 효율적 튜닝(PEFT) 방법을 다루고 있으며, 이는 거대한 계산 자원 필요성을 줄이기 위한 것이다. MoE(전문가 혼합방식) 구조와 LoRA(저순위 적응화) 방법을 결합하여 성능을 향상시키는 전반적인 개념을 소개.

2. **방법론**:
   - MoE-LoRA 구조를 통해 각 레이어에 LoRA 전문가 수를 다양하게 배치하는 MoLA 방법을 제안한다. 이는 저층에서는 전문가 수를 줄이고, 고층에서는 전문가 수를 늘려 모델의 성능을 최적화한다.

3. **실험 및 결과**:
   - 총 여섯 개의 벤치마크에서 실험을 통해 MoLA가 다른 PEFT 기법에 비해 효과적임을 보인다. 특히 MoLA-▽가 최상의 성능을 보여준다. 고층에 더 많은 전문가를 할당하면 모델의 성능 향상에 도움이 된다.

4. **논의**:
   - MoLA는 저층의 전문가 중복을 줄이고 고층의 정보 처리 능력을 향상시킨다. 결과적으로 다양한 태스크에서도 응용할 수 있는 플러그 앤 플레이 방식으로 사용될 수 있다.

5. **결론**:
   - MoLA는 PEFT 접근법의 성능 한계를 넘어서는데 기여하며, 향후 다양한 태스크에 적용될 가능성이 있다. 이 기법은 에너지 절약 및 AI 탈탄소화 촉진에 기여할 수 있다.

### 전체 요약

이 논문은 MoLA라는 새로운 매개변수 효율적 튜닝 방법을 소개하여 평균적인 정보 처리에서 전문가 중복을 최소화하고, 모델 성능을 극대화하는 혁신적인 접근법을 제시하고 있다. MoLA는 저층에 적은 수의 전문가를 할당하고, 고층에 많은 수의 전문가를 할당하여, 고도의 추상적 정보 처리를 가능하게 한다. 결과적으로 MoLA는 다양한 자연어 처리 및 커먼센스 QA 태스크에서 뛰어난 성능을 입증했다. MoLA는 낮은 비용으로 넓은 학습 범위와 뛰어난 효과를 제공하는 플러그 앤 플레이 방식의 접근법이 되어, 미래의 AI 발전에 기여할 수 있음을 보여준다.