# ShiftAddLLM: Accelerating Pretrained LLMs via Post-Training Multiplication-Less Reparameterization
## TL;DR
## Summary
- [https://arxiv.org/pdf/2406.05981.pdf](https://arxiv.org/pdf/2406.05981.pdf)

## 주요 내용 요약

1. **소개**
    예교육된 대형 언어 모델(LLMs)은 언어 이해와 생성 작업에서 뛰어난 성능을 보여주지만, 엄청난 하드웨어 요구 사항과 높은 지연 시간, 메모리 및 에너지 소비 문제 때문에 리소스가 제한된 장치에서의 배포가 어려움. 이에 대한 해결책으로 'ShiftAddLLM'을 제안. 이는 곱셈이 없는 모델로, 하드웨어 친화적인 시프트 및 추가 연산을 사용함으로써 메모리와 에너지를 절약.

2. **관련 연구**
    LLM의 효율성을 높이기 위한 기존 기술은 프루닝, 양자화, 주의 최적화 등이 있었으나 여전히 비용이 많이 든다는 한계가 있음. 'ShiftAddLLM'은 곱셈을 시프트와 추가 연산으로 대체하여 효율성을 높이는 첫 시도.

3. **프로포즈된 방법 (ShiftAddLLM 프레임워크)**
    예교육된 LLM의 가중치를 이진 행렬 및 그룹별 스케일링 팩터로 양자화하고, 곱셈을 시프트와 추가 연산으로 재매개화함. 이를 통해 메모리와 에너지를 절약하고, 여러 가지 최적화 방법을 통해 정확도 손실을 최소화함.

4. **결과**
    실험을 통해 ShiftAddLLM은 다섯 가지 LLM 계열과 여덟 가지 작업에서 일관되게 우수한 정확도와 효율성 성능을 보임. 기존 양자화된 LLM과 비교해 메모리 및 에너지 소비를 80% 이상 절감.

5. **결론**
    ShiftAddLLM은 예교육된 LLM의 효율성을 크게 향상시키는 새로운 접근법이자, 곱셈 연산을 최소화하는 포스트 트레이닝 최적화를 통해 보다 효율적인 LLM 제공 시스템을 가능케 함.

## 논문의 주요 기여와 혁신 부분 요약

ShiftAddLLM의 주요 혁신은 예교육된 LLM의 곱셈 연산을 시프트와 추가 연산으로 대체하여 하드웨어 비용을 낮추는 것입니다. 이는 세 가지 주요 기여를 포함합니다:

1. **곱셈 없는 모델 제안**: 시프트와 추가 연산으로 곱셈을 대체하여 메모리와 에너지 소비를 대폭 줄임.
2. **정확도 손실 최소화**: 가중치와 출력 활성화 오류를 최소화하기 위한 복합 최적화 방법을 통해 정확도를 높임.
3. **비트 할당 전략 개발**: 각 층의 민감도에 따라 최적의 비트 수를 자동으로 할당하여 메모리 사용과 지연 시간을 더욱 줄임.

## 전반적인 요약

이 논문은 기존의 예교육된 대형 언어 모델이 갖고 있는 하드웨어 요구 사항 문제를 해결하는 새로운 방법을 제안합니다. 제안된 'ShiftAddLLM'은 곱셈을 시프트와 추가 연산으로 대체하여 메모리와 에너지 효율성을 크게 높이고, 정확도 손실을 최소화하기 위한 복합 최적화 방법과 자동화된 비트 할당 전략을 제시합니다. 실험 결과, ShiftAddLLM은 다섯 가지 LLM 계열과 여덟 가지 작업에서 일관된 성능 향상을 보여주었으며, 메모리와 에너지 소비를 80% 이상 절감했습니다. 이 방법은 보다 효율적인 LLM 제공 시스템을 가능케 하며, LLM의 배포와 운영 비용을 크게 줄일 수 있는 중요한 기여입니다.

## Similar Papers
- [ZeroQuant-FP: A Leap Forward in LLMs Post-Training W4A8 Quantization Using Floating-Point Formats](2307.09782.md)
- [LLM-FP4: 4-Bit Floating-Point Quantized Transformers](2310.16836.md)
- [OneBit: Towards Extremely Low-bit Large Language Models](2402.11295.md)
- [BiLLM: Pushing the Limit of Post-Training Quantization for LLMs](2402.04291.md)
- [OmniQuant: Omnidirectionally Calibrated Quantization for Large Language Models](2308.13137.md)
- [AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration](2306.00978.md)
- [Grass: Compute Efficient Low-Memory LLM Training with Structured Sparse Gradients](2406.17660.md)
- [T-MAC: CPU Renaissance via Table Lookup for Low-Bit LLM Deployment on Edge](2407.00088.md)
- [Extreme Compression of Large Language Models via Additive Quantization](2401.06118.md)
