# LASP-2: Rethinking Sequence Parallelism for Linear Attention and Its Hybrid
## TL;DR
## Summary
- [https://arxiv.org/pdf/2502.07563.pdf](https://arxiv.org/pdf/2502.07563.pdf)

### 1. 논문 각 섹션 요약

#### 서론
이 논문은 선형 시퀀스 모델링을 위한 새롭게 설계된 LASP-2 방법론을 제안합니다. 기존의 SP(시퀀스 패러렐리즘) 방법들이 통신과 계산의 병렬성에서 비효율성을 보이는 문제를 지적하며, 이를 해결하기 위해 최적화된 실행 메커니즘과 단일 AllGather 통신을 통한 개선을 설명합니다.

#### 방법론
LASP-2는 선형 주의층에서의 시퀀스 패러렐리즘을 재설계하여, 통신과 계산 과정 전반에 걸쳐 효율성을 극대화합니다. 특히 시퀀스 길이에 독립적인 상태의 중간 메모리로 AllGather 연산을 통해 통신 부담을 줄이고, 통신과 계산의 병렬성 및 겹침을 개선합니다. 이 외에도 LASP-2H라는 확장판을 도입하여, 선형과 표준 주의층을 혼합한 하이브리드 모델을 위한 효율적인 SP 솔루션을 제안합니다.

#### 실험
실험 결과는 LASP-2가 기존 방법들에 비해 특히 긴 시퀀스 길이에서 상당한 속도 개선을 보인다는 것을 시사합니다. LASP-2는 LASP-1 및 Ring Attention 대비 2048K의 시퀀스 길이에서 각각 15.2%, 36.6%의 속도 향상을 가져왔습니다. 이러한 결과는 LASP-2의 효율성 및 유용성을 대규모 분산 시스템에서 뒷받침합니다.

#### 결론
LASP-2는 긴 시퀀스 길이에서의 통신과 계산 효율성을 크게 개선하여 선형 주의 모델의 확장성과 실용적 사용성을 강화합니다. 이 방법론은 향후 긴 시퀀스 모델들을 다루는 AI와 머신러닝 응용에 있어서 획기적인 기여를 할 것으로 예측됩니다.

### 2. 총괄 요약
이 논문은 선형 주의 메커니즘에서의 시퀀스 패러렐리즘을 최적화한 LASP-2를 제안하며, 이는 장기 시퀀스를 처리할 때 통신과 계산의 효율성을 획기적으로 개선합니다. LASP-2는 단일 AllGather 통신을 활용하여 메모리 사용량은 줄이고, 계산과 통신의 병렬성은 극대화하여 대규모 AI 모델 학습의 속도를 높입니다. 실험적으로 LASP-2는 기존 방법들에 비해 더 나은 성능을 보였으며, 이는 자연어 처리, 유전자 시퀀스 분석 및 시계열 예측 등 다양한 분야에 걸쳐 활용할 수 있습니다.