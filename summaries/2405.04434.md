# DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model
## TL;DR
## Summary
- [https://arxiv.org/pdf/2405.04434.pdf](https://arxiv.org/pdf/2405.04434.pdf)

### 주요 내용 요약

1. **서론 및 배경**:
   - DeepSeek-V2는 믹스처 오브 엑스퍼트(Mixture-of-Experts, MoE) 구조를 갖는 강력한 언어 모델로, 경제적인 트레이닝과 효율적인 추론이 특징입니다. 이 모델은 236B의 총 파라미터를 가지고 있으며, 토큰당 21B의 파라미터만 활성화시켜 사용합니다.

2. **모델 구조 및 기능**:
   - DeepSeek-V2는 멀티 헤드 라텐트 어텐션(MLA)과 DeepSeekMoE 아키텍처를 도입하여 추론 효율성을 향상시키고, 강력한 모델을 경제적인 비용으로 트레이닝할 수 있습니다. MLA는 주의력 메커니즘에서 키-값 캐시를 상당히 줄이며, DeepSeekMoE는 전문화된 전문가 세분화를 통해 더 정확한 지식 획득이 가능합니다.

3. **프리트레이닝 및 평가**:
   - DeepSeek-V2는 8.1T 토큰으로 구성된 고품질 및 다원 출처 코퍼스에서 사전 트레이닝되었습니다. 이 모델은 영어와 중국어 벤치마크를 포함한 다양한 벤치마크에서 평가되며, 상위 성능을 달성합니다. 특히 21B의 활성화된 파라미터로도 뛰어난 성능을 보입니다.

### 혁신적인 부분
DeepSeek-V2의 혁신적인 점은 MLA를 통해 추론 시 키-값 캐시를 대폭 줄이면서도 성능을 향상시킨 것입니다. 또한, DeepSeekMoE 아키텍처는 전문화된 전문가 세분화를 통해 경제적인 비용으로 강력한 모델을 훈련할 수 있는 능력을 제공합니다. 이러한 기능은 모델의 효율성과 경제성을 동시에 증대시키는 중요한 발전입니다.

이 연구는 언어 모델의 성능과 효율성을 개선하는 데 중요한 기여를 하며, 특히 대규모 언어 모델의 실용적 사용에 큰 영향을 미칠 것으로 기대됩니다.