# DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model
## TL;DR
## Summary
- [https://arxiv.org/pdf/2405.04434.pdf](https://arxiv.org/pdf/2405.04434.pdf)

### 1. 각 섹션 요약

#### 1. 서론 (Introduction)
- 최근 몇 년 동안 대형 언어 모델(LLMs)은 급격한 발전을 이루었으며, 이는 인공지능 일반화(AGI)의 새 시대를 예고하고 있습니다. LLM의 지능은 매개변수 수가 증가함에 따라 향상되지만, 이는 더 많은 컴퓨팅 자원을 요구하고 추론 처리량이 감소하는 문제를 발생시킵니다.
- 이러한 문제를 해결하기 위해 DeepSeek-V2를 소개합니다. 이 모델은 경제적인 훈련과 효율적인 추론을 가능하게 하는 혁신적인 Transformer 아키텍처를 갖춘 오픈 소스 Mixture-of-Experts(MoE) 언어 모델입니다.
- 이 모델은 236B의 총 파라미터를 가지고 있으며, 토큰당 21B의 파라미터만 활성화시켜 사용합니다.

#### 2. 아키텍처 (Architecture)
- DeepSeek-V2는 Transformer 아키텍처를 기반으로 하며, 주목할 만한 혁신 요소로는 Multi-head Latent Attention(MLA)와 DeepSeekMoE가 있습니다.
  - **MLA**: 표준 다중 헤드 어텐션(MHA)에서 발생하는 키-값(KV) 캐시 문제를 해결하기 위해 설계되었습니다. 이는 성능을 저하시키지 않으면서도 KV 캐시의 양을 크게 줄입니다.
  - **DeepSeekMoE**: 전문가 분할을 세분화하고 공유 전문가를 격리하여 경제적인 비용으로 강력한 모델을 훈련할 수 있습니다.

#### 3. 사전 훈련 (Pre-Training)
- DeepSeek-V2는 고품질의 다중 소스 사전 훈련 코퍼스(8.1T 토큰)를 사용하여 사전 훈련되었습니다.
- 기존 DeepSeek 67B와 비교했을 때, 더 많은 데이터(특히 중국어 데이터)를 포함하고 데이터 품질을 향상시켰습니다.
- Supervised Fine-Tuning(SFT)와 Reinforcement Learning(RL)을 통해 모델을 인간의 선호에 맞게 조정하였습니다.

#### 4. 정렬 (Alignment)
- **Supervised Fine-Tuning (SFT)**: 1.5M 개의 대화 세션을 수집하여 다양한 도메인(수학, 코드, 작문, 추론, 안전 등)에 대해 SFT를 수행하였습니다.
- **Reinforcement Learning (RL)**: 인간의 선호에 맞추어 모델을 조정하기 위해 RL을 적용하였습니다.

#### 5. 결론, 제한점 및 미래 작업 (Conclusion, Limitation, and Future Work)
- DeepSeek-V2는 128K 컨텍스트 길이를 지원하는 대형 MoE 언어 모델로, 경제적인 훈련과 효율적인 추론이 가능하며, 뛰어난 성능을 자랑합니다.
- 주요 제한점으로는 지속적인 지식 업데이트가 없다는 점과 중국어 및 영어 이외의 언어에 대한 제한된 성능이 있습니다.
- 미래 작업으로는 GPT-4와 동등한 성능을 목표로 모델을 확장하고, 다중 모달리티를 지원하는 기능을 개발할 계획입니다.

### 2. 전체 요약
DeepSeek-V2는 대형 언어 모델(LLMs)의 효율성을 높이기 위해 개발된 혁신적인 Mixture-of-Experts(MoE) 아키텍처를 채택한 모델입니다. 이 모델은 Multi-head Latent Attention(MLA)와 DeepSeekMoE를 통해 경제적인 훈련과 효율적인 추론을 지원합니다. 고품질의 데이터와 다양한 도메인을 포함한 1.5M 개의 대화 세션을 통해 Supervised Fine-Tuning(SFT)과 Reinforcement Learning(RL)을 수행하여 인간의 선호에 맞춘 모델을 제공합니다. DeepSeek-V2는 경제적인 비용으로 뛰어난 성능을 발휘하며, 향후 GPT-4와 동등한 성능을 목표로 모델을 확장하고 다중 모달리티 지원 기능을 개발할 계획입니다.

### 3. 논문의 주요 목적
DeepSeek-V2의 주요 목적은 대형 언어 모델의 경제적인 훈련과 효율적인 추론을 가능하게 하는 것입니다. 이를 위해, 기존 Transformer 아키텍처의 한계를 극복하고 성능을 향상시키기 위해 Multi-head Latent Attention(MLA)와 DeepSeekMoE라는 혁신적인 메커니즘을 도입하였습니다. MLA는 기존의 다중 헤드 어텐션에서 발생하는 키-값(KV) 캐시 문제를 해결하여 추론 효율성을 높이며, DeepSeekMoE는 전문가 분할을 세분화하고 공유 전문가를 격리하여 더 강력한 모델을 경제적인 비용으로 훈련할 수 있도록 합니다. 이를 통해 DeepSeek-V2는 뛰어난 성능을 발휘하면서도 경제적인 훈련과 효율적인 추론을 지원하는 모델로 자리매김하고자 합니다.