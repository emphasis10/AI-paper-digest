# Applying RLAIF for Code Generation with API-usage in Lightweight LLMs
## TL;DR
## Summary
- [https://arxiv.org/pdf/2406.20060.pdf](https://arxiv.org/pdf/2406.20060.pdf)

### 1. 각 섹션 요약 및 주요 공헌과 혁신 부분
#### 1. 서론 (Introduction)
이 논문의 서론에서는 대규모 언어 모델(LLM)이 자연어 이해와 생성에서 뛰어난 능력을 보여주었음을 강조합니다. 특히 사람의 피드백(RLHF)를 활용한 강화학습이 이 성공의 중요한 요소임을 설명합니다. 그러나, 사람의 피드백은 비용이 많이 들기 때문에 AI의 피드백을 활용하는 RLAIF 방법론이 등장하였습니다. 이 연구는 RLAIF를 사용하여 경량 모델(<1B 파라미터)의 코드 생성 능력을 향상시키는 방법을 제안합니다.

#### 2. 데이터셋 (Dataset)
이 연구에서는 Patil 등이 공개한 Gorilla 데이터셋을 사용했습니다. HuggingFace, TensorFlow, PyTorch 데이터셋으로 구성된 Gorilla 데이터셋 중 HuggingFace 부분만을 초점으로 했고, 다양한 응용 프로그램들에 대한 925개의 고유 API를 포함하고 있습니다.

#### 3. 방법론 (Methodology)
이 연구에서는 RLHF와 유사한 파이프라인을 제시하지만, 인간 평가자 대신 더 큰 LLM(GPT-3.5)을 사용하여 생성된 코드의 품질을 평가하는 새로운 방법을 사용합니다. 먼저 GPT-2-large 모델을 데이터셋에 맞게 학습시킨 후, GPT-3.5의 피드백을 사용하여 보상 모델을 학습시키고 마지막으로 PPO 알고리즘을 사용하여 모델을 세밀하게 조정합니다. 이 접근법은 비싼 인간 평가 비용을 제거하면서도 높은 품질의 코드를 생성합니다.

#### 4. 결과 및 논의 (Results and Discussions)
이 연구의 결과는 MRL 모델이 기존의 MGorilla 모델이나 단순히 학습된 MSFT 모델보다 코드 품질에서 우수함을 보여줍니다. 특히, 코드 실행 가능성, ROUGE, CodeBLEU, AST 등의 다양한 지표에서 뛰어난 성과를 보였습니다. MGorilla와 비교하여 9분의 1의 파라미터만 사용하면서도 더 높은 성능을 나타냅니다.

#### 5. 윤리 성명 (Ethics statement)
이 연구는 ACM 윤리 규칙과 연구 커뮤니티의 광범위한 윤리 지침을 준수합니다. 연구에 사용된 데이터셋은 공개된 저장소에서 수집하였으며, 안전하지 않은 내용을 생성하는 것을 예방하기 위해 주의해야 한다는 점을 강조합니다.

#### 6. 한계점 (Limitations)
이 연구는 사용된 모델의 편향성과 Python 코드로만 한정된 데이터셋의 다양성 부족이 주요 한계점으로 지적되었습니다. 또한, 오프라인 학습 방법론을 사용하여 최신 API를 반영하지 못하는 점도 한계점으로 제시되었습니다. 하지만 RLAIF는 자원이 적게 들어 정기적인 업데이트가 더 쉬워진다는 장점도 있습니다.

### 2. 전체 요약
이 논문은 AI 피드백을 활용한 강화학습(RLAIF) 기법을 통해 경량 언어 모델(LLM)의 코드 생성 능력을 향상시키는 새로운 프레임워크를 제안합니다. 주요 데이터셋은 Gorilla 데이터셋을 사용하였으며, GPT-2-large 모델을 기반으로 GPT-3.5의 피드백을 통해 보상 모델을 학습시키는 방법을 제시하였습니다. 실험 결과, 이 방법론을 통해 코드 품질의 다양한 지표에서 우수한 성과를 나타내었으며, 자원이 적게 들면서도 더 큰 모델보다 높은 성능을 발휘하였습니다. 이 연구는 비용 효율적인 AI 피드백을 통해 코드 생성의 새로운 가능성을 제시하며, 향후 다양한 프로그래밍 언어와 최신 API를 반영할 필요성을 언급하고 있습니다.

## Similar Papers
- [AutoCoder: Enhancing Code Large Language Model with \textsc{AIEV-Instruct}](2405.14906.md)
- [Self-Training with Direct Preference Optimization Improves Chain-of-Thought Reasoning](2407.18248.md)
- [Can Small Language Models Help Large Language Models Reason Better?: LM-Guided Chain-of-Thought](2404.03414.md)
- [DevEval: A Manually-Annotated Code Generation Benchmark Aligned with Real-World Code Repositories](2405.19856.md)
- [Direct Preference Optimization: Your Language Model is Secretly a Reward Model](2305.18290.md)
- [OpenBezoar: Small, Cost-Effective and Open Models Trained on Mixes of Instruction Data](2404.12195.md)
- [DogeRM: Equipping Reward Models with Domain Knowledge through Model Merging](2407.01470.md)
- [Constrained Decoding for Secure Code Generation](2405.00218.md)
- [Accurate Knowledge Distillation with n-best Reranking](2305.12057.md)
