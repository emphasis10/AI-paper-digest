# World Models with Hints of Large Language Models for Goal Achieving
## TL;DR
## Summary
- [https://arxiv.org/pdf/2406.07381.pdf](https://arxiv.org/pdf/2406.07381.pdf)

### 1. 논문의 각 섹션 요약

#### 초록(Abstract)
이 논문은 긴 시간과 희소한 보상을 가진 강화 학습 문제를 해결하기 위해 'Dreaming with Large Language Models (DLLM)'라는 새로운 다중 모달 모델 기반 강화 학습 접근 방식을 제안합니다. DLLM은 LLM(대형 언어 모델)에서 제공하는 힌트 서브골을 모델 롤아웃에 통합하여 효과적인 탐색을 유도합니다. 여러 실험에서 DLLM은 HomeGrid, Crafter, Minecraft 등 다양한 희소 보상 환경에서 최근 강력한 방법들보다 27.7%, 21.1%, 9.9% 더 우수한 성능을 보였습니다.

#### 배경 및 관련 연구(Background and Related Work)
모델 기반 RL(MBRL)은 환경과의 상호작용을 통해 세계 모델을 훈련하여, 보상과 다음 단계를 예측합니다. 최근의 MBRL 방법들은 고차원 관측값과 복잡한 동적 특성을 처리할 수 있습니다. 희소 보상 환경에서는 본질적으로 동기가 부여된 RL 방법이 복잡한 환경에서도 데이터를 수집하고 보상을 제공합니다. DLLM은 이러한 접근 방식을 바탕으로 언어 목표를 생성하여 내적 보상을 만들어 내는 개선된 방법을 제안합니다.

#### 예비 지식(Preliminaries)
논문에서는 부분적으로 관측 가능한 마르코프 결정 프로세스(POMDP)를 다룹니다. 자연 언어 문장 임베딩을 통한 전환 집합과 목표 집합 두 가지를 정의하여 LLM이 에이전트에게 목표를 제공할 수 있도록 합니다. DLLM 에이전트는 내적 보상 함수 및 환경의 보상을 활용하여 정책 최적화를 통해 목표를 달성합니다.

#### DLLM 방법론(Dreaming with LLMs)
이 섹션에서는 DLLM이 어떻게 LLMs에서 목표를 얻고 이를 활용하여 장기적인 의사결정을 유도하는지 설명합니다. 주어진 목표를 생성하거나 특정 유형의 목표에 적합한 목표를 제공하여 정책 학습을 유도합니다. 특히, 동일한 목표에 여러 번 내적 보상을 할당하지 않기 때문에 탐색의 복잡성을 보존합니다.

#### 실험 및 결과(Experiments and Results)
DLLM은 다양한 희소 보상 환경(Crafter, HomeGrid, Minecraft)에서 성능을 검증했습니다. 실험 결과 DLLM은 성능 면에서 기존의 강력한 방법들을 능가했으며 특히 높은 난이도의 작업에서도 큰 이점을 보여주었습니다. 또한, 더 강력한 LLM을 사용할수록 더 나은 성능을 발휘하는 것을 확인했습니다.

#### 결론(Conclusion)
DLLM은 희소 보상 환경과 길어진 학습 시간 문제를 해결할 수 있는 새로운 모델 기반 강화 학습 방법을 제안합니다. 실험을 통해 DLLM이 다양한 환경에서 우수한 성능을 발휘함을 입증했습니다.

### 2. 논문의 전체 요약
이 논문은 강화 학습의 어려운 과제인 긴 시간과 희소한 보상을 다루기 위해 'Dreaming with Large Language Models (DLLM)'이라는 새로운 접근 방식을 제안합니다. DLLM은 LLM(대형 언어 모델)을 활용하여 에이전트가 수행할 목표를 설정하고, 그 목표를 달성하기 위한 효과적인 탐색을 유도합니다. 실험을 통해 DLLM은 HomeGrid, Crafter, Minecraft 등의 환경에서 기존 강력한 방법들보다 월등히 높은 성능을 보였습니다. 특히 내적 보상을 통해 복잡한 환경에서도 에이전트가 목표를 효율적으로 달성하도록 돕습니다. 이 논문의 주요 기여는 LLM을 활용하여 본질적인 내적 보상을 제공하고, 이를 통해 강화 학습의 성능을 대폭 향상시킨 데 있습니다.

 

## Similar Papers
- [Position: Foundation Agents as the Paradigm Shift for Decision Making](2405.17009.md)
- [Make-An-Agent: A Generalizable Policy Network Generator with Behavior-Prompted Diffusion](2407.10973.md)
- [Diffusion Augmented Agents: A Framework for Efficient Exploration and Transfer Learning](2407.20798.md)
- [Learning to Manipulate Anywhere: A Visual Generalizable Framework For Reinforcement Learning](2407.15815.md)
- [VoCo-LLaMA: Towards Vision Compression with Large Language Models](2406.12275.md)
- [Gemini vs GPT-4V: A Preliminary Comparison and Combination of Vision-Language Models Through Qualitative Cases](2312.15011.md)
- [iVideoGPT: Interactive VideoGPTs are Scalable World Models](2405.15223.md)
- [RL for Consistency Models: Faster Reward Guided Text-to-Image Generation](2404.03673.md)
- [Flash-VStream: Memory-Based Real-Time Understanding for Long Video Streams](2406.08085.md)
