# CompCap: Improving Multimodal Large Language Models with Composite Captions
## TL;DR
## Summary
- [https://arxiv.org/pdf/2412.05243.pdf](https://arxiv.org/pdf/2412.05243.pdf)

1. 논문의 각 섹션 요약:

- **서론**: 복합 이미지(CIs)의 중요성과 MLLMs 모델이 이들에 대해 이해하는 데 어려움을 겪고 있다는 점을 강조합니다. 이에 대응하여 복합 이미지를 위해 고품질 설명 데이터가 필요하다고 주장합니다.

- **MLLMs의 필요성**: 현재 MLLMs은 자연 이미지(NIs)는 잘 이해하지만, 복합 이미지를 이해하는 데 한계를 가지며, 이는 고품질 캡션 데이터가 부족하기 때문이라는 점을 설명합니다. 따라서 복합 이미지의 이해도를 높일 필요성이 강조됩니다.

- **CompCap 프레임워크**: CompCap은 고품질 CI 캡션을 생성하기 위한 범용 프레임워크로, MLLMs의 비전-언어 정렬을 강화하고, 복합 이미지에 대한 이해력을 향상시키는 데 도움을 줍니다. 이 프레임워크는 메타데이터를 활용해 복합 이미지와 그에 따른 설명을 생성합니다.

- **실험 결과**: CompCap을 활용하여 118,000개의 CI-설명 쌍을 생성하고, 이를 통해 MLLMs의 성능 검증에 사용하였습니다. 경험적인 결과는 이 데이터 세트가 MLLMs의 복합 이미지 이해도를 크게 향상시킴을 보여줍니다.

- **결론**: 본 연구는 CompCap과 CompCap-118K 데이터 세트가 복합 이미지 이해의 측면에서 MLLMs의 성능을 향상시키고 있음을 강조합니다. 이 데이터 세트는 다양한 벤치마크에서의 MLLMs 성능 향상을 이끌어냈으며, 비전-언어 정렬의 중요성을 부각시킵니다.

2. 전체 요약:

이 논문은 복합 이미지(CI)에 대한 MLLMs의 이해 능력을 향상시키기 위해 고품질 캡션의 필요성을 강조합니다. CompCap이라는 프레임워크를 제안하여, 복합 이미지에 대한 고품질 캡션을 생성하고, 이를 통해 MLLMs의 성능을 여러 벤치마크에서 검증합니다. 결과적으로, CompCap-118K 데이터 세트는 특히 CI 작업에서 MLLMs의 성능을 크게 개선하여, 비전-언어 정렬의 중요성을 강조하게 되었습니다.