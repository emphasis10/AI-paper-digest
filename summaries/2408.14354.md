# SWE-bench-java: A GitHub Issue Resolving Benchmark for Java
## TL;DR
## Summary
- [https://arxiv.org/pdf/2408.14354.pdf](https://arxiv.org/pdf/2408.14354.pdf)

### 요약

#### 1. 각 섹션의 주요 내용 요약

**Introduction (소개)**
- 소프트웨어 엔지니어링 작업을 자동화하는 대형 언어 모델 (LLM)의 중요성이 증가하고 있습니다. 이 논문은 Python 기반 대형 언어 모델 평가 벤치마크인 SWE-bench를 확장하여, Java 버전인 SWE-bench-java-verified를 제안합니다.
 
**Multi-SWE-bench**
- SWE-bench-java-verified 벤치마크의 구축 과정을 설명합니다. 이 벤치마크는 GitHub에서 인기 있는 Java 리포지토리를 수집하고, 결함 데이터베이스에서 사라진 리포지토리들을 포함하여 총 70개의 후보 Java 리포지토리를 선택합니다.

**Benchmark Construction (벤치마크 구성)**
- SWE-bench-java-verified 벤치마크는 여러 단계로 구성됩니다: 후보 리포지토리 수집, 이슈 수집, 데이터 주석화, 평가 환경 구축, 평가 메트릭 설정.

**Workflow Overview (워크플로우 개요)**
- 벤치마크 구축은 다섯 단계로 나누어집니다: 후보 리포지토리 수집, 이슈 수집, 주석 추가, 평가 환경 구축, 평가 메트릭 설치.

**Data Statistics (데이터 통계)**
- SWE-bench-java-verified는 총 6개의 GitHub 리포지토리에서 91개의 이슈를 포함합니다. 리포지토리의 다양성은 데이터 세트의 대표성을 나타내며, 다양한 테스트 환경을 제공합니다.

**Experimental Setup (실험 설정)**
- 평가 메트릭으로 해결된 비율 (%)을 사용하며, SWE-agent를 이용해 여러 LLMs의 성능을 평가합니다. 이 모델들은 GPT-4, GPT-4-mini, DeepSeek-Coder-V2, DeepSeek-V2, Doubao-pro-128k를 포함합니다.

**Evaluation Metrics (평가 메트릭)**
- 해결된 비율 (%)을 사용하여 각 다양한 방법이 SWE-bench-java-verified 이슈를 해결하는 능력을 평가합니다.

**Results (결과)**
- DeepSeek 모델이 GPT 및 Doubao 모델보다 성능이 뛰어난 것으로 나타났습니다. 또한, 문제 설명이 더 직관적일수록 해결할 가능성이 높아지는 경향이 있습니다.

**Related Works (관련 연구)**
- 코드 생성 및 다국어 벤치마크 평가에 관한 언급과 기존 연구들을 참고하여 기술합니다.

**Conclusion and Future Works (결론 및 향후 작업)**
- SWE-bench-java-verified는 Java 프로젝트 이슈를 해결하기 위한 전용 평가 벤치마크로, 향후 더 많은 프로그래밍 언어에 대해 벤치마크를 확장할 계획입니다.

#### 2. 전체 요약

이 논문은 Java의 새로운 벤치마크인 SWE-bench-java-verified를 도입하여 대형 언어 모델(LLM)이 소프트웨어 이슈를 해결하는 능력을 평가하고자 합니다. Python 기반의 기존 벤치마크인 SWE-bench를 확장하여, 다양한 프로그래밍 언어에서 활용할 수 있도록 하는 다국어 벤치마크 구축을 목표로 합니다. Java 리포지토리에서 91개의 이슈를 수집하고 엄격하게 주석을 추가하여 데이터 세트를 구축하였으며, 여러 최신 모델들의 성능을 평가하였습니다. 결과는 DeepSeek 모델이 다른 모델들보다 우수한 성능을 보이며, 문제 설명이 더 상세할수록 해결 능력이 향상되는 것을 보여주었습니다. 향후에는 더 많은 프로그래밍 언어로 벤치마크를 확장하여 LLM의 코딩 능력을 더욱 정확하게 평가할 계획입니다.