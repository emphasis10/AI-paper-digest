# Linguistic Generalizability of Test-Time Scaling in Mathematical Reasoning
## TL;DR
## Summary
- [https://arxiv.org/pdf/2502.17407.pdf](https://arxiv.org/pdf/2502.17407.pdf)

## 1. 논문 각 섹션의 요약 및 설명

### 서론
논문은 다중 언어 기반의 복잡한 수학적 추론 벤치마크인 MCLM(Multilingual Competition Level Math)를 도입하여 테스트 시간 조정(Test-time scaling)이 언어 간 성능에 미치는 영향을 조사합니다. 이 연구는 수학적 추론에서 테스트 시간 조정 스케일링(Outcome Reward Modeling, Process Reward Modeling, Budget Forcing)의 언어적 일반화 한계를 밝히고, MCLM 데이터셋과 MR1-1.5B 모델을 통해 성능 평가를 실시합니다.

### 다중 언어 경쟁 수준 수학
MCLM은 55개 언어로 구성된 경쟁 수준의 수학 문제를 포함한 벤치마크로, 기존의 단순한 수학 문제로부터 더 복잡한 추론 능력을 평가하도록 설계되었습니다. 많은 언어에서 충분히 복잡한 벤치마크가 존재하지 않음을 보완하고자 시도합니다.

### 테스트 시간 조정 방법
이 논문에서는 세 가지 주요 방법을 분석합니다. 결과 보상 모델링(Outcome Reward Modeling, ORM), 과정 보상 모델링(Process Reward Modeling, PRM), 그리고 예산 강제(Budget Forcing, BF)입니다. ORM과 PRM은 쉬운 데이터셋에서는 개선 효과가 뚜렷하지만, 어려운 과제나 다중 언어에서는 불안정하며, BF는 영어권 사용자에게만 유의미한 개선을 제공합니다.

### MR1-1.5B 모델 소개
MR1-1.5B는 GPT-4o-Mini와 비슷한 복잡한 수학적 추론 성능을 달성하면서도 단 1.5B 파라미터만을 사용하는 오픈 멀티링구얼 추론 모델입니다. 이 모델은 Deepseek-R1-1.5B에 기초하여 학습되어, 다양한 언어에서 독해 및 추론 능력을 효과적으로 확장하기 위한 탐구입니다.

### 실험 결과
다양한 벤치마크 테스트에서 MR1-1.5B와 Qwen2.5-1.5B 수학 모델은 유사한 수준의 성능을 보였습니다. 그러나, 테스트 시간 스케일링 전략은 다중 언어의 작업에서 효과적으로 일반화되지 않음을 강조하였습니다.

### 결론
결론적으로, 논문은 테스트 시간 조정 방법이 영어 이외의 언어에서 그다지 효과적이지 않으며, 다중 언어로의 확장은 여전히 제한적이라고 주장합니다. 이 연구는 이러한 제한을 극복하기 위한 미래 연구의 방향으로 MCLM과 같은 데이터셋의 잠재력을 강조합니다.

## 2. 전체 요약
이 논문은 다중 언어 환경에서의 복잡한 수학적 추론을 평가하는 MCLM 벤치마크를 제안하고, 최신의 테스트 시간 조정 방법들이 언어간 깊이 있는 이해를 보장하지 못함을 입증합니다. 이는 새로운 방식의 다중 언어 데이터세트와 MR1-1.5B 같은 모델을 통해 더 나은 다국어 추론 성능을 위한 연구의 기회를 제시합니다. 궁극적으로 테스트 시간 조정을 통한 성능 개선의 한계와 가능성을 심층적으로 논의합니다.