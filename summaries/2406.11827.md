# WPO: Enhancing RLHF with Weighted Preference Optimization
## TL;DR
## Summary
- [https://arxiv.org/pdf/2406.11827.pdf](https://arxiv.org/pdf/2406.11827.pdf)

### 1. 각 섹션의 요약 내용

#### Introduction (서론)
대규모 언어 모델(LLMs)은 사람과 유사한 응답을 생성하는 놀라운 능력을 보여주지만, 높은 신뢰성, 안전성 및 윤리를 요구하는 시나리오에서는 여전히 도전에 직면하고 있습니다. 이를 해결하기 위해 인간 피드백을 통한 강화 학습(RLHF)이 LLMs를 더 잘 인간 가치에 맞추는 유망한 방법으로 제시됩니다. RLHF는 정책 모델을 사용하는 방식을 따라 온-정책과 오프-정책 설정으로 나뉘어질 수 있습니다. 온-정책 설정은 정책 모델을 최적화하는 동안 데이터 생성과 동일한 정책 모델을 사용하며, 오프-정책 설정은 다른 모델에서 데이터가 생성됩니다. 오프-정책 강화학습은 비용 효율성과 데이터 효율성 측면에서 상당한 이점을 가지고 있지만, 데이터와 최적화되는 정책 간의 분포 격차로 인해 성능이 떨어지는 문제점이 있습니다.

#### Related Work (관련 연구)
RLHF와 관련된 기존 연구는 ChatGPT와 같은 LLM의 발전을 이끌었습니다. 주요 모델로는 Zephyr와 GPT-4가 있으며, 이들은 RLHF와 직합 설정을 통해 더 나은 모델 정렬을 달성했습니다. 이러한 접근방식은 RLHF의 목표를 명확히 개선하려는 의도가 있습니다. 또한 직접 내쉬 최적화(DNO) 알고리즘은 DPO의 반복적 프레임워크를 사용하여 실제 및 예측 승률 간의 격차를 평가합니다. 그러나 DPO는 정책이 생성하는 출력과 선호 데이터셋의 출력을 비교할 때 불일치를 자주 드러냅니다.

#### Method (방법론)
제안된 방법론은 오프-정책 선호 데이터로 온-정책 선호 최적화를 시뮬레이션하는 것으로, 오프-정책 데이터의 효율성을 온-정책 RL의 성능 향상과 결합하는 것입니다. 이 과정은 선호 데이터셋을 선호 라벨링 함수로 변환하고, 현재 정책 모델을 사용해 새로운 선호 데이터셋을 재샘플링하는 방식으로 진행됩니다. 결국 가중된 정책 최적화(WPO) 목표가 제시되며, 이는 선호쌍을 재가중하여 분포 격차를 완화하고 최적화를 개선합니다.

##### Weighted Preference Optimization (WPO)
WPO는 가중된 선호 최적화를 통해 체계적인 가중 정렬 메커니즘을 도입하며, 모든 온-정책 생성 쌍이 동일하게 가중되도록 합니다. 이를 통해 WPO는 RL 동안의 분포 격차를 효과적으로 완화하면서 추가 비용 없이 최적화를 향상시킬 수 있습니다. WPO는 주로 Alpaca Eval 2 및 MT-bench에서 평가되었으며, WPO는 DPO보다 성능이 뛰어남을 보여줍니다.

#### Experiment (실험)
실험 설정에서는 오프-정책 및 하이브리드 설정을 사용합니다. 오프-정책 설정에서는 Ultrafeedback 데이터셋을 사용하며, 하이브리드 설정에서는 정책 모델과 다른 모델에서 데이터를 생성합니다. 실험 결과는 Alpaca Eval 2 및 MT-bench에서 평가되며, WPO는 DPO보다 뛰어난 성능을 보여줍니다. 특히 하이브리드 설정에서는 최고의 성과를 기록하였습니다.

### 2. 전체 요약

이 논문은 대규모 언어 모델(LLMs)에서 인간 피드백을 통한 강화 학습(RLHF)의 효율성을 높이고자 합니다. 오프-정책 데이터는 비용 효율성과 데이터 효율성을 제공하지만, 분포 격차 문제로 인해 최적화 성능이 저하됩니다. 이를 해결하기 위해 저자들은 오프-정책 데이터로 온-정책 학습을 시뮬레이션하는 WPO(Weighted Preference Optimization) 방법을 제안합니다. WPO는 선호쌍을 재가중하여 이 분포 격차를 완화하며, 최적화 과정을 개선합니다.

실험 결과, WPO는 기존의 DPO 방법보다 Alpaca Eval 2 및 MT-bench에서 뛰어난 성능을 보여줍니다. 특히 하이브리드 설정에서는 최고의 성능을 기록하였으며, 이러한 결과는 WPO의 우수성을 증명합니다. WPO는 온-정책 데이터와 고품질 오프-정책 데이터를 결합하여 최적화의 효율성과 안정성을 높일 수 있습니다.

## Similar Papers
- [SimPO: Simple Preference Optimization with a Reference-Free Reward](2405.14734.md)
- [Is DPO Superior to PPO for LLM Alignment? A Comprehensive Study](2404.10719.md)
- [Self-Play Preference Optimization for Language Model Alignment](2405.00675.md)
- [Bootstrapping Language Models with DPO Implicit Rewards](2406.09760.md)
- [sDPO: Don't Use Your Data All at Once](2403.19270.md)
- [Self-Exploring Language Models: Active Preference Elicitation for Online Alignment](2405.19332.md)
- [Iterative Length-Regularized Direct Preference Optimization: A Case Study on Improving 7B Language Models to GPT-4 Level](2406.11817.md)
- [RLHF Workflow: From Reward Modeling to Online RLHF](2405.07863.md)
- [Value-Incentivized Preference Optimization: A Unified Approach to Online and Offline RLHF](2405.19320.md)
