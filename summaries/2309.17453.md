# Efficient Streaming Language Models with Attention Sinks
## TL;DR
## Summary
- [https://arxiv.org/pdf/2309.17453.pdf](https://arxiv.org/pdf/2309.17453.pdf)

### 1. 각 섹션의 요약과 주요 기여 및 혁신적 부분

#### Introduction
이 논문은 스트리밍 애플리케이션에서 LLM(대형 언어 모델)을 사용하는 방법에 대해 다루고 있습니다. 구체적으로는, 무한 길이의 텍스트를 효율적으로 처리하면서도 성능을 유지할 수 있는 방법을 제안합니다. 이를 위해 주의 집중 현상(attention sink)을 소개합니다. 이 현상은 초반 토큰에 높은 주의가 할당되는 현상으로, 이를 이용해 스트리밍 애플리케이션에서 LLM을 활용하는 방안을 제시합니다.

#### Related Work
이 섹션에서는 기존 연구를 세 가지 주요 방향으로 분류하여 설명합니다: 길이 외삽, 컨텍스트 창 확장, 긴 텍스트의 활용성 개선. 각 접근법의 장단점을 논의하며, 본 연구가 이 중 첫 번째 방향에 집중하고 있음을 강조합니다.

#### StreamingLLM
스트리밍LLM이라는 새로운 프레임워크를 제안합니다. 이를 통해, 기존의 LLM이 제한된 길이의 주의 창(attention window)으로 학습된 경우에도 무한 길이의 텍스트를 처리할 수 있습니다. 또한 스트리밍 성능을 높이기 위해 프리트레이닝 단계에서 추가의 플레이스홀더 토큰을 사용합니다.

#### The Failure of Window Attention and Attention Sinks
창 주의 창(window attention) 방식의 실패를 설명합니다. 초반 토큰의 키/값(KV)을 유지하면 성능이 크게 회복됨을 보이며, 이는 주의 집중 현상 때문임을 증명합니다.

#### Rolling KV Cache with Attention Sinks
창 주의 창의 문제점을 해결하기 위해 롤링 KV 캐시를 제안합니다. 초반 토큰 몇 개의 KV를 유지하여 모델 성능을 안정적으로 유지합니다.

#### Pre-Training LLMs with Attention Sinks
주의 집중 현상을 고려하여, 프리트레이닝 단계에서 학습 가능한 주의 집중 토큰을 도입하는 방법을 제안합니다. 이는 스트리밍 성능을 향상시키는 데 도움이 됩니다.

#### Experiments
다양한 실험을 통해 제안된 방법의 유효성을 입증합니다. PG19 테스트 세트를 사용하여 20K 이상의 토큰에 대해 모델링 퍼플렉시티(perplexity) 실험을 수행합니다.

#### Results of Pre-Training with a Sink Token
주의 집중 토큰을 사용한 프리트레이닝이 모델 성능에 미치는 영향을 분석합니다. 학습 곡선과 퍼포먼스 결과를 비교하여 효용성을 확인합니다.

#### Results on Streaming Question Answering with Instruction-tuned Models
질문 응답 스트리밍 성능을 평가하며, 실 세계 애플리케이션에의 적용 가능성을 검토합니다.

#### Ablation Studies
제안된 프레임워크의 다양한 구성 요소의 중요성을 독립적으로 평가합니다.

#### Efficiency Results
스트리밍LLM의 디코딩 지연 시간과 메모리 사용을 기존 방법과 비교하여 그 효율성을 증명합니다.

#### Conclusion
스트리밍 애플리케이션에서 LLM을 효율적으로 사용할 수 있는 새로운 방법을 제안하며, 주의 집중 현상과 스트리밍LLM의 성능을 요약합니다.

### 2. 전체 요약

이 논문은 스트리밍 환경에서 효율적으로 작동하는 대형 언어 모델(LLM)을 제안합니다. 스트리밍LLM이라는 프레임워크를 통해 무한 길이의 텍스트를 효율적으로 처리할 수 있으며, 이는 주의 집중 현상(attention sink)을 활용한 것입니다. 주의 집중 현상은 초기 토큰에 높은 주의가 할당되는 현상으로, 이를 이용하여 스트리밍 애플리케이션에서의 성능 저하를 방지합니다. 제안된 방법은 다양한 실험을 통해 검증되었으며, 프리트레이닝 단계에서 학습 가능한 주의 집중 토큰을 도입하여 성능을 더욱 향상시킬 수 있음을 보여줍니다. 이를 통해 스트리밍LLM은 무한한 토큰을 효율적으로 모델링할 수 있으며, 기존의 방법보다 최대 22.2배 빠른 속도를 자랑합니다.

## Similar Papers
- [TriForce: Lossless Acceleration of Long Sequence Generation with Hierarchical Speculative Decoding](2404.11912.md)
- [MLKV: Multi-Layer Key-Value Heads for Memory Efficient Transformer Decoding](2406.09297.md)
- [SnapKV: LLM Knows What You are Looking for Before Generation](2404.14469.md)
- [Layer-Condensed KV Cache for Efficient Inference of Large Language Models](2405.10637.md)
- [Reasoning in Large Language Models: A Geometric Perspective](2407.02678.md)
- [SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models](2211.10438.md)
- [AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration](2306.00978.md)
- [Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention](2404.07143.md)
- [VeRA: Vector-based Random Matrix Adaptation](2310.11454.md)
