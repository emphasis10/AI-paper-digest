# DiJiang: Efficient Large Language Models through Compact Kernelization
## TL;DR
## Summary
- [https://arxiv.org/pdf/2403.19928.pdf](https://arxiv.org/pdf/2403.19928.pdf)

본 논문은 Transformer 모델의 계산 부하를 줄이기 위한 새로운 접근 방식인 DiJiang을 제안합니다. DiJiang은 주파수 영역 커널화(Frequency Domain Kernelization) 방법을 통해 기존에 훈련된 Transformer 모델을 선형 복잡성 모델로 변환할 수 있게 해줍니다. 이 방법은 가중 Quasi-Monte Carlo 샘플링을 사용하여 이론적으로 우수한 근사 효율성을 제공하며, 훈련 계산 복잡성을 줄이기 위해 이산 코사인 변환(DCT) 작업에 기반합니다. 실험을 통해 제안된 방법이 기존 Transformer와 비교할 수 있는 성능을 달성하면서 훈련 비용을 크게 줄이고 추론 속도를 향상시킴을 입증합니다.

### 1. 소개 및 관련 연구

- Transformer 아키텍처는 NLP 분야에서 큰 성공을 거두었으나, attention 메커니즘으로 인해 계산 비용이 크게 증가합니다.
- 이러한 문제를 해결하기 위해 선형 Transformer, locality-sensitive hashing, Reformer, Performer 등 여러 방법이 제안되었습니다.
- 하지만, 대규모 모델의 경우 이러한 방법들도 충분히 효율적이지 못하며, 새로운 모델을 처음부터 훈련시키는 데에는 많은 비용이 듭니다.

### 2. DiJiang: 효율적인 대규모 언어 모델을 위한 컴팩트 커널화

- DiJiang은 기존 Transformer를 선형 복잡성 모델로 변환하는 새로운 접근법을 제안합니다.
- 가중 Quasi-Monte Carlo 방법과 DCT를 통해 attention 계산을 효율적으로 근사합니다.
- 이 방법은 기존 모델과 유사한 성능을 유지하면서 훈련 비용을 크게 줄이고 추론 속도를 향상시킵니다.

### 3. 실험

- 다양한 규모의 언어 모델과 데이터셋을 사용한 실험을 통해 DiJiang의 효율성과 성능을 검증합니다.
- DiJiang은 기존 Pythia 모델과 유사한 성능을 달성하면서 훈련 비용을 약 1/16로 줄이고 추론 속도를 높였습니다.
- 다른 선형 attention 모델과의 비교를 통해 DiJiang이 더 높은 정확도와 효율성을 보임을 확인했습니다.

### 4. 결론 및 광범위한 영향

- DiJiang은 Transformer 모델의 계산 효율성을 크게 향상시키는 새로운 방법을 제시합니다.
- 이 방법은 대규모 언어 모델의 훈련 및 배포 비용을 줄이는 데 기여할 것으로 기대됩니다.

이 논문은 Transformer 모델의 계산 효율성을 개선하는 새로운 방법론을 제시하며, 이는 NLP 분야의 연구 및 응용에 있어 중요한 진전을 의미합니다.