# PM-LLM-Benchmark: Evaluating Large Language Models on Process Mining Tasks
## TL;DR
## Summary
- [https://arxiv.org/pdf/2407.13244.pdf](https://arxiv.org/pdf/2407.13244.pdf)

### 1. 각 섹션 요약 및 주요 기여 내용

#### 서론 (Introduction)
이 논문은 대형 언어 모델(LLMs)이 프로세스 마이닝(PM) 분석을 반자동화하는 잠재력을 탐구하며, 상업용 모델이 분석 작업에 적합한지 평가하고 있습니다. 주요 문제는 여러 PM 아티팩트, 타입, 그리고 코드 라이브러리들이 존재하기 때문에 이를 평가하는 벤치마크가 부재하다는 점입니다.

#### 관련 연구 (Related Work)
LLMs와 PM의 연결 및 평가 방법론을 다룹니다. PM 작업에서 LLMs의 가능성 및 기존 벤치마크의 현재 상태를 설명하고, LLMs가 SQL 구문을 생성하거나 사실 기반 출력을 생성할 수 있음을 강조합니다.

#### 평가 전략 (Evaluation Strategy)
LLMs가 생성한 텍스트 출력을 객관적으로 평가하는 방법론을 제안합니다. 특히 LLMs의 평가자를 활용하는 방법(LLMs-as-Judges)을 탐구하여 더 포괄적인 평가를 수행합니다.

#### 벤치마크 결과 (Benchmark Results)
최신의 상업용 및 오픈 소스 LLM들에 대해 벤치마크를 수행한 결과를 제시합니다. 상업용 모델과 대형 오픈 소스 모델이 PM 작업을 비교적 잘 수행하는 반면에, 작은 모델은 성능이 부족함을 발견했습니다. 각 모델의 점수를 표로 정리하여 비교합니다.

#### 미래 벤치마킹 전략 (Future Benchmarking Strategies)
미래의 벤치마킹 전략으로서, 정보 검색 강화 생성(RAG) 기술과 에이전트 크루 및 가설 생성 과정을 벤치마킹하는 방법을 제안합니다. 또한, 의미론적 이상(anomalies)이나 근본 원인(root causes)을 포함한 동적 데이터 생성의 필요성을 강조합니다.

#### 결론 (Conclusion)
LLMs를 사용한 프로세스 마이닝 벤치마크를 제안하고 그 유용성을 검토합니다. LLMs의 성능을 평가하는 데 한계가 있음을 인정하면서도, 소형 LLMs의 적합성을 평가하고 현재의 상태를 검토하는 데 유용하다고 결론 내립니다.

### 2. 전체 요약

이 논문은 "PM-LLM-Benchmark"라는 대형 언어 모델(LLMs)을 프로세스 마이닝(PM) 작업에 평가하기 위한 첫 종합 벤치마크를 제안합니다. 주로 도메인 지식 및 다양한 구현 전략에 초점을 맞추고 있습니다. 논문은 LLMs가 다양한 PM 작업을 어느 정도 수행할 수 있지만, 특히 소형 모델은 여전히 부족함을 나타냅니다. 이는 LLMs의 성능을 평가하기 위한 포괄적인 도구로 사용될 수 있으며, 향후 연구 방향으로는 고급 가설 생성 및 동적 데이터 생성을 포함한 새로운 벤치마킹 전략을 제안합니다.

행복한 연구가 되기를 바랍니다! ☺️