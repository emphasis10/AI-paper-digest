# Understanding the performance gap between online and offline alignment algorithms
## TL;DR
## Summary
- [https://arxiv.org/pdf/2405.08448.pdf](https://arxiv.org/pdf/2405.08448.pdf)

### 주요 내용 요약

1. **서론 및 배경**:
   - 이 논문에서는 온라인 및 오프라인 정렬 알고리즘의 성능 차이를 조사합니다. 인간 피드백을 통한 강화 학습(RLHF)은 대규모 언어 모델의 정렬을 위한 표준 프레임워크로 자리 잡았습니다. 그러나 최근 오프라인 정렬 알고리즘의 발전으로 인해 RLHF에서 정책 샘플링의 필요성이 도전받고 있습니다. 이 논문은 보상 과최적화의 맥락에서 실험을 통해 온라인 방법이 오프라인 방법보다 명확한 우위를 보임을 보여줍니다.

2. **방법론**:
   - **가설 테스트**: 성능 차이를 설명하기 위한 몇 가지 가설을 세우고, 이를 검증하기 위해 실험적 소거법을 사용합니다. 데이터 커버리지, 최적화 특성, 손실 함수 및 확장성 등의 측면에서 가설을 설정합니다.
   - **실험 설정**: KL 발산을 기준으로 다양한 알고리즘과 하이퍼파라미터 설정을 비교하여 공정한 비교를 수행합니다. 온라인 및 오프라인 알고리즘은 동일한 손실 함수를 사용하며, 동일한 하이퍼파라미터를 공유합니다.

3. **실험**:
   - **데이터 커버리지**: 온라인 알고리즘이 더 다양한 데이터 커버리지를 제공하는지 여부를 조사합니다. 오프라인 알고리즘에 온라인 생성 데이터를 사용하여도 성능 차이는 여전히 존재함을 발견했습니다.
   - **최적화 특성**: 분류 및 생성 성능 간의 상호작용을 조사합니다. 오프라인 알고리즘은 분류 정확도는 높지만, 생성 성능은 낮습니다. 반면 온라인 알고리즘은 생성 성능이 높지만, 분류 정확도는 낮습니다.
   - **손실 함수와 확장성**: 대조적 손실 함수와 비대조적 손실 함수를 비교하여 성능 차이를 조사합니다. 또한, 정책 네트워크를 확장함으로써 성능 차이가 어떻게 변화하는지 연구합니다.

### 혁신적인 부분
이 논문의 혁신성은 온라인 및 오프라인 정렬 알고리즘 간의 성능 차이를 체계적으로 분석하고, 그 원인을 규명하기 위해 다양한 가설을 검증한 데 있습니다. 특히, 온라인 알고리즘이 데이터 커버리지, 최적화 특성, 손실 함수 및 확장성 측면에서 오프라인 알고리즘보다 우수함을 실증적으로 입증했습니다. 이 연구는 AI 정렬의 중요한 역할을 하는 정책 샘플링의 중요성을 강조하고, 오프라인 정렬 알고리즘의 근본적인 도전에 대한 이해를 심화합니다.

## Similar Papers
- [Value-Incentivized Preference Optimization: A Unified Approach to Online and Offline RLHF](2405.19320.md)
- [Offline Regularised Reinforcement Learning for Large Language Models Alignment](2405.19107.md)
- [Dataset Reset Policy Optimization for RLHF](2404.08495.md)
- [WARP: On the Benefits of Weight Averaged Rewarded Policies](2406.16768.md)
- [Self-Exploring Language Models: Active Preference Elicitation for Online Alignment](2405.19332.md)
- [Learn Your Reference Model for Real Good Alignment](2404.09656.md)
- [New Desiderata for Direct Preference Optimization](2407.09072.md)
- [Understanding Reference Policies in Direct Preference Optimization](2407.13709.md)
- [Bootstrapping Language Models with DPO Implicit Rewards](2406.09760.md)
