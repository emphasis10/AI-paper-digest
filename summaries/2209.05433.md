# FP8 Formats for Deep Learning
## TL;DR
## Summary
- [https://arxiv.org/pdf/2209.05433.pdf](https://arxiv.org/pdf/2209.05433.pdf)

### 요약

#### 1. 서론
딥러닝의 성능을 지속적으로 개선하기 위해 신경망 모델의 크기와 연산 자원의 증가가 필요합니다. FP8 포맷은 16비트 포맷보다 계산 요구량을 줄이고, 훈련과 추론 모두에서 효율성을 높이는 자연스러운 진화입니다. 이 논문은 E4M3 (4비트 지수와 3비트 가수)와 E5M2 (5비트 지수와 2비트 가수)의 두 가지 FP8 인코딩 방식을 제안합니다.

#### 2. 딥러닝에서 FP8 사용 측면
FP8 사용에는 두 가지 주요 측면이 있습니다:
1. **동적 범위**: 네트워크가 요구하는 동적 범위에 따라 두 가지 인코딩 포맷이 필요합니다.
2. **유형 변환**: FP8 입력에 대한 수학적 연산은 더 높은 정밀도로 출력되며, 메모리에 기록하기 전에 FP8로 변환될 수 있습니다.

#### 3. FP8 이진 교환 포맷
FP8은 E4M3과 E5M2의 두 가지 인코딩으로 구성됩니다. E4M3는 무한대를 나타내지 않고 NaN을 위해 하나의 가수 비트 패턴만 사용하여 동적 범위를 확장합니다. E5M2는 IEEE 754 규약을 따릅니다.

##### 3.1 특수 값 표현
E4M3의 동적 범위를 확장하기 위해 적은 수의 특수 값을 나타내고, 이를 일반 값으로 재배정합니다. E5M2는 모든 특수 값을 IEEE 규약에 맞게 유지합니다.

##### 3.2 지수 바이어스
E4M3과 E5M2는 각각 7과 15의 지수 바이어스를 유지합니다. 이는 각 텐서의 중요한 값을 더 잘 다루기 위해 소프트웨어 구현에서 스케일링 요소를 유지할 수 있게 합니다.

#### 4. 실험 결과
##### 4.1 훈련
FP8 훈련 실험은 FP16 또는 bfloat16과 일치하는 결과를 달성했습니다. 다양한 모델과 작업에 대해 FP8 훈련 결과는 높은 정밀도의 훈련 결과와 일치했습니다.

##### 4.2 추론
FP8 훈련을 통해 8비트 추론 배포가 간소화됩니다. 이는 32비트 또는 16비트 부동 소수점으로 훈련된 네트워크를 사용한 int8 추론보다 더 간단합니다. GPT3와 BERT 모델의 평가 결과, FP8 포스트 트레이닝 양자화가 int8보다 더 높은 정확도를 유지함을 확인했습니다.

##### 4.3 텐서별 스케일링 요소
FP8에서 모든 텐서 타입에 동일한 스케일링 요소를 사용할 수 있지만, 정확도를 유지하기 위해 텐서별 스케일링 요소가 필요한 경우가 있습니다. 이는 특히 더 많은 텐서를 FP8로 저장할 때 두드러집니다.

#### 5. 결론
FP8 포맷은 IEEE-754 규약과 최소한의 일탈로 설계되었습니다. 이는 소프트웨어 구현이 IEEE FP 속성을 계속 활용할 수 있게 합니다. FP8을 사용하면 훈련과 추론 모두에서 더 작은 수학 파이프라인을 사용하여 계산 효율성을 높이고 메모리 대역폭 압력을 줄일 수 있습니다. FP8은 16비트 양자화된 모델의 정확도를 유지하면서도 리소스를 절약할 수 있습니다.

### 전체 요약
이 논문은 딥러닝 훈련과 추론을 가속화하기 위한 8비트 부동 소수점(FP8) 포맷을 제안합니다. E4M3과 E5M2의 두 가지 인코딩 방식을 통해 FP8 포맷은 성능 저하 없이 메모리 사용량을 줄이고 계산 효율성을 높입니다. 실험 결과, FP8 훈련은 높은 정밀도의 훈련 결과와 일치했으며, FP8 추론은 더 높은 정확도를 유지했습니다. FP8 포맷은 훈련과 추론 모두에서 효율성을 높이며, 더 큰 신경망 모델의 접근성을 확대합니다.

## Similar Papers
- [ZeroQuant-FP: A Leap Forward in LLMs Post-Training W4A8 Quantization Using Floating-Point Formats](2307.09782.md)
- [NIPQ: Noise proxy-based Integrated Pseudo-Quantization](2206.00820.md)
- [Efficient LLM Inference on CPUs](2311.00502.md)
- [Scalify: scale propagation for efficient low-precision LLM training](2407.17353.md)
- [QMoE: Practical Sub-1-Bit Compression of Trillion-Parameter Models](2310.16795.md)
- [Depth-Adaptive Transformer](1910.10073.md)
- [ChatQA 2: Bridging the Gap to Proprietary LLMs in Long Context and RAG Capabilities](2407.14482.md)
- [FlashDecoding++: Faster Large Language Model Inference on GPUs](2311.01282.md)
- [An Image is Worth More Than 16x16 Patches: Exploring Transformers on Individual Pixels](2406.09415.md)
