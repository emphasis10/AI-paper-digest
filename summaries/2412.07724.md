# Granite Guardian
## TL;DR
## Summary
- [https://arxiv.org/pdf/2412.07724.pdf](https://arxiv.org/pdf/2412.07724.pdf)

### 섹션별 요약

1. **서론 (Introduction)**
   - Granite Guardian 모델은 안전하게 대형 언어 모델(LLMs)의 프롬프트와 응답을 감지하는 기능을 제공합니다. 사회적 편견, 욕설, 폭력, 성적 콘텐츠, 비윤리적 행동, 탈옥(jailbreak), 맥락 적합성, 근거, 답변 관련성과 같은 다양한 위험 요소를 포괄적으로 다루며 사용자에게 안전한 AI 사용 환경을 제공합니다.

2. **LLMs의 위험 (Risks in LLMs)**
   - 대형 언어 모델은 일반적으로 사회적, 보안적 위험 등 다양한 위험을 내포하고 있습니다. Granite Guardian은 이러한 위험을 폭넓고 깊이 있게 감지하며, 사회적 편견, 비윤리적 행동 같은 사회적 위험과 탈옥 및 맥락 관련성 문제를 해결하기 위해 설계되었습니다.

3. **데이터셋 (Datasets)**
   - Granite Guardian은 사람의 주석 데이터와 합성 데이터를 결합하여 학습되었습니다. 복잡한 무해 및 유해 데이터를 통해 실용적인 응용 프로그램을 개발하고 실제 위협으로부터 모델을 보호하는 데 기여하고 있습니다.

4. **모델 디자인 및 개발 (Model Design and Development)**
   - Granite Guardian은 감독된 미세 조정을 통해 위험을 계산하고 안전 지침 템플릿을 사용하여 다양한 위험을 탐지합니다. 이는 모든 사용 사례에 적용 가능한 유연성을 제공합니다.

5. **평가 (Evaluation)**
   - 다양한 공개 데이터셋에서의 벤치마킹은 Granite Guardian이 위험 감지에서 최첨단 성과를 이룩했음을 보여줍니다. 또한, 안전 및 성과의 균형을 유지하여 각 베이스라인을 능가합니다.

6. **결론 (Conclusion)**
   - Granite Guardian 모델군은 개방형 소스로 제공되며, AI 시스템의 발전과 책임 있는 개발을 장려합니다. 이러한 모델은 맥락 적합성, 근거 및 답변 관련성과 같은 특수한 위험을 해결하기 위해 사용됩니다.

### 전체 요약

Granite Guardian은 대형 언어 모델의 안전한 활용을 위해 설계된 위험 감지 모델로, 사회적 편견, 욕설, 탈옥, 그리고 RAG 관련 맥락 및 근거 등의 위험을 포괄적으로 다루고 있습니다. 이 모델은 사람의 주석과 합성 데이터를 사용하여 학습되었으며, 다양한 벤치마크에서 뛰어난 성과를 보이고 있습니다. Granite Guardian은 오픈 소스로 제공되며, 책임 있는 AI 개발을 촉진하고 안전한 사용을 위한 기반을 제공합니다.