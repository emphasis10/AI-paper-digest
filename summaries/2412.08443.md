# POINTS1.5: Building a Vision-Language Model towards Real World Applications
## TL;DR
## Summary
- [https://arxiv.org/pdf/2412.08443.pdf](https://arxiv.org/pdf/2412.08443.pdf)

1. 각 섹션의 요약:
- 서론: Vision-language 모델은 최근 몇 년간 놀라운 발전을 이뤘으며, 복잡한 문제 해결에 탁월한 성능을 보이고 있습니다. 특히, 강력한 open-source 모델로 POINTS1.5를 소개하며, 이 모델은 정밀한 데이터 튜닝을 통해 기존 상업 모델과 견줄만한 성능을 발휘합니다.
  
- 모델 아키텍처: POINTS1.5는 NaViT 스타일 비전 인코더와 LLaVA 스타일 아키텍처를 활용하여 이미지의 해상도를 제한 없이 처리할 수 있는 기능을 보유하고 있습니다. 이는 기존의 CLIP 비전 인코더가 해상도 제한으로 인한 이미지 분할 문제를 해결했습니다.

- 주요 혁신: 이 논문의 주요 기여는 세 가지 혁신입니다. 첫째, NaViT 스타일 비전 인코더로 이미지의 원본 해상도를 유지하면서 더욱 정확히 해석합니다. 둘째, 중국어 서포트를 추가하여 다국어에 걸친 효율성을 증대시켰습니다. 셋째, 데이터세트의 정밀한 필터링 방법을 제안하여 더 높은 품질의 학습 데이터를 제공했습니다.

2. 전체 요약:
POINTS1.5는 open-source 비전-언어 모델로, 다양한 실제 응용 분야에서 뛰어난 성능을 발휘합니다. 주요 혁신은 비전 인코더의 해상도 조절 능력 강화, 중국어 포함을 통한 이중 언어 제작, 그리고 데이터 필터링 강화입니다. 이 모델은 기존의 POINTS1.0보다 월등히 개선되어, 다양한 벤치마크에서 최고 순위를 차지했습니다. POINTS1.5는 최대 10억 개 파라미터 이하 모델 중에서 OpenCompass 리더보드에서 1위를 기록할 만큼 강력한 성능을 보이고 있습니다.