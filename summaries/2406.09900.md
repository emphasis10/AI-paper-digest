# GEB-1.3B: Open Lightweight Large Language Model
## TL;DR
## Summary
- [https://arxiv.org/pdf/2406.09900.pdf](https://arxiv.org/pdf/2406.09900.pdf)

### 1. 각 섹션 요약 및 주요 기여와 혁신점

#### 1. 서론
최근 대형 언어 모델(LLM)들의 성과는 놀랍습니다. 예를 들면, ChatGPT, GPT-4, Claude 등이 여러 작업에서 인간 수준을 능가하는 성과를 보였습니다. 그러나 이러한 모델들은 상당한 컴퓨팅 자원을 요구하며, 주로 고성능 서버에서 실행됩니다. 이는 일반 사용자들이 접근하기 어렵게 만듭니다. 이에 따라 CPU 환경에서 효율적으로 동작할 수 있는 경량 모델이 필요하게 되었습니다. GEB-1.3B는 이러한 요구를 충족시키기 위해 개발되었습니다.

#### 2. 사전 학습
##### 데이터 수집 및 처리
모델 성능을 보장하기 위해 다양한 고품질 데이터를 수집하여 활용했습니다. 영문 데이터는 주로 C4, Github, StackExchange에서 수집되었고, 중문 데이터는 Common Crawl, WuDaoCorpus, SkyPile에서 수집되었습니다. 이러한 데이터를 정리하고 불필요한 부분을 제거하여 품질 높은 학습 데이터를 확보했습니다.

##### 모델 아키텍처
GEB-1.3B는 Transformer 구조를 기반으로 하여, Group-Query-Attention, SwiGLU 활성화 함수, Post-RMSNorm, RoPE 등 최신 기술을 통합하여 훈련 효율성과 안정성을 높였습니다. 또한, FP32를 활용하여 정확한 위치 정보를 반영하고, 파라미터 수를 줄이기 위해 그룹화된 토큰화를 도입했습니다.

##### 하이퍼파라미터 튜닝
AdamW 옵티마이저와 BFloat16 혼합 정밀도를 사용해 훈련을 수행했습니다. GPU 메모리 제약으로 작은 배치 사이즈를 사용해야 했으며, 이는 훈련 안정성을 떨어뜨릴 수 있었지만 배치 교체, 특정 반복 건너뛰기, 임베딩 계층 기울기 축소 등의 방법으로 이를 보완했습니다.

#### 3. 정렬(Alignment)
##### 지도 학습
약 1600만 개의 인스트럭션 데이터를 사용하여 모델을 세밀하게 튜닝했습니다. 이 데이터셋은 일반적인 언어 과제부터 수학 문제 해결, 프로그래밍 연습 등 다양한 주제를 포괄합니다. 이를 통해 모델이 다양한 과제에서 일반화할 수 있도록 하였습니다.

##### 직접 선호 최적화
또한, DPO를 사용하여 모델이 비윤리적인 질문에 답하지 않고 유해하지 않은 답변을 생성하도록 유도했습니다. 이 데이터셋은 약 1만 개의 예제로 구성되어 있습니다.

#### 4. 평가
##### 일반 벤치마크
GEB-1.3B는 MMLU, C-Eval, CMMLU 등의 벤치마크에서 다른 경량 모델들보다 우수한 성능을 보였습니다. 특히, MindLLM-1.3B와 TinyLLaMA-1.1B 모델보다 뛰어난 성과를 내었으며, 다소 큰 모델인 LLaMA-7B와 비교해도 우세하게 평가됩니다.

##### 독성 평가
ToxiGen 데이터셋을 사용하여 생성된 텍스트의 독성을 평가한 결과, GEB-1.3B는 작은 모델임에도 불구하고 매우 적은 독성 텍스트를 생성했습니다.

##### 추론 속도
현재 FP32 버전은 CPU 환경에서 초당 12토큰의 속도로 동작하며, 이는 실제 응용 프로그램에서 사용될 수 있습니다. 향후 모델 양자화를 통해 더 빠른 추론 속도를 목표로 하고 있습니다.

### 2. 종합 요약
GEB-1.3B는 대형 언어 모델의 최신 기술을 다수 도입한 경량 모델로, 고성능이 요구되는 서버가 아닌 일반적인 CPU 환경에서도 효율적으로 동작하도록 설계되었습니다. 이 모델은 다양한 고품질 데이터셋을 사용해 훈련되었으며, 지도 학습과 직접 선호 최적화를 통해 인간과의 대화에서 더 자연스럽고 안전한 응답을 제공하도록 설계되었습니다. 벤치마크 테스트 결과, 여러 경쟁 모델들을 능가하는 성능을 보였으며, 특히 다언어 지원 측면에서 탁월한 성과를 보였습니다. GEB-1.3B의 공개는 경량 LLM 개발에 중요한 기여를 하며, 다양한 연구와 혁신의 기반이 될 것으로 기대됩니다.

## Similar Papers
- [Patch-Level Training for Large Language Models](2407.12665.md)
- [Xmodel-VLM: A Simple Baseline for Multimodal Vision Language Model](2405.09215.md)
- [A Survey on Efficient Inference for Large Language Models](2404.14294.md)
- [Tele-FLM Technical Report](2404.16645.md)
- [Arcee's MergeKit: A Toolkit for Merging Large Language Models](2403.13257.md)
- [PowerInfer-2: Fast Large Language Model Inference on a Smartphone](2406.06282.md)
- [Efficient and Effective Vocabulary Expansion Towards Multilingual Large Language Models](2402.14714.md)
- [SeaLLMs 3: Open Foundation and Chat Multilingual Large Language Models for Southeast Asian Languages](2407.19672.md)
- [RAG Foundry: A Framework for Enhancing LLMs for Retrieval Augmented Generation](2408.02545.md)
