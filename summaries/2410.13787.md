# Looking Inward: Language Models Can Learn About Themselves by Introspection
## TL;DR
## Summary
- [https://arxiv.org/pdf/2410.13787.pdf](https://arxiv.org/pdf/2410.13787.pdf)

### 1. 각 섹션의 요약

#### 소개
이 논문은 대규모 언어 모델(LLM)이 자체적으로 자각 없는 지식을 발견할 수 있는 능력을 연구합니다. 주된 아이디어는 LLM이 introspection(내성)이라는 과정을 통해 훈련 데이터에 포함되지 않은 정보에 접근할 수 있다는 것입니다. 이는 LLM이 질문에 답하는 데 있어 성실성을 높일 수 있는 가능성을 시사합니다.

#### 방법론 개요
서로 다른 두 모델을 통해 introspection을 실험합니다. 모델 M1은 자체 행동 예측에 뛰어나며, 이는 훈련 데이터로 단순히 유추할 수 없는 점을 시사합니다. M1의 행동을 예측하는데, 두 번째 모델 M2도 M1의 데이터로 훈련했는데, M1이 M2보다 더 뛰어난 성능을 보였습니다.

#### 결과 및 토론
모델들이 자신의 행동을 예측할 때 다른 모델이 예측하는 것보다 더 정확하다는 결과가 나타났습니다. 이는 introspection를 통해 모델이 스스로를 이해할 수 있는 능력을 가지고 있다는 것을 나타내며, 이는 데이터에서 유추할 수 없는 정보를 스스로 예측할 수 있는 능력을 가질 수 있음을 시사합니다.

#### 결론
논문은 introspection이 LLM의 성실성과 해석 가능성을 높이는데 기여할 수 있다고 결론짓습니다. 이를 통해 모델이 자신의 도덕적 상태나 내부적인 목표를 진단할 수 있는 가능성을 제시합니다.

### 2. 전체 요약

이 논문은 대규모 언어 모델(LLM)의 introspection 능력을 통해 훈련 데이터만으로는 유추할 수 없는 정보를 스스로 알아내는 과정을 설명합니다. 다수의 실험에서, 이 introspection 능력을 통해 모델들이 스스로의 행동을 더 정확하게 예측할 수 있음을 보여줍니다. 이러한 능력은 LLM의 성실성을 높이고, 모델이 내부적인 상태나 목표를 더 명확하게 설명함으로써 해석 가능성을 높일 수 있는 잠재력을 갖고 있습니다. 최종적으로, 이러한 introspection의 발전은 AI의 안전성을 향상시키는 한편, 모델의 도덕적 상태를 평가하는데도 활용될 수 있습니다.