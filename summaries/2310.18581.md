# Accelerating LLaMA Inference by Enabling Intermediate Layer Decoding via Instruction Tuning with LITE
## TL;DR
## Summary
- [https://arxiv.org/pdf/2310.18581.pdf](https://arxiv.org/pdf/2310.18581.pdf)

### 논문 섹션별 요약 및 전체 요약

#### 1. 서론 (Introduction)
최근 대형 언어 모델(LLMs)은 자연어 처리 분야에서 혁신적인 성과를 보였지만, 이 모델들의 큰 규모로 인해 추론이 느리고 계산 비용이 많이 듭니다. 이 논문에서는 중간 레이어에서의 디코딩을 가능하게 하는 'LITE'를 통한 명령어 튜닝 방법을 제안합니다. 이 방법은 중간 레이어에서도 '좋은' 텍스트 생성 능력을 부여하고, 최종 레이어의 생성 능력에 영향을 미치지 않습니다.

#### 2. 관련 연구 (Related Work)
LLM의 추론 효율성을 높이기 위한 다양한 방법들이 연구되었습니다. 모델 크기를 줄이는 방법으로 양자화, 지식 증류, 모델 압축 및 네트워크 가지치기 등이 있습니다. 하지만 이 방법들은 복잡한 구조적 변경이나 추가 모델 파라미터가 필요합니다. 반면, LITE를 통한 명령어 튜닝은 간단하면서도 상당한 개선을 제공합니다.

#### 3. 명령어 튜닝 방법론 (Instruction Tuning with LITE)
기존의 명령어 튜닝 방식에서는 최종 레이어만 텍스트 생성 능력을 가지게 됩니다. LITE를 통해 중간 레이어도 텍스트 생성 능력을 가지게 하여, 중간 레이어의 예측이 최종 레이어의 예측과 잘 맞도록 합니다. 이를 통해 중간 레이어에서의 디코딩이 가능해지며, 추론 효율성을 높일 수 있습니다.

#### 4. 추론 효율성 개선 (Making Inference Efficient)
기존의 고정된 중간 레이어 디코딩 방식은 성능 저하를 야기할 수 있습니다. 이를 개선하기 위해, 중간 레이어의 예측 확률에 기반한 동적 조기 종료 방법을 제안합니다. 이 방법은 중간 레이어의 예측 확률이 충분히 높을 때 해당 레이어에서 디코딩을 종료하여 효율성을 높입니다.

#### 5. 결과 (Results)
종합적인 실험을 통해, LLaMA-2 모델을 Alpaca 데이터셋으로 튜닝하고, Vicuna, WizardLM, Koala, Self-Instruct 등 네 가지 테스트 셋에서 평가를 수행했습니다. 그 결과, 동적 조기 종료 방법은 추론 비용을 크게 줄이면서도 생성 품질을 유지한다는 것을 확인했습니다. 구체적으로, 7B 모델에서는 37.86%, 13B 모델에서는 46.35%의 추론 비용 개선을 달성했습니다.

#### 6. 결론 (Conclusion)
본 연구는 LITE를 통한 명령어 튜닝이 중간 레이어에서도 '좋은' 생성 능력을 부여하고, 동적 조기 종료 방법이 추론 효율성을 높이면서 생성 품질을 유지함을 보여줍니다. 이는 LLM의 폭넓은 채택을 위한 중요한 단계이며, 추가적인 연구 가능성을 열어줍니다.

### 전체 요약
이 논문에서는 대형 언어 모델(LLMs)의 추론 효율성을 개선하기 위해, 중간 레이어에서도 '좋은' 텍스트 생성 능력을 부여하는 LITE를 통한 명령어 튜닝 방법을 제안합니다. 이를 통해 동적 조기 종료 방법을 적용하여, 중간 레이어의 예측 확률이 충분히 높을 때 해당 레이어에서 디코딩을 종료함으로써 추론 비용을 크게 줄이면서도 생성 품질을 유지할 수 있습니다. 종합적인 실험 결과, 이 방법은 LLM의 추론 효율성을 높이는 동시에 생성 품질을 유지하는 데 효과적임을 확인했습니다. 이는 LLM의 실용적인 채택을 위한 중요한 연구이며, 추가적인 연구와 개선 가능성을 제시합니다.