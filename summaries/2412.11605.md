# SPaR: Self-Play with Tree-Search Refinement to Improve Instruction-Following in Large Language Models
## TL;DR
## Summary
- [https://arxiv.org/pdf/2412.11605.pdf](https://arxiv.org/pdf/2412.11605.pdf)

### 1. 각 섹션 요약:

#### 서론 및 동기:
이 논문은 대형 언어 모델(LLM)의 지시 따르기 능력을 향상시키기 위해 SPAR라는 자체 플레이 프레임워크를 제안합니다. LLM이 복잡한 지침을 따르는 데 있어 미묘한 요구 사항을 인식하는 것이 중요하며, 기존의 방식은 여러 독립적인 반응을 샘플링하여 선호 쌍을 생성하는 문제가 있었습니다.

#### 방법론:
SPAR는 나무 탐색(self-play tree-search)을 통해 모델이 Instrucion- following작업에서 자기 개선을 할 수 있도록 도와줍니다. 이 과정에서 배우는 모델은 '행위자'와 '개선자'라는 두 가지 역할을 하며, 모델 자체가 스스로를 판단하고 개선하는 역할을 수행하여 결과적으로 개선을 이루어냅니다.

#### 데이터셋 구성:
제안된 방법론의 데이터셋은 43,000개의 복잡한 지시 따르기 요청을 포함하며, 이 데이터 세트를 통해 LLM의 지시 따르기 능력을 강화할 수 있도록 설계되었습니다.

#### 실험 결과:
여러 실험을 통해 SPAR는 다른 자기 개선 방법보다 뛰어난 성능을 보였으며, 특히 IF Eval 기준에서 LLaMA3-8B-Instruct 모델이 GPT-4 Turbo를 능가하는 성과를 냈습니다. 이 방법론은 추론 시간 계산을 확대하여 성능을 추가로 향상시킬 수 있으며, 스스로 개선할 수 있는 잠재력을 보여주고 있습니다.

#### 결론:
SPAR의 중요한 기여는 모델의 개선을 위한 운동선수 같은 자기 플레이 방식과 함께, 기존 선호 학습의 방식을 개선하여 불필요한 요소를 최소화하고 주요 차이점을 강조함으로써 지시 따르기 작업에서 모델의 성과를 현저히 높인 것입니다.

### 2. 전체 요약:

이 논문은 대형 언어 모델의 지시 따르기 성능을 향상시키기 위한 SPAR라는 혁신적 방법론을 제안합니다. SPAR는 스스로 재생하는 나무 탐색 프레임워크로, 모델이 지시 사항을 정확히 따르고 결과의 미세한 차이를 인식하도록 돕습니다. 세 번의 반복을 통해, LLaMA3-8B-Instruct 모델은 GPT-4 Turbo보다 뛰어난 성과를 보였으며, SPAR는 지속적인 자체 개선을 가능하게 하는 잠재력을 보여줍니다.