# Learning to Decode Collaboratively with Multiple Language Models
## TL;DR
## Summary
- [https://arxiv.org/pdf/2403.03870.pdf](https://arxiv.org/pdf/2403.03870.pdf)

이 논문은 다양한 대규모 언어 모델(LLM)이 협력하여 토큰 수준에서 생성물을 교차시키는 방법을 제안합니다. LLM이 다음 토큰을 생성하는 결정을 잠재 변수로 모델링하며, 훈련 세트의 마진 가능성을 최적화함으로써 기본 LLM이 자체적으로 생성할 때와 "보조" 언어 모델 중 하나에게 생성을 위임할 때를 자동으로 학습합니다.

**1. 도입**

다양한 LLM을 결합하는 기존 방법들은 속도 향상, 생성 제어, 일관성 및 반복 감소 등 다양한 이점을 제공합니다. 이 논문은 LLM들이 각 토큰을 생성할 때마다 협력하도록 학습하는 새로운 접근 방식을 소개합니다. 이러한 방식은 특정 작업에 맞게 각 모델의 전문 지식을 결합할 수 있으며, 특히 일반 모델이 도메인 전문 모델을 호출하여 성능을 향상시키는 크로스 도메인 설정에서 유용합니다.

**2. 협력적 생성을 위한 잠재 변수 프레임워크**

이 프레임워크는 기본 모델이 어떤 토큰을 생성할지, 또는 어떤 보조 모델에게 생성을 위임할지 결정하는 잠재 변수에 기반합니다. 기본 모델 외에도 하나 이상의 보조 모델(일반적으로 더 크거나 특화된 모델)을 사용하여 토큰 시퀀스를 생성합니다. 이 방법은 직접적인 감독 없이도 최적의 협력 패턴을 데이터에서 자연스럽게 학습할 수 있습니다.

**3. Co-LLM: LLM과 함께 디코딩을 협력적으로 학습**

기본 모델이 스스로 생성할지 또는 보조 모델에게 생성을 위임할지를 결정하는 이진 결정을 내리는 방식으로, Co-LLM은 기본 모델과 하나의 보조 모델 사이의 협력을 학습합니다. 이는 기본 모델이 보조 모델을 호출할 최적의 순간을 학습함으로써, 두 모델의 장점을 결합하여 향상된 성능을 달성하도록 합니다.

**4. 실험 설정 및 결과**

Co-LLM은 수학 추론, 도메인별 질문 응답 작업 등 다양한 작업에서 개별 모델을 사용하는 것보다 우수한 성능을 보였습니다. Co-LLM을 사용하여 LLAMA와 도메인별 모델 간의 협력이 성능 향상에 기여함을 입증했습니다. 특히, Co-LLM은 다른 스케일의 모델 간 협력을 가능하게 하여, 작은 모델이 큰 모델의 지식을 활용할 수 있도록 합니다.

**5. 결론**

Co-LLM은 여러 LLM이 협력하여 더 우수한 생성물을 만들 수 있는 새로운 접근 방식을 제시합니다. 이 방식은 각 작업에 가장 적합한 협력 패턴을 자연스럽게 학습할 수 있으며, 다양한 작업에서 모델의 성능을 향상시킬 수 있습니다. 향후 연구에서는 두 개 이상의 모델을 통합하고 더 복잡한 협력 전략을 탐색할 계획입니다.w