# HARP: Hesitation-Aware Reframing in Transformer Inference Pass
## TL;DR
## Summary
- [https://arxiv.org/pdf/2412.07282.pdf](https://arxiv.org/pdf/2412.07282.pdf)

### 1. 섹션 요약

**초록 (Abstract):** 
이 논문은 큰 언어 모델의 성능 향상을 목표로 합니다. 주요 제안 방법은 HARP(Hesitation-Aware Reframed Pass)로, 추론 과정에서 불확실성이 있는 토큰에 추가 계산을 적용하는 방법입니다.

**서론 (Introduction):**
Transformer 아키텍처는 각 토큰 생성 시 동일한 계산 자원을 사용합니다. 이러한 동등한 처리 방식은 복잡한 토큰 처리에 정확성 저하를 초래할 수 있습니다. HARP는 인간의 주저와 입력의 재구성을 통해 이러한 문제를 해결하려고 합니다.

**관련 연구 (Related Works):**
적응형 계산(Adaptive Computation)과 불확실성 평가(Uncertainty Estimation)에 관한 기존 연구들을 소개하며, 특히 HARP와의 비교를 통해 차별점을 강조합니다.

**방법론 (Methods):**
기존 Transformer의 Forward Pass에 토큰 불확실성을 평가하고 입력을 재구성하는 HARP 방식을 통합한 새로운 알고리즘을 제시합니다. 불확실성이 높은 경우, 입력의 임베딩에 노이즈를 주어 재구성하고 추가 계산을 수행합니다.

**실험 결과 (Experimental Results):**
다양한 데이터셋과 모델 크기에 걸쳐 HARP의 성능 개선을 평가합니다. 최대 5.16%의 성능 향상과 기존 빔 서치보다 효율적인 추론을 보입니다.

**결론 (Conclusion):**
HARP는 재훈련이나 아키텍처 수정 없이 언어 모델의 추론 성능을 향상시킬 수 있는 방법입니다. 이는 여러 작업에 걸쳐 성능 향상을 달성하였고, 기존 빔 서치 방법보다 효율적입니다.

### 2. 전체 요약

이 논문은 HARP라는 새로운 방법을 소개하여 언어 모델의 추론 성능을 개선합니다. HARP는 불확실한 순간에 토큰을 재구성하고 추가 계산을 사용하여 성능을 높입니다. 이는 모델의 재훈련 없이도 기존 방법보다 빠르고 효율적이며, 각 토큰에 대한 계산 자원을 더 효과적으로 사용할 수 있도록 설계되었습니다.