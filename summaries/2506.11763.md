# DeepResearch Bench: A Comprehensive Benchmark for Deep Research Agents
## TL;DR
## Summary
- [https://arxiv.org/pdf/2506.11763.pdf](https://arxiv.org/pdf/2506.11763.pdf)

I'm currently processing the paper to provide the summaries as requested. Please allow me a moment to compile the information from the document. 1. 논문의 각 섹션 요약:

- **소개(Introduction)**: 본 논문은 대규모 언어 모델(LLM)을 기반으로 한 심층 연구 에이전트(DRA)의 개발 및 평가를 위한 새로운 벤치마크인 DeepResearch Bench를 소개합니다. 이는 복잡한 연구 작업을 높은 정확도로 처리할 수 있는 에이전트를 평가하기 위한 종합적인 프레임워크를 제공합니다.

- **문제 설정(Problem Setting)**: 현재의 DRA 평가 시스템은 실제 연구 환경을 충분히 반영하지 않으며, 본 논문은 이를 개선하기 위한 방법론을 제안합니다. 데이터 기반 과학 발견 및 보고서 생성의 능력을 측정하기 위한 두 가지 새로운 프레임워크, RACE와 FACT를 도입합니다.

- **시스템 설계(System Design)**: DeepResearch Bench는 22개 다양한 도메인에서 100개의 고급 연구 작업으로 구성되며, 이는 전문적인 연구에 필요한 다양한 실질적인 요구사항을 반영하도록 설계되었습니다.

- **평가 방법(Evaluation Methodology)**: RACE (Reference-based and Adaptive Criteria-driven Evaluation)는 생성된 보고서의 품질 평가에 초점을 맞추고, FACT (Factual Abundance and Citation Trustworthiness)는 정보 검색 능력 및 인용의 정확성을 평가하는 데 중점을 둡니다.

- **결과(Result)**: Gemini-2.5-Pro Deep Research 에이전트는 종합적인 성능에서 앞서 나가며, OpenAI Deep Research와 비교했을 때 교육과 관련된 측면에서 우수함을 보여줍니다.

- **결론(Conclusion)**: DeepResearch Bench는 연구자들이 더욱 효율적이고 인간 중심적인 AI 에이전트를 구축하는 데 기여할 것으로 기대되며, 평가 결과는 인간 판단과 높은 일관성을 불러일으킵니다.

2. 종합 요약:

본 논문은 LLM 기반의 심층 연구 에이전트를 평가하기 위한 최초의 포괄적인 벤치마크인 DeepResearch Bench를 소개합니다. 이 벤치마크는 다양한 분야에서의 100개 이상의 연구 작업을 통해 에이전트의 실제 세계 요구 사항을 충족시키기 위한 능력을 평가합니다. 논문에서는 두 가지 평가 프레임워크인 RACE와 FACT를 도입하여 보고서 생성의 품질과 정보 검색의 정확성을 각각 평가합니다. 이러한 프레임워크는 인간의 평가와 높은 일관성을 보이며, 연구자들이 더욱 강력하고 인간 중심적인 AI 에이전트를 개발하는 데 도움을 줄 수 있습니다.