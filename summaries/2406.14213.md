# Complexity of Symbolic Representation in Working Memory of Transformer Correlates with the Complexity of a Task
## TL;DR
## Summary
- [https://arxiv.org/pdf/2406.14213.pdf](https://arxiv.org/pdf/2406.14213.pdf)

### 섹션별 요약 및 주요 내용

#### 1. 서론 (Introduction)
신경망 모델은 다양한 인공지능 과제를 성공적으로 해결하지만 여전히 해석력과 일반화에서 부족합니다. 본 논문은 트랜스포머 디코더에 상징적 작업 기억을 추가하여 기계 번역에서 모델의 예측 품질을 향상시키는 방법을 탐구합니다.

**주요 기여 및 혁신성:**
- 트랜스포머 모델 디코더에 상징적 작업 기억을 추가
- 모델의 예측 품질을 향상시키고 기계 번역 작업에 필요한 중요한 정보를 저장

#### 2. 관련 연구 (Related Work)
이 섹션은 메모리 확장 신경망(MANN)의 다양한 모델들과 그 작업 기억 구현 방법들을 설명합니다. Recurrent Neural Networks (RNN), Long Short-Term Memory (LSTM), Neural Turing Machines (NTM) 등 다양한 모델이 언급됩니다.

**주요 기여 및 혁신성:**
- 기존 메모리 모델들과의 차별성
- 상징적 작업 기억을 통한 상호 작용 및 업데이트 방법 고유성

#### 3. 트랜스포머 디코더에 작업 기억 적용 (Transformer with Working Memory in Decoder)
바닐라 트랜스포머의 입력 인코딩 방식을 따르며, 디코더는 출력 시퀀스를 생성하는 동안 내부 작업 기억에 토큰을 기록할지 출력 대상 예측에 쓸지 결정합니다.

**주요 기여 및 혁신성:**
- 모델이 결정하는 작업 기억 토큰과 표적 예측 토큰의 조화
- 작업 기억이 자연어 단어 또는 부분 단어를 상징적으로 표현해 메모리 내용을 설명할 수 있게 함

#### 4. 기계 번역 작업을 위한 데이터셋 (Datasets for Machine Translation Task)
TED, Winograd Schema Challenge(WSC), Open Subtitles, IT Documents 등 다양한 데이터셋을 통해 실험을 진행합니다.

**주요 기여 및 혁신성:**
- 데이터셋의 복잡성과 특성이 작업 기억에 미치는 영향 분석

#### 5. 메모리 내용 연구 (Study of Memory Content)
기계 번역 작업을 수행하는 동안 작업 기억에 저장된 내용이 번역 문장과 관련 있는지를 분석합니다. 키워드 발생 빈도를 통해 번역 텍스트의 복잡성과 메모리 다변성과의 상관관계를 조사합니다.

**주요 기여 및 혁신성:**
- 번역 문장 키워드가 작업 기억에 얼마나 자주 발생하는지 분석
- 메모리 내용의 다양성과 복잡한 텍스트의 관계 파악

#### 6. 결론 (Conclusion)
본 연구는 트랜스포머 아키텍처의 신경 작업 기억 요소의 특성을 탐구하고, 작업 기억이 모델 예측에 유용한지를 분석합니다. 다양한 영역의 텍스트에서 작업 기억 다변성이 낮은 수준으로 나타났습니다.

**주요 기여 및 혁신성:**
- 작업 기억이 기계 번역 문제 해결에 어떻게 유용한지를 입증
- 더욱 다양한 텍스트에서 작업 기억의 특성 및 관련성 분석

### 전체 요약
이 논문은 트랜스포머 디코더에 상징적 작업 기억을 도입하여 기계 번역 성능을 향상시키는 방법을 제안합니다. 트랜스포머 디코더는 번역 과정에서 중요한 정보를 작업 기억에 저장하고 이를 바탕으로 더 정확한 예측을 수행합니다. 다양한 데이터셋을 통해 실험을 진행한 결과, 복잡한 텍스트일수록 작업 기억의 다변성이 높아지는 경향을 확인했으며, 이는 작업 기억이 모델의 예측 품질을 향상시키는 데 중요한 역할을 함을 보여줍니다. 연구 결과는 상징적 작업 기억과 트랜스포머 모델의 조합이 기계 번역과 같은 자연어 처리 작업에 있어서 큰 잠재력을 가지고 있음을 나타냅니다.