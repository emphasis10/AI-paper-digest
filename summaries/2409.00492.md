# Accurate Compression of Text-to-Image Diffusion Models via Vector Quantization
## TL;DR
## Summary
- [https://arxiv.org/pdf/2409.00492.pdf](https://arxiv.org/pdf/2409.00492.pdf)

### 섹션별 요약

#### 1. 도입부
이 논문에서는 텍스트-이미지 변환을 위한 현대적인 확산 모델을 저비트 폭으로 압축하는 벡터 양자화 기법인 VQDM(Vector Quantization for Diffusion Models)을 소개합니다. 확산 모델의 특정한 아키텍처와 추론 과정을 고려하여, 벡터 양자화를 더 잘 적용할 수 있도록 조정했습니다. VQDM은 SDXL의 3-4 비트 압축에서 기존 방법을 능가하며, 3 비트로 압축된 모델이 이전 4 비트 양자화 방법과 동등한 성능을 보입니다.

#### 2. 관련 연구
양자화 방법은 대체로 사후-훈련 양자화(PTQ)와 양자화-인식 훈련(QAT)으로 나뉩니다. PTQ는 사전 훈련된 모델에 양자화 및 가중치 조정을 수행하여 비용이 적게 들지만, QAT는 여러 차례의 훈련과 양자화 반복이 필요하며 높은 정확도를 보입니다. 이 연구는 특히 효율적인 모델 압축을 위해 가중치 양자화에 중점을 두며, 기존 연구들과 달리 더 유연한 가중치 양자화 방법을 탐구합니다.

#### 3. 방법론

##### 3.1 텍스트-이미지 모델의 벡터 양자화
기존 벡터 양자화 알고리즘을 텍스트-이미지 변환 모델의 U-Net 아키텍처에 적용하는 전략을 소개합니다. 논문에서는 대규모 언어 모델과 달리, 텍스트-이미지 변환 모델이 여러 이질적인 구성 요소를 포함한다는 점을 강조하며, 이러한 차이를 반영한 양자화 방법을 제안합니다.

##### 3.2 벡터 양자화된 확산 모델의 보정
양자화된 가중치 구성을 최적화하는 보정 절차는 두 단계로 나뉩니다. 첫째, 소량의 보정 프롬프트에 대해 확산 샘플링을 실행하여 보정 세트를 수집합니다. 둘째, 모델의 각 레이어를 순차적으로 보정합니다. 모든 레이어를 한 번에 보정하는 방법과 각 레이어마다 데이터를 다시 수집하는 방법의 절충안으로, 특정 레이어 그룹을 한 번에 보정하는 방식을 사용합니다.

##### 3.3 추론 절차
양자화된 확산 모델의 추론 절차를 논의합니다. 이 절차는 모델의 성능을 유지하면서 압축된 모델로서의 기능을 확인하는 데 중점을 둡니다.

#### 4. 실험

##### 4.1 실험 설정 및 비교
이 논문에서는 SDXL-Turbo 모델의 여러 비트 폭 및 샘플링 단계에 대해 양자화된 모델을 평가합니다. 여러 자동화된 지표와 병렬 평가(SbS 점수)를 통해 양자화 설정의 성능을 비교합니다. 양자화 인식 세밀 조정(FT)을 통해 양자화 품질이 더욱 향상되는 것을 발견했습니다.

##### 4.2 양자화된 확산 모델의 성능
양자화된 VQDM 모델이 원본 모델과 비슷한 성능을 유지하면서 효율적으로 압축될 수 있음을 입증합니다.

#### 5. 결론
벡터 양자화 기법 VQDM을 통해 현대적인 텍스트-이미지 확산 모델을 저비트 폭으로 효율적으로 압축할 수 있음을 시사합니다. VQDM은 기존 방법을 능가하며, 저비트로도 높은 성능을 유지할 수 있음을 증명합니다.

### 전체 요약
이 논문은 텍스트-이미지 변환을 위한 현대적인 확산 모델을 저비트로 압축하는 VQDM(Vector Quantization for Diffusion Models)을 제안합니다. VQDM은 기존의 4 비트 양자화 방법을 능가하며, 특히 3 비트로도 동등한 성능을 보입니다. 양자화 알고리즘을 U-Net 아키텍처에 맞게 조정하고, 두 단계로 이루어진 보정 절차를 통해 양자화된 가중치를 효과적으로 최적화합니다. 실험 결과, VQDM은 여러 비트 폭에서도 원본 모델과 유사한 성능을 유지하여 효율적인 모델 압축을 가능하게 합니다.