# Accurate LoRA-Finetuning Quantization of LLMs via Information Retention
## TL;DR
## Summary
- [https://arxiv.org/pdf/2402.05445.pdf](https://arxiv.org/pdf/2402.05445.pdf)

이 논문에서는 정보 보존을 통한 LLM의 정밀 LoRA-파인튜닝 양자화 방법인 IR-QLoRA를 제안합니다. 주요 내용은 다음과 같습니다:

1. **서론**:
   - 대규모 언어 모델(LLMs)은 자연어 이해에서 강력한 성능을 보여주지만, 많은 계산 자원을 필요로 합니다.
   - 이러한 모델을 자원이 제한된 환경에서도 효율적으로 배포할 수 있도록 하는 기술 연구가 활발히 진행 중입니다.

2. **관련 작업**:
   - 기존의 양자화 기술은 LLM의 비트 너비를 줄여 저장 공간과 계산 비용을 절감하지만, 정확도가 크게 저하되는 문제가 있습니다.
   - LoRA-파인튜닝 양자화는 양자화된 LLM을 고효율로 파인튜닝하여 정확도를 높이는 방법으로 주목받고 있습니다.

3. **IR-QLoRA**:
   - **정보 보정 양자화(Information Calibration Quantization, ICQ)**: 엔트로피 최대화를 통해 원래의 정보를 보존하면서 LLM의 매개변수를 양자화합니다.
   - **정보 탄력 연결(Information Elastic Connection, IEC)**: LoRA를 활용하여 원래의 특성 정보를 다양화하여 변환하는 파라미터-프리 변환을 수행합니다.

4. **실험 및 결과**:
   - MMLU 벤치마크에서 IR-QLoRA는 기존의 양자화 방법보다 뛰어난 정확도를 보여주며, 특히 저비트 양자화(2-4 비트)에서도 우수한 성능을 나타냅니다.
   - 이 성능 향상은 0.31%의 추가 시간 소비만으로 달성됩니다.

5. **결론**:
   - IR-QLoRA는 자원 제한 환경에서 LLM의 정밀 양자화 및 파인튜닝을 가능하게 하며, 다양한 양자화 프레임워크와 호환됩니다.

이 연구는 LLM의 양자화와 파인튜닝을 결합하여 정확도를 유지하면서 모델의 효율성을 크게 향상시키는 새로운 접근 방식을 제시합니다.