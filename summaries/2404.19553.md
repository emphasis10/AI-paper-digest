# Extending Llama-3's Context Ten-Fold Overnight
## TL;DR
## Summary
- [https://arxiv.org/pdf/2404.19553.pdf](https://arxiv.org/pdf/2404.19553.pdf)

이 연구 논문에서는 기존의 Llama-3-8B-Instruct 모델의 컨텍스트 길이를 8K에서 80K로 확장하는 방법을 제시하고 있습니다. 이를 통해 모델은 광범위한 평가 작업에서 뛰어난 성능을 보이며, 동시에 짧은 컨텍스트에서의 기존 능력도 잘 유지하고 있습니다.

### 주요 내용 요약

1. **서론 및 배경**:
   - 긴 컨텍스트를 처리할 수 있는 대형 언어 모델에 대한 관심이 증가하고 있습니다. 본 논문에서는 GPT-4를 사용하여 3.5K개의 긴 컨텍스트 훈련 데이터를 생성하고, 이를 통해 모델의 컨텍스트 길이를 크게 확장하는 방법을 소개합니다.

2. **QLoRA를 이용한 효율적인 훈련**:
   - QLoRA(Q, K, V, O 프로젝션에 LoRA를 적용)를 사용하여 모델을 효율적으로 미세조정합니다. 이는 훈련 시간을 단 8시간 만에 완료할 수 있게 하며, 그 결과로 우수한 성능의 모델을 얻을 수 있습니다.

3. **실험 및 결과**:
   - 다양한 긴 컨텍스트 벤치마크에서 원래 모델과 비교하여 향상된 성능을 보여줍니다. 이는 모델이 80K 길이의 컨텍스트에서도 효과적으로 작동할 수 있음을 시사합니다.

### 혁신적인 부분
이 연구의 가장 큰 혁신은 매우 적은 수의 합성 훈련 샘플(3.5K개)을 사용하여 LLM의 컨텍스트 길이를 상당히 확장한 점입니다. 이는 LLM의 잠재력을 크게 활용할 수 있음을 보여줍니다. 또한, 이 모델, 훈련 데이터 및 코드를 공개하여 커뮤니티의 미래 연구를 촉진합니다.

이 논문은 긴 컨텍스트를 처리할 수 있는 대형 언어 모델의 개발 방향을 제시하며, 이는 다양한 NLP 작업에서 모델의 활용도를 크게 높일 수 있습니다.