# How faithful are RAG models? Quantifying the tug-of-war between RAG and LLMs’ internal prior
## TL;DR
## Summary
- [https://arxiv.org/pdf/2404.10198.pdf](https://arxiv.org/pdf/2404.10198.pdf)

### 요약본

#### 1. 서론
이 논문은 검색 보강 생성(RAG) 모델이 대형 언어 모델(LLMs)의 내부 지식(선행 지식)과 검색된 정보 사이의 긴장 관계를 어떻게 처리하는지 분석합니다. 특히, LLM이 잘못된 정보를 검색했을 때 이를 무시하고 올바른 답변을 제공할 수 있는지, 혹은 잘못된 정보를 반복할지에 대해 연구합니다.

#### 2. 방법론
연구자들은 GPT-4를 포함한 여러 LLM에 대해 검색 보강 질문응답(RAG QA) 능력을 평가합니다. 이를 위해 여러 도메인에서 1,294개의 질문을 수집하고, 참조 문서에 다양한 수준의 수정을 가한 다음 이러한 수정이 모델의 답변에 어떤 영향을 미치는지 관찰합니다.

#### 3. 결과
- **일치성**: RAG를 사용할 때 모델의 응답이 참조 답변과 일치하는 비율이 향상됩니다.
- **RAG 선호도**: 모델이 내부적으로 확신하는 응답일수록 RAG 정보를 따르지 않는 경향이 있습니다. 또한, 참조 정보가 모델의 선행 지식에서 크게 벗어날수록 모델은 참조 정보를 덜 선호합니다.
- **프롬프팅 전략의 영향**: 엄격하게 참조 문맥을 따르도록 요구하는 '엄격한' 프롬프트와, 모델이 참조 문맥을 해석하여 반응하도록 유도하는 '느슨한' 프롬프트를 비교했을 때, 느슨한 프롬프트에서 RAG 선호도가 더 낮게 나타났습니다.

#### 4. 논의
이 연구는 RAG 시스템이 LLM의 환각을 해결하는 데 도움이 될 수 있지만, 모델이 잘못된 정보를 어떻게 처리하는지에 대한 이해도 중요함을 강조합니다. 또한, RAG 시스템을 실제로 구현할 때 발생할 수 있는 다양한 문제점들에 대해 논의합니다.

#### 5. 부록
연구에 사용된 프롬프트의 세부 사항은 부록에 포함되어 있습니다.

### 종합적인 요약
이 논문은 대형 언어 모델이 검색 보강 생성(RAG) 시스템과 상호 작용할 때 발생하는 내부 지식과 검색된 정보 간의 긴장 관계를 조사합니다. 연구 결과에 따르면, 모델이 자신의 선행 지식에 확신을 가질수록 RAG 정보를 따르지 않는 경향이 있으며, 참조 정보가 모델의 선행 지식에서 크게 벗어날 경우 모델은 참조 정보를 덜 선호하는 것으로 나타났습니다. 이러한 발견은 RAG 시스템의 효과적인 구현과 관련된 중요한 고려 사항을 제공합니다.

## Similar Papers
- [Concise Thoughts: Impact of Output Length on LLM Reasoning and Cost](2407.19825.md)
- [Speculative RAG: Enhancing Retrieval Augmented Generation through Drafting](2407.08223.md)
- [RATT: A Thought Structure for Coherent and Correct LLM Reasoning](2406.02746.md)
- [RAFT: Adapting Language Model to Domain Specific RAG](2403.10131.md)
- [Towards Robust Evaluation: A Comprehensive Taxonomy of Datasets and Metrics for Open Domain Question Answering in the Era of Large Language Models](2406.13232.md)
- [GLiNER multi-task: Generalist Lightweight Model for Various Information Extraction Tasks](2406.12925.md)
- [RichRAG: Crafting Rich Responses for Multi-faceted Queries in Retrieval-Augmented Generation](2406.12566.md)
- [LLaMA-NAS: Efficient Neural Architecture Search for Large Language Models](2405.18377.md)
- [EXAONE 3.0 7.8B Instruction Tuned Language Model](2408.03541.md)
