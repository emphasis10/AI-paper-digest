# Whiteboard-of-Thought: Thinking Step-by-Step Across Modalities
## TL;DR
## Summary
- [https://arxiv.org/pdf/2406.14562.pdf](https://arxiv.org/pdf/2406.14562.pdf)

### 논문 요약

#### 1. 각 섹션별 요약

##### 소개 및 배경
논문에서는 사람들은 시각적 사고를 할 때 단순히 텍스트로만 생각하는 것이 아니라 이미지를 그리거나 머릿속에서 시각적인 요소를 구성하여 문제를 해결한다고 설명합니다. 그러한 시각적 사고를 멀티모달 대형 언어 모델(MLLM)에 적용하는 방안을 제안합니다.

##### 화이트보드-오브-사고(Whiteboard-of-Thought)
화이트보드-오브-사고(WoT)는 MLLM이 중간 사고 단계를 이미지로 표현하고 다시 모델로 보내 추가 분석할 수 있게 해주는 방법입니다. 이를 통해 MLLM이 시각적, 공간적 추론 문제를 해결하는 능력을 극대화할 수 있습니다. 주로 Matplotlib와 Turtle 같은 라이브러리의 코드를 작성하여 시각화를 합니다.

##### 실험 및 결과
논문에서는 WoT가 ASCII 아트 이해, 공간 추론 및 네비게이션 등 다양한 시각적 추론 문제에서 우수한 성능을 보임을 입증합니다. 특히 GPT-4를 사용한 실험에서는 텍스트 기반 체인의 사고(CoT)보다 월등히 높은 정확도를 나타냈습니다.

##### 제한 사항
화이트보드-오브-사고(WoT)는 현재의 시각적 인식 능력에 따라 성능이 크게 좌우됩니다. 예를 들어, 기하학적 도형을 이해하는 데 있어 현재의 MLLM은 한계가 있습니다. 이러한 문제는 컴퓨터 비전 기술이 발전함에 따라 개선될 것입니다.

#### 2. 종합 요약

화이트보드-오브-사고(WoT)는 멀티모달 대형 언어 모델(MLLM)의 시각적 추론 능력을 극대화하기 위한 혁신적인 방법입니다. 이 기법은 텍스트 기반의 중간 사고 과정을 그림으로 표현하고, 이를 모델이 다시 분석하여 문제를 해결하는 데 이용합니다. 논문은 이러한 방법이 다양한 시각적 추론 문제에서 우수한 성과를 내는 것을 입증하며, 현재 모델의 한계를 극복하기 위한 향후 발전 가능성을 제시합니다.

이 논문은 AI가 텍스트뿐만 아니라 이미지를 통한 사고 과정에서도 우수한 성능을 발휘할 수 있도록 하는 중요한 기여를 합니다. 향후 컴퓨터 비전 기술이 발전함에 따라, 이 방법은 더욱 많은 영역에서 활용될 수 있을 것입니다.

## Similar Papers
- [Visual Sketchpad: Sketching as a Visual Chain of Thought for Multimodal Language Models](2406.09403.md)
- [Tree of Thoughts: Deliberate Problem Solving with Large Language Models](2305.10601.md)
- [Gemini vs GPT-4V: A Preliminary Comparison and Combination of Vision-Language Models Through Qualitative Cases](2312.15011.md)
- [Visual Haystacks: Answering Harder Questions About Sets of Images](2407.13766.md)
- [Stream of Search (SoS): Learning to Search in Language](2404.03683.md)
- [Benchmarking Multi-Image Understanding in Vision and Language Models: Perception, Knowledge, Reasoning, and Multi-Hop Reasoning](2406.12742.md)
- [Faithful Logical Reasoning via Symbolic Chain-of-Thought](2405.18357.md)
- [LVLM-Interpret: An Interpretability Tool for Large Vision-Language Models](2404.03118.md)
- [Vision language models are blind](2407.06581.md)
