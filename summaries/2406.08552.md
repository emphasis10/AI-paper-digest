# DiTFastAttn: Attention Compression for Diffusion Transformer Models
## TL;DR
## Summary
- [https://arxiv.org/pdf/2406.08552.pdf](https://arxiv.org/pdf/2406.08552.pdf)

#### 1. 섹션별 요약
1. **소개 (Introduction)**
   - **내용 요약**: 최근 Diffusion Transformers(DiTs)는 이미지와 비디오 생성 분야에서 많은 인기를 얻고 있음. 그러나 이 모델들은 높은 해상도의 콘텐츠 생성 시 많은 연산량을 요구함. 이러한 문제를 해결하기 위해 DiTFastAttn이라는 후처리 압축 기법을 제안함.
   - **요약**: Diffusion 트랜스포머(DiTs)는 이미지와 비디오 생성에서 인기를 끌고 있지만, 많은 계산 비용이 든다는 단점이 있습니다. 본 논문에서는 DiTFastAttn이라는 후처리 압축 기법을 제안해 이 문제를 해결하고자 합니다.

2. **관련 연구 (Related Work)**
   - **내용 요약**: Diffusion 모델과 비전 트랜스포머의 압축 기법들(DynamicViT, FlashAttention 등)에 대해 다룸. 이전 연구들은 모델 재훈련이 필요했지만, 본 논문의 기법은 후처리 방식으로 재훈련 없이 적용 가능함.
   - **요약**: 기존의 Diffusion 모델들과 비전 트랜스포머 압축 기술에 대해 검토하며, 우리 모델은 재훈련 없이 후처리 방식으로 적용 가능하다는 점에서 차별화됩니다.

3. **방법론 (Method)**
   - **내용 요약**: DiTFastAttn은 3종류의 중복성(공간, 단계별, 조건 기반)을 줄이기 위한 기법 제안. 공간 중복을 줄이기 위한 윈도우 주의와 잔차 공유(Window Attention with Residual Sharing), 단계별 유사성을 활용한 AST, 조건 기반 유사성을 활용한 ASC가 있음.
   - **요약**: DiTFastAttn은 공간 중복, 단계별 중복, 조건 기반 중복성을 줄이는 여러 압축 기법을 사용합니다. 주요 기술로는 윈도우 주의와 잔차 공유, 단계 간 주의 공유, 조건 기반 주의 공유가 있습니다.

4. **실험 (Experiments)**
   - **내용 요약**: 세 가지 모델(DiT, PixArt-Sigma, OpenSora)에 대한 실험 결과를 제시. 다양한 설정에서 압축 효과 및 생성 품질에 대한 평가 수행. 
   - **결과 요약**: DiTFastAttn은 성능을 유지하면서도 최대 88%의 FLOPs 절감과 최대 1.6배 속도 향상을 달성.

5. **결론 (Conclusion)**
   - **내용 요약**: DiTFastAttn은 Diffusion 모델의 연산 병목 현상을 완화하는 후처리 압축 기법으로, 다양한 디노이징 단계와 CFG에서도 효과적임을 입증. 후속 연구에서는 훈련을 고려한 압축 방법 및 추가적인 모듈 적용을 포함할 예정.
   - **요약**: 후처리 압축 기법인 DiTFastAttn이 Diffusion 모델 성능 향상에 효과적임을 확인했습니다. 향후 연구에서는 훈련 기반 압축 방법과 다른 모듈에 대한 적용도 검토할 것입니다.

#### 2. 전체 요약
논문은 Diffusion Transformers(DiTs)의 연산 비용 문제를 해결하기 위해 DiTFastAttn이라는 후처리 압축 기법을 제안합니다. 이 기술은 주요한 세 가지 중복성(공간, 단계별, 조건 기반)을 줄이기 위한 다양한 압축 방법들을 사용하여, 성능 저하 없이 연산 비용을 크게 절감하고 속도를 향상시킵니다. 실험 결과, DiTFastAttn은 이미지와 비디오 생성에서 높은 해상도에서도 뛰어난 성능을 유지하면서 최대 88%의 연산 작업(FLOPs)을 줄이고 1.6배의 속도 향상을 달성했습니다. 이 기법은 후속 연구에서 다른 모듈들과 훈련 기반 압축 방법에서도 검토될 예정입니다.

## Similar Papers
- [MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression](2406.14909.md)
- [Align Your Steps: Optimizing Sampling Schedules in Diffusion Models](2404.14507.md)
- [IRASim: Learning Interactive Real-Robot Action Simulators](2406.14540.md)
- [LLaVA-PruMerge: Adaptive Token Reduction for Efficient Large Multimodal Models](2403.15388.md)
- [MotionClone: Training-Free Motion Cloning for Controllable Video Generation](2406.05338.md)
- [Fine-gained Zero-shot Video Sampling](2407.21475.md)
- [Diffusion-RWKV: Scaling RWKV-Like Architectures for Diffusion Models](2404.04478.md)
- [VIMI: Grounding Video Generation through Multi-modal Instruction](2407.06304.md)
- [FIFO-Diffusion: Generating Infinite Videos from Text without Training](2405.11473.md)
