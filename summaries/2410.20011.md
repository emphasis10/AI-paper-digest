# A Survey of Small Language Models
## TL;DR
## Summary
- [https://arxiv.org/pdf/2410.20011.pdf](https://arxiv.org/pdf/2410.20011.pdf)

#### **1. 논문의 주요 기여**
이 논문은 **Small Language Models (SLMs)**, 즉 소형 언어 모델에 관한 **종합적인 설문 조사**입니다. 주로 모델의 효율성을 극대화하기 위해 필요한 **모델 아키텍처, 학습 기술, 모델 압축**에 대해 다루고 있으며, 특히 다음과 같은 내용을 정리했습니다:

- **모델 최적화를 위한 새로운 분류 체계** 제안 (압축, 프루닝, 양자화 등)
- **SLM 성능 평가를 위한 벤치마크 데이터셋과 지표** 요약
- **소형 언어 모델의 한계와 연구 과제**에 대한 분석 및 미래 연구 방향 제시

---

#### **2. 각 섹션의 내용 요약**

##### **2.1 도입 (Introduction)**
- **SLM 등장 배경**: 대형 언어 모델(LLM)은 높은 성능을 자랑하지만 **막대한 자원**이 필요하여, **모바일 장치 및 엣지 컴퓨팅**에 적합하지 않습니다. 이에 대한 해결책으로 **SLM**이 주목받고 있습니다.
- SLM의 목표는 **성능과 자원 제약 간의 균형**을 맞추는 것입니다. LLM의 효율성 및 정확성을 유지하되, 메모리, 연산량, 비용을 줄이기 위해 노력합니다.
- **SLM의 정의와 기준**은 시기와 맥락에 따라 변화하며, 예를 들어 GPT-2는 2019년 당시 대형 모델로 간주되었으나 현재 기준으로는 소형에 해당될 수 있습니다.

---

##### **2.2 모델 아키텍처 (Model Architectures)**
- **경량화 아키텍처**: MobileBERT, DistilBERT와 같은 모델들은 BERT의 성능을 유지하면서 **모델 크기와 속도를 대폭 개선**합니다. 예를 들어, MobileBERT는 4.3배 작은 크기와 5.5배 빠른 속도를 자랑합니다.
- **효율적인 자기-주의 메커니즘**: Reformer와 같은 모델은 **O(N²)**의 계산 복잡도를 **O(N log N)**으로 줄여 **메모리와 시간 효율성을 극대화**합니다.
- **신경망 아키텍처 탐색(NAS)**: 자동화된 탐색 기법을 통해 작업에 맞는 최적화된 경량 모델을 설계합니다. 예를 들어, MobileLLM은 모델의 깊이와 너비를 조절하여 성능을 극대화합니다.

---

##### **2.3 학습 기술 (Training Techniques)**
- **혼합 정밀도 학습**: FP16과 BFLOAT16 같은 저정밀도 형식을 사용해 학습 속도와 메모리 효율성을 높입니다. 최신 GPU에서는 FP8까지 지원하며 더욱 효율적인 연산이 가능합니다.
- **파라미터 효율적 미세 조정(PEFT)**: LoRA와 같은 기법을 사용해 **일부 파라미터만 업데이트**하고 대부분은 고정하여 미세 조정 비용을 줄입니다.
- **데이터 증강**: AugGPT, Evol-Instruct 등은 데이터 품질과 다양성을 높여 **모델의 일반화 성능을 개선**합니다.

---

##### **2.4 모델 압축 기법 (Model Compression Techniques)**
- **프루닝(Pruning)**: 불필요한 가중치를 제거해 모델을 경량화합니다. 예를 들어, SparseGPT는 **희소 회귀를 통해 대규모 모델의 일부 가중치만 남깁니다**.
- **양자화(Quantization)**: FP32 대신 INT8 등의 정수 연산을 사용하여 **메모리 사용량을 줄이고 속도를 높입니다**.
- **지식 증류(Knowledge Distillation)**: 대형 모델의 지식을 소형 모델에 전이합니다. 예를 들어, BabyLLaMA는 다수의 대형 모델로부터 지식을 증류하여 작은 모델에 탑재했습니다.

---

##### **2.5 평가 및 벤치마크 (Evaluation & Benchmarks)**
- **SLM 성능 평가 지표**: 
  - **지연 시간**: SuperGLUE, SQuAD 같은 데이터셋을 사용해 모델의 응답 속도를 측정합니다.
  - **메모리 사용량**: 메모리 피크와 압축 비율을 평가합니다.
  - **프라이버시**: PrivacyGLUE와 같은 데이터셋을 통해 차등 프라이버시 성능을 측정합니다.
  - **에너지 효율성**: 에너지 효율 비율(EER)로 모델의 전력 소비를 평가합니다.

---

##### **2.6 응용 분야 (Applications)**
- **실시간 상호작용**: 모바일 및 음성 인터페이스, 번역 등에서 **실시간 응답**을 제공하는 것이 중요합니다.
- **콘텐츠 생성**: 텍스트 요약, 감성 분석, 자동 완성 등에서 **짧은 지연 시간**과 **낮은 메모리 사용량**이 요구됩니다.
- **프라이버시 보호**: 의료, 금융 등 민감한 데이터를 다룰 때 **프라이버시 보장**이 필수적입니다.

---

##### **2.7 연구 과제 및 도전 과제 (Open Challenges)**
- **환각 현상(Hallucination)**: LLM과 SLM 모두 **허위 정보**를 생성할 가능성이 있으며, 이로 인한 피해를 줄이는 것이 과제입니다.
- **편향(Bias)**: 대형 모델은 학습 데이터에 내재된 편향을 반영할 수 있으며, 이를 해결하기 위한 연구가 필요합니다.
- **에너지 효율성**: 배터리로 구동되는 장치에서 사용되는 SLM은 **추론 시 에너지 소비를 줄이는 것**이 중요합니다.
- **프라이버시 보호**: 사용자의 입력이나 학습 데이터가 노출되지 않도록 하는 **프라이버시 강화 기법** 개발이 필요합니다.

---

#### **3. 논문의 전체 요약**
이 논문은 **대형 언어 모델의 한계를 극복하기 위해** 등장한 **소형 언어 모델(SLM)**에 대한 종합적인 조사를 제공하며, 효율적인 **모델 설계, 학습 및 압축 기법**을 소개합니다. SLM의 **경량화와 프라이버시 보호** 측면에서의 강점을 살리면서도, **모델 성능을 유지**하는 것이 주요 목표입니다. 또한 SLM이 사용될 수 있는 다양한 응용 분야와 **앞으로 해결해야 할 연구 과제**를 제시하며, 향후 연구 방향을 이끕니다.

---

#### **4. 논문의 주요 목적**
이 논문의 주된 목적은 **SLM의 최신 동향과 기술들을 체계적으로 정리하여 연구자와 개발자에게 유용한 자료를 제공**하는 것입니다. 특히 **모델의 경량화와 효율성**에 초점을 맞추고, **모바일 및 엣지 환경**에서 사용할 수 있는 실용적인 솔루션을 제시합니다. 이를 통해 연구자들이 **미래의 인공지능 개발**을 주도할 수 있도록 돕는 것을 목표로 합니다.