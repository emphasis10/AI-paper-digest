# Scaling Laws with Vocabulary: Larger Models Deserve Larger Vocabularies
## TL;DR
## Summary
- [https://arxiv.org/pdf/2407.13623.pdf](https://arxiv.org/pdf/2407.13623.pdf)

### 1. 각 섹션 요약

#### 서론 (Introduction)
이 논문은 대형 언어 모델(LLM)을 확장할 때 어휘 크기의 중요성을 연구합니다. 기존 연구들은 주로 모델 파라미터와 학습 데이터 양에 집중했지만, 어휘 크기도 모델 성능에 상당한 영향을 미친다고 주장합니다. 이 연구는 다양한 어휘 구성을 사용하여 모델을 학습한 후, 최적의 어휘 크기를 예측하는 세 가지 접근법을 제안합니다.

#### 사전 지식 (Preliminary)
FLOPs(뜯어보기 작업 수)에 따라 어휘 크기가 LLM의 성능에 어떻게 영향을 미치는지를 이해하기 위해 필요한 배경 정보를 제공합니다. 또한, 모델의 파라미터 수와 학습 데이터 양과의 관계를 설명합니다.

#### 확장 법칙 (Scaling Law)
다양한 모델 크기와 어휘 크기를 사용한 실험을 통해, 어휘 크기와 FLOPs 간의 관계를 분석합니다. FLOPs가 증가함에 따라 최적의 어휘 크기도 증가한다는 것을 발견했습니다. 최적의 어휘 크기는 모델의 나머지 파라미터 수보다 더 천천히 증가해야 한다고 주장합니다.

#### 분석 1: 고정된 정규화 손실의 관점 (Analysis 1: The Perspective of Fixed Normalized Loss)
어휘 크기를 증가시키면 처음에는 성능이 향상되지만, 어느 순간부터는 성능이 감소하게 됩니다. 이는 작은 어휘 크기에서는 토큰화를 통해 성능이 매우 향상되지만, 어휘 크기가 너무 커지면 희귀한 단어들에 대한 학습이 불충분해지기 때문입니다.

#### 분석 2: 고정된 FLOP 예산의 관점 (Analysis 2: The Perspective of Fixed FLOP Budget)
고정된 FLOP 예산 내에서 어휘 크기를 변화시키면서 손실을 조사한 결과, 더 많은 계산 자원이 주어지면 LLM은 더 큰 어휘를 효과적으로 활용하여 손실을 줄일 수 있습니다. 그러나 어휘 크기가 너무 커지면 다시 성능이 저하되는 최적 지점이 존재합니다.

#### 분석 3: 파라미터 증가의 관점 (Analysis 3: The Perspective of Parameter Growing)
모델 파라미터를 늘리는 전통적인 방법들(깊이 또는 너비 증가)과 현재의 경험적 관행을 비교합니다. 어휘 파라미터는 주로 너비를 통해서만 확장될 수 있기 때문에, 파라미터의 균형 잡힌 성장을 유지하려면 어휘 크기도 함께 확장해야 한다고 주장합니다.

#### 결론 및 논의 (Discussion and Conclusion)
본 연구는 어휘 크기와 모델 성능 간의 최적 관계를 규명하고 그 예측 방법을 제안합니다. 이러한 예측을 검증하기 위해 다양한 모델 크기와 FLOPs 예산 하에서의 실험을 수행하였습니다. 추가적으로, 멀티모달 및 다국어 시나리오로의 확장 가능성을 논의합니다. 어휘 크기를 최적화하면 더 가볍고 비용 효율적인 대형 언어 모델을 만들 수 있다는 점에서 큰 사회적 영향을 미칠 수 있음을 강조합니다.

### 2. 전체 요약

이 논문은 대형 언어 모델(LLM)의 성능을 최적화하기 위해 어휘 크기의 중요성을 탐구합니다. 연구자들은 어휘 크기가 너무 작거나 너무 크면 성능이 저하된다는 점을 발견하고, 최적의 어휘 크기를 예측하기 위한 세 가지 접근법을 제안합니다. 실험 결과, 더 큰 FLOPs 예산이 주어지면 LLM은 더 큰 어휘를 효과적으로 활용할 수 있지만, 그에도 최적의 지점이 존재한다는 것을 확인했습니다. 이 연구는 모델 파라미터와 어휘 크기를 함께 고려하여 LLM을 효율적으로 확장하는 방법에 대한 새로운 이해를 제공합니다. 이로 인해 더 가볍고 비용 효율적인 대형 언어 모델의 개발이 가능해질 것입니다.

## Similar Papers
- [Stacking Your Transformers: A Closer Look at Model Growth for Efficient LLM Pre-Training](2405.15319.md)
- [RegMix: Data Mixture as Regression for Language Model Pre-training](2407.01492.md)
- [Bootstrapping Language Models with DPO Implicit Rewards](2406.09760.md)
- [Physics of Language Models: Part 3.3, Knowledge Capacity Scaling Laws](2404.05405.md)
- [Chinchilla Scaling: A replication attempt](2404.10102.md)
- [JailbreakLens: Visual Analysis of Jailbreak Attacks Against Large Language Models](2404.08793.md)
- [AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration](2306.00978.md)
- [Compression Represents Intelligence Linearly](2404.09937.md)
- [Beyond Scaling Laws: Understanding Transformer Performance with Associative Memory](2405.08707.md)
