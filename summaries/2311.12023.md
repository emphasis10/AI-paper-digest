# LQ-LoRA: Low-rank Plus Quantized Matrix Decomposition for Efficient Language Model Finetuning
## TL;DR
## Summary
- [https://arxiv.org/pdf/2311.12023.pdf](https://arxiv.org/pdf/2311.12023.pdf)

### 섹션 요약

#### 1. 요약
이 논문은 대형 언어 모델(LLM)의 효율적인 적응을 위한 방법을 제안합니다. 제안된 방법은 각 사전 학습된 행렬을 높은 정밀도의 저랭크(low-rank) 성분과 메모리 효율적인 양자화된 성분으로 분해하는 단순한 알고리즘을 사용합니다. 미세 조정(finetuning) 중에는 양자화된 성분은 고정되고 저랭크 성분만이 업데이트됩니다. 정수 선형 계획법을 이용하여 양자화 성분의 동적 구성을 가능하게 하여 주어진 메모리 예산에 맞출 수 있습니다. 또한, Fisher 정보 행렬의 근사를 사용하여 데이터 인식 알고리즘을 탐색합니다.

#### 2. 소개 (Introduction)
LLM의 전체 미세 조정은 데이터셋 적응에 많은 비용이 듭니다. 메모리 효율적인 미세 조정 방법으로 저랭크 어댑테이션(LoRA)이 주목받고 있습니다. 이 논문은 이러한 메모리 효율성을 더욱 향상시키기 위해 양자화된 저랭크 행렬 분해(LQ-LoRA)를 제안합니다.

#### 3. 배경 (Background)
기존의 LoRA는 모델의 미세 조정 시 메모리 사용량을 줄이기 위해 저랭크 행렬로 사전 학습된 모델의 가중치를 재구성합니다. 그러나, 매우 낮은 비트로 양자화된 모델의 경우 LoRA의 초기화가 최적이 아닐 수 있습니다.

#### 4. 방법 (Method)
LQ-LoRA는 다음 세 가지 주요 단계로 구성됩니다:
1. **저랭크 + 양자화 행렬 분해**: 사전 학습된 행렬을 저랭크 성분과 양자화된 성분으로 분해.
2. **혼합 구성 양자화**: 정수 선형 계획법을 통해 각 행렬에 대해 다른 양자화 구성을 할당.
3. **데이터 인식 행렬 분해**: Fisher 정보 행렬을 사용하여 재구성 목표를 가중치로 수정.

#### 5. 실험 연구 (Empirical Study)
RoBERTa와 LLaMA-2 모델을 사용하여 LQ-LoRA의 성능을 검증했습니다. 실험 결과, LQ-LoRA는 QLoRA와 GPTQ-LoRA를 능가하는 성능을 보여주며, 메모리 예산에 따라 유연하게 조정할 수 있습니다.

#### 6. 토론 및 한계 (Discussion and Limitations)
LQ-LoRA의 단순 반복 알고리즘은 실용적이지만, 이론적으로 더 정교한 최적화 알고리즘으로 확장할 가능성이 있습니다. 또한, 이 방법은 저랭크 업데이트를 기반으로 하기 때문에 다른 파라미터 효율적인 미세 조정 방법에는 일반적으로 적용되지 않습니다.

#### 7. 결론 (Conclusion)
LQ-LoRA는 저랭크와 양자화된 성분으로 사전 학습된 행렬을 분해하는 간단한 확장 방법을 제안합니다. 이 방법은 강력한 베이스라인 대비 의미 있는 개선을 보여주며, 매우 낮은 비트로의 양자화에서도 성능 저하가 거의 없습니다.

### 전체 요약
이 논문은 대형 언어 모델(LLM)의 메모리 효율적인 적응을 위해 저랭크와 양자화된 행렬 성분으로 사전 학습된 행렬을 분해하는 새로운 방법인 LQ-LoRA를 제안합니다. LQ-LoRA는 정수 선형 계획법을 통해 각 행렬에 대해 동적으로 양자화 구성을 할당할 수 있으며, Fisher 정보 행렬을 사용하여 데이터 인식 알고리즘을 구현합니다. 실험 결과, 이 방법은 기존의 QLoRA와 GPTQ-LoRA 대비 성능이 우수하며, 매우 낮은 비트로의 양자화에서도 성능 저하가 적습니다. LQ-LoRA는 메모리 효율성을 극대화하면서도 높은 성능을 유지하는 데 기여할 수 있습니다.

## Similar Papers
- [Fast Matrix Multiplications for Lookup Table-Quantized LLMs](2407.10960.md)
- [ZeroQuant-FP: A Leap Forward in LLMs Post-Training W4A8 Quantization Using Floating-Point Formats](2307.09782.md)
- [GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection](2403.03507.md)
- [LLM-FP4: 4-Bit Floating-Point Quantized Transformers](2310.16836.md)
- [Accurate LoRA-Finetuning Quantization of LLMs via Information Retention](2402.05445.md)
- [OmniQuant: Omnidirectionally Calibrated Quantization for Large Language Models](2308.13137.md)
- [Efficient Streaming Language Models with Attention Sinks](2309.17453.md)
- [PiSSA: Principal Singular Values and Singular Vectors Adaptation of Large Language Models](2404.02948.md)
- [ZeroQuant-V2: Exploring Post-training Quantization in LLMs from Comprehensive Study to Low Rank Compensation](2303.08302.md)
