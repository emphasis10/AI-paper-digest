# Audio Conditioning for Music Generation via Discrete Bottleneck Features
## TL;DR
## Summary
- [https://arxiv.org/pdf/2407.12563.pdf](https://arxiv.org/pdf/2407.12563.pdf)

### 주요 내용 요약

#### 1. 서론 (Introduction)
본 논문은 기존의 음악 생성 모델이 주로 텍스트 또는 파라메트릭 조건(예: 템포, 하모니, 음악 장르)을 사용하는 것과 달리, 오디오 입력을 조건으로 사용하는 새로운 접근 방식을 제안합니다. 이 연구는 '텍스트 인버전'이라는 사전 훈련된 텍스트-음악 모델을 이용하여 오디오 입력을 텍스트 임베딩 공간의 '가상 단어'로 변환하는 방법과, 텍스트 조건화 및 양자화된 오디오 특징 추출기와 함께 음악 언어 모델을 처음부터 학습시키는 방법을 포함합니다.

#### 2. 관련 연구 (Related Work)
이 섹션에서는 텍스트-음악 생성 모델 관련 연구와 음악 생성 모델에 대한 조건화 방법에 대해 개괄적으로 설명합니다. 예를 들어, CLAP 및 MuLan과 같은 사전 훈련된 텍스트-음악 모델을 사용하여 많은 비주석 오디오 데이터를 학습하고, 추론 시 텍스트를 사용하는 방법 등이 있습니다.

#### 3. 텍스트 인버전 방법 (Textual Inversion Method)
텍스트 인버전 방법은 오토리그레시브 모델링을 이용하여 다음 토큰의 조건부 분포를 추정하고, 텍스트 임베딩을 최적화함으로써 오디오을 텍스트 임베딩 공간으로 매핑합니다. 이 방법은 수백 번의 최적화 단계를 거쳐 텍스트 프롬프트를 생성한 후 음악을 생성하는데, 효율성이 떨어지는 단점이 있습니다.

#### 4. 스타일 조건화 방법 (Style Conditioning Method)
스타일 조건화 모듈을 설계하여 텍스트-음악 MusicGen 모델과 함께 학습시켰습니다. 이 모듈은 몇 초의 오디오를 받아 특징을 추출한 뒤, 이러한 오디오와 텍스트를 혼합하여 음악을 생성할 수 있습니다. 또한, Residual Vector Quantizer (RVQ)의 잔여 스트림 수를 조절하여 조건화 강도를 조절할 수 있습니다. 이를 통해 스타일 조건화가 너무 많은 정보를 전달하지 않도록 정보 병목을 설정하여 최적의 조건화를 달성할 수 있습니다.

#### 5. 실험 결과 (Experimental Results)
다양한 하이퍼파라미터 설정을 통해 텍스트 인버전 방법과 스타일 조건화 방법을 평가했습니다. 또한, 주관적인 평가와 KNN 기반 메트릭, Fréchet Audio Distance (FAD), KL-divergence 등의 객관적인 메트릭을 사용하여 모델의 성능을 측정했습니다. 이 과정에서 스타일 오디오와의 일치성을 잘 유지하면서도 적절한 변동을 제공하는 우리의 방식이 가장 우수한 성능을 보였습니다.

#### 6. 결론 (Conclusion)
스타일 조건화를 통해 텍스트-음악 생성 모델을 보다 유연하고 창의적으로 사용할 수 있는 가능성을 입증했습니다. 새로운 더블 클래스프리 가이던스 방법을 통해 오디오와 텍스트 조건을 혼합하여 균형을 맞출 수 있으며, 이를 통해 다양한 조건을 가진 생성 모델에서도 활용할 수 있는 가능성을 제시했습니다.

### 전체 요약
이 논문은 기존 음악 생성 모델의 한계를 극복하기 위해 오디오 입력을 조건으로 사용하는 두 가지 접근 방식을 제안합니다. 첫 번째는 텍스트 인버전 방법을 통해 오디오를 텍스트 임베딩 공간으로 변환하여 음악을 생성하는 것이며, 두 번째는 텍스트와 오디오 조건화를 혼합하여 사용할 수 있는 스타일 조건화 모듈을 개발하는 것입니다. 이를 통해 생성된 음악이 원래 스타일과 잘 맞으면서도 일정한 변동성을 가지도록 하는 데 성공했습니다. 향후 연구에서는 이러한 조건화 방법을 다양한 생성 모델에 적용함으로써 음악 생성 도구의 창의성과 유연성을 크게 향상시킬 수 있을 것으로 기대됩니다.

## Similar Papers
- [MusiConGen: Rhythm and Chord Control for Transformer-Based Text-to-Music Generation](2407.15060.md)
- [Futga: Towards Fine-grained Music Understanding through Temporally-enhanced Generative Augmentation](2407.20445.md)
- [SoundCTM: Uniting Score-based and Consistency Models for Text-to-Sound Generation](2405.18503.md)
- [DITTO-2: Distilled Diffusion Inference-Time T-Optimization for Music Generation](2405.20289.md)
- [Invertible Consistency Distillation for Text-Guided Image Editing in Around 7 Steps](2406.14539.md)
- [FontStudio: Shape-Adaptive Diffusion Model for Coherent and Consistent Font Effect Generation](2406.08392.md)
- [JEN-1 DreamStyler: Customized Musical Concept Learning via Pivotal Parameters Tuning](2406.12292.md)
- [Naturalistic Music Decoding from EEG Data via Latent Diffusion Models](2405.09062.md)
- [Proofread: Fixes All Errors with One Tap](2406.04523.md)
