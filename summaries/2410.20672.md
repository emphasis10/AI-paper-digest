# Relaxed Recursive Transformers: Effective Parameter Sharing with Layer-wise LoRA
## TL;DR
## Summary
- [https://arxiv.org/pdf/2410.20672.pdf](https://arxiv.org/pdf/2410.20672.pdf)

### 1. 섹션별 요약

#### 서론
이 논문은 대형 언어 모델(LLM)의 효율적인 배치를 위해 성능과 자원 간의 균형을 맞추는 방법을 제시합니다. 대형 모델은 성능이 뛰어나지만, 많은 메모리와 계산 비용이 들기 때문에 이를 효과적으로 줄이는 방법으로 파라미터 공유를 강조합니다.

#### 주요 기여
논문은 Recursive Transformers라는 새롭고 효율적인 모델 구조를 제안하여, 기존 모델 대비 성능 손실이 거의 없는 상태에서 파라미터를 공유하며 크기를 줄일 수 있도록 합니다. 이를 통해 적은 자원으로도 높은 성능을 유지할 수 있습니다.

#### 모델 압축과 초기화 기법
파라미터를 거의 공유하지 않는 상태에서도 성능을 개선할 수 있도록 Recursive Transformers를 도입하고, 모델의 루프 레이어를 초기화하는 효과적인 기술을 소개합니다. 이 방법은 메모리 사용량을 줄이면서 성능을 최적화하는 데 도움이 됩니다.

#### 실험과 결과
Gemma 모델과 같은 대형 LLM을 Recursive 구조로 변환하였을 때, 적은 양의 트레이닝 데이터를 사용해도 원본 모델의 성능에 근접할 수 있음을 입증했습니다. 이 과정에서 'Stepwise'라는 초기화 방법은 최적의 정확도를 도출해냈습니다.

#### 미래 연구 방향
논문은 아직 이론적인 가설에 초점을 맞추었으며, 실제 환경에서의 응용 가능성을 높이기 위한 추가 연구의 필요성을 제안합니다. 또한, 이 방법론을 더 큰 모델을 포함하도록 확장할 가능성을 염두에 두고 있습니다.

### 2. 전체 요약
이 논문은 큰 언어 모델(LLM)의 효율성을 더 높이기 위한 새로운 접근법을 제시합니다. Recursive Transformers라는 혁신적 모델을 도입하여, 기존 대비 적은 자원으로도 높은 성능을 유지할 수 있도록 설계되었습니다. 이 모델은 주로 파라미터 공유를 통해 메모리 사용량을 줄이고, 새로운 초기화 기법을 이용해 모델의 성능을 극대화합니다. 실험 결과, 적은 데이터 양으로도 기존 큰 모델에 근접하는 성능을 얻었으며, 이는 큰 LLM을 효율적으로 운영할 수 있는 새로운 가능성을 열어주었습니다. 앞으로는 이 구조를 더 큰 모델과 더 많은 상황에 적용할 수 있도록 연구를 계속할 예정입니다.