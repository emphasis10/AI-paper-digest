# TransformerFAM: Feedback attention is working memory
## TL;DR
## Summary
- [https://arxiv.org/pdf/2404.09173.pdf](https://arxiv.org/pdf/2404.09173.pdf)

본 문서는 인공지능(AI) 및 기계학습 분야의 논문, 특히 "TransformerFAM: Feedback attention is working memory"을 다루고 있습니다. 이 논문에서는 트랜스포머(Transformer) 아키텍처의 발전과 함께, 주의력(Attention) 매커니즘의 중요성과 그 한계를 지적하며, 이를 극복하기 위한 새로운 아키텍처인 피드백 주의 메모리(Feedback Attention Memory, FAM)를 제안합니다. 아래는 간략한 내용 요약입니다:

### 1. 서론 및 개요

트랜스포머 아키텍처는 입력의 길이가 늘어날수록 복잡도가 기하급수적으로 증가하는 문제가 있습니다. 이 문제를 해결하고자 피드백 주의 메모리(FAM)라는 새로운 구조를 제안합니다. 이 구조는 네트워크가 자신의 잠재적 표현(latent representations)에 주의를 기울일 수 있도록 하는 피드백 루프를 활용합니다. 이런 설계는 워킹 메모리(작업 기억)의 형성을 촉진시키며, 무한히 긴 시퀀스를 처리할 수 있게 합니다.

### 요약

이 논문에서는 기존 트랜스포머 아키텍처의 한계를 극복하기 위해 새로운 구조인 TransformerFAM(Feedback Attention Memory)을 제안합니다. 이 아키텍처는 피드백 루프를 통해 자신의 잠재적 표현에 주의를 기울이는 것을 가능하게 하여 작업 메모리의 등장을 촉진합니다. 이를 통해 무한히 긴 입력 시퀀스를 처리할 수 있습니다. 이 모델은 추가적인 가중치 없이 기존의 사전 훈련된 모델들과의 호환성을 유지하면서 성능을 크게 향상시킵니다.

#### 섹션별 요약:

1. **서론**:
   - 트랜스포머 아키텍처의 발전 및 영향력을 소개하며, 기존의 한계점인 장기적 맥락 처리의 어려움을 지적합니다.
   - TransformerFAM 아키텍처가 이러한 문제를 해결할 수 있는 새로운 접근 방식을 제시합니다.

2. **TransformerFAM**:
   - 피드백 주의 메모리(FAM)의 개념과 이를 구현하는 데 사용된 기술적인 세부사항을 설명합니다.
   - 피드백 루프를 통해 자신의 잠재 표현에 주의를 기울이도록 하여, 트랜스포머 내에 작업 기억이 형성되게 합니다. 
   - TransformerFAM은 추가적인 가중치 없이 사전 학습된 모델과의 원활한 통합을 가능하게 합니다.
   - FAM은 트랜스포머의 각 블록 사이에서 글로벌 맥락 정보를 유지하고 전달하는 역할을 합니다.
   - 무한히 긴 입력을 처리하는 능력뿐만 아니라 장기적인 의존성을 보존하면서 트랜스포머의 성능을 크게 향상시키는 방법을 제시합니다.

3. **실험**:
   - 다양한 모델 크기(1B, 8B, 24B)에서의 장문 맥락 태스크에서 TransformerFAM의 성능을 평가하고 기존 모델과 비교합니다.
   - FAM이 장기적인 정보를 효과적으로 압축하고 기억하여 성능 향상에 기여함을 보여줍니다.

4. **논의**:
   - TransformerFAM이 어떻게 기존의 한계를 극복하고 있는지, 그리고 이것이 언어 모델링 분야에서 가질 수 있는 잠재적인 영향을 논의합니다.

5. **결론**:
   - TransformerFAM은 기존 트랜스포머 모델이 직면한 문제들을 해결할 수 있는 유망한 솔루션으로서, 더욱 복잡하고 긴 맥락을 처리할 수 있는 능력을 제공합니다.

#### 전체 요약:

이 논문은 트랜스포머 아키텍처의 한계를 극복하고자 TransformerFAM을 제안하며, 이를 통해 트랜스포머가 무한히 긴 시퀀스를 처리할 수 있도록 합니다. TransformerFAM은 작업 메모리의 개념을 도입하여 입력의 글로벌 맥락을 유지하며, 이를 통해 더욱 긴 맥락의 정보를 효과적으로 처리할 수 있습니다. 실험 결과는 이 아키텍처가 기존 모델 대비 우수한 성능을 보임을 확인시켜 줍니다.

## Similar Papers
- [Associative Recurrent Memory Transformer](2407.04841.md)
- [BioMamba: A Pre-trained Biomedical Language Representation Model Leveraging Mamba](2408.02600.md)
- [Complexity of Symbolic Representation in Working Memory of Transformer Correlates with the Complexity of a Task](2406.14213.md)
- [Sparser is Faster and Less is More: Efficient Sparse Attention for Long-Range Transformers](2406.16747.md)
- [FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning](2307.08691.md)
- [Enhancing LLM's Cognition via Structurization](2407.16434.md)
- [Samba: Simple Hybrid State Space Models for Efficient Unlimited Context Language Modeling](2406.07522.md)
- [You Only Cache Once: Decoder-Decoder Architectures for Language Models](2405.05254.md)
- [MambaVision: A Hybrid Mamba-Transformer Vision Backbone](2407.08083.md)
