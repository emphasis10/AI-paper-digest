# Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity
## TL;DR
## Summary
- [https://arxiv.org/pdf/2101.03961.pdf](https://arxiv.org/pdf/2101.03961.pdf)

### 1. 섹션 별 요약 (논문의 주요 기여와 혁신적인 부분)
#### 1.1 Introduction (소개)
논문은 기본적으로 깊은 학습(Deep Learning) 모델이 입력에 동일한 파라미터를 사용하는 것 대신, Mixture of Experts (MoE) 모형을 사용하여 각 입력 예제에 다른 파라미터를 선택하는 Switch Transformer라는 모델을 제안합니다. 이는 대규모 희소 모델을 만들면서도 일정한 계산 비용을 유지하며, 복잡성, 통신 비용, 훈련 불안정성을 극복할 수 있습니다.

#### 1.2 Switch Transformer
Switch Transformer의 디자인 원리는 단순하고 계산 효율적인 방법으로 변환기 모델의 파라미터 수를 극대화하는 것입니다. MoE의 라우팅 알고리즘을 간소화하고, 통신 및 계산 비용을 줄인 모델을 제안합니다. 또한, bfloat16 형식을 사용해 훈련 불안정성을 완화하고, T5-Base 및 T5-Large 모형에 비해 훈련 속도를 7배까지 증가시키며 동일한 계산 자원을 사용합니다.

#### 1.3 Scaling Properties (확장 특성)
모델의 확장 특성을 연구하며, 고정된 계산 예산 내에서 큰 희소 모델을 훈련할 수 있도록 합니다. 전문가 수를 증가시킴으로써 계산 비용을 유지하면서 모델의 파라미터 수를 확장할 수 있습니다. 또한, 희소 모델이 밀집 모델보다 더 효율적임을 시뮬레이션 결과를 통해 보여줍니다.

#### 1.4 Downstream Results (다운스트림 결과)
다양한 자연 언어 처리 과제들에 대해 미세 조정 실험을 수행하여, Switch Transformer가 T5-Base 및 T5-Large 모델에 비해 성능이 우수함을 확인했습니다. SuperGLUE, Winogrande, XSum 등에서 큰 성능 향상을 나타내며, 희소 모델을 밀집 모델로 증류(distilling)하여 모델 크기를 99%까지 줄이면서도 성능 개선의 30%를 유지합니다.

### 2. 전체 요약
Switch Transformer 논문은 큰 희소 모델(sparse model)에 대한 문제를 해결하고, 효율적인 훈련 및 배포 가능성을 제시합니다. MoE 구조를 사용하여 각 입력 예제에 대해 다른 전문가를 선택하는 방식으로, 계산 효율성을 유지하면서 모델의 파라미터 수를 증가시킵니다. 실험 결과, Switch Transformer는 다양한 자연 언어 처리 과제에서 기존 모델(T5)보다 더 빠르고 효율적이며, 동일한 계산 자원 내에서 성능을 높일 수 있음을 보여줍니다. 또한, 희소 모델을 밀집 모델로 증류하여 모델 크기를 획기적으로 줄일 수 있으며, 이러한 방법은 대규모 모델을 배포하고 활용하는 데 실질적인 이점을 제공합니다.

## Similar Papers
- [FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning](2307.08691.md)
- [A Closer Look into Mixture-of-Experts in Large Language Models](2406.18219.md)
- [Beyond Scaling Laws: Understanding Transformer Performance with Associative Memory](2405.08707.md)
- [Fast Inference from Transformers via Speculative Decoding](2211.17192.md)
- [SpecInfer: Accelerating Generative Large Language Model Serving with Tree-based Speculative Inference and Verification](2305.09781.md)
- [Yuan 2.0-M32: Mixture of Experts with Attention Router](2405.17976.md)
- [Mixture of A Million Experts](2407.04153.md)
- [Jamba: A Hybrid Transformer-Mamba Language Model](2403.19887.md)
- [Efficient Memory Management for Large Language Model Serving with PagedAttention](2309.06180.md)
