# I-SHEEP: Self-Alignment of LLM from Scratch through an Iterative Self-Enhancement Paradigm
## TL;DR
## Summary
- [https://arxiv.org/pdf/2408.08072.pdf](https://arxiv.org/pdf/2408.08072.pdf)

### 1. 각 섹션 요약

#### Introduction - 소개
LLM(Large Language Models, 대형 언어 모델) 분야는 많은 발전을 이루었지만, 기존의 학습 패러다임은 LLM을 수동적인 정보 저장소로 취급하여 능동적 학습과 맞춤형 학습의 잠재력을 간과하는 한계가 있었습니다. 본 논문은 인간의 학습 프로세스를 모방한 새로운 패러다임인 "I-SHEEP"을 소개합니다. I-SHEEP는 초기 데이터 없이 LLM이 자체적으로 지식 정리를 통해 지속적으로 자가 정렬(self-alignment)이 가능하게 합니다. 이는 인간의 지속적이고 자동적인 학습 방법과 유사합니다.

#### Related Work - 관련 연구
자동 데이터 선택과 합성 데이터 생성 방안을 다루는 기존 연구들을 검토하였습니다. 몇몇 연구는 고품질 데이터 세트를 선별하는데 주력했으며, 다른 연구들은 LLM의 생성 능력을 활용하여 합성 데이터를 만들어 모델의 성능을 향상시키는 방안들을 제안하였습니다. I-SHEEP는 이러한 데이터 선택 및 생성 방법들과는 달리, 지속적이고 자동적인 자가 정렬 및 자가 향상 프로세스를 기본으로 합니다.

#### Methodology - 방법론
I-SHEEP의 학습 과정은 두 단계로 나눌 수 있습니다:
1. **Self-Driven Data Synthesis(자가 구동 데이터 합성)**: 초기 소수의 시드 데이터로부터 새로운 지시사항과 응답 데이터를 생성합니다.
2. **Self-Assessment and Data Filtering(자가 평가 및 데이터 필터링)**: 생성된 데이터의 품질을 스스로 평가하여 낮은 품질의 데이터를 걸러냅니다.
이를 통해 LLM은 반복적인 학습과 개선 과정을 거칩니다.

#### Experiments - 실험
Qwen-1.5 및 Llama-3 모델을 사용하여 I-SHEEP의 효과와 일반화를 검증하였습니다. 다양한 모델 크기와 자기 평가 수준에 따른 성능 변화를 연구하였습니다. 실험 결과, I-SHEEP를 통한 반복 학습 과정에서 모델 성능이 지속적으로 향상됨을 확인할 수 있었습니다.

#### Results - 결과
I-SHEEP는 Alpaca Eval, MT Bench 및 여러 표준 벤치마크 테스트에서 기존 모델 대비 우수한 성능을 보였습니다. 특히 Alpaca Eval에서는 최대 78.2%까지 성능 향상을 보였습니다. 다양한 필터링 설정을 통해 데이터 품질을 관리함으로써 모델의 자가 정렬 능력이 크게 향상되었습니다.

#### Discussion - 토론
I-SHEEP의 반복적인 자기 평가와 데이터 필터링 과정은 모델의 자가 학습 효율성을 높이는 데 중요한 역할을 합니다. 이는 과거의 수동적인 학습 방식과 비교할 때, 모델이 스스로 데이터를 평가하고 필터링하여 고품질 데이터로 지속적으로 학습하는 중요한 차별화 포인트가 됩니다.

#### Conclusion - 결론
I-SHEEP는 외부 데이터나 도구에 의존하지 않고, 모델 스스로 지속적인 향상이 가능함을 보였습니다. 이는 LLM의 자가 학습 능력을 크게 강화할 수 있는 가능성을 보여줍니다. 그러나 RLHF(인간 피드백을 통한 강화 학습) 과정의 최종 향상 정도나 윤리적 우려 등 추가 연구가 필요합니다.

### 2. 전체 요약

본 논문은 새로운 LLM 학습 패러다임인 I-SHEEP를 소개합니다. I-SHEEP는 초기 데이터 없이도 LLM이 지속적으로 자가 학습과 자가 향상이 가능하도록 설계되었습니다. 이를 위해 자가 구동 데이터 합성과 자가 평가 및 필터링 과정을 도입하였습니다. 실험 결과, I-SHEEP는 기존 모델들에 비해 여러 벤치마크 테스트에서 우수한 성능을 보였으며, 특히 자가 평가를 통해 데이터 품질을 스스로 관리하는 능력이 모델 성능 향상에 중요한 역할을 했습니다. 그러나, 향후 RLHF 과정에 대한 추가 연구와 윤리적 우려 해결이 필요합니다. 이는 LLM의 자가 학습 능력을 한 단계 더 발전시키는 데 중요한 기여를 할 것입니다.