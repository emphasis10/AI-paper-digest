# ToolHop: A Query-Driven Benchmark for Evaluating Large Language Models in Multi-Hop Tool Use
## TL;DR
## Summary
- [https://arxiv.org/pdf/2501.02506.pdf](https://arxiv.org/pdf/2501.02506.pdf)

**1. 각 섹션 요약 및 주요 기여와 혁신 부분 설명**

**도입부**
본 연구에서는 대규모 언어 모델(LLM)의 도구 사용 능력을 평가하기 위해 ToolHop이라는 새로운 데이터셋을 소개합니다. 이 데이터셋은 사용자 쿼리를 확장하여 다중 도구 사용 시나리오에서 필요한 철저한 평가를 가능하게 합니다. 

**ToolHop 소개**
ToolHop는 995개의 사용자 쿼리와 3,912개의 관련 도구를 포함하며, 이를 통해 LLM이 복합적인 쿼리를 단계별로 분해하고 적절한 도구를 호출하여 최종 답변을 도출하기 위한 능력을 평가합니다. 

**ToolHop 데이터 구성 및 분석**
ToolHop 데이터셋은 쿼리 주도형 데이터 구축 과정을 통해 구성됩니다. 이 과정은 도구 생성, 문서 보완 및 코드 생성을 포함하며, 다양한 도구 집합과 정교한 피드백을 통해 신뢰성을 높입니다.

**평가 및 결과**
14개의 LLM 리더 모델에 대해 ToolHop을 사용하여 도구 사용 능력을 평가한 결과, 도구 사용이 답변의 정확성을 향상시키지만, 여전히 개선의 여지가 있음을 발견했습니다. GPT-4 모델은 다중 도구 사용 시나리오에서 49.04%의 정확성을 기록하여 가장 높은 성능을 보였습니다.

**결론 및 한계**
ToolHop은 LLM의 도구 사용 능력을 평가할 수 있는 새로운 표준을 설정하며, 복합적인 문제 해결을 위한 능력을 증진시키는 토대가 됩니다. 그러나 즉각적인 능력 향상을 위한 전략은 제시되지 않았습니다.

---
 
**2. 전반적인 요약**

이 논문은 LLM에서 복잡한 다중 도구 사용 시나리오를 평가하기 위한 ToolHop이라는 혁신적인 데이터셋과 그 평가 방법을 제안합니다. 해당 데이터셋은 사용자 쿼리를 확장하는 과정을 통해 도구 사용 능력을 평가하며, 이를 통해 LLM의 도구 호출과 문제 해결 능력을 높이는 데 기여하고자 합니다. 본 연구를 통해 GPT-4 모델이 여전히 개선의 여지가 있지만, 주어진 도구 사용 시나리오에서 상대적으로 높은 성능을 기록하였음을 확인하였습니다.