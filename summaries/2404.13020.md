# Stronger Random Baselines for In-Context Learning
## TL;DR
## Summary
- [https://arxiv.org/pdf/2404.13020.pdf](https://arxiv.org/pdf/2404.13020.pdf)

### 논문 요약

#### 1. 서론
이 연구는 인컨텍스트 학습(ICL)에서 표준 랜덤 기준선의 문제점을 지적하고, 새로운 강화된 랜덤 기준선을 제안합니다. 기존의 표준 랜덤 기준선은 단순히 레이블을 무작위로 추측하는 것에 기반하고 있으나, 이 연구에서 제안하는 기준선은 여러 무작위 분류기들 중 최고의 성능을 기대치로 삼아 더 강화된 평가 기준을 제공합니다.

#### 2. 연구 배경
표준 랜덤 기준선은 분류 작업에서 레이블을 무작위로 추측했을 때의 예상 정확도를 의미하며, 평가 집합이 한 번만 사용되거나 데이터셋이 클 때 안정적입니다. 그러나 ICL 같은 설정에서는 작은 데이터셋과 검증 세트의 재사용으로 인해 이러한 기준이 부적절할 수 있습니다.

#### 3. 새로운 랜덤 기준선
연구진은 무작위 분류기 중 최대 정확도를 기대치로 하는 새로운 기준선을 제안합니다. 이는 여러 랜덤 분류기를 비교할 때, 가장 성능이 좋은 분류기의 정확도를 계산함으로써 얻어집니다. 이를 통해 더 엄격하고 현실적인 평가 기준을 설정할 수 있습니다.

#### 4. 실험 및 결과
BIG-bench Lite 벤치마크 스위트에서 수행된 실험에서는 이 새로운 기준선이 기존 기준선보다 훨씬 높은 임계값을 제공함을 확인했습니다. 예를 들어, 200개의 랜덤 분류기를 비교했을 때, 이 새로운 기준선은 이진 분류 작업에서 7% 이상의 정확도 향상을 요구합니다.

#### 5. 결론 및 토론
이 연구는 ICL 평가에 있어서 기존 랜덤 기준선이 가지는 한계를 극복하고자 합니다. 새로운 기준선은 무작위 성능과의 비교를 통해 모델의 성능을 보다 정확하게 평가할 수 있게 도와줍니다. 또한, 이 기준은 향후 연구에서 모델과 작업의 특성에 따라 더욱 발전될 수 있을 것입니다.

### 종합적인 요약
이 논문은 인컨텍스트 학습(ICL) 성능 평가를 위한 새로운 기준을 제안합니다. 기존의 랜덤 기준선을 개선하여, 여러 랜덤 분류기 중 최고의 성능을 기대치로 하는 강화된 랜덤 기준선을 도입함으로써, 모델의 실제 성능을 더 정확하고 엄격하게 평가할 수 있습니다. 이는 ICL에서의 모델 평가 방식에 중요한 변화를 제시하며, 더 높은 신뢰성을 가진 결과를 도출할 수 있게 합니다.

## Similar Papers
- [In-Context Learning with Long-Context Models: An In-Depth Exploration](2405.00200.md)
- [T-FREE: Tokenizer-Free Generative LLMs via Sparse Representations for Memory-Efficient Embeddings](2406.19223.md)
- [$\text{Memory}^3$: Language Modeling with Explicit Memory](2407.01178.md)
- [Many-Shot In-Context Learning in Multimodal Foundation Models](2405.09798.md)
- [Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning](2205.05638.md)
- [Estimating Knowledge in Large Language Models Without Generating a Single Token](2406.12673.md)
- [Knowledge Card: Filling LLMs' Knowledge Gaps with Plug-in Specialized Language Models](2305.09955.md)
- [Why Larger Language Models Do In-context Learning Differently?](2405.19592.md)
- [In-Context Example Selection via Similarity Search Improves Low-Resource Machine Translation](2408.00397.md)
