# Do Large Language Models Reason Causally Like Us? Even Better?
## TL;DR
## Summary
- [https://arxiv.org/pdf/2502.10215.pdf](https://arxiv.org/pdf/2502.10215.pdf)

1. 각 섹션의 주요 내용 요약:

1. **서론:**
   이 논문은 대형 언어 모델(LLMs)이 인간과 같이 인과적 추론을 할 수 있는지, 또는 더 나은지를 탐구합니다. 최근 LLMs의 발전으로 인해 여러 분야에서 인간과 유사한 텍스트를 생성할 수 있는 능력이 있으며, 이러한 모델들이 진정한 이해를 반영하는지 아니면 학습된 통계적 패턴에 의존하는지를 확인합니다.

2. **목표와 범위:**
   LLMs가 인간과 비교하여 인과적 추론을 어떻게 잘 수행하는지, 그리고 이들의 판단이 학습된 도메인 지식에 기반하는지를 조사합니다.

3. **방법론:**
   이 연구에서는 Rehder와 Waldmann(2017)의 인간 데이터를 이용하여, 다수의 LLMs와 인간의 인과적 추론을 비교합니다. 추론 과제로는 두 개의 독립적인 원인이 하나의 공통 결과에 영향을 미치는 콜라이더 그래프를 사용하였습니다.

4. **결과:**
   GPT-4o와 Claude는 인간보다 더 규범적인 추론을 보였으며, Gemini-Pro와 GPT-3.5는 덜 규범적이었습니다. 전반적으로 도메인 지식을 기반으로 추론을 수행합니다.

5. **토론:**
   LLMs는 인간과 유사한 인과적 추론 패턴을 보이나, 특정 조건에서는 차이가 드러났습니다. 특히, "explaining away"라는 패턴에서 GPT-4o와 Claude는 강한 인과적 이해를 나타냈으나, Gemini-Pro와 GPT-3.5는 잘못된 추론을 하는 경향이 있었습니다.

2. 전체 요약:
   이 연구는 대형 언어 모델들이 인과적 추론에서 인간과 어떤 차이를 보이는지를 종합적으로 분석하였습니다. 특히, GPT-4o와 Claude는 더 정교한 인과적 추론 능력을 가지고 있음을 보여주며, 이는 AI 시스템이 인간의 의사결정을 도울 때의 편향성을 연구하는 데 중요한 기초 자료를 제공합니다. 이는 LLMs의 발전 가능성을 보여주고, 향후 AI 연구 및 실무 응용에 유용한 인사이트를 제공합니다.