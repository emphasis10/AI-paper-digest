# SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models
## TL;DR
## Summary
- [https://arxiv.org/pdf/2211.10438.pdf](https://arxiv.org/pdf/2211.10438.pdf)

### 요약

#### 1. 서론 (Introduction)
대규모 언어 모델(LLM)은 뛰어난 성능을 보이나, 막대한 메모리와 연산 자원이 필요하다. 이를 해결하기 위해, 이 논문은 SmoothQuant라는 새로운 양자화 방법을 제안한다. SmoothQuant는 훈련 없이도 정확성을 유지하며, LLM의 8비트 양자화를 가능하게 한다. 특히, 가중치보다 양자화하기 어려운 활성값을 부드럽게 조정하여 양자화의 어려움을 줄인다.

#### 2. 사전 지식 (Preliminaries)
양자화는 고정밀 값을 이산적인 단계로 매핑하는 것이다. 논문에서는 특히 INT8 양자화를 연구하며, 이는 하드웨어 지원과 효율성을 높이기 위해 사용된다.

#### 3. 양자화의 어려움 검토 (Review of Quantization Difficulty)
LLM의 활성값에 존재하는 이상치(outliers)로 인해 양자화가 어렵다. 가중치는 균일하고 평탄한 분포를 가지므로 쉽게 양자화할 수 있지만, 활성값은 그렇지 않다. 이러한 활성값의 이상치는 양자화 시 큰 오류를 초래한다.

#### 4. SmoothQuant
SmoothQuant는 활성값의 이상치를 줄이기 위해, 양자화의 어려움을 가중치로 옮기는 방법을 사용한다. 이는 활성값의 채널 간 스케일 변화를 부드럽게 하여 모델을 양자화 친화적으로 만든다. 세 가지 효율성 수준(O1-O3)으로 구현되어 다양한 하드웨어에서 사용할 수 있다.

#### 5. 실험 (Experiments)
SmoothQuant의 성능을 평가하기 위해 OPT, BLOOM, GLM-130B 모델을 사용하였으며, 다양한 제로샷 평가 과제에서 테스트를 진행하였다. SmoothQuant는 LLM의 정확성을 유지하면서도, 최대 1.56배의 속도 향상과 메모리 절감을 달성하였다.

#### 6. 관련 연구 (Related Work)
LLM의 양자화에 관한 다양한 기존 연구들이 있지만, 대부분의 방법들은 정확성과 하드웨어 효율성을 동시에 유지하는 데 실패했다. SmoothQuant는 이러한 문제를 해결하기 위한 새로운 접근법이다.

#### 7. 결론 (Conclusion)
SmoothQuant는 대규모 언어 모델의 8비트 양자화를 가능하게 하여, 추론 속도를 높이고 메모리 사용을 절감한다. 이를 통해, LLM의 사용을 더욱 민주화하고 비용을 절감할 수 있다.

### 전반적인 요약
SmoothQuant는 대규모 언어 모델(LLM)의 양자화를 통해 성능을 유지하면서도 하드웨어 자원을 절감할 수 있는 혁신적인 방법을 제안한다. 기존 양자화 방법의 문제점을 해결하고, 다양한 LLM에 적용하여 뛰어난 효율성을 입증하였다. 이 방법은 모델의 활성값 이상치를 부드럽게 조정하여 양자화의 어려움을 가중치로 옮기며, 최대 1.56배의 속도 향상과 메모리 절감을 제공한다. 이를 통해, LLM의 사용이 보다 효율적이고 비용 효과적으로 가능해진다.

## Similar Papers
- [AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration](2306.00978.md)
- [BASS: Batched Attention-optimized Speculative Sampling](2404.15778.md)
- [Efficient Streaming Language Models with Attention Sinks](2309.17453.md)
- [QMoE: Practical Sub-1-Bit Compression of Trillion-Parameter Models](2310.16795.md)
- [OneBit: Towards Extremely Low-bit Large Language Models](2402.11295.md)
- [SparseGPT: Massive Language Models Can Be Accurately Pruned in One-Shot](2301.00774.md)
- [T-MAC: CPU Renaissance via Table Lookup for Low-Bit LLM Deployment on Edge](2407.00088.md)
- [KV-Runahead: Scalable Causal LLM Inference by Parallel Key-Value Cache Generation](2405.05329.md)
- [LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale](2208.07339.md)
