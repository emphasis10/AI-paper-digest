# CLIP meets Model Zoo Experts: Pseudo-Supervision for Visual Enhancement
## TL;DR
## Summary
- [https://arxiv.org/pdf/2310.14108v1.pdf](https://arxiv.org/pdf/2310.14108.pdf)

#### 1. 서론
기초 모델(FM)은 컴퓨터 비전과 자연어 처리 등 인공지능의 여러 영역을 혁신하고 있습니다. CLIP는 이미지와 텍스트 표현을 정렬하기 위해 대조 학습을 사용하는 방법 중 하나로, 데이터 분포 이동에 대한 강인함과 뛰어난 제로샷 및 크로스모달 검색 기능을 제공합니다. 그러나 CLIP는 객체 위치 추적 능력이 부족해, 텍스트와 이미지 내 객체를 연결하는 데 어려움을 겪습니다. 이 연구는 CLIP 모델을 모델 동물원의 특화된 비전 모델로 보강해 시각 표현을 개선할 수 있는지에 대한 질문을 탐구합니다.

#### 2. 관련 연구
- **비전 FM**: 비전 FM은 수억 개의 이미지가 포함된 방대한 데이터셋을 사전 학습에 활용하며, 대표적으로 CLIP가 있습니다.
- **전문가를 통한 의사 감독**: 의사 감독의 주된 목적은 전문가가 훈련한 모델을 사용해 라벨이 없는 데이터에 의사 라벨을 생성하여 모델 학습을 돕는 것입니다. 이 접근 방식은 데이터셋의 긍정적 전이를 목표로 합니다.
- **다중 과제 학습**: 여러 과제를 동시에 훈련하는 표준 방법으로, 긍정적 전이를 촉진하기 위해 다중 과제 데이터셋을 통합합니다.

#### 3. CLIPTeX
CLIPTeX는 공개된 전문 태스크 모델로부터 의사 감독을 받아 CLIP의 시각 표현을 개선합니다. 이 접근 방식은 라벨링된 데이터를 수집하지 않고도 CLIP의 표현력을 강화합니다.

- **모델링**: CLIPTeX는 이미지 인코더와 텍스트 인코더를 사용하며, CLIP와 유사하게 대조 손실을 사용하여 학습합니다. 또한, 다중 스케일 모듈을 통해 더 나은 시각 표현을 학습하도록 합니다.
- **훈련**: 여러 태스크에 대해 의사 라벨을 생성하고, CLIPTeX를 이 의사 라벨과 이미지-텍스트 쌍으로 학습합니다. 이를 통해 CLIPTeX는 다양한 태스크에서 시각 표현을 크게 향상시킵니다.

#### 4. 실험 설정
- **태스크별 전문가**: 공개된 전문가 모델을 사용해 의사 라벨을 생성합니다. 예를 들어, Mask-RCNN은 세그멘테이션을, DPT는 깊이 추정을, NLL-AngMF는 표면 법선 추정을 수행합니다.
- **CLIP 기준선**: 대규모 데이터셋으로 사전 학습된 CLIP 모델을 사용합니다. 높은 해상도 이미지를 사용한 훈련이 필요할 경우, CC3M 데이터셋으로 CLIP를 미세 조정합니다.

#### 5. 결과
- **시각 표현 향상**: CLIPTeX는 CLIP와 비교하여 다양한 태스크에서 시각 표현을 향상시킵니다. 예를 들어, PASCAL VOC 데이터셋에서 선형 프로빙 정확도가 10% 이상 향상되었습니다.
- **제로샷 능력 유지**: CLIPTeX는 기존 CLIP의 제로샷 능력을 유지하면서도 시각 표현을 향상시킵니다.

#### 6. 결론
CLIPTeX는 공개된 전문 모델을 활용해 CLIP의 시각 표현을 개선하며, 라벨링된 데이터를 수집하지 않고도 다양한 태스크에서 뛰어난 성능을 발휘합니다. 이 연구는 CLIP의 적용 범위를 넓히고, 다양한 컴퓨터 비전 분야에서 활용될 수 있는 가능성을 보여줍니다.