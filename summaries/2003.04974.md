# Transformer++
## TL;DR
## Summary
- [https://arxiv.org/pdf/2003.04974.pdf](https://arxiv.org/pdf/2003.04974.pdf)

### 섹션 요약

#### I. 소개
이 논문은 신경 기계 번역(NMT)에 관한 연구로, Transformer 아키텍처 기반의 기존 방법론들의 한계를 개선하고자 합니다. Transformer는 2017년에 발표되었고, 주어진 문장의 단어들 간의 의존성을 잘 모델링하여 우수한 성과를 보였습니다. 하지만 이 연구에서는 특정 의존성들이 중간 컨텍스트를 통해 학습될 때 더 효과적으로 학습될 수 있다고 주장합니다. 이를 위해 다중-헤드 주의 메커니즘에 컨볼루션을 추가하는 새로운 방식을 제안하면서, 이를 Transformer++라고 명명하였습니다.

#### II. 배경
Transformer는 재귀 신경망(RNN) 없이도 신경 기계 번역에서 최고 성능을 기록했습니다. RNN은 순차적 특성 때문에 긴 범위의 의존성을 모델링하는데 어려움이 있었고, Transformer는 이런 문제를 해결했습니다. 다양한 연구가 순차적 계산 요구를 줄이기 위해 컨볼루션 신경망(CNN)을 사용했지만, Transformer가 여전히 최고의 성능을 자랑합니다.

#### III. 모델 아키텍처
Transformer++ 아키텍처는 기존 Transformer의 인코더-디코더 구조를 따르면서, 새로운 다중-헤드 주의 메커니즘을 도입했습니다. 다중-헤드 주의 메커니즘은 H/2는 기존의 자가 주의를 사용하고, 나머지 H/2는 컨텍스트를 통한 의존성을 학습하는 컨볼루션 기반 주의를 사용합니다. 이를 통해 단어-컨텍스트 의존성을 효과적으로 캡처할 수 있습니다.

##### A. 인코더 스택
기초 인코더는 두 개의 서브 레이어로 구성되며, 첫 번째 레이어는 다중-헤드 주의 메커니즘(자가 주의 + 제안된 컨텍스트-단어 주의)이고, 두 번째 레이어는 전치사/명명 엔티티 인식을 위한 간단한 피드 포워드 네트워크입니다. 이는 문법적 정보와 정보 추출을 강화하여 번역 성능을 향상시킵니다.

##### B. 디코더 스택
디코더 구조도 인코더와 유사하지만, POS 및 NER 분류를 위한 추가 레이어는 없습니다. 디코더는 예측 시퀀스를 방지하기 위해 마스킹을 사용하여 미래 정보를 참조하지 않게 합니다.

##### C. 주의 메커니즘
이 연구에서는 스케일드 닷-프로덕트 주의와 제안된 컨텍스트-단어 주의 메커니즘을 함께 사용했습니다. 이는 단어-단어 의존성을 넘어 단어-컨텍스트 의존성을 모델링하여 번역 성능을 향상시킵니다.

#### IV. 데이터셋 및 학습
WMT 2014 영어-독일어 및 영어-프랑스어 데이터셋을 사용하여 학습하고, Newstest2013과 Newstest2014에서 성능을 평가했습니다. 학습은 Adam 옵티마이저를 사용하여 수행되었고, 문장 페어는 동일한 시퀀스 길이를 가진 페어들로 배치했습니다.

#### V. 결과 및 결론
Transformer++는 기존 최고의 성능을 1.4 BLEU 및 1.9 BLEU 만큼 초과하며, 새로운 상태의 기술(SOTA)을 달성했습니다. 또한, 단어 임베딩의 코사인 유사도를 계산하여 제안된 방법이 단어-컨텍스트 의존성을 더욱 효과적으로 학습함을 확인했습니다.

### 전체 요약
Transformer++는 기존 Transformer 모델의 한계를 극복하기 위해 다중-헤드 주의 메커니즘에 컨볼루션 기반 주의를 도입한 혁신적인 모델입니다. 이 모델은 단어 간의 의존성을 더욱 효과적으로 학습하며, 전치사/명명 엔티티 인식을 통한 추가적인 정보로 번역 성능을 크게 향상시킵니다. WMT 2014 데이터셋에서 새로운 최고의 성능을 기록하며, 신경 기계 번역 연구에 새로운 방향을 제시합니다.

## Similar Papers
- [Attention Is All You Need](1706.03762.md)
- [Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity](2101.03961.md)
- [RoFormer: Enhanced Transformer with Rotary Position Embedding](2104.09864.md)
- [Complexity of Symbolic Representation in Working Memory of Transformer Correlates with the Complexity of a Task](2406.14213.md)
- [Pointer Networks](1506.03134.md)
- [Online normalizer calculation for softmax](1805.02867.md)
- [Depth-Adaptive Transformer](1910.10073.md)
- [Sparser is Faster and Less is More: Efficient Sparse Attention for Long-Range Transformers](2406.16747.md)
- [Enhancing Semantic Similarity Understanding in Arabic NLP with Nested Embedding Learning](2407.21139.md)
