# Bi-Mamba: Towards Accurate 1-Bit State Space Models
## TL;DR
## Summary
- [https://arxiv.org/pdf/2411.11843.pdf](https://arxiv.org/pdf/2411.11843.pdf)

### 1. 각 섹션 요약

#### 서론
Selective State-Space Model (SSM)은 트랜스포머 모델의 한계를 해결하기 위해 개발되었습니다. SSM은 작은 크기에서부터 중간 크기까지 트랜스포머 모델에 비해 우수한 성능을 보이며, 훈련 중에 선형적인 시퀀스 길이 증가와 일정한 상태 크기로 인해 더 적은 메모리를 소모하여 효율적입니다.

#### Bi-Mamba 소개 및 기술
이 논문에서는 다양한 규모(780M, 1.3B, 2.7B)의 효율적인 대형 언어 모델을 위한 1비트 Mamba 아키텍처인 Bi-Mamba를 제안합니다. Bi-Mamba는 기존의 풀 프리시전(base) 모델에 비해 메모리 사용량과 에너지 소비를 크게 줄이면서 비슷한 성능을 확보합니다.

#### 주요 기여사항
Bi-Mamba는 처음으로 Mamba 아키텍처를 1비트로 성공적으로 이진화하여 강력한 성능을 유지합니다. 이를 통해 Mamba의 이진화 가능한 파라미터 공간을 탐색하며, 자원 제한적인 시나리오에서도 강력한 기반 모델로 동작할 수 있게 하여 후속 연구에 유용한 통찰을 제공합니다.

#### 실험결과
Bi-Mamba는 다양한 초대형 언어 모델과 비교하여 다운스트림 작업에서 경쟁력 있는 성능과 낮은 복잡도를 보여주었습니다. 특히, 비-LLM이나 GPTQ-3비트 모델에 비해 더 낮은 난해도를 자랑하며, 모델의 크기가 커질수록 최고의 성능을 유지합니다.

#### 결론 및 한계
우리의 연구는 메모리 사용량과 계산 비용을 대폭 줄인 새로운 1비트 기반의 선형 계산 복잡성 LLM 프레임워크를 제안합니다. 다만, 복잡한 작업이나 전문 하드웨어가 요구되는 경우 정확도에 있어 일부 손실이 있을 수 있으며, 데이터 집합의 편향을 증폭시킬 가능성도 존재합니다.

### 2. 전반적인 요약
Bi-Mamba는 Selective State-Space Model(SSM)에 기초한 혁신적인 1비트 Mamba 아키텍처로, 효율적인 대형 언어 모델의 구현을 가능하게 하며, 메모리와 에너지 소비를 줄이는 동시에 강력한 성능을 유지합니다. 이 연구는 저전력 소비 및 메모리 효율성을 극대화하며 다양한 다운스트림 작업에서 탁월한 성능을 발휘할 수 있는 초석을 제공합니다. 앞으로도 이러한 연구는 더욱 발전할 가능성이 높으며, AI의 잠재력을 끌어올리는 중요한 발판이 될 것입니다.