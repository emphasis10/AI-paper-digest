# PECC: Problem Extraction and Coding Challenges
## TL;DR
## Summary
- [https://arxiv.org/pdf/2404.18766.pdf](https://arxiv.org/pdf/2404.18766.pdf)

### 요약

#### 1. 서론 (Introduction)
대규모 언어 모델(LLMs)은 다양한 생성 작업에서 뛰어난 능력을 보여주었으며, 코드 생성, 문제 해결 및 추론 등에서도 신뢰할 수 있는 도구로 자리 잡았습니다. 기존의 벤치마크는 주로 특정 작업에 국한되어 있지만, LLMs가 서술형 문제를 이해하고, 근본적인 문제를 식별하여 적절한 코드 솔루션을 생성하는 능력을 평가하는 것은 아직 충분히 탐구되지 않았습니다. 이를 해결하기 위해 PECC라는 새로운 벤치마크를 소개하며, 이 벤치마크는 Advent of Code(AoC)와 Project Euler에서 가져온 2396개의 문제로 구성되어 있습니다. PECC는 LLMs가 서술된 문제를 이해하고 요구 사항을 추출하여 실행 가능한 코드를 생성하는 능력을 평가합니다. 이를 통해 LLMs의 범용 문제 해결 능력을 모니터링하고 평가할 수 있는 프레임워크를 제공합니다.

#### 2. PECC 데이터셋 (PECC Dataset)
PECC 데이터셋은 AoC와 Project Euler의 문제를 기반으로 구축되었습니다. AoC는 이야기 형식의 서술된 문제를 포함하고 있으며, Project Euler는 수학적이고 컴퓨팅적인 문제를 중심으로 구성되어 있습니다. 두 데이터셋은 서술형과 중립형 문제로 변환되어 다양한 문제 형태에 대한 모델의 적응력을 평가할 수 있도록 설계되었습니다. 총 2,352개의 문제로 구성된 이 데이터셋은 모델이 자연어 사양을 정확하게 이해하고 올바른 코드를 생성하는 능력을 평가합니다.

#### 3. 실험 (Experiments)
PECC 벤치마크를 사용하여 여러 최신 언어 모델을 평가했습니다. 이 실험은 LLMs가 복잡한 코딩 작업을 해결하는 데 있어 얼마나 뛰어난 능력을 발휘하는지 조사하기 위해 설계되었습니다. 평가된 모델에는 GPT-3.5-turbo, PALM 2, Codey, Claude 3 Haiku 등이 포함되었습니다. 실험 결과는 다양한 문제 유형과 난이도에 따른 모델의 성능을 분석하고, 문제 해결 과정에서 발생하는 오류를 상세히 분석했습니다.

#### 4. 결과 (Results)
모델 성능 평가 결과, 다중 샘플링 접근 방식(k=3)을 사용하면 단일 샘플링(k=1)에 비해 Pass@k 점수가 향상되는 것으로 나타났습니다. 또한, 서술형 문제는 모델이 더 잘 수행할 수 있도록 도와주지만, 수학적인 Project Euler 문제에서는 성능이 저하되는 경향이 있습니다. 전반적으로, 모델은 복잡한 코딩 문제에서 어려움을 겪는 것으로 나타났으며, 이를 해결하기 위해 모델의 세부 조정이나 코드 생성에 특화된 모델의 활용이 필요합니다.

#### 5. 결론 (Conclusion)
PECC 데이터셋은 LLMs의 코드 생성 능력을 평가하기 위한 유용한 도구로서, 서술형과 중립형 문제 모두에서 모델의 성능을 평가할 수 있습니다. 이 연구는 자연어 프롬프트와 채팅 기반 평가 접근 방식을 결합하여 실제 문제 해결 시나리오를 반영한 평가를 수행하였습니다. 서술형 문제와 중립형 문제 간의 성능 차이를 비교한 결과, 모델이 서술형 환경에서 더 잘 수행할 수 있음을 확인할 수 있었습니다. 이 연구는 LLMs의 능력을 평가하고 향후 연구 방향을 제시하는 데 중요한 기여를 할 것입니다.

### 전체 요약
이 논문은 PECC라는 새로운 벤치마크를 소개하며, 이는 LLMs가 서술형 문제를 이해하고 적절한 코드를 생성하는 능력을 평가하는 데 중점을 둡니다. PECC 데이터셋은 AoC와 Project Euler의 문제를 포함하며, 모델의 자연어 이해 및 문제 해결 능력을 종합적으로 평가할 수 있습니다. 실험 결과, 모델은 복잡한 문제에서 어려움을 겪었지만, 서술형 문제에서는 상대적으로 더 나은 성능을 보였습니다. 이 연구는 LLMs의 능력을 평가하고 향후 발전 방향을 제시하는 데 중요한 기여를 할 것입니다.

## Similar Papers
- [A Survey on Retrieval-Augmented Text Generation for Large Language Models](2404.10981.md)
- [LAB: Large-Scale Alignment for ChatBots](2403.01081.md)
- [Efficient and Effective Vocabulary Expansion Towards Multilingual Large Language Models](2402.14714.md)
- [MMAU: A Holistic Benchmark of Agent Capabilities Across Diverse Domains](2407.18961.md)
- [AutoCoder: Enhancing Code Large Language Model with \textsc{AIEV-Instruct}](2405.14906.md)
- [Non Verbis, Sed Rebus: Large Language Models are Weak Solvers of Italian Rebuses](2408.00584.md)
- [REPOEXEC: Evaluate Code Generation with a Repository-Level Executable Benchmark](2406.11927.md)
- [Towards Scalable Automated Alignment of LLMs: A Survey](2406.01252.md)
- [ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools](2406.12793.md)
