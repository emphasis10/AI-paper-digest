# MM-IQ: Benchmarking Human-Like Abstraction and Reasoning in Multimodal Models
## TL;DR
## Summary
- [https://arxiv.org/pdf/2502.00698.pdf](https://arxiv.org/pdf/2502.00698.pdf)

### 1. 각 섹션의 주요 내용 요약 (한글)

**서론 (Introduction)**  
이 논문에서는 대규모 멀티모달 모델(LMM)의 인간과 같은 추상적 사고 및 추론 능력을 평가하기 위해 MM-IQ라는 포괄적인 평가 프레임워크를 제안합니다. 기존의 멀티모달 모델은 자신의 경험에 기반하여 누적된 분야 지식 및 언어 능력을 측정하는 데 한계를 보이고 있으며, 이를 해결하기 위해 지식에 구애받지 않는 추상적 사고 능력을 독립적으로 평가할 필요가 있습니다. MM-IQ는 8개의 서로 다른 추론 패러다임을 포함하는 2,710개의 문제로 구성되어 있습니다.

**관련 연구 (Related Work)**  
현재 사용되는 여러 AVR(추상적 시각 추론) 벤치마크에 대해 설명하며, 이들이 입력 형태, 문제 구성, 추론 패러다임이 제한적이라는 점을 지적합니다. 대부분의 벤치마크는 도메인 지식에 의존하거나 컴퓨터 프로그램에 의해 생성된 데이터에 의존하고 있습니다.

**MM-IQ의 구축 (Construction of MM-IQ)**  
MM-IQ의 데이터 수집 과정은 세 단계로 나누어집니다. 첫 번째는 기존 AVR 데이터셋의 검토와 압축이 없고 다양성이 부족한 문제를 선택하는 것입니다. 두 번째 단계는 데이터를 여러 패러다임으로 분류하고, 세 번째 단계는 인간 검수를 통해 중복 문제를 제거하고, 정답을 추출하는 것입니다.

**실험 (Experiments)**  
MM-IQ 데이터셋에 대해 성능을 평가하기 위해 여러 공개 소스와 폐쇄 소스 LMM들을 비교합니다. 결과적으로 인간의 평균 정확도는 51.27%이며, 최상위 모델조차 27.49%로, 랜덤 추측보다 약간 높은 수준입니다. 특히 LMM들은 논리 연산 및 추상적 추론 관련 작업에서 큰 한계를 보였습니다.

**결론 (Conclusion)**  
MM-IQ는 멀티모달 모델에 대한 포괄적인 추상적 시각 추론 평가 기준을 제시합니다. 이 시스템은 현재의 LMM들이 인간 수준의 사고력을 아직 충족하지 못함을 잘 보여주며, 향후 연구 및 발전의 방향성을 제시합니다.

### 2. 전체 요약 (한글)

이 논문은 MM-IQ라는 새로운 벤치마크를 통해 멀티모달 모델의 추상적 사고 및 추론 능력을 평가하는 새로운 기준을 제안합니다. MM-IQ는 2,710개의 문제와 8개의 다양한 추론 패러다임을 포함하며, 기존의 데이터셋들이 지니고 있는 특정한 문제를 해결하는 데 있어 겪는 한계들을 극복하기 위한 목적으로 만들어졌습니다. 현재의 최첨단 멀티모달 모델들은 인간의 평균 정확도에 비해 매우 낮은 성과를 보이며, 이는 모델들이 기본적인 인간 사고 능력을 재현하지 못하고 있다는 점을 시사합니다. 따라서 MM-IQ는 향후 멀티모달 모델의 연구와 개발 방향성을 제시하는 중요한 자료로서 기능할 것입니다. 

MM-IQ는 멀티모달 지능 평가에서 언어적 또는 도메인 지식의 편향을 배제하여, 보다 진정한 인간과 유사한 인지적 능력을 평가할 수 있는 가능성을 제시합니다.