# Molmo and PixMo: Open Weights and Open Data for State-of-the-Art Multimodal Models
## TL;DR
## Summary
- [https://arxiv.org/pdf/2409.17146.pdf](https://arxiv.org/pdf/2409.17146.pdf)

**1. 서론 (Introduction)**

Molmo라는 새로운 비전-언어 모델(VLM) 가족을 소개합니다. Molmo는 기존의 폐쇄적인 모델에 의존하지 않고, 인간 주석자들로부터 수집한 새로운 이미지 캡션 데이터셋을 활용하여 성능을 높였습니다. 이 모델은 VLM의 가용성 문제를 해결하고, 다양한 사용자의 상호작용을 가능하게 하는 데이터셋을 제공함으로써 관찰 가능한 데이터를 최대한 활용합니다.

**2. 아키텍처 (Architecture)**

Molmo의 아키텍처는 단순하면서도 표준적인 설계를 따릅니다. 이 모델은 이미지 전처리기, 비전 인코더, 커넥터 및 변환기 LLM(언어 모델)로 구성됩니다. 이러한 단순한 구조와 새로운 데이터셋을 포함한 잘 다듬어진 훈련 파이프라인을 통해 높은 성능을 달성합니다.

**3. 데이터 및 훈련 (Data and Training)**

두 단계의 훈련 과정으로 이루어져 있습니다:
- **단계 1: 캡션 생성 (Caption Generation)**: 최신 이미지 캡션 데이터(PixMo-Cap)를 활용하여 비전 인코더와 언어 모델을 결합하여 훈련합니다.
- **단계 2: 지도 학습 (Supervised Fine-Tuning)**: 학문적 데이터셋과 새로 수집한 데이터셋을 사용해 모델을 미세 조정합니다.

**4. 평가 (Evaluation)**

최신 학문적 벤치마크 11개와 인간 평가를 통해 모델의 성능을 평가했습니다. 인간 평가에서는 사용자의 선호도를 기반으로 모델을 순위 매겼으며, Molmo-72B가 높은 평가를 받았습니다. 이 모델은 GPT-4o와 같은 비밀 모델과 경쟁할 수 있을 정도로 높은 성능을 자랑합니다.

**5. 결론 및 출시 계획 (Conclusion and Release Plan)**

Molmo의 성과는 모델 아키텍처의 세부 사항, 잘 다듬어진 훈련 파이프라인, 그리고 새로 수집한 데이터셋의 품질 덕분입니다. 향후 모든 모델 가중치, 캡션 및 미세 조정 데이터, 소스 코드를 공개할 예정입니다.

### 전체 요약

이 논문은 기존의 폐쇄적인 비전-언어 모델(VLM)들의 한계를 극복하기 위해, Molmo라는 새로운 오픈 VLM 모델 가족을 소개했습니다. Molmo는 인간 주석자들로부터 수집한 고품질 이미지 캡션 데이터셋과 다양한 상호작용을 위한 데이터셋을 활용하여 성능을 극대화했습니다. 
모델 아키텍처는 이미지 전처리기, 비전 인코더, 커넥터, 및 변환기 LLM로 구성된 단순한 구조로, 이러한 구성과 고도로 다듬어진 훈련 파이프라인을 통해 높은 성과를 냈습니다. 평가 결과, Molmo-72B 모델은 GPT-4o와 같은 비밀 모델과 경쟁할 수 있을 정도로 높은 성능을 보여주었으며, 인간 평가에서도 높은 평가를 받았습니다. 향후 모든 모델 가중치와 데이터셋을 공개할 계획입니다. 