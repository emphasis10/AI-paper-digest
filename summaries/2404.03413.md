# MiniGPT4-Video: Advancing Multimodal LLMs for Video Understanding with Interleaved Visual-Textual Tokens
## TL;DR
## Summary
- [https://arxiv.org/pdf/2404.03413.pdf](https://arxiv.org/pdf/2404.03413.pdf)

### 주요 기여 및 혁신
이 논문은 동영상 이해를 위해 설계된 멀티모달 대규모 언어 모델(LLM)인 MiniGPT4-Video를 소개합니다. 이 모델은 시각적 데이터와 텍스트 데이터를 함께 처리할 수 있어 동영상의 복잡한 내용을 이해하는 데 적합합니다. MiniGPT-v2의 성공을 기반으로, 이 논문은 시각적 특징을 LLM 공간으로 번역하는 능력을 동영상 처리로 확장합니다. MiniGPT4-Video는 기존의 최첨단 방법들을 능가하며, MSVD, MSRVTT, TGIF, TVQA 벤치마크에서 각각 4.22%, 1.13%, 20.82%, 13.1%의 향상을 보였습니다.

### 섹션별 요약

#### 1. 소개
최근 LLM 연구는 놀라운 발전을 이루었지만, 주로 텍스트 중심의 작업에 한정되어 있습니다. 이미지 통합에 대한 연구는 있었으나, 동영상의 시간적 정보를 통합하는 연구는 상대적으로 미진합니다. 동영상은 정적 이미지와 달리 시간적 차원을 가지고 있어 동적인 시각적 내용과 텍스트 입력을 함께 이해해야 합니다. 기존 연구들은 공간적 및 시간적 풀링 기법을 사용하여 프레임 간 정보를 결합하려 했지만, 정보 손실이 발생할 수 있습니다. MiniGPT4-Video는 이러한 문제를 해결하기 위해 매 4개의 시각 토큰을 하나의 토큰으로 결합하여 정보 손실을 최소화합니다.

#### 2. 관련 연구
NLP 및 LLM의 발전은 크로스 모달리티를 이해하기 위한 다양한 방법과 모델(Vision Language Models, VLM)을 탄생시켰습니다. 최근에는 LLM을 활용한 동영상 이해 연구가 활발히 진행되고 있습니다. 예를 들어, Video-ChatGPT는 단일 스트림을 사용하여 각 프레임을 인코딩한 후 공간적 및 시간적 풀링 과정을 거쳐 LLM에 매핑합니다. MiniGPT4-Video는 시각적 프레임과 텍스트 설명을 정렬하여 동영상 내용을 더 포괄적으로 이해할 수 있도록 합니다.

#### 3. MiniGPT4-Video
MiniGPT-v2는 단일 이미지 이해에 성공적으로 적용되었으며, 이를 확장하여 다중 프레임 동영상 이해를 가능하게 합니다. 동영상은 LLM의 컨텍스트 윈도우에 맞게 프레임을 샘플링하여 처리됩니다. 시각적 프레임은 사전 학습된 EVA-CLIP 모델을 통해 시각적 특징을 추출하고, 이를 텍스트 토큰과 결합합니다. 이 접근 방식은 시각적 및 텍스트적 쿼리에 모두 응답할 수 있는 모델을 만듭니다.

#### 4. 실험
MiniGPT4-Video의 유효성을 검증하기 위해 다양한 벤치마크에서 평가를 수행했습니다. Video-ChatGPT 벤치마크에서는 정보의 정확성, 세부 지향성, 맥락 이해, 시간적 이해, 일관성 등의 측면에서 평가되었습니다. 또한, MSVD, MSRVTT, TGIF, TVQA 등의 벤치마크에서도 높은 성과를 보였습니다. 특히 TVQA 벤치마크에서 자막 정보를 통합했을 때 성능이 크게 향상되었습니다.

#### 5. 질적 결과
모델의 성능을 시각적으로 보여주기 위해 몇 가지 예시 결과를 제시했습니다. 각 예시는 동영상 링크와 함께 제공되어 동영상 내용을 직접 확인할 수 있습니다.

#### 6. 결론
MiniGPT4-Video는 시각적 및 대화적 이해를 통합하여 동영상 질문 응답에 효과적인 솔루션을 제공합니다. 이 모델은 동영상 프레임 간의 복잡한 관계를 이해하는 능력을 보여주며, 시간적 역학을 이해하는 데 유망한 성능을 입증했습니다. 그러나 현재 LLM의 컨텍스트 윈도우 제한으로 인해 동영상 길이에 제한이 있습니다. 향후 연구에서는 이 모델의 능력을 확장하여 더 긴 동영상도 처리할 수 있도록 할 예정입니다.

### 전체 요약
MiniGPT4-Video는 동영상 이해를 위한 멀티모달 대규모 언어 모델로, 시각적 및 텍스트 데이터를 함께 처리하여 동영상의 복잡한 내용을 이해합니다. 이 모델은 기존의 방법들을 능가하며, 다양한 벤치마크에서 우수한 성능을 입증했습니다. 자막 정보를 통합하여 성능을 더욱 향상시킬 수 있으며, 향후 더 긴 동영상을 처리할 수 있도록 모델의 능력을 확장할 계획입니다.

## Similar Papers
- [Pegasus-v1 Technical Report](2404.14687.md)
- [Goldfish: Vision-Language Understanding of Arbitrarily Long Videos](2407.12679.md)
- [On Speculative Decoding for Multimodal Large Language Models](2404.08856.md)
- [PLLaVA : Parameter-free LLaVA Extension from Images to Videos for Video Dense Captioning](2404.16994.md)
- [Video-MME: The First-Ever Comprehensive Evaluation Benchmark of Multi-modal LLMs in Video Analysis](2405.21075.md)
- [MMBench-Video: A Long-Form Multi-Shot Benchmark for Holistic Video Understanding](2406.14515.md)
- [MovieDreamer: Hierarchical Generation for Coherent Long Visual Sequence](2407.16655.md)
- [Octopus v3: Technical Report for On-device Sub-billion Multimodal AI Agent](2404.11459.md)
- [ShareGPT4Video: Improving Video Understanding and Generation with Better Captions](2406.04325.md)
