# MiniGPT4-Video: Advancing Multimodal LLMs for Video Understanding with Interleaved Visual-Textual Tokens
## TL;DR
## Summary
- [https://arxiv.org/pdf/2404.03413.pdf](https://arxiv.org/pdf/2404.03413.pdf)

### 주요 기여 및 혁신
논문은 대규모 언어 모델(LLMs)의 효율적인 파인튜닝을 위한 새로운 방법인 PiSSA(Principal Singular values and Singular vectors Adaptation)를 제안합니다. PiSSA는 모델의 주요 특이값 및 특이벡터를 사용하여 파라미터 공간을 최적화하여 전체 파라미터 파인튜닝보다 뛰어난 성능을 제공합니다. LoRA와 같은 기존 방법과 비교하여 PiSSA는 모델의 본질적인 부분을 초기에 변경하고 '노이즈' 부분을 고정함으로써 더 빠르게 수렴하고 성능을 향상시킵니다.

### 섹션별 요약

#### 1. 소개
대규모 언어 모델의 파인튜닝은 높은 비용이 듭니다. 이를 해결하기 위해 PiSSA라는 파라미터 효율적인 파인튜닝 방법을 제안합니다. PiSSA는 특이값 분해(SVD)를 통해 모델의 행렬 W를 두 개의 학습 가능한 행렬 A와 B의 곱으로 나타내며, 잔차 행렬 W_res는 고정합니다. 이 방법은 초기화 시에 주요 특이값 및 특이벡터를 사용하여 A와 B를 초기화하고, 이를 통해 파인튜닝 성능을 향상시킵니다.

#### 2. 관련 연구
파라미터 효율적인 파인튜닝(PEFT)은 대규모 모델의 파인튜닝 비용을 줄이기 위한 다양한 방법을 포함합니다. 기존의 방법으로는 부분 파인튜닝, 소프트 프롬프트 파인튜닝, 비선형 어댑터 파인튜닝 등이 있으며, 그 중 LoRA는 모델 파라미터의 변경이 저순위 행렬이라는 가정을 기반으로 합니다.

#### 3. 방법론
PiSSA는 SVD를 사용하여 모델의 행렬 W를 주 특이값 및 특이벡터로 나눕니다. 주 행렬 W_pri는 저순위 행렬로 A와 B의 곱으로 표현되며, 잔차 행렬 W_res는 고정됩니다. 이를 통해 PiSSA는 파인튜닝 초기에 모델의 주요 능력을 유지하면서도 효율적으로 학습할 수 있습니다.

#### 4. 실험
PiSSA, LoRA, 및 전체 파라미터 파인튜닝을 비교 실험했습니다. PiSSA는 LoRA와 동일한 설정에서 더 우수한 성능을 보였으며, 특히 수학 문제 해결, 코딩 능력, 대화 능력 등 다양한 작업에서 뛰어났습니다. 또한, PiSSA는 양자화 오류를 줄이는 데 효과적이었으며, 빠른 SVD 기법을 통해 초기화 속도를 높였습니다.

#### 5. 결론
PiSSA는 대규모 언어 모델의 파인튜닝에서 효율적이고 성능이 뛰어난 방법을 제공합니다. 주 특이값 및 특이벡터를 사용한 초기화는 LoRA보다 더 빠르고 정확하게 수렴하며, 다양한 작업에서 우수한 성능을 입증했습니다. PiSSA는 앞으로 더 많은 모델과 작업에서 성능을 입증할 필요가 있으며, LoRA와의 결합을 통한 추가 개선 가능성도 탐구할 것입니다.

### 전체 요약
PiSSA는 대규모 언어 모델의 파인튜닝에서 효율성을 높이기 위한 새로운 접근 방식을 제안합니다. 이 방법은 SVD를 통해 모델의 행렬을 분해하여 주요 특이값 및 특이벡터를 학습 가능한 파라미터로 사용하며, 이를 통해 초기화 시점부터 모델의 주요 능력을 유지하고 빠르게 학습할 수 있습니다. PiSSA는 기존의 LoRA보다 더 빠르고 성능이 뛰어나며, 다양한 작업에서 이를 입증했습니다. 앞으로 PiSSA는 더 많은 모델과 작업에 적용될 것이며, LoRA와의 결합을 통해 추가적인 개선 가능성을 탐구할 것입니다.