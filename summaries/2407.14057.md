# LazyLLM: Dynamic Token Pruning for Efficient Long Context LLM Inference
## TL;DR
## Summary
- [https://arxiv.org/pdf/2407.14057.pdf](https://arxiv.org/pdf/2407.14057.pdf)

### 1. 각 섹션 요약 및 기여 부분

**1. 서론**
이 논문은 긴 문맥의 대규모 언어 모델(LLM)의 추론 효율성을 개선하기 위해 "LazyLLM"이라는 새로운 방식을 제안합니다. LazyLLM은 프리필링(prefilling) 단계와 디코딩 단계에서 중요한 토큰만을 선택적으로 계산하여 첫 번째 토큰을 생성하는 시간을 단축하고 전체적인 생성 속도를 개선합니다.

**2. 관련 연구**
긴 문맥을 포함하는 경우 LLM의 추론 효율성을 높이는 여러 연구가 있습니다. 하지만 대부분의 연구는 디코딩 단계를 중심으로 초점을 맞추었으며, 프리필링 단계를 최적화하는 방법에 대한 연구는 부족했습니다. LazyLLM은 기존 LLM과 원활하게 통합될 수 있는 효율적이고 효과적인 방법을 제공합니다.

**3. LazyLLM 방법론**
이 섹션에서는 LazyLLM의 동작 방식을 설명합니다. LazyLLM은 각 생성 단계에서 이전 단계에 사용할 수 있는 토큰을 동적으로 선택합니다. 이 방법은 정적인 프루닝(pruning)과 달리, 이전 단계에서 제거된 토큰을 다시 사용할 수 있게 합니다.

**4. 구현 세부사항**
LazyLLM은 Llama 2와 XGen 모델에 구현되었고, LongBench 벤치마크를 통해 평가되었습니다. LazyLLM은 별도의 훈련 없이 기존에 공개된 체크포인트(checkpoint)만을 사용합니다.

**5. 실험 및 결과**
LazyLLM은 여러 실험을 통해 다양한 작업(예: 문서 요약, 질문 응답 등)에서 기존의 방법보다 더 나은 성능을 보였습니다. 특히 LazyLLM은 프리필링 단계에서 Llama 2 모델을 2.34배 빠르게 하고, 정확도를 유지했습니다.

**6. 결론**
LazyLLM은 긴 문맥 시나리오에서 효율적인 LLM 추론을 위한 새로운 방법입니다. 중요한 토큰만을 계산하고 나머지 토큰은 필요할 때까지 연기하여 계산 시간을 줄입니다. LazyLLM은 별도의 조정 없이 기존의 Transformer 기반 LLM과 통합할 수 있으며, 여러 작업에서 성능 저하 없이 추론 속도를 개선했습니다.

---

### 2. 전체 요약

이 논문은 긴 문맥을 포함하는 대규모 언어 모델(LLM)의 추론 효율성을 개선하기 위해 "LazyLLM"이라는 새로운 방법을 제안합니다. LazyLLM의 가장 큰 혁신은 프리필링(prefilling) 단계와 디코딩 단계에서 중요한 토큰만 선택적으로 계산하여 첫 번째 토큰 생성 시간을 단축하고 전체 생성을 가속하는 것입니다. 이 방법은 정적인 토큰 프루닝(pruning) 방법과 달리, 이전 단계에서 제거된 토큰을 다시 사용할 수 있게 하여 정확도를 유지합니다.

LazyLLM은 Llama 2와 XGen 모델에 구현되었고, LongBench 벤치마크를 통해 여러 작업(단일 문서 질문 응답, 다중 문서 질문 응답, 요약, 소수 예제 학습 등)에서 평가되었습니다. 평가 결과, LazyLLM은 기존 방법보다 최대 2.34배 더 빠른 속도로 프리필링 단계를 수행하면서도 정확도를 거의 유지하였습니다. 

결론적으로, LazyLLM은 별도의 훈련이나 모델 조정 없이 기존의 Transformer 기반 LLM과 쉽게 통합될 수 있으며, 긴 문맥 시나리오에서 성능 저하 없이 추론 효율성을 크게 개선할 수 있는 혁신적인 접근방식입니다.

## Similar Papers
- [LLM in a flash: Efficient Large Language Model Inference with Limited Memory](2312.11514.md)
- [Superposition Prompting: Improving and Accelerating Retrieval-Augmented Generation](2404.06910.md)
- [TriForce: Lossless Acceleration of Long Sequence Generation with Hierarchical Speculative Decoding](2404.11912.md)
- [KV-Runahead: Scalable Causal LLM Inference by Parallel Key-Value Cache Generation](2405.05329.md)
- [Efficient Streaming Language Models with Attention Sinks](2309.17453.md)
- [MemServe: Context Caching for Disaggregated LLM Serving with Elastic Memory Pool](2406.17565.md)
- [Block Transformer: Global-to-Local Language Modeling for Fast Inference](2406.02657.md)
- [Kangaroo: Lossless Self-Speculative Decoding via Double Early Exiting](2404.18911.md)
- [Medusa: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads](2401.10774.md)
