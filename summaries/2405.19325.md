# Nearest Neighbor Speculative Decoding for LLM Generation and Attribution
## TL;DR
## Summary
- [https://arxiv.org/pdf/2405.19325.pdf](https://arxiv.org/pdf/2405.19325.pdf)

**1. Introduction**
이 논문은 "Nearest Neighbor Speculative Decoding (Nest)"라는 새로운 방법을 소개합니다. 이는 대형 언어 모델(LLMs)이 종종 발생하는 허구 생성 문제를 해결하고 출처를 명확히 하는 데 도움을 줍니다. Nest는 LLM의 출력을 개선하고 실시간 텍스트를 통합하여 신뢰할 수 있는 정보 생성을 목표로 합니다.

**2. Nearest Neighbor Speculative Decoding (Nest)**
Nest는 두 단계로 작동합니다. 첫째, 각 추론 단계에서 텍스트 스팬을 검색하여 잠재적인 스팬 연속성을 식별합니다. 둘째, 예측된 스팬의 접두사를 수용하거나 새 토큰을 생성하는 추론 절차를 사용합니다. 이를 통해 생성 품질을 향상시키고 출처를 명확히 할 수 있습니다.

**3. Experiments**
실험 결과, Nest는 기존의 kNN-LM 방법을 능가하며, 다양한 지식 집약적 작업에서 성능을 크게 향상시켰습니다. 또한, Llama-2-Chat 70B에 적용했을 때 추론 시간을 1.8배 단축하는 성과를 보였습니다.

**4. Conclusion**
Nest는 LLM의 생성 품질과 출처 명확성을 개선하면서도 속도까지 향상시키는 혁신적인 방법입니다. 이는 정보 검색과 생성 간의 균형을 맞추어 정확하고 신뢰할 수 있는 텍스트 생성을 가능하게 합니다.

**5. Broader Impacts**
Nest의 개발은 허구 생성 문제를 줄이고, 더욱 신뢰할 수 있는 AI 시스템 구축에 기여할 수 있습니다. 이는 다양한 지식 집약적 응용 분야에서 유용할 것입니다.

### 종합 요약
이 논문은 "Nearest Neighbor Speculative Decoding (Nest)"이라는 새로운 접근 방식을 제안합니다. Nest는 대형 언어 모델이 텍스트를 생성할 때 발생하는 허구 문제를 해결하고 출처를 명확히 할 수 있도록 합니다. 이를 통해 LLM의 생성 품질을 향상시키고 속도를 높일 수 있습니다. 실험 결과, Nest는 기존 방법을 능가하는 성과를 보였으며, 이는 다양한 지식 집약적 작업에서 유용하게 사용될 수 있습니다. 이 방법은 허구 생성을 줄이고, 더욱 신뢰할 수 있는 AI 시스템을 구축하는 데 기여할 것입니다.

## Similar Papers
- [SEACrowd: A Multilingual Multimodal Data Hub and Benchmark Suite for Southeast Asian Languages](2406.10118.md)
- [FLAME: Factuality-Aware Alignment for Large Language Models](2405.01525.md)
- [MoMa: Efficient Early-Fusion Pre-training with Mixture of Modality-Aware Experts](2407.21770.md)
- [Discrete Flow Matching](2407.15595.md)
- [CoverBench: A Challenging Benchmark for Complex Claim Verification](2408.03325.md)
- [Knowledge Card: Filling LLMs' Knowledge Gaps with Plug-in Specialized Language Models](2305.09955.md)
- [Efficient Streaming Language Models with Attention Sinks](2309.17453.md)
- [On LLMs-Driven Synthetic Data Generation, Curation, and Evaluation: A Survey](2406.15126.md)
- [Language Models are Super Mario: Absorbing Abilities from Homologous Models as a Free Lunch](2311.03099.md)
