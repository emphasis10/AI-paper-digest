# Self-Exploring Language Models: Active Preference Elicitation for Online Alignment
## TL;DR
## Summary
- [https://arxiv.org/pdf/2405.19332.pdf](https://arxiv.org/pdf/2405.19332.pdf)

#### 1. 소개
대규모 언어 모델(LLMs)은 사람의 의도를 따르는 능력으로 인해 큰 성공을 거두고 있습니다. LLM을 사람의 의도에 맞게 조정하기 위한 일반적인 방법은 인간 피드백에서 강화 학습(RLHF)입니다. 그러나, 기존의 방식은 종종 로컬 최적화에 갇히게 되어 잠재적으로 높은 보상을 제공하는 영역을 탐색하지 못하는 문제가 있습니다. 이를 해결하기 위해, 우리는 낙관적 편향을 사용하여 높은 보상을 제공할 가능성이 있는 응답을 적극적으로 탐색하는 방법인 자가탐색 언어 모델(SELM)을 제안합니다.

#### 2. 배경
LLMs는 주어진 입력에 대해 텍스트 응답을 생성합니다. 이 모델들은 사전 훈련과 감독 하에 미세 조정됩니다. RLHF는 모델이 보상 함수를 최대화하도록 최적화합니다. 일반적인 방법은 인간이 레이블을 매긴 선호 데이터에서 보상 모델(RM)을 학습한 후, 이를 기반으로 LLM을 최적화하는 것입니다. 최근 연구는 별도의 보상 모델 없이 직접 선호 데이터를 활용하여 모델을 최적화하는 방법을 탐구하고 있습니다.

#### 3. 자가탐색 언어 모델(SELM)
SELM은 낙관적 편향을 적용하여 보상 적합 목표에 낙관적 용어를 추가합니다. 이를 통해 관측된 데이터에 국한되지 않고 잠재적으로 높은 보상을 제공할 수 있는 응답을 적극적으로 탐색합니다. 이 방식은 보상 모델 없이 LLM을 최적화하는 방법을 도입하여 탐색 효율성을 높입니다. 또한, SELM은 임의로 보지 않은 데이터를 무차별적으로 선호하는 기존 방법의 문제를 완화합니다.

#### 4. 관련 연구
기존의 많은 연구들은 RLHF를 통해 LLM의 성능을 향상시키기 위해 데이터를 합성하거나, 사람의 피드백을 활용한 강화 학습 방법을 사용했습니다. 그러나, 이러한 방법들은 종종 국지적인 최적화에 갇히거나, 전체 언어 공간을 탐색하는 데 어려움을 겪습니다. SELM은 낙관적 탐색 방법을 사용하여 이러한 문제를 해결합니다.

#### 5. 실험
SELM을 Zephyr-7B-SFT와 Llama-3-8B-Instruct 모델에 적용하여 실험한 결과, AlpacaEval 2.0과 MT-Bench 등 여러 벤치마크에서 성능이 크게 향상됨을 확인했습니다. 특히, Zephyr-7B-SFT 모델의 경우 LC 승률이 16.24% 증가했으며, Llama-3-8B-Instruct 모델의 경우 11.75% 증가했습니다.

#### 6. 결론 및 향후 연구
이번 연구에서는 온라인 정렬을 위한 능동적 선호 유도 방법을 제안하였습니다. 제안된 SELM 방법은 관측된 데이터와 잠재적 고보상 영역을 탐색하여 모델의 성능을 향상시킵니다. 향후 연구에서는 SELM을 더 정교한 정렬 프레임워크에 적용하여 그 효과를 검증하고자 합니다.

### 전체 요약
이 논문은 대규모 언어 모델(LLMs)의 성능을 향상시키기 위한 새로운 방법인 자가탐색 언어 모델(SELM)을 제안합니다. SELM은 낙관적 편향을 사용하여 보상 모델 없이 LLM을 최적화하며, 이를 통해 탐색 효율성을 높이고 로컬 최적화 문제를 해결합니다. Zephyr-7B-SFT와 Llama-3-8B-Instruct 모델에 대한 실험 결과, 여러 벤치마크에서 성능이 크게 향상됨을 확인했습니다. 이번 연구는 LLM의 정렬 및 성능 향상을 위한 새로운 접근법을 제시하며, 향후 더 정교한 정렬 프레임워크에 적용될 수 있는 가능성을 보여줍니다.

## Similar Papers
- [SimPO: Simple Preference Optimization with a Reference-Free Reward](2405.14734.md)
- [Value-Incentivized Preference Optimization: A Unified Approach to Online and Offline RLHF](2405.19320.md)
- [WPO: Enhancing RLHF with Weighted Preference Optimization](2406.11827.md)
- [Learn Your Reference Model for Real Good Alignment](2404.09656.md)
- [New Desiderata for Direct Preference Optimization](2407.09072.md)
- [RLHF Workflow: From Reward Modeling to Online RLHF](2405.07863.md)
- [Bootstrapping Language Models with DPO Implicit Rewards](2406.09760.md)
- [From $r$ to $Q^*$: Your Language Model is Secretly a Q-Function](2404.12358.md)
- [sDPO: Don't Use Your Data All at Once](2403.19270.md)
