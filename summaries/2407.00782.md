# Step-Controlled DPO: Leveraging Stepwise Error for Enhanced Mathematical Reasoning
## TL;DR
## Summary
- [https://arxiv.org/pdf/2407.00782.pdf](https://arxiv.org/pdf/2407.00782.pdf)

## 1. 섹션 요약과 주요 공헌 및 혁신 부분 요약

### Abstract
이 논문에서는 큰 언어 모델(LLMs)의 성능을 개선하기 위해 직접 선호 최적화(DPO)가 효과적임을 보여줍니다. 특히 체계적 오류 감독을 자동으로 제공하는 스텝-컨트롤 DPO(SCDPO) 방법을 제안합니다. 수학적 추론의 특정 단계에서 오류를 내는 부정적인 샘플을 생성하여 모델이 오류를 이해하고 정확한 단계를 출력할 수 있도록 합니다. 이 방법은 코드 통합 및 체인-오브-생각(CoT) 솔루션 모두에 적용하여 성과를 향상시킵니다. SCDPO는 여러 SFT 모델에 대해 일관되게 더 나은 성능을 보였으며, InternLM2-20B 모델을 적용하여 높은 점수를 기록했습니다.

### Introduction
다양한 수학 문제 해결에 있어 LLM의 잠재력이 크게 증가하고 있습니다. 기존 연구는 DPO가 LLM의 수학적 추론 능력을 제어하는 데 유용하다고 보여주었습니다. 하지만 이러한 접근 방식은 여러 가지 다양한 오류 경로를 다루지 못했습니다. 본 연구에서는 특정 단계에서 오류를 발생시켜 체계적 감독을 제공하는 SCDPO를 제안합니다. 이 방법은 디테일한 추론 단계를 학습하는 데 효과적입니다.

### Related Work
수학적 추론 향상을 위해 여러 가지 방법이 시도되었습니다. Chain-of-Thought (CoT)와 같은 프롬프트 방식이나 파라미터 최적화 방식 등이 있습니다. 최근 연구는 RL 알고리즘을 적용하여 수학적 정확성을 높이거나 다양한 모델을 미세 조정하여 성과를 개선하고 있습니다. 이에 비해 SCDPO 방식은 추가 인간 작업 없이 감독 데이터를 자동으로 생성하여 더 나은 성과를 보여줍니다.

### Step-Controlled DPO
SCDPO는 두 단계로 구성됩니다. 첫째, 모델은 초기 수학 문제 해결 능력을 바탕으로 정답을 생성합니다. 둘째, 오류를 발생시키기 위해 온도를 조절하여 부정적인 샘플을 생성합니다. 이 샘플들은 학습 데이터로 사용되며, 더 나은 추론 능력을 달성합니다.

### Experiments
다양한 실험을 통해 SCDPO의 성능을 평가했습니다. 세 가지 다른 7B SFT 모델에 대해 일관된 성능 향상을 보였으며, 20B 모델에서는 최고 점수를 기록했습니다. 성능 면에서 SCDPO가 DPO보다 더 우수함을 입증했습니다.

### Conclusion
SCDPO는 체계적 오류 감독을 통해 수학적 추론 능력을 크게 향상시킬 수 있는 방법입니다. 본 연구는 이 방법이 여러 모델에 걸쳐 일관되게 더 나은 성능을 낸다는 것을 보여주며, 다양한 데이터셋에서 탁월한 성과를 도출했습니다.

## 2. 전체 요약
본 논문은 큰 언어 모델(LLMs)의 수학적 추론 능력을 향상시키기 위해 Step-Controlled DPO(SCDPO) 방법을 제안합니다. 기존의 DPO 방식은 최종 답변을 기준으로 선호와 비선호를 판단했기에 세부적인 추론 단계를 잘 다루지 못했습니다. SCDPO는 특정 단계에서 오류를 내는 부정적인 샘플을 자동으로 생성하여, 이러한 문제를 해결하려고 합니다. 다양한 실험을 통해 여러 모델에서 일관된 성능 향상을 확인하였으며, 20B 모델에서는 최고 수준의 성과를 보여줬습니다. 이 방법은 추가적인 인간 작업 없이 자동으로 체계적 오류 데이터를 생성하는 점에서 혁신적입니다. SCDPO를 통해 LLMs의 수학적 문제 해결 능력을 크게 향상시킬 수 있습니다.