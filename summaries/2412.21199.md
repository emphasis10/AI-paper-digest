# HumanEval Pro and MBPP Pro: Evaluating Large Language Models on Self-invoking Code Generation
## TL;DR
## Summary
- [https://arxiv.org/pdf/2412.21199.pdf](https://arxiv.org/pdf/2412.21199.pdf)

1. **섹션별 요약**

    - **소개 및 문제 정의**
      이 논문은 인간이 수행하는 평가기준의 확장된 버전인 HumanEval Pro와 MBPP Pro를 도입하여, 대형 언어 모델(LLM)의 자기 호출 코드 생성 능력을 평가하는 새로운 과제를 제안합니다. LLM은 기본 문제와 연관된 더 복잡한 문제를 해결해야 하며, 자기 생성 코드를 사용하여 복잡한 문제를 해결합니다.

    - **관련 작업**
      LLM의 코드 생성 능력이 향상되었으며, 기존 벤치마크는 주로 코드 생성에 초점이 맞춰져 있었지만, 자기 호출 코드 생성의 중요성을 발견하고 이를 평가할 수 있는 새로운 벤치마크의 필요성을 설명합니다.

    - **벤치마크 생성 방법론**
      HumanEval Pro와 MBPP Pro는 기존의 HumanEval과 MBPP 문제를 기반으로 복잡한 자기 호출 문제를 생성하고, Python의 명령어를 이용해 테스트 케이스를 구축합니다. 문제의 솔루션을 실행하여 출력 결과를 검증하고, 전문가의 검토를 거쳐 해결방안을 최적화합니다.

    - **실험 결과**
      다양한 LLM을 평가한 결과, 대다수의 LLM이 기존의 코드 생성 과제에 비해 자기 호출 문제 해결에서는 성능이 떨어짐을 발견했습니다. 예를 들어, o1-mini는 HumanEval에서 96.2%의 정답률을 보였으나 HumanEval Pro에서는 76.2%로 감소하였습니다.

    - **결론 및 제한점**
      자기 호출 코드 생성에 대한 평가를 통해 LLM의 현재 한계를 명확히 하고, 향후 발전을 위해 새로운 방향을 제시합니다. 다만, 사용한 벤치마크는 Python에 한정되어 있으며, 더 다양한 문제와 다국어 지원의 필요성을 강조합니다.

2. **전체 요약**

    이 논문의 가장 큰 공헌은 LLM의 코드 생성 능력을 보다 철저하게 평가하기 위해 새로운 벤치마크인 HumanEval Pro와 MBPP Pro를 도입한 것입니다. 기존 방식에서 벗어나 LLM이 복잡한 문제를 해결하기 위해 자기 생성 코드를 사용하는 능력이 향상될 필요가 있음을 보였습니다. 이러한 연구는 코드 생성 및 문제 해결에서 LLM의 진정한 능력을 이해하는데 중요한 가이드라인을 제공하며, LLM의 실행 가능한 솔루션을 평가하고 발전시키기 위한 새로운 방법론을 제시합니다.