# Auditing Prompt Caching in Language Model APIs
## TL;DR
## Summary
- [https://arxiv.org/pdf/2502.07776.pdf](https://arxiv.org/pdf/2502.07776.pdf)

### 1. 각 섹션의 주요 내용 요약 (한국어로)

#### 서론
대형 언어 모델(LLM)의 프롬프트 캐싱은 데이터에 따라 처리 시간이 달라지며, 이로 인해 사이드 채널 공격의 위험이 증가한다. 본 논문에서는 실제 API 제공업체에서 프롬프트 캐싱을 감시하기 위한 통계적 감사를 개발하고 수행하였다.

#### 프롬프트 캐싱 설명
프롬프트 캐싱은 요청 간 키-값(KV) 캐시를 재사용하여 빠른 응답을 생성하게 해준다. 캐시된 프롬프트는 호출 속도가 빠르지만, 이는 사용자의 개인 정보를 유출할 위험이 있다. 저자는 API 제공업체의 캐싱 정책의 투명성이 중요하다고 강조한다.

#### 캐싱 감시 결과
7개의 API 제공업체에서 전역 캐시 공유를 발견했으며, 이는 다른 사용자들의 프롬프트에 대한 정보를 유출할 수 있는 가능성을 보여준다. 특히 OpenAI의 임베딩 모델이 디코더 전용이라는 구조적 정보도 유출되었다.

#### 감시 방법
저자들은 서로 다른 캐시 공유 수준에 따라 다양한 통계 감사 방법을 수립하였으며, 각 감사에서 발견된 p-값을 보고하였다. 프롬프트의 형식적 특성이 신뢰도 및 정확도에 미치는 영향을 분석하였다.

#### 결론
LLM과 기타 기계 학습 시스템이 널리 배포됨에 따라 보안 및 개인정보 보호 측면의 고려가 중요해진다. 저자들은 이러한 시스템의 견고함과 신뢰성을 보장하기 위한 지속적인 평가와 감사가 필요하다고 주장한다.

### 논문의 기여 및 혁신적인 부분
- **프롬프트 캐싱의 위험성**: 프롬프트 캐싱이 개인 정보 유출을 초래할 수 있는 메커니즘을 상세히 설명하였다.
- **통계적 감사 개발**: 실제 API 제공업체에 대한 정밀한 감사 방법론을 개발하여 발견된 문제를 수치적으로 나타냈다.
- **정보 유출 기록**: 감시에 포함된 API들의 프롬프트 캐시 사용에 대한 구체적 기록을 제공하여, 향후 유사한 연구에 기초 데이터를 제공한다.

---

### 2. 전체 요약 (한국어로)
본 논문은 대형 언어 모델의 프롬프트 캐싱의 보안과 개인정보 보호 문제를 다룬다. 프롬프트 캐싱은 응답 속도를 증가시킬 수 있지만, 이는 사용자 프롬프트 데이터의 유출로 이어질 수 있는 위험이 있다. 저자들은 7개의 API 제공업체에서 전역 캐시 공유를 식별하였으며 이를 통해 개인 정보가 어떻게 유출될 수 있는지를 증명하였다. 또한, 디코더 전용 구조와 같은 모델 아키텍처 정보를 드러내는 흥미로운 결과도 도출되었다. 이 연구는 향후 LLM의 보안성과 신뢰성을 높이기 위한 방법론을 제안하며, 지속적인 감사의 중요성을 강조한다.