# RetroLLM: Empowering Large Language Models to Retrieve Fine-grained Evidence within Generation
## TL;DR
## Summary
- [https://arxiv.org/pdf/2412.11919.pdf](https://arxiv.org/pdf/2412.11919.pdf)

1. **섹션별 요약**

   - **서론**
     이 연구는 대형 언어 모델(LLM)들의 생성 능력은 우수하지만 가끔 “환각”을 일으킨다는 문제를 언급하며 시작됩니다. 이를 해결하기 위해, 외부 지식을 통합하는 검색-증강 생성(RAG)이 효과적인 해결책으로 부상하고 있으며, RetroLLM이라는 새로운 프레임워크를 제안합니다. RetroLLM은 검색 및 생성을 하나의 과정으로 통합하여 더욱 정밀한 증거를 자동으로 생성합니다.
   
   - **본 논문 기여**
     연구의 주요 기여는 RetroLLM을 제안하여 검색과 생성을 하나의 자기 회귀적 프로세스로 통합했다는 점입니다. 이를 통해 별도의 검색기를 사용할 필요가 없어지고 RAG 작업의 공동 최적화가 가능해졌습니다.
   
   - **문제 정의 및 연구 배경**
     기존의 RAG 방법들은 별도의 밀집 검색기를 사용하여 텍스트 청크를 검색하는데, 이는 배치 및 추가 비용을 증가시키고, 생성과 검색의 공동 최적화를 방해합니다. 이 연구는 이러한 문제를 해결하고자 합니다.
   
   - **RetroLLM 프레임워크**
     RetroLLM은 FM-인덱스를 사용하여 정밀한 증거를 검색하는 자기 회귀적 생성 과정을 사용하며, 생성 중에는 추가 증거 생성을 결정하거나 최종 응답을 제공합니다. 이 과정은 보다 정교한 검색 및 생성의 관계를 이해하게 하며 전반적인 성능을 향상시킵니다.
   
   - **실험 및 결과**
     RetroLLM의 성능은 다양한 공개 도메인 QA 데이터셋에서 기존의 RAG 방법들에 비해 우수한 결과를 보여줍니다. 이 연구는 또한 다양한 LLM들에 대한 성능 평가를 통해 모든 모델이 RetroLLM의 효율성을 확인했습니다.
   
   - **제약 사항 및 미래 연구**
     RetroLLM은 다양한 QA 시나리오에서 강력한 성능을 보여주었지만, 향후 연구에서는 단서를 확장하는 메커니즘을 설계하는 것이 필요합니다.

2. **종합 요약**

   이 연구는 대형 언어 모델들이 때때로 환각을 일으키는 문제를 해결하기 위해 검색-증강 생성(RAG) 접근 방식을 제안합니다. RetroLLM은 검색과 생성을 하나의 자기 회귀적 프로세스로 통합하여 대형 언어 모델의 정밀한 증거 생성을 가능케 하고, 이를 통해 별도의 검색기를 사용할 필요가 없어집니다. 결과적으로, 다양한 QA 데이터셋에서 다른 기법들보다 뛰어난 성능을 보입니다. 향후 연구에서는 시스템의 완전한 최적화를 위한 단서 확장을 목표로 하고 있습니다.