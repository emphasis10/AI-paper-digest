# Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets
## TL;DR
## Summary
- [https://arxiv.org/pdf/2201.02177.pdf](https://arxiv.org/pdf/2201.02177.pdf)

### 1. 섹션별 요약

#### 서론
이 논문에서는 작은 알고리즘적으로 생성된 데이터셋에서 신경망의 일반화를 연구합니다. 일반화는 훈련 데이터에 대한 성능과는 별개로 나타나며, 데이터 효율성, 암기화, 학습 속도 등 다양한 질문을 다룰 수 있습니다. 특히, 과적합 이후에 일반화 성능이 크게 향상되는 'grokking' 현상을 설명합니다.

#### 방법론
모든 실험에서 작은 변환기 모델을 사용하여 각 이진 연산을 포함하는 수식을 학습합니다. 실험 세부사항과 하이퍼파라미터 조정 방법이 자세히 설명되어 있습니다.

#### 실험
1. **과적합 이후의 일반화**:
   - 신경망이 훈련 셋에서는 매우 빨리 최적화되지만, 검증 셋에서는 많은 시간이 지나야만 일반화 성능이 향상된다는 것을 보여줍니다.
   - 일반화 시간 곡선에서 데이터 셋의 크기가 감소할수록 최적화 시간이 급격히 증가하는 현상을 관찰합니다.
   
2. **다양한 문제에서의 Grokking**:
   - 여러 이진 연산에서 평균 정확도를 측정하고, 대칭적인 연산이 비대칭적인 연산보다 일반화에 더 적은 데이터를 요구한다는 것을 발견했습니다.

3. **소거 및 트릭**:
   - 가중치 감소와 같은 다양한 정규화 기법이 데이터 효율성을 크게 개선시킵니다. 특히, 가우시안 노이즈 추가 및 적정 학습률 조정이 효과적입니다.

#### 질적 시각화
모듈러 덧셈과 S5 연산에서 출력 레이어 행 벡터의 t-SNE 시각화를 통해, 네트워크가 학습한 기호 임베딩의 구조적 특성을 보여줍니다. 특히, 가중치 감소를 통해 최적화된 네트워크는 더 명확한 구조를 드러냅니다.

#### 토의
작은 알고리즘 데이터 셋에서 관찰된 다양한 일반화 현상과 이를 개선하기 위한 방법들을 논의합니다. 미래 연구 방향으로, 최솟값 평탄도의 측정이 일반화와의 상관관계를 확인하는 것에 대한 가능성을 제시합니다.

### 2. 전체 요약

이 논문은 신경망이 작은 알고리즘 학습 데이터 셋에서 일반화하는 방법을 연구합니다. 본 연구의 주요 기여는 다음과 같습니다:
1. **Grokking 현상 설명**: 과적합 이후에 검증 성능이 극적으로 향상되는 현상을 관찰하고 설명합니다.
2. **효율적인 방법 제시**: 데이터 효율성을 높이는 다양한 정규화 및 최적화 기법을 실험적으로 검증합니다.
3. **질적 시각화 제공**: 네트워크가 학습한 기호의 구조적 특성을 시각적으로 확인합니다.

연구를 통해 신경망의 일반화 패턴을 이해하고, 작은 데이터셋에서도 높은 일반화 성능을 달성할 수 있는 방법을 제시하며, 이는 향후 더 복잡한 모델의 학습 및 일반화 연구에 기여할 것입니다.