# JetMoE: Reaching Llama2 Performance with 0.1M Dollars
## TL;DR
## Summary
- [https://arxiv.org/pdf/2404.07413.pdf](https://arxiv.org/pdf/2404.07413.pdf)

### 섹션별 요약

1. **서론 (Introduction):**
   JetMoE-8B는 'ModuleFormer' 아키텍처를 기반으로 하는 새로운 대형 언어 모델로, Sparse Mixture-of-Experts(SMoE) 구조를 활용하여 성능을 높이면서 비용을 줄였습니다. JetMoE-8B는 대형 모델의 성능을 보여주면서도 상대적으로 낮은 비용($0.1M 이하)으로 훈련되었습니다.

2. **모델 아키텍처 (Model Architecture):**
   JetMoE-8B 모델은 Sparse Mixture of Experts 구조를 사용하여, 전체 파라미터 중 일부만 활성화시킴으로써 계산 비용을 대폭 줄였습니다. 이로 인해, 동일한 데이터 입력에 대해 활성화되는 파라미터 수가 제한되어 효율성을 높이며, 이는 기존의 밀집 모델의 비효율성을 극복하는 데 중요한 역할을 합니다.

3. **주요 기여 및 혁신 (Key Contributions and Innovations):**
   - JetMoE-8B는 오픈 소스 데이터를 활용하여 훈련됨으로써 연구자들에게 공개되었고, 사용상의 장벽을 줄였습니다.
   - SMoE를 사용하여, 이전 모델에 비해 70% 이상의 계산 비용 절감이 가능하게 하였습니다.
   - 공개된 데이터 혼합과 훈련 매개변수가 제공되어, 다른 연구자들이 이를 기반으로 연구를 확장할 수 있도록 하였습니다.

4. **결론 (Conclusion):**
   JetMoE-8B는 오픈 소스 모델 중에서 최첨단 성능을 유지하면서도 높은 효율성을 달성한 모델로, 언어 모델의 민주화를 위한 중요한 진전을 나타냅니다. 이를 통해 연구자들이 고급 언어 기술을 더 널리 이용할 수 있게 되었으며, 재현성을 위한 훈련 매개변수 및 데이터 혼합 정보가 제공되었습니다.

### 전체 요약

JetMoE-8B 모델은 대규모 언어 모델의 계산 비용을 크게 줄이면서도 높은 성능을 유지하는 혁신적인 SMoE 아키텍처를 바탕으로 하고 있습니다. 이 모델의 주요 기여는 저비용으로 대규모 언어 모델을 효과적으로 훈련할 수 있는 방법을 제시한 것과 오픈 소스 데이터를 통해 학계와의 협력을 촉진한 것입니다. JetMoE-8B는 언어 모델의 발전에 중요한 기여를 했으며, 앞으로 더 많은 연구자들이 이 모델을 기반으로 새로운 연구를 진행할 수 있도록 관련 정보를 공개하였습니다. 이를 통해 JetMoE-8B는 언어 기술의 민주화에 크게 기여하게 되었습니다.