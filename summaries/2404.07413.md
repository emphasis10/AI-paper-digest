# JetMoE: Reaching Llama2 Performance with 0.1M Dollars
## TL;DR
## Summary
- [https://arxiv.org/pdf/2404.07413.pdf](https://arxiv.org/pdf/2404.07413.pdf)

논문 "JetMoE: Reaching Llama2 Performance with 0.1M Dollars"를 요약하면 다음과 같습니다:

### 1. 서론
이 논문은 저비용으로 Llama2 모델과 비슷한 성능을 달성하는 새로운 대규모 언어 모델인 JetMoE-8B를 소개합니다. 이 모델은 공개 데이터셋과 오픈 소스 코드만을 사용하여 훈련되었으며, 고비용의 언어 모델 훈련에 대한 대안을 제공하고자 합니다【43†source】.

### 2. 모델 아키텍처
JetMoE-8B는 모듈포머 아키텍처에서 영감을 받아 자체 주의(Self-attention) 및 전방 피드(Feed-forward) 레이어에 모두 희소 활성화를 적용한 혁신적인 구조입니다. 이 구조는 계산 비용을 크게 줄이면서 성능을 유지합니다【43†source】.

### 3. 훈련 데이터 및 파라미터
JetMoE-8B는 1.25T 토큰을 사용하여 30,000 H100 GPU 시간 동안 훈련되었습니다. 이 모델은 두 단계의 훈련 과정을 거치며, 고품질 데이터의 비중을 점차 늘려가며 학습합니다【43†source】.

### 4. 평가 및 성능
JetMoE-8B는 Llama2-7B 모델을 능가하며, 특히 저비용에도 불구하고 인상적인 성능을 보여줍니다. 이는 언어 모델의 훈련이 일반적으로 생각하는 것보다 훨씬 비용 효율적일 수 있음을 시사합니다【43†source】.

### 5. 결론
JetMoE-8B는 개방성과 학계 친화적인 특성, 비용 효율적인 훈련 과정을 통해 대규모 언어 모델의 개발 방향에 새로운 가능성을 제시합니다. 이 모델은 연구자들이 자체적으로 실험을 반복하고 확장할 수 있도록 모든 훈련 파라미터와 데이터 구성을 공개하고 있습니다【43†source】.