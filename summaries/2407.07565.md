# On Leakage of Code Generation Evaluation Datasets
## TL;DR
## Summary
- [https://arxiv.org/pdf/2407.07565.pdf](https://arxiv.org/pdf/2407.07565.pdf)

### 논문 섹션별 요약 (한국어)

#### 1. 서론
코드 생성은 대형 언어 모델(LLM)이 마스터해야 하는 중요한 기술로 부각되었습니다. 이러한 코드 생성 능력을 평가하는 데 사용되는 주요 벤치마크에는 HumanEval과 MBPP가 있으며, 이는 최근 대부분의 코드 생성 모델 비교에 사용되고 있습니다. 그러나 이 벤치마크들은 데이터를 훈련 세트에 노출시켜 평가의 신뢰성을 저해할 수 있는 문제, 즉 데이터 오염 문제를 가지고 있습니다. 본 논문은 이 데이터 오염 문제를 다루기 위해 새로운 평가 데이터셋인 LBPP를 제안합니다.

#### 2. 관련 연구
HumanEval과 MBPP는 가장 많이 보고된 코드 생성 평가 데이터셋이며, 다른 경쟁력 있는 데이터셋들도 존재합니다. 이러한 데이터셋은 대부분 간단한 Python 코딩 문제들과 이에 대한 솔루션으로 구성되어 있습니다. 최근에는 더 어려운 문제를 포함한 지속적으로 업데이트되는 데이터셋도 제안되었습니다.

#### 3. 오염의 가능한 원천
- **직접적인 데이터 누출**: 훈련 데이터에 평가 데이터셋이 포함되어 코드 생성 모델의 일반화 능력 평가가 신뢰성을 잃게 되는 경우가 있습니다.
- **합성 데이터로 인한 오염**: 합성 데이터는 코드 생성 능력을 높이기 위해 널리 사용되며, 이는 기존 평가 데이터셋과 높은 유사성을 나타낼 수 있습니다.
- **테스트 세트에 대한 과적합**: 모델 선택 과정에서 벤치마크 성능이 지나치게 중요시되면 특정 벤치마크에 과적합하여 실제 코드 생성 능력의 평가가 왜곡될 수 있습니다.

#### 4. 결론
HumanEval과 MBPP의 데이터 오염은 대형 언어 모델 스케일에서 불가피하다는 것을 나타냅니다. 본 연구는 이러한 오염 문제를 해결하기 위해 LBPP라는 새로운 코드 생성 벤치마크를 제안합니다. 이 데이터셋은 기존의 평가 세트를 대체하거나 보완할 수 있게 설계되었으며, 보다 어려운 문제들로 구성되어 있어 모델 성능 비교에 더 많은 시그널을 제공합니다.

### 종합 요약
이 논문은 코드 생성 능력을 평가하는 데 사용되는 기존 벤치마크 데이터셋(특히 HumanEval과 MBPP)이 평가 신뢰성을 저해할 수 있는 데이터 오염 문제를 가지고 있음을 지적하고 있습니다. 이러한 문제를 해결하기 위해 새로운 평가 데이터셋인 LBPP를 소개합니다. LBPP는 기존 데이터셋보다 더 어려운 문제들로 구성되어 있으며, 데이터 오염 가능성을 최소화하여 현재의 코드 생성 능력을 보다 정확하게 평가할 수 있도록 설계되었습니다. 

LBPP의 도입으로 단순하고 직관적인 평가 방식에서 벗어나 보다 복잡하고 도전적인 기준을 통해 모델 성능을 평가할 수 있게 되며, 이를 통해 대형 언어 모델의 코드 생성 능력을 보다 신뢰성 있게 평가할 수 있을 것으로 기대됩니다.