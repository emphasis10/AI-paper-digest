# The Power of Scale for Parameter-Efficient Prompt Tuning
## TL;DR
## Summary
- [https://arxiv.org/pdf/2104.08691.pdf](https://arxiv.org/pdf/2104.08691.pdf)

1. 각 섹션 요약:

1. **서론 (Introduction)**:
   이 논문에서는 "프롬프트 튜닝(prompt tuning)"이라는 간단하면서도 효과적인 메커니즘을 제안하여, 고정된 언어 모델이 특정 다운스트림 작업을 수행하도록 조건을 설정했습니다. 이는 GPT-3의 이산 텍스트 프롬프트와 달리, 역전파를 통해 학습된 소프트 프롬프트를 활용하여 학습 데이터의 신호를 통합할 수 있습니다.

2. **프롬프트 튜닝 (Prompt Tuning)**:
   T5의 "텍스트-텍스트(text-to-text)" 접근 방식을 따라 모든 작업을 텍스트 생성 문제로 변환합니다. 기존에는 클래스를 주어진 입력으로 모델링했지만, 이제는 클래스 레이블을 나타내는 토큰 시퀀스 Y로 모델링합니다. 프롬프트 튜닝은 새롭게 도입된 프롬프트에 대해 전용 매개변수를 사용하여 모델 파라미터를 고정시킵니다.

3. **모델 크기의 영향 (The Power of Scale)**:
   대규모 언어 모델에서는 프롬프트 튜닝이 모델 튜닝과 비교하여 경쟁력을 가지게 됩니다. 이는 고정된 모델을 여러 다운스트림 작업에 재사용할 수 있도록 하여 저장과 서비스를 효율적으로 만드는 장점이 있습니다.

4. **비교 연구 (Comparison to Similar Approaches)**:
   프롬프트 튜닝이 얼마나 적은 작업 특정 매개변수를 사용하는지에 대해 논의하고, 유사한 접근 방식과의 비교를 통해 프롬프트 엔섬블링의 효율성을 강조합니다.

5. **도메인 시프트에 대한 내성 (Resilience to Domain Shift)**:
   프롬프트 튜닝은 도메인 시프트 문제에서 모델 튜닝보다 더 성능이 좋은 것으로 나타났습니다. 이는 입력의 분포가 다를 때도 모델의 일반 언어 이해 능력을 유지하도록 합니다.

6. **프롬프트 엔섬블링 (Prompt Ensembling)**:
   동일한 작업을 위해 여러 프롬프트를 학습함으로써 품질을 높이고 효율적으로 모델 엔섬블링을 대체할 수 있음을 보입니다.

2. 전체 요약:

이 논문의 주요 기여는 프롬프트 튜닝이라는 기법을 제안하고, 이를 통해 대규모 언어 모델 환경에서 모델 튜닝과의 경쟁력을 갖춘다는 것을 보여주는 것입니다. 프롬프트 튜닝은 모델 파라미터를 고정시킨 채로 소프트 프롬프트만을 학습하여 대규모 데이터 세트로부터 최적의 신호를 모을 수 있게 하며, 도메인 시프트 문제에 대한 내성을 높임으로써 모델을 보다 다양한 작업에 효과적으로 적용할 수 있음을 입증하였습니다.

이는 AI와 기계 학습 분야를 발전시키려는 연구에 중요한 영향력을 미치며, 대규모 모델을 활용할 수 있는 효율적이고 새로운 방법론을 제공합니다.