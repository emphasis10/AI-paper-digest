# SpeechGuard: Exploring the Adversarial Robustness of Multimodal Large Language Models
## TL;DR
## Summary
- [https://arxiv.org/pdf/2405.08317.pdf](https://arxiv.org/pdf/2405.08317.pdf)

### 주요 내용 요약

1. **서론 및 배경**:
   - 이 논문에서는 통합 음성 및 언어 모델(SLM)의 안전성과 견고성을 조사합니다. SLM은 음성 명령을 따르고 관련 텍스트 응답을 생성할 수 있는 모델로, 최근 들어 인기를 얻고 있습니다. 그러나 이러한 모델의 안전성과 견고성은 아직 명확하지 않습니다. 이 논문은 SLM이 적대적 공격과 탈옥(jailbreaking) 공격에 취약할 수 있음을 보여주고, 이를 방지하기 위한 대책을 제안합니다.

2. **방법론**:
   - **공격 알고리즘**: 백박스(white-box) 및 블랙박스(black-box) 공격 설정에서 인간 개입 없이 적대적 예제를 생성하는 알고리즘을 설계합니다. 백박스 공격에서는 모델의 기울기에 접근하여 공격을 수행하고, 블랙박스 공격에서는 다양한 모델 간의 전이 공격을 탐구합니다.
   - **대책**: 탈옥 공격을 막기 위한 대책으로, 입력 음성 신호에 무작위 잡음을 추가하는 방식을 제안합니다. 이 방법은 모델의 프론트엔드 음성 인코더가 무작위 잡음에 견고하도록 하여, 적대적 방해를 효과적으로 무력화합니다.

3. **실험**:
   - **음성 질의응답(Spoken QA) 작업**: 다양한 SLM을 사용하여 안전성, 유용성, 생성된 응답의 관련성을 평가합니다. 백박스 공격에서는 기울기에 접근하여 barely perceptible perturbations(거의 인식할 수 없는 방해)으로 모델을 탈옥시키는 데 성공했습니다. 전이 공격에서는 한 모델에서 생성된 방해를 다른 모델에 적용하여 탈옥 성공률을 측정했습니다.
   - **대책 효과**: 무작위 잡음 추가 방식을 적용하여 탈옥 공격 성공률을 크게 줄일 수 있음을 보였습니다. 추가된 잡음의 신호 대 잡음비(SNR) 값에 따라 방어 효과가 달라졌습니다.

### 혁신적인 부분
이 논문의 혁신성은 통합 음성 및 언어 모델(SLM)의 안전성과 견고성을 처음으로 체계적으로 평가하고, 적대적 공격에 대한 대책을 제안한 데 있습니다. 백박스 및 블랙박스 공격 설정에서 탈옥 공격의 성공률을 분석하고, 무작위 잡음 추가 방식이 효과적인 방어 대책이 될 수 있음을 입증하였습니다. 이 연구는 SLM의 잠재적 취약성을 이해하고 이를 방지하기 위한 기초 연구로서 중요한 기여를 합니다.