# Towards Fast Multilingual LLM Inference: Speculative Decoding and Specialized Drafters
## TL;DR
## Summary
- [https://arxiv.org/pdf/2406.16758.pdf](https://arxiv.org/pdf/2406.16758.pdf)

### 1. 논문의 각 섹션 요약

#### 1. 서론 (Introduction)
이 논문은 대규모 언어 모델(LLM)의 다국어 맥락에서의 추론 시간을 줄이기 위한 새로운 방법을 제시합니다. 특히, '추측 디코딩(speculative decoding)'과 '언어별 드래프터 모델(drafter models)'을 사용하여 속도를 개선하는 방법을 탐구합니다. 이러한 모델은 다양한 언어의 텍스트를 빠르게 이해하고 생성하는 데 있어 중요한 역할을 합니다.

#### 2. 방법론 (Method)
##### 2.1 초기 기반: 추측 디코딩 (Preliminaries: speculative decoding)
추측 디코딩은 보조 모델을 활용하여 미래의 토큰을 예측하고, 이를 타겟 LLM이 검증하는 과정입니다. 이 과정은 LLM 추론의 속도를 높이기 위해 메모리 대역폭 문제를 해결합니다.

##### 2.2 동기 (Motivation)
다양한 추측 디코딩 모델을 평가한 결과, 특정 작업(domain)에 따라 성능 차이가 큽니다. 특히, 다국어 번역 작업에서 성능이 저조한 문제점을 식별하고, 이를 해결하기 위해 언어별 드래프터 모델을 개발합니다.

##### 2.3 언어별 보조 모델 훈련 (Training specialized assistant models)
작은 모델은 다양한 언어 토큰 분포를 포착하는 데 한계가 있으므로, 각 언어에 특화된 드래프터 모델을 제안합니다. 먼저, 보조 모델을 대규모 데이터셋(C4 및 ShareGPT)을 통해 사전 훈련(pretrain)한 후, 목표 언어 작업에 대해 미세 조정(finetune)합니다. 이를 통해 모델의 성능과 속도를 크게 향상시킵니다.

#### 3. 실험 (Experiment)
##### 3.1 실험 설정 (Experimental setup)
Vicuna 7B, Gemma-Instruct 7B, Llama2-chat와 같은 타겟 LLM을 사용하고, 이에 대응하는 드래프터 모델(Vicuna 68M, Gemma 250M drafter, Llama 68M)을 활용합니다. 다양한 설정에서 이들 모델의 성능을 검증합니다.

##### 3.2 주요 결과 (Main result)
사전 훈련 후 미세 조정된 드래프터 모델을 사용한 결과, 추론 속도가 크게 향상되었습니다. 특히, 사전 훈련 및 미세 조정이 이루어진 모델이 단독 미세 조정 모델보다 성능이 뛰어납니다. 또한, 드래프터 모델이 다양한 데이터셋에서 높은 속도 향상 비율을 기록함을 보여줍니다.

#### 4. 결론 (Conclusion)
추측 디코딩과 언어별 드래프터 모델을 활용한 접근 방식이 다국어 LLM 추론 속도를 현저히 개선할 수 있음을 확인하였습니다. 이 방법은 다양한 언어 환경에서 LLM의 상용화를 촉진할 수 있는 중요한 발전 가능성을 나타냅니다.

#### 부록 (Appendix)
부록은 이 논문의 주요 내용을 보완하는 추가 자료들을 제공합니다. 이에는 더 광범위한 영향, 향후 연구 방향, 관련 문헌 검토, 알고리즘 세부 사항 및 추가 실험 결과가 포함됩니다.

### 2. 전체 요약
이 논문은 다국어 환경에서의 LLM 추론 시간을 줄이기 위해 추측 디코딩 및 언어별 드래프터 모델을 활용하는 새로운 방법을 제시합니다. 보조 모델을 통해 미래의 토큰을 예측하고 이를 타겟 LLM이 검증하는 과정에서, 모델을 사전 훈련하고 미세 조정하여 성능을 대폭 향상시키는 전략을 탐구합니다. 실험 결과, 사전 훈련된 드래프터 모델이 다양한 언어 작업에서 추론 속도를 크게 개선했음을 확인하였습니다. 이러한 접근 방식은 다국어 LLM의 상용화를 촉진할 수 있는 중요한 기술적 발전을 의미합니다.