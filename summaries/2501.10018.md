# DiffuEraser: A Diffusion Model for Video Inpainting
## TL;DR
## Summary
- [https://arxiv.org/pdf/2501.10018.pdf](https://arxiv.org/pdf/2501.10018.pdf)

### 논문의 각 섹션 요약

1. **서론**
   - 비디오 인페인팅은 가려진 부분을 공간적, 시간적으로 자연스럽게 채워 넣는 기술을 목표로 합니다. 기존 알고리즘은 주로 광학 흐름 기반의 픽셀 전파와 변환기 기반의 방법을 결합하여 사용합니다.

2. **관련 작업**
   - 디퓨전 모델은 이미지 및 비디오 생성의 품질을 크게 향상시켰습니다. Propainter는 이러한 기술을 활용하여 구조를 완성하는 대표적인 접근법이지만, 큰 마스크를 처리할 때는 제너레이티브 능력이 부족하여 아티팩트를 남긴다는 한계가 있습니다.

3. **제안된 방법론** 
   - **DiffuEraser**는 안정적인 디퓨전 모델을 기반으로 한 비디오 인페인팅 모델이며, 이동 모듈을 결합하여 더욱 섬세한 생성이 가능합니다. 이는 불필요한 객체 생성을 억제하고 시간적 연속성을 강화합니다.

4. **실험**
   - Panda-70M 데이터 세트를 이용하여 모델을 훈련했습니다. 비디오 시퀀스를 나누고, 여러 장치를 이용해 병렬적으로 학습하는 방식을 사용하여 속도를 개선하였으며, 결과물은 알고리즘 효율성을 입증합니다.

5. **결론**
   - DiffuEraser는 최첨단 방법보다 더욱 정교한 콘텐트 완전성과 시간적 일관성을 보여주며, 다른 비디오 편집 작업에도 적용 가능합니다. 향후 작업에서는 상호 참조를 통해 이러한 기법을 더 확장할 계획입니다.

### 논문의 전체적인 요약

논문은 주로 비디오 인페인팅 작업에서 발생하는 블러, 모자이크 아티팩트 등을 효과적으로 해결하는 데 중점을 둡니다. DiffuEraser는 알려진 픽셀 전파, 알려지지 않은 픽셀 생성, 그리고 소프트웨어적 시간적 일관성 문제를 각각 해결하기 위한 독창적인 솔루션을 제안합니다. 디퓨전 모델의 제너레이티브 능력을 활용하여 기존의 한계를 극복하였으며, Priors와의 결합을 통해 더 정확한 결과를 얻었습니다. 접근 방식은 특히 긴 시퀀스의 일관성을 유지하고, 비디오 편집의 다른 영역으로의 응용 가능성을 지닙니다.