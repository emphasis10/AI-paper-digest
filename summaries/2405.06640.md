# Linearizing Large Language Models
## TL;DR
## Summary
- [https://arxiv.org/pdf/2405.06640.pdf](https://arxiv.org/pdf/2405.06640.pdf)

### 1. 중요 섹션 요약

#### 소개 (Introduction)
이 연구는 최근 몇 년간 연산 효율성과 척도 성능 측면에서 순환 신경망(RNNs)을 대체하고 있는 트랜스포머의 발전을 살펴봅니다. 그럼에도 불구하고 트랜스포머는 매개변수 수에 따라 비례하는 메모리 비용이 존재하며, 고정 크기의 상태로 효율적인 모델링을 지원하는 회귀 모델에 대한 관심이 높아지고 있습니다.

#### 방법론 (Methodology)
본 연구는 Kasai 등(2021)의 선형화 기법을 기반으로 한 SUPRA(Scalable UPtraining for Recurrent Attention) 기법을 제안합니다. 이 방법은 기존의 고성능 트랜스포머를 RNN으로 변환하여 적은 예산으로 학습하여, 품질 좋은 사전 학습 데이터를 활용할 수 있게 합니다.

#### 실험 (Experiments)
다양한 1B에서 7B 범위 모델을 RNN으로 업트레이닝하여 표준 언어 이해 벤치마크와 장문 맥락 평가를 수행했습니다. SUPRA 모델이 상대적으로 더 높은 성능을 보이며, 각각의 건축적 선택 및 학습 전략의 차이를 분석하였습니다.

#### 논의 (Discussion)
SUPRA가 구현된 RNN과 기존의 트랜스포머 모델 간의 성능 격차를 이야기하며, SUPRA가 강력한 사전 학습된 트랜스포머 모델에서 시작할 때 더 나은 결과를 제공함을 보여줍니다. 이에 대한 지속적인 연구와 개선이 필요함을 언급하고 있습니다.

#### 관련 작업 (Related Work)
다양한 선형 트랜스포머와 RNN 구조들이 비교됩니다. 기존 트랜스포머는 많은 데이터로 자원을 소비하는 반면, SUPRA는 이를 회피하고 고성능을 유지하는 방법을 소개합니다.

#### 결론 (Conclusion)
SUPRA는 고성능 트랜스포머를 RNN으로 변환하는 기술로, 적은 컴퓨팅 비용으로 대규모 모델의 장점과 단점을 분석하는데 기여합니다. 본 연구는 모델의 효율성을 유지하면서 최대한의 성능을 양립하는데 있어 중요한 통찰을 제공합니다.

### 2. 전체 요약
이 논문은 트랜스포머 기반의 모델과 생성순환신경망(RNN)의 혼합 접근법을 통해, 고성능 언어 모델을 효율적으로 변환하고 사용하는 방법론인 SUPRA를 제안합니다. SUPRA는 고성능 트랜스포머를 이용해 RNN으로 전환함으로써, 적은 자원으로도 높은 성능을 유지하며 다양한 벤치마크에서 경쟁력 있는 성과를 내옵니다. 이 연구의 주요 기여는 강력한 사전 학습 트랜스포머를 바탕으로 한 새로운 선형화 전략을 통해 성능을 유지하면서 효율성을 획득한 점입니다. 더 나아가 장문 맥락에서의 지속 가능한 성능을 보장하는 여러 아이디어와 차후 연구 방향성을 제시합니다.