# Linearizing Large Language Models
## TL;DR
## Summary
- [https://arxiv.org/pdf/2405.06640.pdf](https://arxiv.org/pdf/2405.06640.pdf)

### 주요 내용 요약

1. **서론 및 배경**:
   - 이 논문에서는 대규모 언어 모델(LLMs)을 순환 신경망(RNNs)으로 변환하는 새로운 접근 방식인 Scalable UPtraining for Recurrent Attention (SUPRA)를 제안합니다. 이 방법은 기존 LLM의 강력한 사전 훈련 데이터와 성능을 활용하면서 훈련 비용의 5%만을 요구합니다.

2. **방법론**:
   - SUPRA는 기존 트랜스포머를 RNN으로 변환하는 과정을 설명하며, 이를 위해 softmax 정규화 대신 GroupNorm을 사용하고, 쿼리와 키에 소규모 MLP를 도입합니다.

3. **실험**:
   - 1B에서 7B 범위의 모델을 RNN으로 변환하여 언어 이해 벤치마크와 긴 문맥 평가에서 평가합니다. 실험 결과는 변환된 모델이 일반적으로 기존 순환 LLM과 경쟁력 있는 성능을 보여줍니다.

### 혁신적인 부분
SUPRA의 혁신성은 기존의 강력한 LLM을 효율적으로 RNN으로 전환하여, 향상된 성능과 감소된 훈련 비용의 이점을 동시에 제공한다는 점에 있습니다. 이는 특히 메모리 효율성이 중요한 언어 및 다모달 모델에서 추론 비용을 절감할 수 있는 방법을 제공합니다.

이 연구는 대규모 언어 모델의 잠재력을 더욱 확장하며, 특히 긴 문맥 작업에서 순환 모델의 한계를 극복하고자 하는 새로운 방법론을 탐구합니다.

## Similar Papers
- [HGRN2: Gated Linear RNNs with State Expansion](2404.07904.md)
- [Your Transformer is Secretly Linear](2405.12250.md)
- [FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning](2307.08691.md)
- [VisualRWKV: Exploring Recurrent Neural Networks for Visual Language Models](2406.13362.md)
- [Mamba: Linear-Time Sequence Modeling with Selective State Spaces](2312.00752.md)
- [The Remarkable Robustness of LLMs: Stages of Inference?](2406.19384.md)
- [MultiLoRA: Democratizing LoRA for Better Multi-Task Learning](2311.11501.md)
- [BiLLM: Pushing the Limit of Post-Training Quantization for LLMs](2402.04291.md)
- [GoldFinch: High Performance RWKV/Transformer Hybrid with Linear Pre-Fill and Extreme KV-Cache Compression](2407.12077.md)
