# ReLiK: Retrieve and LinK, Fast and Accurate Entity Linking and Relation Extraction on an Academic Budget
## TL;DR
## Summary
- [https://arxiv.org/pdf/2408.00103.pdf](https://arxiv.org/pdf/2408.00103.pdf)

### 1. 섹션별 요약

#### 초록
이 논문은 ReLiK(리트리버-리더 아키텍처)을 소개합니다. 이를 통해 텍스트에서 엔티티(실체) 연결 및 관계 추출 작업을 수행할 수 있습니다. ReLiK은 학문적 예산으로도 동급 최고의 성능을 달성할 수 있으며, 경쟁자들보다 최대 40배 빠른 추론 속도를 자랑합니다. 특히, 새로운 입력 표현 방식을 통해 하나의 순방향 통과로 엔티티를 연결하고 관계를 추출할 수 있게 하여 사전학습된 언어 모델의 컨텍스트화 능력을 최대한 활용합니다.

#### 도입
비정형 텍스트에서 구조화된 정보를 추출하는 것은 다양한 AI 문제의 핵심입니다. 정보 검색, 지식 그래프 구축, 지식 발견, 자동 텍스트 요약, 언어 모델링, 자동 텍스트 추론 및 의미적 구문 분석 등의 분야에서 중요합니다. 이전의 접근 방법들은 성능, 유연성, 속도 중 두 가지 특성에만 집중했지만 ReLiK은 세 가지 모두를 만족시킵니다. ReLiK은 동일한 아키텍처를 사용해 엔티티 연결과 관계 추출 작업에서 뛰어난 성능을 보이며 짧은 실험 주기와 학문적 예산으로 쉽게 연구할 수 있습니다.

#### 배경
엔티티 연결(EL)은 텍스트에서 특정 엔티티를 확인하고 이를 참조 지식 베이스의 항목과 연결하는 작업입니다. 관계 추출(RE)은 주어진 텍스트에서 엔티티 간의 의미적 관계를 추출하는 작업입니다. 이 두 가지 작업을 결합하여 폐쇄형 정보 추출(cIE)라고도 합니다.

#### 리트리버-리더 아키텍처
ReLiK은 곧 리트리버-리더 아키텍처로 구성되며, 리트리버는 텍스트에서 추출 가능한 엔티티나 관계를 검색하고, 리더는 원본 텍스트와 검색된 엔티티나 관계를 기반으로 적절한 스팬(spans)를 연결합니다. 이 아키텍처는 언어 모델의 최신 컨텍스트화 기능을 최대한 활용하여 성능과 처리 속도를 동시에 향상시킵니다.

#### 실험 및 결과
ReLiK은 다양한 데이터셋에서 엔티티 연결과 관계 추출 작업에서 최신 성능을 달성했습니다. 특히, AIDA-CoNLL 데이터셋과 다양한 외부 도메인 데이터셋에서 검증되었습니다. 또한, 속도 면에서도 경쟁자들보다 월등히 우수하며, 이는 특히 실제 응용 프로그램에서 중요한 요소입니다.

#### 한계 및 미래 연구
이 연구는 일부 한계점도 가지고 있습니다. 예를 들어, 특정 데이터셋에서만 실험이 진행되었고, 다양한 도메인 또는 텍스트 종류에 대한 적응성은 추가 연구가 필요합니다. ReLiK의 경량화된 계산 요구사항과 성능 향상 덕분에, 향후 다양한 실제 응용 프로그램에서도 유용하게 사용될 것으로 기대됩니다.

### 2. 전체 요약

이 논문은 ReLiK라는 엔티티 연결과 관계 추출을 위한 혁신적인 리트리버-리더 아키텍처를 소개하고 있습니다. ReLiK은 최신 언어 모델의 컨텍스트화 능력을 최대한 활용하여 하나의 순방향 통과로 엔티티를 연결하거나 관계를 추출할 수 있으며, 성능, 유연성, 속도 세 가지 특성을 모두 만족합니다. 학문적 예산으로도 높은 성능을 달성할 수 있어 연구자들에게 매우 유익할 것입니다. 실험 결과, ReLiK은 다양한 데이터셋에서 최신 성능을 보였으며, 이는 특히 실제 응용 프로그램에서 중요한 효율성을 입증합니다. 다만, 다양한 도메인에서의 추가 연구가 필요하며, 이를 통해 더 널리 응용될 수 있을 것입니다.

## Similar Papers
- [Entity Disambiguation via Fusion Entity Decoding](2404.01626.md)
- [RelBench: A Benchmark for Deep Learning on Relational Databases](2407.20060.md)
- [DiJiang: Efficient Large Language Models through Compact Kernelization](2403.19928.md)
- [Chain-of-Knowledge: Integrating Knowledge Reasoning into Large Language Models by Learning from Knowledge Graphs](2407.00653.md)
- [LAB: Large-Scale Alignment for ChatBots](2403.01081.md)
- [Text-Driven Neural Collaborative Filtering Model for Paper Source Tracing](2407.17722.md)
- [rLLM: Relational Table Learning with LLMs](2407.20157.md)
- [Video-to-Audio Generation with Hidden Alignment](2407.07464.md)
- [How multilingual is Multilingual BERT?](1906.01502.md)
