# CLLMs: Consistency Large Language Models
## TL;DR
## Summary
- [https://arxiv.org/pdf/2403.00835.pdf](https://arxiv.org/pdf/2403.00835.pdf)

### 주요 내용 요약

1. **서론 및 배경**:
   - 대형 언어 모델(LLMs)의 추론 속도는 사용자 경험과 서비스 품질에 중요한 역할을 합니다. 기존의 자동회귀(AR) 추론 방식은 한 번에 하나의 토큰을 생성하기 때문에 긴 응답을 생성하는 데 많은 시간이 소요됩니다. 이를 개선하기 위해 본 연구에서는 자코비 디코딩 방법을 개선한 새로운 접근법을 제안합니다.

2. **CLLMs의 구조 및 기능**:
   - 자코비 디코딩은 n개의 토큰을 동시에 예측하는 것을 목표로 하며, 이를 통해 AR 디코딩의 한계를 극복하고자 합니다. 이 연구에서는 일관성 대형 언어 모델(CLMMs)을 도입하여, 더 빠른 수렴을 위해 목표 LLM이 주어진 모든 상태에서 고정점을 일관성 있게 예측하도록 세밀하게 조정합니다.

3. **성능 평가 및 응용**:
   - CLMMs는 다양한 벤치마크에서 기존 방법들과 비교하여 2.4배에서 3.4배의 속도 향상을 보여줍니다. 특히, 자코비 디코딩을 사용하여 거의 정확도 손실 없이 추론 속도를 크게 개선할 수 있음을 입증했습니다.

### 혁신적인 부분
CLLMs의 혁신성은 기존 LLMs를 조정하여 자코비 디코딩 방식을 개선함으로써 추론 속도를 크게 향상시킨 것입니다. 특히, 복잡한 변형이나 추가 구성 요소 없이 기존 모델을 활용함으로써 메모리 효율성 및 적응성을 높였습니다. 이러한 접근 방식은 특히 대규모 언어 모델의 효율적인 추론에 있어서 중대한 진전을 이루었으며, 실제 응용에서의 추론 속도를 크게 개선할 수 있을 것으로 기대됩니다.

이 연구는 자코비 디코딩을 이용한 언어 모델의 추론 속도를 획기적으로 향상시키는 방법을 제시함으로써, 실시간 언어 처리 응용 분야에서 큰 영향을 미칠 수 있을 것입니다.

## Similar Papers
- [EAGLE-2: Faster Inference of Language Models with Dynamic Draft Trees](2406.16858.md)
- [Layer-Condensed KV Cache for Efficient Inference of Large Language Models](2405.10637.md)
- [Draft & Verify: Lossless Large Language Model Acceleration via Self-Speculative Decoding](2309.08168.md)
- [Never Miss A Beat: An Efficient Recipe for Context Window Extension of Large Language Models with Consistent "Middle" Enhancement](2406.07138.md)
- [Unlocking Efficiency in Large Language Model Inference: A Comprehensive Survey of Speculative Decoding](2401.07851.md)
- [Efficient Memory Management for Large Language Model Serving with PagedAttention](2309.06180.md)
- [OpenBezoar: Small, Cost-Effective and Open Models Trained on Mixes of Instruction Data](2404.12195.md)
- [Meta-Rewarding Language Models: Self-Improving Alignment with LLM-as-a-Meta-Judge](2407.19594.md)
- [TriForce: Lossless Acceleration of Long Sequence Generation with Hierarchical Speculative Decoding](2404.11912.md)
