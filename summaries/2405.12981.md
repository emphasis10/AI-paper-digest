# Reducing Transformer Key-Value Cache Size with Cross-Layer Attention
## TL;DR
## Summary
- [https://arxiv.org/pdf/2405.12981.pdf](https://arxiv.org/pdf/2405.12981.pdf)

### 섹션별 요약

#### 1. 서론
Transformer 기반의 대규모 언어 모델(LLM)에서 키-값(KV) 캐싱은 디코딩을 가속화하는 데 중요한 역할을 합니다. 하지만 긴 시퀀스 길이와 큰 배치 크기에서는 KV 캐시를 저장하는 데 필요한 메모리 양이 과도해질 수 있습니다. 이 논문에서는 인접한 레이어 사이에서도 키와 값 헤드를 공유하여 KV 캐시 크기를 줄이는 새로운 주의 설계인 Cross-Layer Attention (CLA)을 제안합니다. CLA는 기존의 MQA와 비교하여 거의 동일한 정확도를 유지하면서 KV 캐시 크기를 추가로 2배 줄일 수 있습니다.

#### 2. 관련 연구
기존의 연구들은 KV 캐시 크기를 줄이기 위한 다양한 방법을 제안했습니다. Multi-Query Attention (MQA)와 Grouped-Query Attention (GQA)은 여러 쿼리 헤드가 단일 키/값 헤드를 공유하도록 하여 KV 캐시 크기를 크게 줄였습니다. CLA는 이러한 접근 방식을 한 단계 더 나아가 인접한 레이어 간에도 키와 값을 공유함으로써 추가적인 메모리 절감을 달성합니다.

#### 3. Cross-Layer Attention (CLA)의 설계 및 기술적 세부 사항
CLA는 모델의 일부 레이어만 키와 값을 계산하고, 나머지 레이어는 이전 레이어의 키와 값을 재사용합니다. 이를 통해 메모리 사용량을 크게 줄일 수 있습니다. CLA는 MQA 및 GQA와 함께 사용할 수 있으며, 다양한 CLA 구성에 따라 성능을 평가했습니다.

#### 4. 실험
1B 및 3B 파라미터 모델을 대상으로 다양한 CLA 구성을 실험하여 메모리 사용량과 정확도 간의 트레이드오프를 평가했습니다. CLA는 MQA와 함께 사용될 때 가장 우수한 성능을 보였으며, 기존 MQA보다 KV 캐시 크기를 2배 줄이면서도 유사한 정확도를 유지했습니다.

### 논문의 주요 기여와 혁신적인 부분
1. **Cross-Layer Attention (CLA) 제안**: 인접한 레이어 간 키/값 공유를 통해 KV 캐시 크기를 줄이는 새로운 주의 메커니즘을 제안했습니다.
2. **확장된 실험 평가**: 다양한 모델 크기와 학습률 설정에서 CLA의 성능을 평가하여 정확도와 메모리 사용량 간의 최적의 균형점을 찾았습니다.
3. **MQA와의 통합**: CLA를 MQA와 결합하여 기존 MQA보다 메모리 효율성을 개선하면서도 거의 동일한 정확도를 달성했습니다.

### 전체 요약
이 논문에서는 Transformer 기반 언어 모델에서 KV 캐시 크기를 줄이기 위해 Cross-Layer Attention (CLA)을 제안했습니다. CLA는 인접한 레이어 간 키와 값을 공유함으로써 메모리 사용량을 크게 줄일 수 있습니다. 다양한 실험을 통해 CLA가 MQA와 결합될 때 가장 우수한 성능을 보였으며, 기존의 방법보다 더 효율적인 메모리 사용을 가능하게 했습니다. 이 연구는 긴 시퀀스 길이와 큰 배치 크기에서도 효율적으로 작동할 수 있는 Transformer 모델 설계에 기여할 수 있습니다.