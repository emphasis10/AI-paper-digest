# Adam-mini: Use Fewer Learning Rates To Gain More
## TL;DR
## Summary
- [https://arxiv.org/pdf/2406.16793.pdf](https://arxiv.org/pdf/2406.16793.pdf)

### 1. Section Summaries and Main Contributions

#### Introduction

**요약:** 
이 논문은 큰 언어 모델(LLM)을 위한 새로운 최적화 방법인 Adam-mini를 제안합니다. Adam-mini는 기존의 AdamW 보다 메모리 사용을 45-50% 줄이면서도 비슷하거나 더 나은 성능을 보여줍니다. 이는 학습 속도와 GPU/CPU 간 통신 부하를 줄여서 달성할 수 있습니다.

**주요 공헌:** 
Adam-mini는 매개변수의 학습률을 줄임으로써 메모리 사용량을 크게 줄입니다.

#### Methodology

**요약:** 
Adam-mini는 두 가지 주요 단계로 구성됩니다. 첫 번째는 모델 파라미터를 블록으로 나누는 것입니다. 두 번째는 각 블록에 단일 학습률을 적용하는 것입니다. 이를 통해 메모리 사용을 크게 줄입니다.

**혁신적인 부분:** 
Adam-mini는 파라미터 블록에 단일 학습률을 적용하여 메모리 효율성을 극대화합니다.

#### Experiments

**요약:** 
여러 LLM과 비-LLM 작업에서 Adam-mini의 성능을 테스트했습니다. 결과는 Adam-mini가 AdamW와 비교하여 메모리 사용을 줄이면서도 유사하거나 더 나은 성능을 보였습니다.

**주요 공헌:** 
여러 작업과 모델 크기에서 Adam-mini의 효율성과 성능을 입증했습니다.

#### Results

**요약:** 
LLM의 사전 훈련, 감독 하에 미세 조정 및 인간 피드백을 통한 강화 학습(RLHF)에서 Adam-mini는 더 나은 성능과 높은 처리량을 확인했습니다.

**혁신적인 부분:** 
Adam-mini는 특히 큰 모델에서 훈련 시간이 절반으로 단축될 수 있음을 보여줍니다.

#### Conclusion

**요약:** 
Adam-mini는 메모리 사용을 45-50% 줄일 수 있는 효과적인 최정화 방법입니다. 이는 더욱 효율적인 모델 훈련을 가능하게 합니다.

**주요 공헌:** 
최적화 방법을 단순화하여 큰 모델의 훈련을 더욱 효율적으로 만들었습니다.

### 2. Overall Summary

이 논문은 기존의 AdamW 대비 메모리를 덜 사용하면서도 높은 성능을 발휘하는 최적화 방법 Adam-mini를 제안합니다. Adam-mini는 모델 파라미터를 블록으로 나누고, 각 블록에 단일 학습률을 사용함으로써 메모리 사용을 줄입니다. 여러 LLM과 비-LLM 작업에서 테스트한 결과, Adam-mini는 메모리 효율성이 높고 훈련 시간이 절반으로 단축될 수 있음을 확인했습니다. 이로써 Adam-mini는 AI 모델의 훈련을 더욱 효율적이고 빠르게 만들 수 있는 혁신적인 방법임을 증명했습니다.