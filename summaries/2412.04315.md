# Densing Law of LLMs
## TL;DR
## Summary
- [https://arxiv.org/pdf/2412.04315.pdf](https://arxiv.org/pdf/2412.04315.pdf)

### 섹션 요약

#### 1. 도입 (Introduction)
본 논문은 인공지능 분야에서 중요한 역할을 하는 대규모 언어 모델(LLM)에 대해 논의하며, 모델 크기가 증가함에 따라 성능이 향상되지만, 리소스 제약이 있는 환경에서의 효율적인 추론 및 훈련에서의 과제를 인식합니다. 이를 위해 "용량 밀도"라는 개념을 새롭게 제시하여 LLM의 여러 규모에 따른 품질을 평가하고, 효율성 및 효과성 측면에서의 경향을 설명합니다.

#### 2. 주요 발견 및 법칙 (Key Findings and Densing Law)
29개의 오픈 소스 모델을 분석한 결과, "압축 법칙"이라 명명된 법칙이 도출되었습니다. 최대 용량 밀도는 시간이 지남에 따라 지수적으로 증가하며, 약 3개월마다 두 배가 되는 경향을 보입니다. 이는, 모델이 적은 매개변수만으로도 기존의 최첨단 모델과 비슷한 성능을 낼 수 있게 된다는 것을 의미합니다.

#### 3. 대규모 언어 모델의 밀도 (Density for Large Language Models)
모델의 효율성과 성능 균형을 나타내는 용량 밀도를 LLM의 평가 지표로 도입합니다. 주어진 모델의 실제 매개변수 크기 대비 요구되는 매개변수 크기의 비율로 정의됩니다.

#### 4. 논의 및 한계점 (Discussion and Limitations)
현재 LLM의 용량 밀도를 평가하기 위해 사용하는 평가 기준의 제한이 있으며, 이는 정확한 평가를 방해합니다. 증가하는 모델의 복잡성과 새로운 멀티모달 모델에 대한 요구로 인해, 평가 방법과 데이터셋의 지속적인 갱신이 필요합니다.

#### 5. 결론 (Conclusion)
이 논문은 LLM의 효율성을 향상하기 위한 경향을 보여주며, 모델의 밀도가 시간이 지남에 따라 지수적으로 증가한다는 경험적인 법칙을 제시합니다. 이는 LLM 커뮤니티가 모델의 용량 밀도를 지속적으로 개선하여 최소한의 계산 비용으로 최적의 성능을 달성하는 데 기여할 것입니다.

### 전체 요약
본 논문은 LLM의 효율성 및 효과성을 새로운 관점에서 평가하기 위해 "용량 밀도"라는 새로운 개념을 도입했습니다. 이를 통해 LLM의 성능은 매개변수의 수와 무관하게 기존 혹은 더 적은 수의 매개변수로 동일한 성능을 달성할 수 있음을 확인했습니다. 이 연구는 LLM의 지속적인 발전을 지원하는 동시에, 최소한의 자원 소모로 최적의 성능을 달성하도록 새로운 방향성을 제시합니다.