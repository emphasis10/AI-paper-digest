# Conditioned Language Policy: A General Framework for Steerable Multi-Objective Finetuning
## TL;DR
## Summary
- [https://arxiv.org/pdf/2407.15762.pdf](https://arxiv.org/pdf/2407.15762.pdf)

### 섹션 요약 및 주요 기여

#### 1. 소개 
Reinforcement Learning (RL) 미세 조정은 언어 모델(LM)을 원하는 행동으로 정렬하는 데 결정적입니다. 다중 목표 미세 조정(MOFT)은 이러한 LM을 특정 보상 가중치로 다양한 출력물을 생성할 수 있도록 조정합니다. CLP(Conditioned Language Policies) 프레임워크는 여러 목표 간의 균형을 최적화하면서 이는 다양한 보상 가중치를 목표로 학습합니다.

#### 2. 관련 연구 
다중 보상 정렬 방법은 크게 두 가지로 나뉩니다: 프롬프트 기반과 파라미터 기반 방식. 프롬프트 기반 방식은 보상 가중치를 프롬프트에 포함시키는 방법을 사용하고, 파라미터 기반 방식은 LM의 파라미터를 조정하는 방법을 활용합니다. 이 논문은 RL 과정 내에서 CLP 프레임워크를 통해 이러한 방식을 결합한 새로운 접근 방식을 제안합니다.

#### 3. CLP 이론
CLP는 다양한 보상 간의 트레이드오프를 최적화하기 위해 파라미터 공간을 조정하고 다중 과제 학습을 활용합니다. 제로샷 방식으로도 일부 보상 간 근사치를 제공할 수 있지만, 새로운 행동을 생성하는 데 한계가 있습니다. 다중 과제 학습을 통해 CLP는 더 효율적이면서도 질 높은 출력을 생성할 수 있습니다.

#### 4. 실험
다양한 데이터셋과 모델 크기를 활용한 실험을 통해 CLP가 기존 방법보다 우수한 성능을 보임을 입증했습니다. 특히, CLP는 더 많은 보상 가중치와의 균형을 제공하며, NLI와 TLDR 사이의 파레토 프론트에서 견고한 성능을 보였습니다.

#### 5. 결론
CLP 프레임워크는 다중 목표 미세 조정을 위한 유연한 방법입니다. 이 논문은 여러 조건에서 CLP가 기존 방식보다 우수한 퍼포먼스를 제공하며, 특히 더 높은 질의 출력을 생성함을 실험적으로 입증했습니다.

---

### 전체 요약
이 논문은 'Conditioned Language Policies (CLP)'라는 다중 목표 미세 조정을 위한 새로운 프레임워크를 제안합니다. CLP는 파라미터 공간 조정과 다중 과제 학습을 결합하여 다양한 보상 가중치 간의 균형을 최적화합니다. 이를 통해 여러 목표를 동시에 충족하는 언어 모델을 효율적으로 훈련할 수 있습니다. 실험 결과, CLP는 기존 방법보다 우수한 성능을 보이며 높은 질의 출력을 생성함을 입증했습니다.

## Similar Papers
- [BOND: Aligning LLMs with Best-of-N Distillation](2407.14622.md)
- [Omnipredictors for Regression and the Approximate Rank of Convex Functions](2401.14645.md)
- [WARP: On the Benefits of Weight Averaged Rewarded Policies](2406.16768.md)
- [On Computationally Efficient Multi-Class Calibration](2402.07821.md)
- [Direct Preference Optimization: Your Language Model is Secretly a Reward Model](2305.18290.md)
- [Low-Resource Machine Translation through the Lens of Personalized Federated Learning](2406.12564.md)
- [Value-Incentivized Preference Optimization: A Unified Approach to Online and Offline RLHF](2405.19320.md)
- [Why Larger Language Models Do In-context Learning Differently?](2405.19592.md)
- [DogeRM: Equipping Reward Models with Domain Knowledge through Model Merging](2407.01470.md)
