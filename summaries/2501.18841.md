# Trading Inference-Time Compute for Adversarial Robustness
## TL;DR
## Summary
- [https://arxiv.org/pdf/2501.18841.pdf](https://arxiv.org/pdf/2501.18841.pdf)

1. 각 섹션 요약 및 설명

**서론**
이 논문의 서론에서는 인공지능(AI)과 머신러닝(ML)의 최근 발전을 언급하며, 적대적 공격에 대한 로버스트니스(강건성)가 여전히 해결하지 못한 문제로 남아 있음을 강조합니다. 적대적 공격은 모델이 미세한 변화를 가진 입력을 통해 오작동하도록 만드는 방법이며, 이러한 취약성이 고도의 성능을 자랑하는 모델에서도 발생하고 있습니다. 이는 특히 대규모 언어 모델(LLM)에서 우려되는 점입니다.

**결과**
결과 섹션에서는 입력 시점의 계산 비용을 증가시키는 것이 다양한 적대적 공격에 대한 로버스트니스를 향상시키는 것과 관련이 있음을 발견하였습니다. 본 연구에서는 OpenAI의 o1-preview 및 o1-mini 모델을 통해 실험하였으며, 테스트에서 계산량이 증가함에 따라 모델이 공격에 더 강해지는 경향을 관찰했습니다. 전반적으로 계산량을 조정하여 모델의 성능을 향상시킬 수 있음을 제안했습니다.

**새로운 공격 기법**
이 논문에서는 두 가지 새로운 공격 기법이 소개되었습니다. 첫 번째는 '생각하지 않기'(think-less) 공격으로, 모델이 최소한의 계산만 하도록 유도하여 취약점을 노리는 방법입니다. 두 번째는 '너드 사냥'(nerd sniping) 공격으로, 상대방이 예상치 못한 방식으로 모델의 사고를 방해하여 오작동을 초래할 수 있도록 만드는 공격 기법입니다. 이러한 기법들은 모델의 로버스트니스를 시험하는 데 중요한 역할을 합니다.

**결론 및 논의**
이 논문은 적대적 로버스트니스 향상을 위한 계산 접근 방식의 가능성을 확인했습니다. 특히, 입력 시점의 계산량을 조정하는 방법이 안전성을 높이는데 기여할 수 있음을 보여주었습니다. 그러나 여전히 해결해야 할 여러 질문이 남아 있으며, 이 방법이 모든 상황에서 적용될 수 있는지에 대한 추가 연구가 필요합니다.

2. 전체 요약
이 논문은 인공지능 모델의 적대적 공격에 대한 강건성을 향상시키기 위해 입력 시점에서 계산량을 증가시킬 필요성을 강조합니다. 적대적 공격은 AI 모델의 오작동을 유도하는 방법으로, 이 논문에서는 새로운 공격 기법을 제안하고 이를 실험적으로 검증하였습니다. 연구 결과, 계산량 증가가 여러 상황에서 로버스트니스를 개선할 수 있음을 입증하였습니다. 이는 AI 모델의 안전성을 높이기 위한 중요한 기초 자료가 될 것입니다.