# Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling
## TL;DR
## Summary
- [https://arxiv.org/pdf/2412.05271.pdf](https://arxiv.org/pdf/2412.05271.pdf)

1. 각 섹션의 중요한 내용 요약:

    - **소개 섹션**:
      최근 몇 년간 멀티모달 대형 언어 모델(MLLMs)이 자연어 처리, 컴퓨터 비전, 인간-컴퓨터 상호작용 분야에서 매우 중요한 기술로 떠올랐습니다. InternVL 2.5는 이러한 MLLMs의 대표적인 사례로, 이전의 InternVL 2.0 모델을 기반으로 하여 훈련 및 테스트 전략과 데이터 품질을 개선했습니다.

    - **모델 아키텍처**:
      InternVL 2.5는 'ViT-MLP-LLM' 패러다임을 따르며 기존 InternVL 1.5 및 2.0과 동일한 아키텍처를 유지합니다. 모델의 입력 이미지를 타일 형태로 나누어 처리하며, 여러 크기의 사전 훈련된 LLM을 통합하여 다양한 유형의 데이터를 처리합니다.

    - **주요 기여**:
      InternVL 2.5는 개방형 멀티모달 AI 시스템 개발에 기여하기 위한 강력한 도구를 제공하며, 모듈을 축소하거나 확장하여 필요한 성능을 발휘하도록 설계되었습니다. 데이터 품질 향상을 위해 엄격한 필터링을 적용하고, 여러 벤치마크에서 탁월한 성능을 보여줍니다.

    - **평가 및 결과**:
      다양한 벤치마크에서 다중 학문적 추론, 문서 이해, 비디오 이해 및 다중 언어 처리 등의 과제를 통해 모델의 성능을 평가합니다. MMMU 벤치마크에서 70% 이상의 성과를 최초로 기록한 개방형 소스 MLLM으로 주목 받으며, 상용 모델과의 성능 격차를 좁히고 있습니다.

2. 전체 요약:

    InternVL 2.5는 복합적인 멀티모달 AI 문제 해결을 위한 혁신적인 접근을 통해 상용 모델과 경쟁할 수 있는 성능을 발휘하는 오픈 소스 MLLM입니다. 기존의 모델 아키텍처를 유지하면서 성능 및 데이터 품질을 대폭 개선하여 여러 벤치마크에서 우수한 결과를 보여주며, 이는 개방형 AI 연구를 위한 중요한 도약이 될 것입니다.