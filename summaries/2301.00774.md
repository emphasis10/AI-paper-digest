# SparseGPT: Massive Language Models Can Be Accurately Pruned in One-Shot
## TL;DR
## Summary
- [https://arxiv.org/pdf/2301.00774.pdf](https://arxiv.org/pdf/2301.00774.pdf)

### 1. 섹션별 요약

**섹션 1: 소개**
이 연구는 GPT 계열의 대규모 언어 모델을 가능한 높은 정확도로 손실 없이 'pruning' 하는 SparseGPT 방법을 소개합니다. GPT-175B 및 BLOOM-176B 같은 대규모 모델도 한 번에 최대 60%까지 구조를 줄일 수 있습니다. 이 방법은 기존의 'magnitude pruning'보다 훨씬 효과적이며, 재훈련 없이도 높은 정확도를 유지할 수 있습니다.

**섹션 2: 배경**
모델 압축의 한 방법으로 'post-training pruning'이 있습니다. 이 방법은 훈련 후에 모델 파라미터를 줄여 메모리 용량과 계산 비용을 감소시킵니다. 기존의 방법들은 주로 작은 모델에 적용되었고, 대규모 모델에는 한계가 있었습니다.

**섹션 3: SparseGPT 알고리즘**
SparseGPT는 층별로 분리된 여러 개의 대규모 가중치 업데이트 문제를 해결하는 알고리즘입니다. 이 알고리즘은 각 층에서 원래의 입력-출력 관계를 최대한 유지하며 구조를 줄이도록 설계되었습니다. 또한, 가중치 양자화(weight quantization)와도 호환이 가능해 병합할 수 있습니다.

**섹션 4: 실험**
SparseGPT의 성능은 OPT와 BLOOM 모델 계열에서 이론적으로 검증되었습니다. 이 알고리즘은 최대 60%의 'sparsity' 수준에서 세밀한 정확도를 유지하며, 여러 Zero-shot 작업에서도 원래 모델과 유사한 정확도를 보였습니다.

**섹션 5: 관련 연구**
기존의 'pruning' 방법들은 주로 작은 모델에 적용되었고, 대규모 모델에 적용하기에는 시간이 많이 드는 한계가 있었습니다. SparseGPT는 이를 극복하여 대규모 모델에서도 재훈련 없이 신속하게 높은 정확도를 유지할 수 있습니다.

**섹션 6: 논의**
SparseGPT는 대규모 GPT 계열 모델을 높은 정확도로 'pruning' 할 수 있는 방법론입니다. 가장 큰 공개된 GPT 모델인 OPT-175B 및 BLOOM-176B에서도 50-60%의 'sparsity'를 달성하며, 정확도 손실은 미미합니다. 이는 미래의 대규모 모델 압축 연구에 매우 고무적인 발견입니다.

**섹션 7: 감사의 글**
이 연구는 유럽 연구 위원회(ERC)의 후원으로 이루어졌으며, 실험 지원 팀에게 감사의 말을 전합니다.

### 2. 전체 요약
이 논문은 Large Language Models (LLM)인 GPT 계열의 모델을 재훈련 없이도 높은 'sparsity'로 손실 없이 압축할 수 있는 SparseGPT 방법을 제안합니다. SparseGPT는 OPT-175B와 BLOOM-176B 같은 매우 큰 모델도 단 몇 시간 안에 최대 60%까지 구조를 줄일 수 있습니다. 실험 결과, 다양한 Zero-shot 작업에서 원래 모델과 유사한 정확도를 유지할 수 있음이 확인되었습니다. 이 연구는 대규모 언어 모델의 효율적인 'pruning'을 가능하게 하여, 계산 비용과 메모리 사용을 크게 감소시킬 수 있는 중요한 기여를 합니다. 이 발견은 향후 대규모 모델 압축 연구에 매우 고무적인 발견입니다.