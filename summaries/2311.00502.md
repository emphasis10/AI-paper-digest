# Efficient LLM Inference on CPUs
## TL;DR
## Summary
- [https://arxiv.org/pdf/2311.00502.pdf](https://arxiv.org/pdf/2311.00502.pdf)

### 1. 섹션별 요약 및 설명

#### Abstract (초록)
이 논문은 CPU에서 대형 언어 모델(LLM)의 추론을 효율적으로 수행할 수 있는 방법을 제안합니다. INT4 양자화와 최적화된 런타임을 사용하여 CPU에서 LLM의 성능을 향상시키며, 이를 Llama2, GPT-NeoX 등의 인기 모델에 적용하여 극도의 추론 효율성을 입증합니다.

#### 1. Introduction (서론)
대형 언어 모델(LLM)은 뛰어난 성능을 보여주지만, 엄청난 양의 모델 파라미터로 인해 배포가 어렵습니다. 양자화는 이러한 문제를 해결하는 기술로, 특히 INT8 양자화가 널리 사용됩니다. 하지만 INT8 양자화에는 한계가 있어 INT4와 같은 더 낮은 비트 양자화가 주목받고 있습니다. 이 논문은 CPU에서의 LLM 추론을 효율적으로 수행할 수 있는 자동 INT4 양자화 흐름과 최적화된 런타임을 제안합니다.

#### 2. Approach (방법론)
##### 2.1 Automatic INT4 Quantization Flow (자동 INT4 양자화 흐름)
Intel Neural Compressor를 기반으로 한 자동 INT4 양자화 흐름을 개발했습니다. 이 흐름은 여러 양자화 레시피와 설정을 사용하여 INT4 모델을 생성하고 정확성을 평가합니다.

##### 2.2 Efficient LLM Runtime (효율적인 LLM 런타임)
LLM 추론을 가속화하기 위한 CPU 텐서 라이브러리를 설계하고, LLM 최적화를 포함한 런타임을 개발했습니다. 이 런타임은 AVX2, AVX512 등의 최신 CPU 명령어 세트를 지원합니다.

#### 3. Results (결과)
##### 3.1 Experimental Setup (실험 설정)
다양한 LLM 모델을 사용하여 정확성과 성능을 평가했습니다. 실험은 Intel Xeon Scalable Processors를 사용하여 수행되었습니다.

##### 3.2 Accuracy (정확성)
INT4 모델은 FP32 모델과 비교하여 1% 미만의 정확도 손실만을 보였습니다.

##### 3.3 Performance (성능)
INT4 양자화를 적용한 LLM 런타임은 ggml 기반 솔루션보다 최대 1.6배 빠른 토큰 생성 지연 시간을 보여주었습니다.

##### 3.4 Discussion (논의)
성능을 더욱 향상시키기 위해 스레드 스케줄러, CPU 텐서 라이브러리의 블로킹 전략 등의 추가적인 최적화가 필요합니다.

#### 4. Summary and Future Work (요약 및 향후 작업)
이 논문은 CPU에서 효율적인 LLM 추론을 위한 자동 INT4 양자화와 최적화된 런타임을 제시했습니다. 향후에는 CPU 텐서 라이브러리를 개선하고, Hugging Face Transformer API에 INT4 LLM 추론을 지원할 계획입니다.

### 2. 전체 요약

이 논문은 CPU에서 대형 언어 모델(LLM)의 효율적인 추론을 가능하게 하는 방법을 제안합니다. INT4 양자화와 최적화된 런타임을 통해 모델의 정확성을 거의 손상시키지 않으면서도 추론 속도를 크게 향상시킵니다. Intel Neural Compressor를 사용하여 자동 INT4 양자화를 수행하고, 다양한 LLM 모델에 이를 적용하여 실험을 통해 그 성능을 입증했습니다. 향후에는 CPU 텐서 라이브러리의 성능을 더욱 개선하고, 보다 많은 하드웨어와의 호환성을 확대할 계획입니다.

이 논문의 주요 기여는 다음과 같습니다:
- 자동 INT4 양자화 흐름을 제안하고, FP32 모델 대비 1% 미만의 정확도 손실로 고품질 INT4 모델을 생성함
- 다양한 CPU 명령어 세트를 지원하는 텐서 라이브러리를 설계하고, 이를 기반으로 효율적인 LLM 런타임을 개발함
- 3B에서 20B 파라미터 크기의 LLM 모델에 적용하여, 기존 솔루션보다 빠른 토큰 생성 지연 시간을 달성함

이러한 기여를 통해 대형 언어 모델의 실용성을 높이고, CPU에서의 효율적인 추론을 가능하게 합니다.