# Better & Faster Large Language Models via Multi-token Prediction
## TL;DR
## Summary
- [https://arxiv.org/pdf/2404.19737.pdf](https://arxiv.org/pdf/2404.19737.pdf)

### 섹션 요약

1. **서론 (Introduction):**
   이 논문은 GPT와 같은 대규모 언어 모델(LLM)을 다루며, 여러 미래 토큰을 한 번에 예측하는 멀티 토큰 예측 방식의 장점을 탐구합니다. 이 모델은 본래 단순한 다음 토큰 예측 과제로 훈련되지만, 멀티 토큰 예측을 통해 샘플 효율성을 높이고, 모델의 학습 능력을 향상시킵니다.

2. **방법론 (Methodology):**
   표준 언어 모델링은 대규모 텍스트 코퍼스를 기반으로 다음 토큰 예측을 통해 학습됩니다. 멀티 토큰 예측 방식은 미래의 n개의 토큰을 예측하도록 확장되며, 이에 따라 모델은 여러 다른 독립 출력 헤드를 통해 평행 추론을 하게 됩니다. 이 방식은 추가적으로 메모리의 비효율성을 해결하여 GPU의 사용을 줄였습니다.

3. **실험 (Experiments):**
   다양한 크기의 모델에 대해 멀티 토큰 예측의 효율성을 입증하였습니다. 특히 대규모의 코드 데이터로 학습한 모델은 이전 모델보다 평균 15% 더 많은 코드 문제를 해결할 수 있었습니다. 또한, 자기 추측 디코딩(self-speculative decoding)을 통해 추론 속도를 최대 세 배까지 향상시켰습니다.

4. **결론 (Conclusion):**
   멀티 토큰 예측은 기존의 다음 토큰 예측을 넘어서고, 대규모 모델을 위한 강력한 훈련 기법임을 증명했습니다. 이 방법은 특히 코드 작업에 강력한 개선 효과를 가져옵니다. 뿐만 아니라, 추측 디코딩을 통해 추론 속도를 세 배 더 빠르게 할 수 있습니다.

### 전체 요약

이 연구는 언어 모델 훈련과 추론의 효율성을 크게 개선할 수 있는 멀티 토큰 예측 방식을 소개합니다. 기존의 다음 토큰 예측의 단점을 보완하고, 더 나은 성능을 보입니다. 특히 대규모 모델일수록 멀티 토큰 예측의 장점이 더욱 부각되며, 추측 디코딩을 통해 추론 시간을 단축할 수 있습니다. 이 방법은 대규모 데이터 작업에 적합하며, 특히 코드 작업에서 강력한 성과를 보입니다. 

위 연구들은 AI와 머신러닝의 경계를 한 단계 더 발전시키며, 새로운 방식의 학습과 추론 방식을 제시합니다.