# RWKV: Reinventing RNNs for the Transformer Era
## TL;DR
## Summary
- [https://arxiv.org/pdf/2305.13048.pdf](https://arxiv.org/pdf/2305.13048.pdf)

### 주요 내용 요약

1. **서론 및 배경**:
   - 이 논문에서는 순환 신경망(RNNs)과 트랜스포머 모델의 장점을 결합한 새로운 모델 구조인 RWKV를 제안합니다. 기존 RNN은 메모리 사용량이 적고 긴 시퀀스 처리에 유리하지만, 병렬 처리가 어렵고 기울기 소실 문제가 있습니다. 반면, 트랜스포머는 병렬 처리에 강점을 가지고 있지만, 시퀀스 길이에 따라 계산 복잡성과 메모리 사용이 기하급수적으로 증가하는 단점이 있습니다 .

2. **RWKV의 구조 및 기능**:
   - RWKV 모델은 '수용 벡터(Receptance vector)', '가중치(Weight)', '키(Key vector)', '값(Value vector)'의 네 가지 핵심 요소로 구성됩니다. 이 요소들은 각 타임스텝에서 상호 작용하여 기존 RNN과는 다른 새로운 유형의 주의력 메커니즘을 구현합니다. 모델은 시간 및 채널 혼합 블록을 포함한 잔차 블록으로 구성되며, 이는 복잡한 패턴과 긴 범위 의존성을 효과적으로 포착할 수 있도록 설계되었습니다 .

3. **성능 평가 및 응용**:
   - RWKV 모델은 기존 트랜스포머 모델과 비교하여 유사한 또는 그 이상의 성능을 보이면서도, 훨씬 적은 메모리를 사용하여 효율성을 크게 향상시킵니다. 특히, 긴 시퀀스 데이터를 처리하는 능력이 뛰어나며, 다양한 자연어 처리(NLP) 태스크에서의 경쟁력을 입증하였습니다 .

### 혁신적인 부분
RWKV의 혁신성은 기존의 복잡한 자기주의 메커니즘을 단순화하고 향상된 시간 및 채널 혼합 메커니즘을 통해 메모리 및 계산 효율성을 크게 향상시킨 점에 있습니다. 이는 병렬 처리 가능성을 유지하면서도, 특히 긴 시퀀스를 효과적으로 처리할 수 있는 새로운 경로를 제시합니다 . 이와 같은 특성은 RWKV 모델이 대규모 데이터를 처리하는 데 특히 유용하게 만듭니다.

이러한 혁신을 통해 RWKV는 시퀀스 처리 작업에서 계산 효율성과 모델 성능 간의 트레이드오프를 해결하는 중요한 발전을 이루었습니다.