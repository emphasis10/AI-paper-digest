# Not All Language Model Features Are Linear
## TL;DR
## Summary
- [https://arxiv.org/pdf/2405.14860.pdf](https://arxiv.org/pdf/2405.14860.pdf)

### 1.1. Introduction
이 논문은 언어 모델에서의 일차원 표현 가설(linear representation hypothesis) 대신 몇몇 언어 모델 표현이 본질적으로 다차원적임을 탐구합니다. 언어 모델이 다음 토큰 예측을 통해 학습한 후, "월요일 후 이틀" 또는 "1월 후 네 달"과 같은 모듈러 산수를 다차원적인 원형 표현으로 해결하는 것을 발견합니다.

### 1.2. Related Work
- **일차원 표현**: GloVe 및 Word2vec 같은 초기 단어 임베딩 방법은 단순한 공차 데이터만을 사용하여 의미적 개념을 나타내는 방향을 가지고 있음을 발견했습니다. 더욱 최근에는 GPT-2 같은 대규모 언어 모델이 다양한 선형적 개념들을 표현한다는 연구 결과가 있습니다.
- **비선형 표현**: 비교적 적은 연구만이 비선형적 기능에 대해 다룹니다. 최근 몇 가지 연구는 장난감 모델을 통해 비선형 기능이 존재함을 증명했습니다.
- **알고리즘 문제 해석가능성**: 일부 연구는 모듈러 산수 문제를 해결할 때 모델이 원형 표현을 사용함을 발견했습니다.

### 1.3. Multi-Dimensional Features
이 섹션에서는 다차원적 기능에 대한 엄격한 정의를 도입하고, 분해할 수 없는 기능(irreducible features)을 분류하는 방법을 설명합니다. 특히, 희소 자동 인코더(Sparse Autoencoders, SAE)를 사용하여 다차원적 기능을 자동으로 찾는 방법을 제안합니다.

### 1.4. Sparse Autoencoders Find Multi-Dimensional Features
희소 자동 인코더를 사용하여 다차원적인 기능을 찾는 방법을 설명합니다. 이렇게 발견된 기능들 중에서 원형 표현이 가장 인상적이었습니다. 예를 들어, GPT-2에서는 요일 및 월이 원형으로 정렬되어 있었습니다.

### 1.5. Circular Representations in Large Language Models
이 섹션에서는 다차원적 기능이 진정한 계산의 단위인지 확인하기 위해 모델이 사용하는 경우를 찾습니다. "요일의 모듈러 산수"와 "달력의 모듈러 산수" 문제를 정의하고, 이러한 문제에서 원형 표현을 사용하는 것을 실험으로 증명합니다. 결과적으로 원형 기능이 해당 문제를 해결하는 기본 단위임을 확인했습니다.

### 2. 전반적인 요약
이 논문은 언어 모델이 모든 개념을 일차원 선형 공간에서 표현하는 기존의 가설을 넘어, 다차원적 표현을 사용하여 계산을 수행할 수 있음을 실험적으로 입증하고 있습니다. 희소 자동 인코더를 이용해 원형의 다차원적 표현을 찾아냈고, 이를 통해 자연어 모듈러 산수 문제를 해결하는 데 있어 원형 표현이 사용됨을 확인했습니다. 이러한 발견은 언어 모델의 이해와 해석 가능성을 새롭게 열어주는 중요한 기여라고 할 수 있습니다.

## Similar Papers
- [The Remarkable Robustness of LLMs: Stages of Inference?](2406.19384.md)
- [Cost-Effective Hallucination Detection for LLMs](2407.21424.md)
- [VCR: Visual Caption Restoration](2406.06462.md)
- [Confidence Regulation Neurons in Language Models](2406.16254.md)
- [Measuring Progress in Dictionary Learning for Language Model Interpretability with Board Game Models](2408.00113.md)
- [Probing the 3D Awareness of Visual Foundation Models](2404.08636.md)
- [Planetarium: A Rigorous Benchmark for Translating Text to Structured Planning Languages](2407.03321.md)
- [RULER: What's the Real Context Size of Your Long-Context Language Models?](2404.06654.md)
- [Toto: Time Series Optimized Transformer for Observability](2407.07874.md)
