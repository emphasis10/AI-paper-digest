# DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning
## TL;DR
## Summary
- [https://arxiv.org/pdf/2501.12948.pdf](https://arxiv.org/pdf/2501.12948.pdf)

### 1. 섹션 요약 및 주요 기여 점

#### 소개 (Introduction)
이 논문은 대규모 강화 학습(RL)을 이용하여 초거대 언어 모델(LLM)의 추론 능력을 향상시키는 방법을 탐구합니다. DeepSeek-R1-Zero는 어떠한 감독 데이터 없이도 자가 진화 과정을 통해 강력한 추론 성능을 발휘하지만, 가독성의 문제와 언어 혼합 문제를 겪습니다. 이러한 문제를 해결하고자 DeepSeek-R1을 소개하며, 이는 콜드 스타트 데이터를 포함한 다단계 학습 파이프라인을 사용합니다.

#### 기여 (Contributions)
- **포스트 트레이닝**: 대규모 강화 학습을 기반 모델에 직접 적용하여, 예비 단계로서의 감독 세부 조정 없이도 체계적인 사고(Chain-of-Thought, CoT)를 탐색하고 복잡한 문제를 해결하는 DeepSeek-R1-Zero를 개발했습니다.
- **증류**: 대형 모델의 추론 패턴을 소형 모델로 증류하여, 소형 모델에서도 강화 학습으로 발견된 추론 패턴보다 더 나은 성능을 발휘할 수 있음을 보였습니다.

#### 방법론 (Approach)
DeepSeek-R1-Zero는 GRPO(Group Relative Policy Optimization) 기반의 강화 학습 알고리즘을 통해 감독 데이터 없이도 자가 진화를 촉진하였습니다. 강력한 성능을 보였지만, 추론 과정의 복잡성과 이해하기 어려운 결과를 생성하는 문제를 극복하기 위해 콜드 스타트 데이터를 포함한 DeepSeek-R1을 개발하게 되었습니다.

#### 실험 결과 요약 (Summary of Evaluation Results)
DeepSeek-R1은 다양한 추론 과업에서 높은 성능을 기록하며, 특히 수학 및 코딩 관련 태스크에서 뛰어난 성과를 보였습니다. AIME 2024에서 79.8%, MATH-500에서 97.3%의 점수를 기록하며, Codeforces에서는 96.3%의 인간 참가자를 능가하는 실력을 보여주었습니다.

#### 결론, 제한점 및 미래 작업 (Conclusion, Limitations, and Future Work)
DeepSeek-R1은 다양한 언어에서의 혼합 문제 및 소프트웨어 엔지니어링 작업에서의 RL 적용 제한점을 극복하기 위한 여러 방향에서의 추가 연구가 필요합니다. 향후 버전에서는 이러한 문제 해결을 위해 많은 발전이 기대되며, 긴 체계적 사고를 활용한 향상된 기능을 탐구할 계획입니다.

### 2. 전체 요약

이 논문은 LLM의 추론 능력을 특별한 감독 데이터 없이 강화 학습만으로 향상시키는 방법을 연구했습니다. DeepSeek-R1-Zero는 자가 진화를 통해 이러한 목표를 달성했지만, 여러 문제점이 존재하여 다량의 데이터와 단계적 학습을 통해 더욱 향상된 DeepSeek-R1을 개발하게 되었습니다. 이 방법론은 대형 모델의 패턴을 소형 모델로 증류하여, 더 작은 컴퓨팅 파워로도 높은 성능을 발휘할 수 있게 했다는데 큰 의의가 있으며, 결과적으로 다양한 추론 및 코딩 관련 태스크에서 우수한 성능을 보여주었습니다. 앞으로도 이와 같은 접근 방식을 기반으로 모델의 제한성을 극복하고 더욱 향상된 결과를 목표로 연구를 계속할 계획입니다.