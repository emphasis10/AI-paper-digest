# The Perfect Blend: Redefining RLHF with Mixture of Judges
## TL;DR
## Summary
- [https://arxiv.org/pdf/2409.20370.pdf](https://arxiv.org/pdf/2409.20370.pdf)

[4]에서 파일을 기반으로 AI 및 기계 학습 논문을 요약하겠습니다.

### 1. 각 섹션의 요약

#### 소개
논문에서는 큰 언어 모델(LLM)을 미세 조정할 때 인공지능을 사람의 피드백과 결합하는 방법인 RLHF(사람 피드백을 활용한 강화 학습)을 설명합니다. RLHF는 다중 작업 학습(MTL)에서 보상 해킹과 같은 문제가 있어, 이 논문에서는 이러한 문제를 해결하기 위해 새로운 포스트 트레이닝 패러다임인 CGPO(제약 있는 생성 정책 최적화)를 소개합니다.

#### 기여
이 논문은 보상 해킹 문제를 해결하기 위해 새로운 제약 기반 RL 방법인 CGPO를 제안합니다. CGPO는 규칙 기반과 LLM 기반으로 두 가지 유형의 심판을 사용하여 보상 해킹 패턴을 식별하고 이를 방지합니다.

#### 실험
CGPO는 5가지 작업(일반 채팅, 지시 따르기, 수학 및 코딩 추론 등)에서 실험을 통해 더욱 향상된 성능을 보여줍니다. CGPO는 기존 방법보다 다중 작업 설정에서 더 나은 결과를 보여준다.

#### 결론
논문은 CGPO가 다중 작업 학습에서 발생하는 여러 가지 도전 과제를 효과적으로 해결하며, 이는 다중 목적 최적화 전략에 기반합니다.

### 2. 전체 요약

이 논문은 큰 언어 모델의 다중 작업 학습에서 보상 해킹과 작업 목표의 상충 문제를 해결하기 위해 CGPO라는 새로운 프레임워크를 제안합니다. CGPO는 새로운 제약 이해 기반의 RL 방법론을 사용하여 이 문제를 해결하며, 실험을 통해 많은 작업에서 우수한 성능을 입증하였습니다. 이 논문은 다중 목적 최적화의 최적점을 찾기 위해 강력한 이론적 보장과 실증적 결과를 통해 RLHF의 최신 상태를 발전시킵니다.