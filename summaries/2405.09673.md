# LoRA Learns Less and Forgets Less
## TL;DR
## Summary
- [https://arxiv.org/pdf/2405.09673.pdf](https://arxiv.org/pdf/2405.09673.pdf)

### 주요 내용 요약

1. **서론 및 배경**:
   - 이 논문은 **LoRA(Low-Rank Adaptation)**의 성능과 잊어버림(Forgetfulness) 효과를 비교하여 분석합니다. LoRA는 대규모 언어 모델을 파인튜닝할 때 메모리 효율성을 높이는 방법으로, 선택된 가중치 행렬에 대해 저차원 변동만을 훈련합니다. 이 연구는 LoRA와 완전 파인튜닝의 성능을 프로그래밍과 수학이라는 두 가지 대상 도메인에서 비교합니다.

2. **방법론**:
   - **LoRA와 완전 파인튜닝 비교**: Llama-2 7B와 13B 모델을 사용하여 프로그래밍 및 수학 도메인에서 LoRA와 완전 파인튜닝의 성능을 비교합니다. 여기에는 명령어 파인튜닝(Instruction Finetuning, IFT)과 연속 사전 훈련(Continued Pretraining, CPT)의 두 가지 훈련 방식이 포함됩니다.
   - **평가 지표**: HumanEval 및 GSM8K 벤치마크를 사용하여 목표 도메인의 성능을 평가하고, HellaSwag, WinoGrande 및 ARC-Challenge를 사용하여 원본 도메인의 잊어버림 정도를 평가합니다.

3. **실험**:
   - **코드와 수학 도메인에서의 성능**: LoRA는 완전 파인튜닝에 비해 대부분의 설정에서 성능이 떨어지지만, 수학 도메인에서는 더 작은 격차를 보입니다. 코드 도메인에서는 완전 파인튜닝이 LoRA보다 훨씬 높은 성능을 보였습니다.
   - **잊어버림 비교**: LoRA는 완전 파인튜닝에 비해 원본 도메인에서 잊어버림이 적었습니다. 이는 LoRA가 더 강력한 정규화 형태를 제공한다는 것을 나타냅니다.
   - **정규화 특성**: LoRA는 드롭아웃 및 가중치 감소와 같은 일반적인 정규화 기법보다 더 강력한 정규화를 제공합니다. 또한, LoRA는 생성된 솔루션의 다양성을 유지하는 데 도움이 됩니다.

4. **결론 및 권장 사항**:
   - LoRA는 목표 도메인 성능에서 완전 파인튜닝에 비해 떨어지지만, 원본 도메인의 성능을 더 잘 유지합니다. 
   - LoRA의 학습률에 대한 민감도는 완전 파인튜닝보다 높으며, 최적의 학습률을 설정하는 것이 중요합니다.
   - LoRA는 특정 모듈(Attention, MLP, All)을 타겟으로 하는 것이 성능에 큰 영향을 미칩니다. 'All' 모듈을 타겟으로 하는 것이 가장 좋은 성능을 제공합니다.

### 혁신적인 부분
이 논문의 혁신성은 LoRA와 완전 파인튜닝을 두 가지 도메인(코드 및 수학)에서 체계적으로 비교하고, LoRA가 더 강력한 정규화를 제공하며 원본 도메인 성능을 더 잘 유지한다는 것을 입증한 데 있습니다. 이는 대규모 언어 모델의 파인튜닝 방법을 선택할 때 중요한 고려 사항을 제공하며, LoRA의 하이퍼파라미터 최적화에 대한 실질적인 권장 사항을 제시합니다.