# OpenVLA: An Open-Source Vision-Language-Action Model
## TL;DR
## Summary
- [https://arxiv.org/pdf/2406.09246.pdf](https://arxiv.org/pdf/2406.09246.pdf)

#### 1. 서론 (Introduction)
로봇 조작을 위한 학습된 정책은 훈련 데이터 외에 일반화하는 데 한계가 있습니다. 예를 들어, 개별 기술이나 언어 지시를 위해 훈련된 기존 정책은 객체 위치나 조명과 같은 새로운 초기 조건으로 행동을 외삽할 수 있지만, 장면 방해 요소나 새로운 객체에 대해서는 잘 대응하지 못합니다. 반면, CLIP, SigLIP 및 Llama 2와 같은 비전 및 언어를 위한 기초 모델은 이러한 일반화를 수행할 수 있습니다. 로봇 조작을 위한 기초 모델은 데이터의 규모와 다양성 덕분에 더 강력한 일반화 성능을 발휘할 수 있습니다. OpenVLA는 이러한 기초 모델을 활용하여 훈련 데이터 외에 객체, 장면 및 작업에 일반화할 수 있는 로봇 정책을 훈련하는 데 중점을 둡니다.

#### 2. OpenVLA 모델 (OpenVLA Model)
OpenVLA는 7B 성능의 오픈 소스 VLA(비전-언어-행동) 모델로, 970k 로봇 조작 데이터셋을 활용하여 훈련되었습니다. 이 모델은 Google의 RT-2-X보다 16.5% 높은 절대 성공률을 기록했으며, 이는 29개의 평가 작업과 다양한 로봇 환경에서 테스트된 결과입니다. OpenVLA는 효과적인 미세 조정 전략을 활용하여 양산형 GPU에서도 높은 효율성을 보입니다. 모델의 체크포인트, 미세 조정 노트북 및 PyTorch 코드베이스가 모두 공개되었습니다.

#### 3. 미세 조정 (Fine-Tuning)
OpenVLA는 효율적인 미세 조정을 통해 새로운 작업에 쉽게 적용될 수 있습니다. 예를 들어, LoRA(Low-Rank Adaptation) 기법을 사용하면 GPU 메모리 사용량을 최소화하면서도 높은 성능을 유지할 수 있습니다. 이는 OpenVLA를 새로운 작업에 맞춰 신속하게 미세 조정하는 데 특히 유용합니다.

#### 4. 모델 성능 및 효율성 (Model Performance and Efficiency)
OpenVLA는 다양한 FPGA 및 대규모 데이터셋을 활용하여 높은 효율성을 가지고 있습니다. 특히, 파인튠된 모델이 기존의 From-Scratch 학습 접근법보다 성과가 뛰어나다는 점을 입증했습니다. 이 모델은 bfloat16 및 int4 정밀도를 사용하여 메모리 사용량을 줄이면서도 높은 추론 속도를 보였습니다.

#### 5. 결론 (Conclusion)
OpenVLA는 로봇 조작을 위한 최고의 오픈 소스 VLA 모델로, 다양한 작업과 로봇 환경에서 뛰어난 성능과 효율성을 입증했습니다. 이 모델은 널리 공개되고 접근 가능하며, 미세 조정을 통한 새로운 작업 적용에서도 높은 성능을 보장합니다.

---

### 전체 요약
이 논문은 로봇 조작을 위한 학습된 정책의 일반화 문제를 다루며, 비전 및 언어 기초 모델의 장점을 활용하여 새로운 OpenVLA 모델을 제안합니다. OpenVLA는 7B 성능을 가진 오픈 소스 모델로, 970k 로봇 조작 데이터셋으로 훈련되었습니다. 이 모델은 기존의 폐쇄형 모델인 RT-2-X보다 성능이 뛰어나며, 효율적인 미세 조정을 통해 다양한 로봇 환경과 작업에 쉽게 적용될 수 있습니다. 더불어, OpenVLA의 체크포인트, 코드베이스 및 미세 조정 노트북이 모두 공개되어 있어, 연구자와 개발자들이 쉽게 사용할 수 있습니다.