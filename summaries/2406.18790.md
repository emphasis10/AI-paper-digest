# MUMU: Bootstrapping Multimodal Image Generation from Text-to-Image Data
## TL;DR
## Summary
- [https://arxiv.org/pdf/2406.18790.pdf](https://arxiv.org/pdf/2406.18790.pdf)

### 1. 각 섹션 요약

#### Introduction (소개)
이 논문에서는 간단한 텍스트 프롬프트를 사용해 상세한 이미지를 생성하는 텍스트-이미지 생성 AI에 대해 설명합니다. 이 모델은 사용자 의도를 더 잘 반영하기 위해 텍스트와 이미지를 혼합한 멀티모달 프롬프트를 이용합니다. 기존의 텍스트-이미지 데이터를 활용하여 멀티모달 트레이닝 세트를 구성하고, 이를 통해 'MUMU'라는 멀티모달 이해 모델을 훈련시켰습니다. 

#### Related Work and Background (관련 연구 및 배경)
텍스트 및 이미지 프롬프트를 이용해 이미지를 생성하는 기존 연구와, 이를 향상하기 위한 다양한 기법들에 대해 설명합니다. 주요내용으로는 Vision Language Models (VLMs)와 Diffusion Models가 있습니다. 

#### Diffusion Models (디퓨전 모델)
디퓨전 모델은 노이즈를 점진적으로 제거하여 이미지를 생성합니다. 상용 처리 과정, 노이즈 제거 방식, 예측 대상 및 모델 아키텍처 등 여러 방법이 활발히 연구되고 있습니다. 

#### Vision Language Models (비전 랭귀지 모델)
텍스트와 이미지의 숨겨진 상태를 상호작용하는 VLMs에 대해 설명합니다. MUMU는 표준 비전-랭귀지 아키텍처를 사용하여 텍스트와 이미지의 인코더를 결합해 훌륭한 시각적 추론을 가능하게 합니다. 

#### MUMU Architecture (MUMU 아키텍처)
MUMU 모델은 기본적으로 SDXL의 사전 훈련된 유넷과 클립 숨겨진 상태를 교차참조하는 변환기 블록을 비롯해, Idefics2 VLM을 결합하여 구성됩니다. 특히 사용된 Idefics2는 퍼시버 트랜스포머를 통해 이미지 임베딩을 고정된 길이로 풀링합니다. 

#### Constructing MUMU Captioned Datasets (MUMU 캡션 데이터셋 구축)
이미지 캡션이 포함된 텍스트-이미지 데이터를 활용해 멀티모달 프롬프트 데이터셋을 부트스트랩해 구성합니다. 여기에는 GroundingDINO의 오픈 보캡 오브젝트 감지 방법을 사용해 이미지 크롭을 추출합니다. 

#### Training Details (트레이닝 세부사항)
MUMU 모델은 SDXL과 Idefics2의 기본 모델에서 약 30만 단계를 싱글 8xH100 GPU 노드로 훈련합니다. 이때 약 6일이 소요됩니다. 

#### Evaluation (평가)
MUMU의 성능은 텍스트-이미지 생성 모델의 질적 평가를 통해 측정됩니다. 여기에는 클립 점수와 페이스 임베딩 일관성 등을 포함합니다. 

#### Conclusion (결론)
이 논문에서는 텍스트와 이미지를 혼합한 멀티모달 프롬프트를 이용해 이미지를 생성하는 MUMU 모델을 소개합니다. 향후 연구 방향으로는 데이터셋의 확장과 모델의 세부적인 개선사항 등이 제시됩니다. 

### 2. 전체 요약
이 논문은 멀티모달 프롬프트(텍스트 + 이미지)를 이용해 텍스트 기반 이미지 생성 모델인 MUMU를 개발한 내용을 담고 있습니다. 주요 내용은 기존의 텍스트-이미지 데이터를 활용해 멀티모달 트레이닝 세트를 구성하고, 이를 통해 MUMU 모델을 훈련시킨 것입니다. MUMU는 SDXL과 Idefics2를 결합하여 텍스트와 이미지의 조합을 통해 효과적으로 이미지를 생성하는 모델입니다. 모델 훈련 과정, 성능 평가, 그리고 향후 연구 방향에 대한 논의가 포함되어 있습니다. 주요 기여는 멀티모달 데이터를 통해 사용자 의도를 명확히 반영하는 이미지 생성 기술의 발전입니다.

## Similar Papers
- [Lumina-mGPT: Illuminate Flexible Photorealistic Text-to-Image Generation with Multimodal Generative Pretraining](2408.02657.md)
- [IPAdapter-Instruct: Resolving Ambiguity in Image-based Conditioning using Instruct Prompts](2408.03209.md)
- [ANOLE: An Open, Autoregressive, Native Large Multimodal Models for Interleaved Image-Text Generation](2407.06135.md)
- [Zero-shot Image Editing with Reference Imitation](2406.07547.md)
- [Streetscapes: Large-scale Consistent Street View Generation Using Autoregressive Video Diffusion](2407.13759.md)
- [ML-Mamba: Efficient Multi-Modal Large Language Model Utilizing Mamba-2](2407.19832.md)
- [Multimodal Structured Generation: CVPR's 2nd MMFM Challenge Technical Report](2406.11403.md)
- [Visual Autoregressive Modeling: Scalable Image Generation via Next-Scale Prediction](2404.02905.md)
- [Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone](2404.14219.md)
