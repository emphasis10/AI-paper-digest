# Why do LLMs attend to the first token?
## TL;DR
## Summary
- [https://arxiv.org/pdf/2504.02732.pdf](https://arxiv.org/pdf/2504.02732.pdf)

1. 각 섹션의 중요 내용 요약:

- 소개:
   이 연구는 대형 언어 모델(LLMs)이 시퀀스의 첫 번째 토큰에 집중하는 '어텐션 싱크' 현상을 다루며, 이는 정보 '과혼합'을 피하는 방법으로 유용하다고 주장합니다.

- Transform Block의 Over-Mixing 회피:
   연구는 Transformer의 값을 평균 공간으로 수렴시키는 '순위 붕괴'를 탐구하며, 깊이와 문맥 길이에 따라 어텐션 싱크가 형성된다고 설명합니다.

- 어텐션 싱크 형성에 대한 분석:
   특별히 <bos> 토큰이 어텐션 싱크 형성에 미치는 영향을 분석하며, 문맥 길이나 데이터 패킹 전략에 따라 싱크가 어떻게 형성되는지 탐구합니다.

- 결론:
   어텐션 싱크는 Transformers의 정보 손실과 혼합 억제를 돕는 자연스러운 반응으로 제안됩니다. 본 연구는 이러한 패턴이 실제로 유용한 이유를 설명하며, 어텐션 패턴 이해를 심화시키는 새로운 관점을 제공합니다.

2. 종합 요약:
   이 논문은 대형 언어 모델 내에서 시퀀스 첫 토큰에 주목하는 어텐션 패턴이 정보 '과혼합'을 피하는 데 유용하다는 새로운 시각을 제공합니다. LLMs의 깊이와 문맥 길이가 증가할수록 어텐션 싱크가 강화되며, 이는 모델의 복잡성을 효과적으로 관리하는 데 기여한다고 설명합니다.