# Order Matters in Hallucination: Reasoning Order as Benchmark and Reflexive Prompting for Large-Language-Models
## TL;DR
## Summary
- [https://arxiv.org/pdf/2408.05093.pdf](https://arxiv.org/pdf/2408.05093.pdf)

### 1. 각 섹션 요약 

#### Introduction
자연 언어 처리(NLP) 분야에서 대규모 언어 모델(LLM)은 방대한 파라미터와 광범위한 훈련 데이터를 활용하여 우수한 성능을 달성했습니다. 이러한 모델들은 교육, 의료, 금융 등 다양한 분야에서 널리 활용되며, GPT-3와 GPT-4와 같은 모델의 개발로 인해 언어 이해 및 생성 과제에 혁신을 일으켰습니다. 그러나 이러한 AI의 광범위한 도입은 "할루시네이션 문제"를 노출시켰으며, 이는 모델이 사실상 부정확하거나 완전히 만들어낸 정보를 생성하는 현상을 말합니다.

#### Related Works
할루시네이션 문제는 AI 모델이 문법적으로나 논리적으로 일관성 있는 출력을 생성하지만, 이는 항상 정확하고 사실인 것은 아닙니다. 이러한 문제는 특히 의학, 법률 상담 및 자율 주행 등 민감한 윤리적 영역에서 심각한 결과를 초래할 수 있습니다. 할루시네이션의 주요 원인으로는 데이터 부족, 지식 격차 및 최적화 과정이 있습니다.

#### Methods
- **Error Reproduction: 9.11 > 9.9**: 대규모 언어 모델이 "9.11이 9.9보다 크다"고 잘못 대답하는 현상을 관찰했습니다. 이는 주로 순차적으로 출력을 생성하는 과정에서 발생할 수 있으며, 출력 순서가 결과에 영향을 미친다는 가설을 제기했습니다.
- **Reasoning Order as Benchmark**: 모델의 논리 일관성을 측정하기 위해 "답변 우선" 프롬프트와 "논리 우선" 프롬프트를 생성하고 결과를 비교하는 방법을 제안했습니다.
- **Reflexive Prompting**: 두 가지 프롬프트 방법을 사용하여 서로 다른 결과를 얻고 이를 비교해 최종 결과를 도출하는 전략으로, 모델의 일관성과 정확성을 높이는 데 효과적입니다.

#### Experiments
다양한 대규모 언어 모델을 대상으로 설계한 실험에서, "Reasoning Order as Benchmark"과 "Reflexive Prompting" 전략의 효과를 평가했습니다. 실험 결과, 여러 데이터셋과 모델에서 새로운 프롬프트 전략이 기존 방법보다 더 높은 정확성을 나타냈습니다.

#### Discussion
모델의 할루시네이션 문제를 완화하기 위한 다양한 접근법이 제안되었으며, 특히 "Reflexive Prompting" 방법은 모델이 일관된 논리와 정확한 결과를 제공하도록 보장합니다. 그러나 실험의 한계점으로는 예산 및 자원 부족으로 인해 더 큰 데이터셋과 더 확장된 모델로 실험하지 못한 점이 있습니다.

#### Conclusion and Limitations
제안된 방법들이 모델의 성능을 향상시키는 데 효과적임을 보여주었지만, 예산 및 자원 부족으로 인해 더 큰 규모의 실험을 수행하지 못했습니다. 향후 연구에서는 디코더 네트워크를 수정하여 모델이 이전 응답을 볼 수 있는 기능을 추가하는 방향으로 할루시네이션 문제를 완화할 수 있을 것입니다.

### 2. 전체 요약

이 논문에서는 대규모 언어 모델(LLM)의 할루시네이션 문제를 해결하기 위한 두 가지 주요 방법, "Reasoning Order as Benchmark"와 "Reflexive Prompting"을 제안합니다. 할루시네이션 문제는 모델이 문법적으로 일관성 있지만 사실상 부정확한 출력을 생성하는 현상을 말합니다. "Reasoning Order as Benchmark"는 답변 우선 프롬프트와 논리 우선 프롬프트를 통해 모델의 일관성을 측정하며, "Reflexive Prompting"은 이 두 가지 결과를 비교하여 최종 결과를 도출하는 방법입니다. 실험 결과, 이 두 가지 방법이 모델의 정확성을 높이는 데 효과적임을 확인할 수 있었습니다. 다만, 예산과 자원의 제약으로 인해 더 큰 데이터셋에 대한 실험은 이루어지지 않았습니다. 향후 연구에서는 디코더 네트워크를 수정하여 모델이 이전 응답을 참조할 수 있는 기능을 추가하는 방향으로 나아갈 것입니다.