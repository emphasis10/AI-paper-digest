# All Roads Lead to Likelihood: The Value of Reinforcement Learning in Fine-Tuning
## TL;DR
## Summary
- [https://arxiv.org/pdf/2503.01067.pdf](https://arxiv.org/pdf/2503.01067.pdf)

1. 섹션별 요약:

- **서론 및 배경**:
  연구의 시작 부분에서는 강화 학습을 통한 정교한 파인 튜닝(FT) 방법에 대해 소개합니다. 주요 이슈는 복잡한 두 단계의 훈련 절차임을 밝힙니다. MLE 대신 RM을 먼저 학습한 후 RL로 이어지는 과정이 데이터 정보 손실을 초래할 수도 있음을 논의합니다.

- **정보 기하학 관점에서의 분석**:
  이 섹션에서는 온라인과 오프라인 FT의 차이를 정보 이론의 관점에서 분석합니다. 결과적으로 온라인과 오프라인 PFT 기법이 같은 함수 클래스를 사용할 때 해당 기법들이 유사한 품질의 정책을 만들어낸다는 점을 입증합니다.

- **기술적 기여**:
  연구는 여러 가지 가설을 제시하고 이를 검증합니다. 특히 생성-검증 차이가 있는 경우 RL이 단순한 검증기를 찾아주고, 그 후 적절한 정책만 검색 수행함으로써 성능이 매우 향상될 수 있음을 밝힙니다.

- **실험 및 결과**:
  온라인 PFT가 오프라인보다 우수한 성능을 보여주는 실험 결과가 발표됩니다. 특히 온라인 DPO가 오프라인보다 높은 성능을 나타내며 인류의 피드백이 부족한 상태에서도 효과적임을 증명합니다.

- **논의 및 결론**:
  연구에서 검토된 가설들을 바탕으로, 성능 차이를 설명하기 위해 향후 연구 방향을 제안합니다. 고난도 문제를 해결하기 위해 RLHF의 지속적인 사용이 요구된다고 강조합니다.

2. 전체 요약:

이 연구는 강화 학습을 통한 파인 튜닝에서의 가치에 대해 심층적으로 탐색합니다. 온라인과 오프라인 방법의 비교를 통해, 특히 생성과 검증의 복잡도가 다를 때 RL의 효용성을 입증합니다. 실험적으로도, 온라인 PFT는 오프라인보다 일관된 성능 향상을 보였으며, 이는 RL을 통한 정책 학습이 검증기 기반의 올바른 검증 방식을 지원함으로써 정책의 품질을 향상시키기 때문임을 밝혀냅니다. 연구는 정보 기하학의 틀 내에서 이론적, 실험적 비교를 통해 결론에 이르렀으며, 이러한 통찰은 AI의 학습 방법론의 발전에 기여하는 바가 큽니다.