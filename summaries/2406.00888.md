# Show, Don't Tell: Aligning Language Models with Demonstrated Feedback
## TL;DR
## Summary
- [https://arxiv.org/pdf/2406.00888.pdf](https://arxiv.org/pdf/2406.00888.pdf)

### 논문의 주요 내용 요약

**1. 서론 (Introduction)**
- 대규모 언어 모델(LLM)은 일반적인 용도로 훈련되지만, 실제로는 특정 사용자와 특정 작업에 적용됩니다.
- 이메일 작성과 같은 간단한 작업도 개인의 글쓰기 스타일이나 특정 이메일 작업, 대상 청중에 따라 달라집니다.
- 기존의 방법들(감독 학습, RLHF 등)은 효과적이지만, 새로운 작업에 대해 큰 데이터셋이 필요하고, 이는 비현실적입니다.
- 본 논문에서는 Demonstration ITerated Task Optimization(DITTO)를 소개하여 소수의 시연(<10)을 사용해 언어 모델을 특정 설정에 맞추는 방법을 제안합니다.

**2. 관련 연구 (Related Work)**
- 기존의 LLM은 주의 깊은 프롬프트 작성으로 잘 작동하지만, 프롬프트 작성은 매우 번거롭고 변동에 민감합니다.
- RLHF와 같은 방법은 많은 양의 비교 데이터를 필요로 하여 비용이 많이 듭니다.
- DITTO는 시연에서 직접 비교 데이터를 생성하여 이러한 문제를 해결합니다.

**3. DITTO 방법론 (DITTO Methodology)**
- DITTO의 핵심 통찰은 전문가 시연과 모델 자체가 비교 데이터셋을 생성할 수 있다는 것입니다.
- 이는 대조적 목표를 달성하게 하며, 전문가 시연을 긍정적 데이터로 사용합니다.
- DITTO는 시간적 학습 과정을 고려하여 반복된 정책 업데이트를 통해 성능을 향상시킵니다.

**4. 실험 (Experiments)**
- DITTO는 작가별 데이터셋과 사용자 평가를 통해 벤치마크 테스트를 수행했습니다.
- DITTO는 다른 방법들(SFT, SPIN, few-shot prompting)보다 우수한 성능을 보였습니다.
- 사용자 연구(N=16)에서도 DITTO는 다양한 사용자 생성 프롬프트와 SFT 방법보다 높은 승률을 기록했습니다.

**5. 결론 (Conclusion)**
- DITTO는 적은 수의 시연을 사용하여 LLM을 사용자/작업별로 맞추는 새로운 방법을 제공합니다.
- 이는 대규모 비교 데이터를 필요로 하지 않아 효율적이며, 다양한 실제 응용 프로그램에 적용할 수 있습니다.
- 본 연구는 LLM의 사용자 맞춤화를 위한 새로운 방향을 제시합니다.

### 전체 요약

이 논문은 Demonstration ITerated Task Optimization(DITTO)이라는 새로운 방법론을 통해, 소수의 시연을 사용하여 대규모 언어 모델(LLM)을 특정 사용자와 특정 작업에 맞추는 방법을 제안합니다. DITTO는 기존의 감독 학습이나 RLHF 방법에 비해 적은 데이터로도 효과적으로 작동하며, 시연 데이터를 사용해 모델을 맞춤화합니다. 실험 결과, DITTO는 다른 방법들에 비해 높은 성능을 보였으며, 사용자 연구에서도 긍정적인 평가를 받았습니다. 이는 LLM의 효율적이고 사용자 맞춤화된 응용을 가능하게 하는 중요한 기여를 합니다.