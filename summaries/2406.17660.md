# Grass: Compute Efficient Low-Memory LLM Training with Structured Sparse Gradients
## TL;DR
## Summary
- [https://arxiv.org/pdf/2406.17660.pdf](https://arxiv.org/pdf/2406.17660.pdf)

# 논문 요약

## 1. 도입 (Introduction)
대형 언어 모델(LLM)의 사전 학습 및 미세 조정(fine-tuning)은 종종 메모리 한계에 의해 제약됩니다. 예를 들어, LLaMA-13B 모델을 학습하는 데 필요한 메모리는 최소 102GB로, 이는 Nvidia A100과 같은 고급 GPU에서도 감당하기 어렵습니다. 기존의 메모리 효율적인 시스템 수준 기술들은 학습 속도를 희생하면서 메모리를 절약하지만, 본 논문에서는 이를 넘어서는 최적화 알고리즘이 필요하다고 주장합니다.

## 2. 관련 연구 (Related Work)
기존의 메모리 효율적인 최적화 기법에는 Low-Rank Adaptation(LoRA)이 포함되어 있으며, 이는 효율적이지만 성능 면에서 완전한 학습 방식에 비해 미흡합니다. 또한, 메모리 효율적 서브스페이스 최적화 기법에도 불구하고, 이들 기법은 고차원 그래디언트를 낮은 차원 공간으로 투영하여 메모리를 절약하지만, 여전히 상당한 메모리와 계산 자원이 필요합니다.

## 3. 방법 (Method)
본 논문에서는 GRASS(GRAdient Structured Sparsification)라는 새로운 접근법을 제안합니다. 이는 희소 투영 기법을 활용하여 그래디언트를 구조화된 희소 업데이트로 변환합니다. 이 디자인은 최적화 상태의 메모리 사용량을 줄일 뿐만 아니라, 그래디언트 메모리, 계산, 통신 비용도 최소화합니다. 본 논문은 GRASS를 통해 LLaMA 모델의 학습을 가능하게 하여, 이전 방법들로는 불가능했던 것을 실현하였습니다.

## 4. 실험 (Experiments)
광범위한 실험을 통해 GRASS가 완전한 학습 방식 및 기존의 투영 기반 방법과 대등한 성능을 달성함을 보였습니다. 또한, GRASS는 다양한 모델 크기 및 작업에서 메모리 절감과 처리량 증가를 제공했습니다. 예를 들어, 13B 파라미터 LLaMA 모델의 절반 정밀도 학습을 단일 40GB A100 GPU에서 가능하게 했고, 8-GPU 시스템에서는 최대 2배의 처리량 개선을 이루었습니다.

## 5. 결론 및 향후 연구 (Conclusion and Future Work)
GRASS는 최적화 상태와 그래디언트의 메모리 발자국을 크게 줄이고, 투영 단계에서 전체 그래디언트를 필요로 하지 않게 하여 상당한 계산 효율성을 제공합니다. 본 연구 결과는 GRASS가 완전 순위 학습 및 기존의 투영 기반 방법과 대등한 성능을 달성하면서 메모리 절감과 처리량 증가를 제공함을 보여줍니다. 향후 연구 방향으로는 다양한 구조적 희소성 패턴을 활용한 GRASS의 확장 및 하드웨어 및 모델 크기에 따른 동적 투영 순위를 조정하는 전략을 탐구하는 것이 포함됩니다.

# 전체 요약
이 논문은 대형 언어 모델(LLM)의 학습에서 GPU 메모리 제약을 극복하고자 GRASS(GRAdient Structured Sparsification)라는 획기적인 방법을 제안합니다. GRASS는 희소 투영을 활용하여 최적화 상태와 그래디언트의 메모리 사용량을 줄이고, 학습 과정에서의 계산 및 통신 비용도 절감합니다. 이를 통해, 기존 방법들로는 불가능했던 LLaMA-13B 모델의 단일 40GB A100 GPU를 활용한 학습을 가능하게 하였으며, 8-GPU 시스템에서는 최대 2배의 처리량 개선을 이루었습니다. 이 논문은 GRASS가 메모리와 계산 효율성을 크게 향상시킬 수 있음을 실험적으로 입증하였으며, 향후 다양한 구조적 희소성 패턴을 탐구하여 성능을 더욱 향상시킬 계획입니다.