# RLHF Workflow: From Reward Modeling to Online RLHF
## TL;DR
## Summary
- [https://arxiv.org/pdf/2405.07863.pdf](https://arxiv.org/pdf/2405.07863.pdf)

### 주요 내용 요약

1. **서론 및 배경**:
   - 이 논문은 인간 피드백을 통한 온라인 반복 강화 학습(RLHF) 워크플로우를 소개합니다. RLHF는 대규모 언어 모델(LLM)을 인간의 가치와 선호에 맞추는 데 중요한 기술로 자리 잡았으며, 특히 ChatGPT와 같은 모델에서 큰 성과를 거두었습니다. 기존의 오프라인 학습 방식에 비해 온라인 반복 RLHF는 더 우수한 성능을 보여주지만, 오픈 소스 커뮤니티에서는 아직 충분히 탐구되지 않았습니다.

2. **방법론**:
   - 이 논문에서는 인간 피드백을 대체하기 위해 다양한 오픈 소스 데이터셋을 사용하여 선호 모델을 구축하고, 이를 통해 프록시 선호 신호를 생성하는 접근 방식을 제안합니다. Bradley-Terry 보상 모델과 선호 모델을 사용하여 선호 신호를 근사화하고, 이를 통해 온라인 반복 RLHF의 이론적 통찰과 알고리즘 원칙을 제시합니다.
   - RLHF 워크플로우는 초기 정책 모델을 감독 학습으로 미세 조정하고, 반복 학습 과정에서 온라인 데이터를 수집하여 정책을 최적화하는 방식으로 구성됩니다. 이를 통해 새로운 데이터가 추가됨에 따라 정책 모델의 성능이 점진적으로 향상됩니다.

3. **실험**:
   - 실험 결과, 학습된 LLM(SFR-Iterative-DPO-LLaMA-3-8B-R)은 AlpacaEval-2, Arena-Hard, MT-Bench 등의 LLM 챗봇 벤치마크에서 우수한 성능을 보였습니다. 또한 HumanEval, TruthfulQA 등의 학술 벤치마크에서도 인상적인 성과를 나타냈습니다.
   - 특히, 온라인 반복 RLHF는 대화 품질을 크게 향상시키며, 기존의 오프라인 DPO 알고리즘을 사용한 모델에 비해 일관된 성능 향상을 보여줍니다. 이는 온라인 데이터 수집이 모델의 성능을 크게 향상시킬 수 있음을 시사합니다.

### 혁신적인 부분
이 논문의 혁신성은 오픈 소스 데이터를 활용하여 인간 피드백을 근사화하는 프록시 선호 모델을 구축하고, 이를 통해 온라인 반복 RLHF를 구현한 점에 있습니다. 이 접근 방식은 인간 피드백을 직접 수집하는 데 드는 비용과 자원을 절약하면서도, 모델의 성능을 효과적으로 향상시킬 수 있는 방법을 제시합니다. 또한, 다양한 벤치마크에서 우수한 성능을 보임으로써, RLHF의 가능성을 확장하고 실질적인 적용 가능성을 높입니다.

## Similar Papers
- [Self-Exploring Language Models: Active Preference Elicitation for Online Alignment](2405.19332.md)
- [WPO: Enhancing RLHF with Weighted Preference Optimization](2406.11827.md)
- [Value-Incentivized Preference Optimization: A Unified Approach to Online and Offline RLHF](2405.19320.md)
- [Bootstrapping Language Models with DPO Implicit Rewards](2406.09760.md)
- [Self-Play Preference Optimization for Language Model Alignment](2405.00675.md)
- [SimPO: Simple Preference Optimization with a Reference-Free Reward](2405.14734.md)
- [Meta-Rewarding Language Models: Self-Improving Alignment with LLM-as-a-Meta-Judge](2407.19594.md)
- [Xwin-LM: Strong and Scalable Alignment Practice for LLMs](2405.20335.md)
- [Self-play with Execution Feedback: Improving Instruction-following Capabilities of Large Language Models](2406.13542.md)
