# Towards Hierarchical Multi-Step Reward Models for Enhanced Reasoning in Large Language Models
## TL;DR
## Summary
- [https://arxiv.org/pdf/2503.13551.pdf](https://arxiv.org/pdf/2503.13551.pdf)

1. **논문의 중요 내용 요약**

   - **서론**
     이 논문은 대규모 언어 모델(LLM)의 논리 추론 능력을 향상시키기 위한 새로운 방법을 제안합니다. 기존의 보상 모델에서 PRM은 보상 해킹 문제를 가지고 있어 문제가 발생합니다. 이를 해결하기 위해, 다단계 추론의 일관성과 자기 반성을 보장하는 계층적 보상 모델(HRM)을 제안합니다.

   - **주요 기여**
     - HRM: 기존의 PAM을 개선하여, 세부적인 추론 단계와 연속된 단계 평가를 통해 초래 단계를 교정할 수 있게 합니다. 이를 통해 PRM보다 더 안정적인 평가가 가능하다는 것을 실험을 통해 보여주었습니다.
     - 계층적 노드 압축(HNC): PRM800K 데이터셋의 자동 주석 생성 과정에서 사용되며, 데이터 세트의 다양성을 증가시키면서도 계산 부담을 최소화하는 방법입니다.
     - 정책 모델 고도화: MCTS로부터 필터링된 고품질 추론 데이터를 통해 정책 모델의 추론 성능을 강화합니다.

   - **결론**
     HRM은 다단계 추론의 일관성을 평가하기 위해 세부와 대강의 분석을 결합하여 자기 반성을 강화합니다. PRM800K 데이터셋에서 강력한 일반화 가능성과 견고함을 입증하였으며, 다양한 도메인에서도 동등하게 우수한 결과를 보였습니다.

2. **전반적인 요약**
   
   이 논문은 AI와 기계 학습 분야의 중요한 발전에 대해 논의하고 있습니다. 특히, 기존의 보상 평가 모델의 한계를 보완하는 HRM 및 HNC의 제안과 함께 새롭게 맞춤화된 정책 모델의 효과를 보여줍니다. 다양한 실험에서 HRM은 PRM 대비 더 높은 안정성과 일반화 가능성을 제공하며, 새로운 데이터 기반의 정책 학습 방법이 대규모 언어 모델의 논리적 추론을 더욱 강화할 수 있음을 보여줍니다.