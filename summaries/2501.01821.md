# SDPO: Segment-Level Direct Preference Optimization for Social Agents
## TL;DR
## Summary
- [https://arxiv.org/pdf/2501.01821.pdf](https://arxiv.org/pdf/2501.01821.pdf)

**Section 요약**

1. **소개:**
   최근 대형 언어 모델(LLM)의 발전은 인간-기계 상호작용 분야에서의 언어 이해와 생성 능력을 향상시켰습니다. 그러나 협상, 경쟁, 협력 등의 복잡한 목표 지향적 사회 시나리오에서 LLM은 여전히 인간 사회적 상호작용의 특징인 미묘한 의사결정 능력을 보여주지 못하고 있습니다.

2. **SDPO 개념:**
   Segment-Level Direct Preference Optimization (SDPO)는 에이전트 행동을 세그먼트 수준에서 정렬하는 새로운 접근법입니다. 이는 다중 턴 상호작용에서 에이전트의 목표 달성도를 향상시킵니다.

3. **실험 및 결과:**
   SDPO는 SOTOPIA 벤치마크에서 GPT-4o와 같은 상용 대형 언어 모델을 포함한 기존 방법보다 월등한 성능을 보여주었습니다. 이 결과는 세그먼트 수준 정렬의 효율성을 강조합니다.

4. **제약 사항 및 미래 방향:**
   SDPO는 현재 긍정 세그먼트와 부정 세그먼트의 길이가 동일해야 하는 제약을 가지고 있습니다. 앞으로는 이 제약을 극복하여 다양한 에이전트 태스크에 SDPO를 적용할 계획입니다.

**종합 요약**

이 논문은 대형 언어 모델 기반 사회적 대화 에이전트를 위한 새로운 최적화 방법인 세그먼트 수준 직접 선호 최적화(SDPO)를 소개합니다. SDPO는 대화 세션 내의 특정 핵심 세그먼트를 최적화하여 에이전트 행동을 정렬하는 방식으로, 복잡한 목표 지향적 사회 시나리오에서의 에이전트의 목표 달성도를 극대화합니다. SOTOPIA 벤치마크에서의 성능은 기존의 상태 대비 우수함을 입증하였으며, 이는 SDPO의 적용이 다양한 에이전트 태스크에도 확장될 수 있음을 시사합니다.