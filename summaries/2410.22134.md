# ProMoE: Fast MoE-based LLM Serving using Proactive Caching
## TL;DR
## Summary
- [https://arxiv.org/pdf/2410.22134.pdf](https://arxiv.org/pdf/2410.22134.pdf)

### 주요 섹션 요약

**1. 서론:**
이 논문은 ProMoE라는 새로운 시스템을 소개하여 Mixture-of-Experts (MoE) 기반 대형 언어 모델(LLM)을 소비자용 하드웨어에서 실행할 때 발생하는 문제를 해결합니다. ProMoE는 예측 캐싱 방식을 사용해 전문가 오프로딩 및 캐싱과 관련된 성능 병목 현상을 해소합니다.

**2. 배경:**
MoE 기반 LLM은 각 토큰에서 소수의 전문가만 활성화 시킵니다. 이렇게 하면 소비자용 하드웨어의 제한된 GPU 메모리에서도 실행할 수 있지만, CPU 메모리로부터의 높은 대기시간 때문에 성능 저하 문제가 발생합니다.

**3. ProMoE 개요:**
ProMoE는 기존의 부하 반응형 데이터 전송에서 발생하는 고대기시간 문제를 해결하기 위해 사전 예측 캐싱 방식으로, 중요한 데이터 전송을 해소하여 처리 시간을 단축하고, GPU 활용을 개선합니다.

**4. GOODPRED 및 학습 기반 예측기:**
ProMoE는 높은 예측 정확도를 위해 학습 기반 예측기를 사용하여 다양한 층의 전문가 선택을 사전에 예측합니다. GOODPRED라는 용어를 통해 예측 방식의 질을 평가하며 이를 통해 필요한 전문가 설정을 사전 캐시에 저장합니다.

**5. 예측 및 추론 조정:**
다양한 기술(청크 기반 선행, 선행 권한 포기, 재정렬된 추론)로 예측 및 추론을 조절하여 각 프로세스 간 간섭을 최소화하고 성능을 높이도록 시스템을 설계했습니다.

**6. 구현 및 평가:**
ProMoE 시스템은 주요 LLM 프레임워크에 통합되어 높은 효율성을 자랑하며, 최신 솔루션과 비교하여 앞선 성능을 보입니다.

### 전체 요약
ProMoE는 MoE 기반 대형 언어 모델을 소비자 수준 하드웨어에서 더 효율적으로 실행할 수 있는 혁신적인 시스템입니다. 사전 예측 캐싱을 통해 전문가 관련 데이터를 사전에 준비하고, 시스템 간섭을 줄여 GPU 활용을 최적화합니다. 결과적으로 ProMoE는 추론 지연을 줄이고, 기존의 오프로딩 방식보다 성능 면에서 두 배 이상의 개선을 보여줍니다.