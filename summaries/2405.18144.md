# 4-bit Shampoo for Memory-Efficient Network Training
## TL;DR
## Summary
- [https://arxiv.org/pdf/2405.18144.pdf](https://arxiv.org/pdf/2405.18144.pdf)

#### 1. 서론
딥 러닝의 성공은 주로 1차 최적화 알고리즘에 기반합니다. 2차 최적화 알고리즘은 이론적으로나 실무적으로나 우수하지만, 메모리 사용량이 많아 큰 모델을 훈련시키는 데 제약이 있습니다. 이 논문에서는 2차 최적화 알고리즘인 샴푸(Shampoo)를 4비트로 압축하여 메모리 효율성을 높이는 방법을 제안합니다.

#### 2. 기초
여기서는 샴푸 알고리즘과 4비트 압축 방법을 소개합니다. 샴푸 알고리즘은 행렬의 4제곱근 역행렬을 이용하여 최적화를 수행하며, 이는 높은 메모리 비용을 초래합니다. 이를 해결하기 위해, 업데이트된 최적화 상태를 양자화하고 사용 전에 역양자화하는 방법을 사용합니다.

#### 3. 방법론
- **고유벡터 행렬 양자화**: 단순히 PD 행렬을 양자화하는 것보다 고유벡터 행렬을 양자화하는 것이 더 정확합니다.
- **고유벡터 행렬의 직교성 수정**: Björck 방법을 사용하여 양자화된 고유벡터 행렬의 직교성을 수정합니다.
- **양자화기 선택**: 고유벡터 행렬에 대해 선형 제곱 양자화(Linear-2)를 사용하여 양자화 오류를 줄입니다.
- **전체 알고리즘**: 업데이트 주기를 조절하여 메모리와 계산 비용을 줄입니다.

#### 4. 이론적 분석
양자화된 고유벡터 행렬의 직교성 수정이 양자화 오류를 어떻게 줄이는지에 대해 수학적으로 분석합니다. 이를 통해 제안된 방법이 기존 방법보다 우수함을 증명합니다.

#### 5. 실험
CIFAR-100 및 Tiny-ImageNet 데이터셋에서 실험을 수행하여 제안된 4비트 샴푸가 32비트 샴푸와 유사한 성능을 유지하면서 메모리 사용량을 크게 줄일 수 있음을 확인합니다. 또한, K-FAC 및 AdaBK와 같은 다른 2차 최적화 알고리즘에서도 제안된 방법을 적용하여 유사한 성능 향상을 확인합니다.

#### 6. 결론
이 논문에서는 2차 최적화 알고리즘의 메모리 사용량을 줄이기 위해 고유벡터 행렬을 양자화하고, 직교성을 수정하는 4비트 샴푸를 제안했습니다. 제안된 방법은 이론적으로나 실험적으로나 기존 방법보다 우수한 성능을 보여주었으며, 대규모 모델 훈련에 효율적으로 사용될 수 있습니다.

### 전체 요약
이 논문은 2차 최적화 알고리즘의 메모리 사용량 문제를 해결하기 위해 4비트 샴푸 알고리즘을 제안합니다. 샴푸 알고리즘은 행렬의 4제곱근 역행렬을 이용하여 최적화를 수행하지만, 메모리 비용이 높습니다. 이를 해결하기 위해, 고유벡터 행렬을 양자화하고 Björck 방법을 사용하여 직교성을 수정함으로써 메모리 효율성을 높입니다. 제안된 방법은 다양한 데이터셋에서 기존의 32비트 알고리즘과 유사한 성능을 유지하면서 메모리 사용량을 크게 줄일 수 있음을 실험을 통해 확인했습니다. 이는 대규모 딥 러닝 모델의 훈련에 효율적으로 적용될 수 있습니다.

## Similar Papers
- [Adam-mini: Use Fewer Learning Rates To Gain More](2406.16793.md)
- [Thermodynamic Natural Gradient Descent](2405.13817.md)
- [The Road Less Scheduled](2405.15682.md)
- [Mixture of Scales: Memory-Efficient Token-Adaptive Binarization for Large Language Models](2406.12311.md)
- [$μ$LO: Compute-Efficient Meta-Generalization of Learned Optimizers](2406.00153.md)
- [BitNet: Scaling 1-bit Transformers for Large Language Models](2310.11453.md)
- [TurboTransformers: An Efficient GPU Serving System For Transformer Models](2010.05680.md)
- [VSSD: Vision Mamba with Non-Causal State Space Duality](2407.18559.md)
- [Conditional LoRA Parameter Generation](2408.01415.md)
