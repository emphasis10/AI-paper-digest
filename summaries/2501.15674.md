# TensorLLM: Tensorising Multi-Head Attention for Enhanced Reasoning and Compression in LLMs
## TL;DR
## Summary
- [https://arxiv.org/pdf/2501.15674.pdf](https://arxiv.org/pdf/2501.15674.pdf)

### 각 섹션의 주요 내용 요약 (한국어)

1. **서론 (Introduction)**  
   대형 언어 모델(LLM)의 성공은 매우 방대한 매개변수와 데이터의 도움으로 실현되었으며, 기존의 연구는 이러한 모델들이 모든 매개변수를 필요로 하지 않는다는 것을 보여주었다. 따라서 LLM을 더 효율적으로 만들기 위한 압축 기술에 대한 탐구가 이루어졌다. 이 연구에서는 LLM의 추론 능력을 향상시키고 파라미터 압축을 동시에 달성할 수 있는 새로운 방법론을 제안한다.

2. **배경 (Background)**  
   이 섹션에서는 수학적 표기법과 텐서 전제 지식이 제공된다. MHA(다중 헤드 주의) 블록과 FFN(피드 포워드 네트워크) 블록 간의 차이점과 기존 방법들의 한계가 논의되며, MHA 블록의 정보 공유 필요성이 강조된다.

3. **제안된 방법론 (Proposed Methodology)**  
   MHA의 가중치를 텐서로 변환하고, Tucker 분해를 통해 각 주의 헤드의 특징을 공유하는 구조로 압축하고 제거하는 방법이 설명된다. 이 과정을 통해 LLM의 추론 능력이 향상되고 파라미터의 크기를 줄일 수 있다.

4. **실험 결과 (Experimental Results)**  
   제안된 방법론이 여러 기준 데이터셋에 대해 기존 방법과 비교하여 성능 향상을 보여준다는 것을 입증하기 위해 실험이 수행되었다. 이를 통해 압축 비율과 정확도가 모두 향상되었음을 발견하였다.

5. **결론 (Conclusion)**  
   본 연구에서는 LLM의 추론 능력을 개선하고 파라미터 압축을 동시에 수행하기 위한 새로운 프레임워크를 제안하였다. 이 프레임워크는 MHA의 가중치를 구조적으로 노이즈 제거하고 압축 하는 방법을 통해 MHA 블록에 적합하게 설계되었다. 향후 연구는 다양한 하이퍼파라미터 설정을 쉽게 조정할 수 있는 방법을 모색할 예정이다.

### 전체 요약 (한국어)

이 연구에서는 대형 언어 모델(LLM)의 추론 능력을 개선하고 동시에 파라미터 압축을 수행하는 새로운 방법론을 제안하였다. 이 방법론은 다중 헤드 주의(MHA) 블록의 가중치를 구조적으로 노이즈 제거 및 압축함으로써 LLM의 성능을 향상시킨다. 본 연구는 다양한 기준 데이터셋에서 MHA 압축의 효과를 입증하였으며, 기존 FFN 자세를 함께 사용하여 성능을 더욱 향상시킬 수 있음을 보여 주었다. 이로 인해 LLM이 메모리 제약 상황에서도 높은 성능을 유지할 수 있도록 하는 길을 제시하고 있다.