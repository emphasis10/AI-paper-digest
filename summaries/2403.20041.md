# Transformer-Lite: High-efficiency Deployment of Large Language Models on Mobile Phone GPUs
## TL;DR
## Summary
- [https://arxiv.org/pdf/2403.20041.pdf](https://arxiv.org/pdf/2403.20041.pdf)

### 1. 섹션별 요약

#### 도입부
대형 언어 모델(LLM)은 지능형 비서, 텍스트 요약, 번역, 멀티모달 작업 등과 같은 모바일 기기 어플리케이션에서 널리 사용되고 있습니다. 그러나 현재의 모바일 기기에서 LLM 배포 방법은 느린 추론 속도로 인해 사용자 경험을 저하시킵니다. 이를 해결하기 위해 저자들은 변형 경량 모델(Transformer-Lite)을 제안했습니다. 이 엔진은 기호 표현 기반 동적 형태 추론, 연산자 최적화 및 실행 우선 설정, FP4 양자화 방법, 그리고 KV 캐시 카피 제거 기법을 사용하여 효율적으로 LLM을 모바일 GPU에서 실행할 수 있습니다. 다양한 아키텍처와 매개변수를 가진 LLM의 성능을 평가한 결과, 기존 CPU 또는 GPU 기반 엔진보다 훨씬 빠른 속도 개선을 확인했습니다.

#### 방법론
이 섹션에서는 변형 경량 모델의 최적화 기법에 대해 자세히 설명합니다.

1. **기호 표현 기반 동적 형태 추론**:
   - 동적 형태 모델 추론을 지원하기 위해 기호 표현 접근법을 사용합니다. 이를 통해 메모리 재사용과 실행 스케쥴링이 가능해집니다.
   - 동적 형태를 가진 텐서들 간의 메모리 재사용을 수행하고, 형상 업데이트 동안의 시간 소비를 줄이기 위한 방법을 설명합니다.
  
2. **연산자 및 지연 최적화**:
   - 성능 향상과 폰의 지연 감소를 위해 연산자 최적화와 실행 우선 설정을 사용합니다.
   - 전화 렉을 줄이기 위해 최신 OpenCL 확장을 이용하여 실행 우선 수준을 최저 수준으로 설정합니다.

3. **FP4 양자화(E0M4)**:
   - FP4 양자화 방법을 제안하여 디양자화 오버헤드를 줄이고 효율적인 매트릭스 곱셈을 가능하게 합니다.
   - 더불어, 일부 연산의 형상 업데이트 시간을 줄이는 기법들을 설명합니다.

4. **서브 텐서 기반 방법**:
   - 동적 형태 모델 출력에서 입력으로 KV 캐시를 복사할 필요를 제거하는 서브 텐서 기반 기법을 사용합니다.

#### 결과
이 섹션에서는 Transformer-Lite 엔진의 성능 평가 결과를 제시합니다.

- 채트GLM2 6B 모델에서 121 토큰/초의 프리필 속도와 14 토큰/초의 디코딩 속도를 달성했습니다.
- Gemma 2B 모델에서는 330 토큰/초의 프리필 속도와 30 토큰/초의 디코딩 속도를 기록했습니다.
- 기존 CPU 기반 FastLLM과 GPU 기반 MLC-LLM과 비교할 때, 프리필 속도에서 10배 이상, 디코딩 속도에서 2~3배 이상의 향상을 보였습니다.

#### 결론
결론적으로, 이 연구는 대형 언어 모델의 모바일 기기 GPU에서의 효율적인 배포를 가능하게 하는 다양한 최적화 기법을 제안합니다. 이는 사용자 경험을 대폭 향상시키며, 기존 기법들에 비해 월등한 성능 개선을 이루었습니다. 이 연구는 향후 LLM의 모바일 디바이스 배포에 중요한 기초 자료가 될 것입니다.

### 2. 전체 요약
이 논문은 대형 언어 모델(LLM)을 모바일 디바이스의 GPU에서 효율적으로 배포하기 위한 변형 경량 모델(Transformer-Lite)을 제안합니다. 성능 향상을 위해 기호 표현 기반 동적 형태 추론, 연산자 최적화, FP4 양자화 기법(E0M4), 서브 텐서 기반 방법 등의 네 가지 핵심 최적화 기법을 사용했습니다. 실험 결과, 기존의 CPU 및 GPU 기반 엔진에 비해 프리필과 디코딩 속도가 크게 향상되었으며, 이는 사용자 경험을 크게 개선할 수 있음을 보여줍니다. 이 논문은 LLM의 효율적이고 고성능 배포를 위한 중요한 기초를 제공하며, 향후 연구와 응용에 크게 기여할 것입니다.