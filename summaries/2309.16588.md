# Vision Transformers Need Registers
## TL;DR
## Summary
- [https://arxiv.org/pdf/2309.16588.pdf](https://arxiv.org/pdf/2309.16588.pdf)

## 요약

### 1. 서론
- **문제 정의**: Vision Transformer (ViT) 모델들이 이미지의 배경 부분에서 높은 정규화 값을 가지는 아티팩트를 생성함.
- **해결 방안**: 추가적인 토큰을 입력 시퀀스에 제공하여 이러한 아티팩트를 제거하는 방법 제안.

### 2. 문제 설정
- **아티팩트 분석**: DINOv2 모델에서 높은 정규화 값을 가지는 토큰들이 주로 배경 영역에 나타나며, 이는 내부 계산을 위해 재활용됨.
- **관찰**: 이러한 아티팩트는 모델이 충분히 큰 경우와 충분히 긴 훈련 시간 후에 나타남.

### 3. 제안 방법
- **레지스터 토큰 추가**: 입력 시퀀스에 새로운 학습 가능한 토큰을 추가하여 모델이 이들을 레지스터로 사용하도록 함.
- **결과**: 아티팩트가 제거되고, 모델 성능이 향상되며, 피처 맵과 주의 맵이 매끄러워짐.

### 4. 실험
- **모델 평가**: DeiT-III, OpenCLIP, DINOv2 모델에 레지스터 토큰을 추가하여 성능 평가.
- **결과**: 레지스터 토큰을 추가함으로써 모든 모델에서 아티팩트가 제거되고 성능이 향상됨.

### 5. 관련 연구
- **기존 연구**: 이전 연구들은 Transformer 모델에 다양한 형태의 토큰을 추가하여 성능을 개선함.
- **차별점**: 본 연구에서는 추가된 토큰이 새로운 정보를 제공하지 않고, 단순히 정보 저장 및 검색을 위해 사용된다는 점에서 차별화됨.

### 6. 결론
- **주요 기여**: 레지스터 토큰을 추가함으로써 ViT 모델의 아티팩트를 제거하고 성능을 향상시키는 간단하면서도 효과적인 방법 제안.
- **미래 연구**: 레지스터 토큰의 최적 개수와 위치 등에 대한 추가 연구 필요.

---

## 전체 요약
본 논문은 Vision Transformer(ViT) 모델에서 발생하는 아티팩트 문제를 해결하기 위해 레지스터 토큰을 추가하는 방법을 제안합니다. 기존의 DINOv2 모델은 배경 영역에서 높은 정규화 값을 가지는 아티팩트를 생성하였으며, 이는 모델의 성능을 저하시킬 수 있습니다. 본 연구에서는 이러한 아티팩트를 제거하기 위해 입력 시퀀스에 새로운 학습 가능한 토큰을 추가하고, 이를 레지스터로 사용하여 문제를 해결했습니다. 실험 결과, DeiT-III, OpenCLIP, DINOv2 모델 모두에서 아티팩트가 제거되고 성능이 향상됨을 확인했습니다. 이 방법은 간단하면서도 효과적인 해결책을 제공하며, 미래 연구를 통해 레지스터 토큰의 최적 개수와 위치 등을 추가적으로 탐구할 필요가 있습니다.

## Similar Papers
- [SparseCraft: Few-Shot Neural Reconstruction through Stereopsis Guided Geometric Linearization](2407.14257.md)
- [Theia: Distilling Diverse Vision Foundation Models for Robot Learning](2407.20179.md)
- [4M-21: An Any-to-Any Vision Model for Tens of Tasks and Modalities](2406.09406.md)
- [Automatic Data Curation for Self-Supervised Learning: A Clustering-Based Approach](2405.15613.md)
- [SHIC: Shape-Image Correspondences with no Keypoint Supervision](2407.18907.md)
- [An Image is Worth 32 Tokens for Reconstruction and Generation](2406.07550.md)
- [Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture](2301.08243.md)
- [Knowledge Composition using Task Vectors with Learned Anisotropic Scaling](2407.02880.md)
- [CatLIP: CLIP-level Visual Recognition Accuracy with 2.7x Faster Pre-training on Web-scale Image-Text Data](2404.15653.md)
