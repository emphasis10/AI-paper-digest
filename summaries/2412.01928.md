# MALT: Improving Reasoning with Multi-Agent LLM Training
## TL;DR
## Summary
- [https://arxiv.org/pdf/2412.01928.pdf](https://arxiv.org/pdf/2412.01928.pdf)

### 1. 각 섹션 요약 및 논문의 주요 기여와 혁신적인 부분

**서론 및 연구 배경**  
이 논문은 '다중 에이전트 대형 언어 모델 훈련(MALT)'을 제안하여 기존 개별 에이전트 학습에 대한 한계를 극복하고자 합니다. 다중 에이전트 시스템 내에서 협력적인 문제 해결을 가능하게 하기 위한 첫 단계로, 서로 다른 역할을 한 모델(생성기, 검증기, 세련기)이 순차적으로 협력하여 문제를 해결합니다. 이 과정에서 생성되는 합성 데이터를 활용하고 이를 통해 각 모델의 역할별 기능을 향상시킵니다.

**다중 에이전트 훈련 방법 및 발전**  
MALT는 생성, 검증, 세련 모델의 연속적 협력과 강화 학습 및 합성 데이터를 통한 훈련을 통해 각 에이전트의 능력을 최적화합니다. 다중 에이전트 LLM 시스템의 성능을 개선하고 다중 문제 해결 기술을 탐색하며, 이는 수학 문제 및 상식 추론 문항에서 긍정적인 실험 결과를 가져왔습니다.

**실험 및 결과**  
논문의 MALT 시스템은 수학(MATH), 상식 추론(CSQA), GSM8k 벤치마크에서 14.14%, 9.40%, 7.12%의 향상을 보여주며, 이는 기존의 개별 모델 기반 기법보다 우수한 성능을 보였습니다. 다중 에이전트 시스템의 협력적 학습은 문제 해결에 효율적이라는 점을 입증했습니다.

**결론 및 향후 연구**  
논문은 다중 에이전트 시스템을 통한 대형 언어 모델의 협력적 학습 기법의 초기 단계를 성공적으로 제시하였으며, 향후 더 넓은 응용 범위에 대한 연구를 제안합니다. MALT 시스템은 신뢰도 높은 성능과 안전성을 확보하면서 작은 모델들의 역할 기반 협업을 강화하는 데 주목했습니다.

### 2. 전체 요약

이 논문은 '다중 에이전트 대형 언어 모델 훈련(MALT)'을 소개하며, 이는 다양한 역할을 수행하는 언어 모델들이 협력하여 복잡한 문제를 해결할 수 있도록 설계되었습니다. MALT는 생성기, 검증기, 세련 모델을 포함한 다중 역할 기반의 LLM 시스템 훈련을 통해 개별 모델의 능력을 강화하고, 실험을 통해 MATH, CSQA, GSM8k 등 다양한 벤치마크에서 기존의 개별 모델 대비 상당한 성능 향상을 입증하였습니다. 이는 향후 다중 문제 해결 환경에서 대형 언어 모델의 협력적 학습 방법론 개발에 중요한 기초를 형성합니다.