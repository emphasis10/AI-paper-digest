# LongBench v2: Towards Deeper Understanding and Reasoning on Realistic Long-context Multitasks
## TL;DR
## Summary
- [https://arxiv.org/pdf/2412.15204.pdf](https://arxiv.org/pdf/2412.15204.pdf)

1. 각 섹션 요약:

- **서론:** 이 논문은 LongBench v2라는 벤치마크를 소개하며, 이는 LLM(Large Language Model)의 긴 문맥 문제를 해결하는 능력을 평가하는 데 중점을 둡니다. 이 벤치마크는 총 503개의 다중선택 질문으로, 8천에서 200만 단어까지의 다양한 문맥을 포함합니다. 연구진은 현재 대형 언어 모델이 긴 텍스트의 정보를 완전하게 이해하고 있는지에 대해 질문을 제기하며, 이를 통해 모델의 이해력과 추론 능력을 평가하려고 합니다.

- **LongBench v2의 설계 원칙:** 이 섹션에서는 LongBench v2의 설계 원칙을 설명합니다. 주요 초점은 1) 긴 문맥 범위, 2) 도전적인 문제, 3) 실세계 긴 문맥 시나리오를 폭넓게 다룰 것, 4) 모든 자료가 영어로 되어 있으며 신뢰성을 갖추도록 설계된 점을 포함합니다. 20개의 하위 작업으로 구성된 6개의 주요 작업 카테고리를 제안하고 있습니다.

- **작업 개요:** LongBench v2의 6가지 주요 작업은 단일 문서 QA, 다중 문서 QA, 긴 문맥 학습, 긴 대화 역사 이해, 코드 저장소 이해, 긴 구조화 데이터 이해로 구성됩니다. 이는 LLM이 긴 대화 역사와 복잡한 대화에서 정보를 처리할 수 있는지를 평가하기 위해 설계되었습니다.

- **LongBench v2의 제한사항:** 연구진은 LongBench v2의 제한사항으로 데이터세트의 크기가 충분히 크지 못하다는 점과 영어로 한정된 언어 환경을 들어 문제를 제기했습니다. 또한, 다양한 길이 범위에서의 공정한 모델 성능 비교에 있어 문제를 인식하고 있습니다.

- **결론:** LongBench v2는 인간과 최첨단 AI 시스템 모두에게 도전적인 검증 도구로 자리 잡고 있습니다. 인간의 성능은 50.1%의 정확도를 보이고 있으며, 최고의 LLM은 57.7%를 기록하였습니다. 이 결과는 차세대 초인적 AI 개발에 신뢰할 수 있는 평가 표준을 제공합니다.

2. 전체 요약:

이 논문은 LLM의 긴 문맥 이해 능력을 평가하기 위해 LongBench v2를 도입합니다. 이 벤치마크는 503개의 다양한 다중 선택 질문을 포함하고 있으며, 이는 단일 문서, 다중 문서, 긴 대화 역사, 코드 저장소 이해 등의 작업을 통해 모델의 깊은 이해와 추론 능력을 시험합니다. 또한, 데이터세트는 인간 전문가가 검토하고, 긴 문맥의 처리와 이해에 있어 모델의 한계를 드러내고 있습니다. LongBench v2의 실험 결과는 인간 능력을 초월한 AI 시스템 개발을 위한 중요한 기준점을 제공합니다.