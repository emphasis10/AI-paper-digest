# COAT: Compressing Optimizer states and Activation for Memory-Efficient FP8 Training
## TL;DR
## Summary
- [https://arxiv.org/pdf/2410.19313.pdf](https://arxiv.org/pdf/2410.19313.pdf)

1. 각 섹션 요약:

   - **소개 (Introduction):**
     COAT (Compressing Optimizer states and Activations for memory-efficient FP8 Training)는 대형 모델을 훈련할 때 메모리 사용량을 줄이는 혁신적인 방법을 제안합니다. FP8 형식을 통해 옵티마이저 상태와 활성화를 양자화함으로써 효율성을 높이고자 합니다.

   - **관련 연구 (Related Work):**
     기존의 저정밀 훈련 기법에 대해 논의하며, FP16과 BF16 훈련 방법에서 FP8으로의 발전을 설명합니다. FP8은 메모리와 시간적 효율성을 더욱 높이는 방향으로 나아가고 있습니다.

   - **다이나믹 범위 확장 (Dynamic Range Expansion):**
     FP8의 표현 범위를 최대한 활용하기 위해 옵티마이저 상태의 다이나믹 범위를 확장하는 방법을 제안합니다. 이를 통해 양자화 오류를 줄이는 것을 목표로 합니다.

   - **혼합 그래뉼러리티 활성화 양자화 (Mixed-Granularity Activation Quantization):**
     여러 레이어에서의 양자화 그래뉼러리티를 다양화함으로써 정확도를 유지하면서 효율성을 높일 수 있는 방법을 설명합니다. 비선형 레이어에는 세밀한 양자화를, 선형 레이어에는 텐서 단위 양자화를 적용해 효율성을 극대화합니다.

   - **실험 결과 (Experiments):**
     LLM과 VLM을 포함한 다양한 작업에 대한 COAT의 정확성을 입증합니다. BF16 대비 메모리를 1.54배 줄이고, 훈련 속도를 1.43배 높이는 결과를 보여주며, 대규모 모델 훈련의 실용적인 솔루션임을 강조합니다.

   - **결론 (Conclusion):**
     COAT는 대형 모델을 메모리 제약 하에서 효율적으로 훈련할 수 있게 해주는 매우 효율적인 솔루션을 제시합니다. 향후 연구에서는 통신 오버헤드를 줄이기 위한 다른 저정밀 그래디언트 압축 방법과의 결합을 탐색하고자 합니다.

2. 전체 요약:

이 논문은 COAT(Compressing Optimizer states and Activations for memory-efficient FP8 Training)라는 새로운 기법을 소개하여, 대형 모델 학습 시 메모리를 효율적으로 사용하는 방법을 제시하고 있습니다. 메모리 사용량을 줄임과 동시에 성능 저하 없이 훈련 속도를 크게 향상시킵니다. FP8 양자화 기술을 사용하여 옵티마이저 상태와 활성화를 효과적으로 관리함으로써, 기존의 저정밀 학습 방법을 넘어서는 성능을 보여줍니다. 이러한 접근은 굉장히 큰 규모의 언어 및 비전 모델 훈련을 지원하며, 더 적은 GPU 사용으로도 대량의 데이터를 처리할 수 있는 가능성을 열어줍니다.