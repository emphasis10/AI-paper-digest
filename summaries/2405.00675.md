# Self-Play Preference Optimization for Language Model Alignment
## TL;DR
## Summary
- [https://arxiv.org/pdf/2405.00675.pdf](https://arxiv.org/pdf/2405.00675.pdf)

이 논문에서는 선호 기반 강화 학습(RLHF)을 언어 모델 조정에 적용하여, 언어 모델의 인간 선호도와 일치성을 높이는 새로운 자체 대화(Self-Play) 기반 최적화 방법인 '자체 대화 선호 최적화(Self-Play Preference Optimization, SPPO)'를 제안하고 있습니다. 이 방법은 인간의 선호를 더 유연하고 정확하게 반영할 수 있도록 설계되었습니다.

### 주요 내용 요약

1. **서론 및 배경**:
   - 전통적인 RLHF 접근법은 Bradley-Terry 모델과 같은 모수적 모델을 사용하지만, 이는 인간의 선호의 비진행성 및 비합리성을 포착하는데 한계가 있습니다. SPPO는 이러한 문제를 해결하고자 두 플레이어 상수합 게임을 통해 나쉬 균형을 찾는 문제로 접근합니다.

2. **SPPO의 구조 및 기능**:
   - SPPO는 반복적인 정책 업데이트를 통해 나쉬 균형을 근사화하고, 선호된 응답의 로그 가능도를 증가시키며, 거부된 응답의 로그 가능도를 감소시키는 것을 목표로 합니다. 이 과정은 이론적 수렴 보장을 가지며, 심층 신경망을 이용한 미세조정으로 대규모 언어 모델에 효과적으로 적용될 수 있습니다.

3. **성능 평가 및 응용**:
   - 실험 결과, SPPO는 AlpacaEval 2.0에서 GPT-4-Turbo와의 비교에서 최고의 길이 제어 승률을 달성하였으며, 다양한 벤치마크에서 기존의 DPO 및 IPO 방법을 초과하는 성능을 보여줍니다. 이는 SPPO가 언어 모델을 인간 선호도에 보다 잘 맞추도록 조정할 수 있음을 의미합니다.

### 혁신적인 부분
SPPO의 혁신성은 기존의 강화 학습 접근 방식을 넘어서 자체 대화 기반의 반복적인 최적화 절차를 도입함으로써, 언어 모델의 인간 선호도 조정을 개선한 점에 있습니다. 특히, SPPO는 외부의 강력한 언어 모델로부터의 추가적인 감독 없이도 강한 성능을 달성하였습니다.

이 연구는 언어 모델의 인간 선호도와의 일치성을 높이는 방법론을 제시하며, 다양한 자연 언어 처리 작업에 유용하게 적용될 수 있습니다.