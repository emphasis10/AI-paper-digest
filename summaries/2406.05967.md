# CVQA: Culturally-diverse Multilingual Visual Question Answering Benchmark
## TL;DR
## Summary
- [https://arxiv.org/pdf/2406.05967.pdf](https://arxiv.org/pdf/2406.05967.pdf)

### 섹션 요약

#### 1. 초록 (Abstract)
이 논문은 문화적으로 다양한 다국어 시각적 질문 응답(CVQA) 벤치마크를 소개합니다. 기존의 VQA 데이터셋이 영어와 몇몇 주요 언어에 집중되어 있고, 이미지가 서구 중심적이라는 문제를 해결하기 위해 CVQA는 다양한 국가와 문화의 이미지를 포함하도록 설계되었습니다. 이 데이터셋은 28개국에서 수집된 9000개의 질문을 포함하며, 26개의 언어와 11개의 문자를 포함합니다.

#### 2. 서론 (Introduction)
시각적 질문 응답(VQA)은 텍스트 질문에 대한 응답을 이미지 컨텍스트를 바탕으로 하는 과제로, 다중 모달 대형 언어 모델(MLLM)의 이해와 추론 능력을 평가합니다. 그러나 현재 대부분의 VQA 벤치마크는 영어 중심이며 문화적 맥락을 제대로 반영하지 못합니다. 이를 해결하기 위해 CVQA는 다양한 언어와 문화를 포함한 데이터를 제공하여, 문화적 능력과 편향을 평가할 수 있는 새로운 벤치마크를 제공합니다.

#### 3. CVQA 데이터 수집 (CVQA Data Collection)
CVQA 데이터셋은 28개국 26언어로 구성된 문화 다양성을 반영하고 있습니다. 데이터 수집 과정은 지역 커뮤니티와 협력하여 이루어졌으며, 9,000개의 질문이 포함되어 있습니다. 각 데이터 샘플은 10개의 다양한 카테고리로 분류되며, 영어와 현지 언어로 작성되었습니다.

#### 4. 실험 설정 (Experimental Setup)
평가는 다양한 다중 모달 시각 언어 모델을 통해 수행되며, 영어 전용 모델과 다국어 지원 모델이 포함됩니다. 평가 방식은 각 국가와 언어별로 로케이션 기반 프롬프트를 사용하여 이루어집니다.

#### 5. 결과 (Results)
CVQA 데이터셋에서 기존의 MLLM들이 보인 성능을 분석한 결과, 영어 질문에 비해 현지 언어 질문에서 성능이 저하되는 경향이 확인되었습니다. GPT-4o와 같은 폐쇄형 모델이 가장 높은 정확도를 보였고, 개방형 모델들은 상대적으로 낮은 성능을 보였습니다. 이는 데이터의 다양성과 미세조정 과정의 필요성을 시사합니다.

#### 6. 결론 (Conclusion)
이 논문은 CVQA라는 새로운 벤치마크를 소개하고, 다양한 다중 모달 모델들이 이 데이터셋에서 보인 성능을 평가하였습니다. CVQA는 영어 중심이 아닌 모델과 벤치마킹에 더 많은 관심을 유도하여 다국어, 다중 모달 연구의 발전을 촉진하기를 희망합니다.

### 전반적인 요약
이 논문은 시각적 질문 응답(VQA) 분야에서 언어와 문화의 다양성을 반영한 새로운 벤치마크 CVQA를 제안합니다. 기존의 VQA 데이터셋이 영어와 서구 중심적 이미지에 치우쳐 있다는 한계를 극복하고자, CVQA는 28개국의 26개 언어로 구성된 데이터를 포함합니다. 이 데이터셋은 9,000개의 질문을 포함하며, 다양한 문화적 맥락을 반영합니다. 실험 결과, 다국어 지원 모델들이 영어 질문에 비해 현지 언어 질문에서 성능이 저하되는 경향이 발견되었습니다. 이 연구는 다국어 및 다문화 모델의 중요성을 강조하며, 이를 통해 AI 연구의 새 지평을 열 수 있기를 기대합니다.

## Similar Papers
- [SEACrowd: A Multilingual Multimodal Data Hub and Benchmark Suite for Southeast Asian Languages](2406.10118.md)
- [The FIGNEWS Shared Task on News Media Narratives](2407.18147.md)
- [VolDoGer: LLM-assisted Datasets for Domain Generalization in Vision-Language Tasks](2407.19795.md)
- [WalledEval: A Comprehensive Safety Evaluation Toolkit for Large Language Models](2408.03837.md)
- [Aya 23: Open Weight Releases to Further Multilingual Progress](2405.15032.md)
- [AgentGym: Evolving Large Language Model-based Agents across Diverse Environments](2406.04151.md)
- [SPIQA: A Dataset for Multimodal Question Answering on Scientific Papers](2407.09413.md)
- [AgentInstruct: Toward Generative Teaching with Agentic Flows](2407.03502.md)
- [Data Mixture Inference: What do BPE Tokenizers Reveal about their Training Data?](2407.16607.md)
