# Accelerating LLM Inference with Staged Speculative Decoding
## TL;DR
## Summary
- [https://arxiv.org/pdf/2308.04623.pdf](https://arxiv.org/pdf/2308.04623.pdf)

### 1. 각 섹션 요약

**1. Introduction (소개)**

이 논문은 대규모 언어 모델(LLM)의 추론을 가속화하기 위해 '단계적 예측 디코딩'이라는 새로운 알고리즘을 제안합니다. 주로 소규모 배치와 장치 내 시나리오에 초점을 맞춰, 기존의 예측 디코딩 방식을 개선하고, 사용자가 모델을 개인화하고 데이터 프라이버시를 보장하면서 응답 시간을 줄이는 방법을 제시합니다. 주요 개선점은 예측 배치를 트리 구조로 재편성하고 두 번째 예측 디코딩 단계를 추가한 것입니다. 이를 통해 762M 파라미터 GPT-2-L 모델에서 단일 배치 디코딩 지연 시간을 3.16배 줄이면서 출력 품질을 완벽히 보존했습니다.

**2. Background (배경)**

자기 회귀 LLM 추론의 기본 원리에 대해 설명하며, 자주사용되는 GPU 최적화 기법과 기존 LLM 추론 최적화 작업을 다룹니다. 자기 회귀 LLM의 기본적인 디코딩 과정은 두 단계로 이루어집니다: 처음엔 입력 프롬프트를 모델에 입력해 첫 번째 출력 logits을 생성하고, 이후 로그잇을 반복적으로 입력해 원하는 길이의 토큰을 생성합니다. 작은 배치에서 디코딩은 주로 대기 시간이 많이 소모되는 작업으로, GPU의 저조한 산술 강도를 극복하는 것이 주요 과제입니다.

**3. Methods (방법)**

단계적 예측 디코딩의 핵심 개선점은 트리 구조의 배치와 추가적인 예측 디코딩 단계입니다. 이 방식은 더 많은 예상 토큰을 포함한 배치를 만들고, 비용을 줄이며, 예측 모델의 병렬 처리를 개선합니다. 구현 방법으로는 자기 주의력을 배치 내에서 교차 주의력으로 나누고, 트리 구조의 배치를 만들기 위해 위치 임베딩을 제어하는 방법이 포함됩니다.

**4. Results (결과)**

실험을 통해 새로운 방식의 디코딩이 기존 방식 대비 성능을 크게 향상시킨다는 것을 확인했습니다. GPT-2 Large 모델에서, 단계적 예측 디코딩은 각기 다른 샘플링 방식에서 평균 3.16배의 성능 향상을 보였습니다. 특히 결정론적 샘플링에서 3.16배, topk 샘플링에서 1.98배의 성능 향상이 있었습니다. 이 방식은 작은 모델이 예측하는 더 쉬운 토큰을 빠르게 디코딩할 수 있게 해줍니다.

**5. Conclusions (결론)**

이 연구에서 제안한 단계적 예측 디코딩 방식은 기존의 단일 배치 추론보다 약 3.16배 빠르며, 트리 구조의 배치와 두 번째 예측 디코딩 단계를 통해 성능을 크게 향상시켰습니다. 향후 연구 방향으로는 더 큰 모델 사용, 더 효율적인 초기 모델 개발 등이 제시되었습니다.

### 2. 전체 요약

이 논문에서는 대규모 언어 모델(LLM)의 추론 속도를 대폭 향상시키기 위해 '단계적 예측 디코딩'이라는 새로운 알고리즘을 제안하고 검증했습니다. 주요 기여는 예측 디코딩 배치를 트리 구조로 재편성하고 추가적인 예측 디코딩 단계를 도입하여 디코딩 지연 시간을 줄이고 성능을 극대화한 것입니다. 실험 결과, 새로운 방식은 기존 방식 대비 GPT-2 Large 모델에서 약 3.16배의 성능 향상을 보였으며, 특히 작은 모델이 예측할 수 있는 쉬운 토큰을 효율적으로 처리함으로써, 모델의 응답 시간을 크게 줄였습니다. 이는 사용자 개별 맞춤화와 데이터 프라이버시 보장 등의 장점을 제공하며, 미래 AI 기술 발전에 크게 기여할 수 있을 것입니다.

## Similar Papers
- [Mixture-of-Agents Enhances Large Language Model Capabilities](2406.04692.md)
- [AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration](2306.00978.md)
- [Confident Adaptive Language Modeling](2207.07061.md)
- [LLMCad: Fast and Scalable On-device Large Language Model Inference](2309.04255.md)
- [SARATHI: Efficient LLM Inference by Piggybacking Decodes with Chunked Prefills](2308.16369.md)
- [LLM in a flash: Efficient Large Language Model Inference with Limited Memory](2312.11514.md)
- [Accelerating LLaMA Inference by Enabling Intermediate Layer Decoding via Instruction Tuning with LITE](2310.18581.md)
- [FlashDecoding++: Faster Large Language Model Inference on GPUs](2311.01282.md)
- [SpecInfer: Accelerating Generative Large Language Model Serving with Tree-based Speculative Inference and Verification](2305.09781.md)
