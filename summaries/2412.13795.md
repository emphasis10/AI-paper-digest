# Mix-LN: Unleashing the Power of Deeper Layers by Combining Pre-LN and Post-LN
## TL;DR
## Summary
- [https://arxiv.org/pdf/2412.13795.pdf](https://arxiv.org/pdf/2412.13795.pdf)

### 1. 각 섹션의 주요 내용 요약

#### 서론
이 논문은 대형 언어 모델(LLMs)의 깊은 층이 일반적으로 효과가 떨어지는 이유와 이를 개선하기 위한 새로운 방법론 Mix-LN을 제안합니다. 대부분의 현대 LLM은 Pre-LN(레이어 정규화)을 사용하고 있어, 깊은 층에서는 그라디언트(norms)가 작아져 효율성이 떨어지는 문제를 겪고 있습니다.

#### 가설 평가
논문은 Pre-LN이 깊은 층의 비효율성의 원인임을 주장하며, 실험을 통해 이를 입증합니다. 또한, Post-LN을 사용하면 초기 층에서 그라디언트 소실 문제가 발생하는 것을 확인했습니다.

#### 새로운 방법: Mix-LN
Mix-LN은 Pre-LN과 Post-LN의 강점을 결합하여 모든 층이 학습에 효과적으로 기여할 수 있도록 합니다. 이는 네트워크의 초기 층에는 Post-LN을, 깊은 층에는 Pre-LN을 적용하여 그라디언트의 균형을 맞추는 방식입니다.

#### 실험 결과
다양한 모델 크기에서 Mix-LN이 Pre-LN과 Post-LN보다 일관되게 더 나은 성능을 보였습니다. Mix-LN은 훈련 과정의 불안정을 피하면서도 깊은 층의 질을 향상시키며, 지도 학습 및 인간 피드백 강화 학습에서 더 나은 결과를 제공합니다.

#### 결론
Mix-LN은 현대 LLM들이 겪고 있는 깊은 층의 비효율성을 해결하며, 전체 모델의 성능과 효율을 높이는 데 기여합니다.

### 2. 전반적인 요약
이 논문은 LLM의 깊은 층 비효율성 문제를 해결하기 위해 고안된 Mix-LN 기법을 소개합니다. Mix-LN은 Pre-LN과 Post-LN의 장점을 결합하여, 모델의 초기 층과 깊은 층 모두에 균형 잡힌 그라디언트를 제공함으로써, 모델의 학습 효율성을 높입니다. 실험 결과, Mix-LN은 Pre-LN과 Post-LN의 성능을 일관되게 능가하였으며, 이로 인해 LLM의 전반적인 성능이 향상됨을 보여줍니다.