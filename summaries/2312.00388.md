# LinguaLinked: A Distributed Large Language Model Inference System for Mobile Devices
## TL;DR
## Summary
- [https://arxiv.org/pdf/2312.00388.pdf](https://arxiv.org/pdf/2312.00388.pdf)

## 섹션 요약 및 주요 기여 
### 1. 서론
이 논문에서는 모바일 장치에서 대규모 언어 모델(LLM)을 효율적으로 추론할 수 있도록 하는 **LinguaLinked** 시스템을 소개합니다. 전통적으로 LLM은 서버에서 실행되지만, 이는 프라이버시 문제와 높은 대역폭 요구사항을 동반합니다. 이 논문은 LLM 분할 및 장치 간 분산 실행을 통해 이러한 문제를 해결합니다.

### 2. 배경과 동기
주요 문제는 모바일 장치의 제한된 메모리와 계산 능력입니다. 기존에는 모델 양자화, 증류, 가지치기 등의 방법이 사용되었지만, 이는 모델 성능에 영향을 미칠 수 있습니다. LinguaLinked는 LLM을 작은 세그먼트로 분할하고 이러한 세그먼트를 여러 장치에 분산하여 처리하도록 합니다. 이를 통해 단일 장치에서의 메모리 부담을 줄이고 모델 정확도를 유지할 수 있습니다.

### 3. 시스템 설계
#### 3.1 시스템 모니터
LinguaLinked는 모바일 장치의 현재 상태(메모리, CPU 사용량 등)를 모니터링하여 최적의 성능을 유지합니다.
#### 3.2 최적화된 모델 할당
모델 세그먼트를 각 장치의 계산 및 네트워크 조건에 맞게 할당하여 데이터 전송 오버헤드를 최소화합니다.
#### 3.3 러ntime 로드 밸런싱
시스템은 실시간으로 장치 상태를 모니터링하며, 로드를 재분배하여 병목 현상을 방지하고 시스템 효율성을 극대화합니다.
#### 3.4 최적화된 통신
데이터 전송 맵을 사용하여 모델 세그먼트 간 데이터 전송을 최적화하고 지연을 감소시킵니다.

### 4. 평가
싱글 스레드 환경에서 LinguaLinked는 기존 시스템 대비 1.11배에서 1.61배의 성능 가속을 달성했으며, 멀티 스레드 환경에서는 최대 2.65배까지 가속할 수 있음을 확인했습니다. 또한, 러ntime 로드 밸런싱은 전체 성능을 1.29배에서 1.32배 향상시켰습니다.

### 5. 논의
기계적 하드웨어 및 소프트웨어 프레임워크의 제약으로 인한 로드 밸런싱 오버헤드가 주요 도전과제로 제시되었습니다. 향후 연구 방향으로는 에너지 효율 향상과 사용자 맞춤형 AI 애플리케이션 개발이 제안되었습니다.

### 6. 결론
이 연구는 세계 최초로 모바일 장치에서 LLM을 분산하여 실행하는 시스템인 LinguaLinked를 소개합니다. 최적화된 모델 할당 전략과 러ntime 로드 밸런싱을 통해 복잡한 LLM 추론을 모바일 환경에서 효과적으로 수행할 수 있음을 보여주었습니다.

## 전체 요약
이 논문은 모바일 장치에서 대규모 언어 모델(LLM)을 분산하여 효율적으로 추론할 수 있도록 하는 LinguaLinked 시스템을 소개합니다. 모델 세그먼트를 각 장치에 분할 배치하고, 러ntime 로드 밸런싱을 통해 성능을 최적화합니다. 이는 전통적인 서버 기반 처리 방식의 프라이버시 문제와 높은 대역폭 요구사항을 해결하며, 모바일 장치의 메모리 및 계산 제약을 극복할 수 있습니다. 평가 결과, LinguaLinked는 기존 시스템 대비 성능을 크게 향상시키는 것으로 나타났습니다.

**주요 기여**:
- 모바일 장치에서 처음으로 분산 처리하는 LLM 시스템 개발
- 기기의 상태와 네트워크 조건을 반영한 최적의 모델 할당과 로드 밸런싱
- 실시간 모니터링과 과제 재분배를 통해 병목 현상 방지
- 평가에서 최대 2.65배 성능 향상 확인

LinguaLinked는 다양한 장치에서 LLM의 효과적 운영을 가능하게 하며, 향후 연구는 에너지 효율 및 사용자 맞춤형 AI 애플리케이션으로 확대될 계획입니다.

## Similar Papers
- [LLMCad: Fast and Scalable On-device Large Language Model Inference](2309.04255.md)
- [LLM as a System Service on Mobile Devices](2403.11805.md)
- [Draft & Verify: Lossless Large Language Model Acceleration via Self-Speculative Decoding](2309.08168.md)
- [Talaria: Interactively Optimizing Machine Learning Models for Efficient Inference](2404.03085.md)
- [EdgeMoE: Fast On-Device Inference of MoE-based Large Language Models](2308.14352.md)
- [ShiftAddLLM: Accelerating Pretrained LLMs via Post-Training Multiplication-Less Reparameterization](2406.05981.md)
- [Efficient Memory Management for Large Language Model Serving with PagedAttention](2309.06180.md)
- [PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU](2312.12456.md)
- [Accelerating LLM Inference with Staged Speculative Decoding](2308.04623.md)
