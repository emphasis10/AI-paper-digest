# LLaMAX: Scaling Linguistic Horizons of LLM by Enhancing Translation Capabilities Beyond 100 Languages
## TL;DR
## Summary
- [https://arxiv.org/pdf/2407.05975.pdf](https://arxiv.org/pdf/2407.05975.pdf)

### 1. 각 섹션의 요약 (한글로)

#### 서론
이 논문은 LLMs(Large Language Models)의 다국어 번역 능력을 향상시키기 위한 연구를 다룬다. 특히 저자들은 다양한 훈련 전략을 사용하여 100개 이상의 언어를 지원하는 모델 LLaMAX를 개발하였다. 주요 특징으로는 어휘 확장과 데이터 증강을 포함한 훈련 전략의 종합적 분석을 통해, 기존 개방형 LLM보다 10 spBLEU 이상 높은 성능을 달성하였다는 점이다. LLaMAX는 Flores-101 벤치마크에서 M2M-100-12B라는 전문 번역 모델과 유사한 성능을 보인다.

#### 데이터 구축
다국어 번역을 지원하는 강력한 LLM을 구축하기 위해, 충분한 데이터 수집 및 구축이 중요하다. 102개 언어를 포함한 수집된 데이터는 크게 단언어 데이터와 평행 데이터로 구성된다. 데이터 부족 문제를 해결하기 위해 다국어 사전 (MUSE, PanLex)을 활용하여 의사 평행 데이터를 생성한다.

#### 훈련 전략
LLaMAX는 데이터 증강과 어휘 확장을 통해 번역 성능을 획기적으로 개선하였다. 데이터 증강은 각 언어 사전에 따라 분류되며, 평행 데이터를 사용한 증강이 가장 효과적이다. LLaMA 시리즈 모델들 (LLaMA2, LLaMA3)에도 이 전략을 적용하여, 보다 나은 번역 성능을 이끌어냈다. 또한, LLaMAX 시리즈 모델은 광범위한 다국어 훈련을 거쳐 일반 번역 모델에서도 10 spBLEU 이상의 성능 향상을 달성하였다.

#### 주요 기여
- LLaMAX 모델은 100개 이상의 언어에서 번역 성능을 크게 향상시켰다.
- 다국어 지속 훈련 핵심 기술, 즉 어휘 확장과 데이터 증강에 대한 종합적 분석을 제공하였다.
- 다양한 모델들을 대상으로 한 광범위한 실험과 번역 벤치마크 평가를 통해 LLaMAX의 우수성을 입증하였다.

### 2. 전체 요약

이 논문은 LLaMAX라는 다국어 번역 모델을 개발하여 기존 LLMs의 한계를 넘는 성과를 거두었다. LLaMAX는 100개 이상의 언어에 대한 번역을 지원하며, 기존 개방형 LLM보다 10 spBLEU 이상의 성능을 달성하였다. 이를 위해, 연구팀은 어휘 확장과 데이터 증강과 같은 여러 훈련 전략을 적용하고 종합적으로 분석하였다. 결론적으로, LLaMAX는 번역 성능 향상뿐만 아니라 다양한 다국어 응용 프로그램을 위한 강력한 기반 모델로 작동할 수 있는 가능성을 보여주었다.

## Similar Papers
- [SambaLingo: Teaching Large Language Models New Languages](2404.05829.md)
- [SeaLLMs 3: Open Foundation and Chat Multilingual Large Language Models for Southeast Asian Languages](2407.19672.md)
- [Yuan 2.0-M32: Mixture of Experts with Attention Router](2405.17976.md)
- [Towards Robust Speech Representation Learning for Thousands of Languages](2407.00837.md)
- [LLM-jp: A Cross-organizational Project for the Research and Development of Fully Open Japanese LLMs](2407.03963.md)
- [Chat Vector: A Simple Approach to Equip LLMs with Instruction Following and Model Alignment in New Languages](2310.04799.md)
- [In-Context Example Selection via Similarity Search Improves Low-Resource Machine Translation](2408.00397.md)
- [Accessing GPT-4 level Mathematical Olympiad Solutions via Monte Carlo Tree Self-refine with LLaMa-3 8B](2406.07394.md)
- [OpenBezoar: Small, Cost-Effective and Open Models Trained on Mixes of Instruction Data](2404.12195.md)
