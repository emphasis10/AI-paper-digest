# Improving GFlowNets for Text-to-Image Diffusion Alignment
## TL;DR
## Summary
- [https://arxiv.org/pdf/2406.00633.pdf](https://arxiv.org/pdf/2406.00633.pdf)

### 1. 각 섹션 요약 (한글)

#### 소개
이 논문은 텍스트-이미지 모델을 인간 피드백을 통해 정렬하는 데 초점을 맞추고 있습니다. 전통적인 확산 모델의 한계를 극복하기 위해 Generative Flow Networks (GFlowNets) 알고리즘을 도입하였습니다. 이 방법은 보상 함수를 통해 특정 기준을 충족하는 샘플을 생성하도록 훈련합니다.

#### 확산 모델 및 GFlowNets 개요
확산 모델은 노이즈가 있는 데이터를 점차 제거하여 고품질의 시각 데이터를 생성합니다. GFlowNets는 주어진 밀도를 효율적으로 추론하기 위해 설계되었으며, 보상 함수를 기반으로 샘플을 생성하는 기능이 있습니다.

#### 제안된 방법: DAG 알고리즘
Diffusion Alignment with GFlowNet (DAG) 알고리즘을 제안합니다. 이는 GFlowNets를 사용하여 대규모 텍스트-이미지 모델을 블랙박스 보상 함수와 정렬시킵니다. 특히, 보상 기반으로 확산 모델을 최적화하기 위해 KL 기반 목적 함수를 도입하여 샘플 효율성을 향상시켰습니다. 실험 결과, DAG는 기존 강화 학습 기반 방법보다 우수한 성능을 보였습니다.

#### 실험 및 결과
Stable Diffusion과 다양한 보상 명세를 사용해 광범위한 실험을 수행했습니다. 그 결과, DAG는 주어진 보상 정보와 더 잘 정렬된 샘플을 생성했으며, 기존 방법보다 더 나은 샘플 효율성과 보상-다양성 균형을 이끌어냈습니다.

### 2. 논문의 주요 기여 및 혁신 부분 (한글)
이 논문의 주요 기여는 다음과 같습니다:
- **Diffusion Alignment with GFlowNet (DAG):** GFlowNet과 확산 모델의 구조를 결합하여 보상 기반 텍스트-이미지 정렬을 개선했습니다.
- **KL 기반 최적화 방법:** 샘플 효율성을 높이기 위해 KL 기반 최적화 방식(DAG-KL)을 제안했습니다.
- **샘플 효율성 및 보상-다양성 균형 개선:** 기존 강화 학습 방법보다 더 나은 샘플 효율성 및 보상-다양성 균형을 달성했습니다.

### 3. 전체 요약 (한글)
이 논문에서는 텍스트-이미지 생성 모델을 최적화하기 위한 새로운 접근 방식을 제안합니다. 기존의 강화 학습 기반 방법의 한계를 극복하기 위해, GFlowNet과 확산 모델을 결합한 Diffusion Alignment with GFlowNet (DAG) 알고리즘을 도입했습니다. 특히, 보상 함수를 기반으로 샘플을 생성하는 데 중점을 두며, KL 기반 최적화 방법을 통해 샘플 효율성 및 보상-다양성 균형을 향상시켰습니다. 실험 결과, DAG는 Stable Diffusion과 같은 대규모 모델에서 기존 방법보다 훨씬 더 나은 성능을 보였습니다. 이 논문의 기여는 AI 모델의 훈련 및 최적화 방법에 새로운 방향을 제시하며, 다양한 응용 분야에서 활용될 수 있을 것입니다.