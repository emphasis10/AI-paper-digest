# Is DPO Superior to PPO for LLM Alignment? A Comprehensive Study
## TL;DR
## Summary
- [https://arxiv.org/pdf/2404.10719.pdf](https://arxiv.org/pdf/2404.10719.pdf)

### 논문 요약

#### 1. 서론
이 논문은 대규모 언어 모델(LLM)의 성능을 인간의 선호도와 일치시키기 위해 사용되는 강화 학습 기반 방법론들, 특히 직접 선호 최적화(Direct Preference Optimization, DPO)와 근접 정책 최적화(Proximal Policy Optimization, PPO)를 비교 분석합니다. 이 논문의 주된 목표는 DPO가 실제로 PPO보다 우수한지, 그리고 PPO의 성능을 향상시킬 수 있는지를 조사하는 것입니다.

#### 2. 관련 연구
LLM을 학습시키는데 있어서 강화 학습에서 인간의 피드백(RLHF)을 통해 모델을 보다 인간의 선호에 맞추려는 연구가 활발히 이루어지고 있습니다. 이러한 방법은 크게 보상 기반과 보상 비기반으로 나누어집니다. DPO는 보상 함수를 명시적으로 사용하지 않는 보상 비기반 방법의 하나로, 정책 최적화에만 집중합니다.

#### 3. 이론적 및 경험적 분석
DPO와 PPO의 알고리즘적 특성에 대한 이론적 및 경험적 연구를 통해 DPO가 모델 출력과 선호 데이터셋 사이의 분포 변화에 의해 성능이 크게 영향을 받을 수 있음을 발견했습니다. PPO에 대한 연구를 통해 성능 향상에 중요한 요소들을 파악하고, 다양한 RLHF 벤치마크에서 두 방법을 평가하였습니다.

#### 4. 실험 결과
실제 대화 생성과 코드 생성과 같은 다양한 작업을 포함한 실험에서 PPO가 일관되게 DPO보다 우수한 성능을 보였습니다. 특히 코드 생성 경연에서는 PPO가 최신 기록을 세우는 결과를 달성하였습니다.

#### 5. 결론
이 연구는 DPO의 기본 한계를 밝히고, PPO의 실제 성능을 향상시키는 데 기여하는 중요한 요소들을 탐색합니다. 논문은 PPO가 다양한 작업에서 강력한 효과를 보이며 최고의 결과를 달성할 수 있음을 보여줍니다.

### 종합적인 요약
이 논문은 LLM의 인간 선호도와의 일치를 개선하기 위한 두 가지 강화 학습 방법, DPO와 PPO를 비교 분석합니다. 이론적 및 경험적 분석을 통해 DPO의 한계를 드러내고, PPO의 성능을 최적화하는 핵심 요소들을 확인합니다. 실험 결과 PPO는 다양한 RLHF 작업에서 DPO를 일관되게 능가함으로써, 보다 효과적인 LLM fine-tuning 방법임을 입증합니다. 이 연구는 LLM의 효과적인 학습 방법론 개발에 중요한 기여를 합니다.

## Similar Papers
- [WPO: Enhancing RLHF with Weighted Preference Optimization](2406.11827.md)
- [Back to Basics: Revisiting REINFORCE Style Optimization for Learning from Human Feedback in LLMs](2402.14740.md)
- [sDPO: Don't Use Your Data All at Once](2403.19270.md)
- [Direct Preference Optimization: Your Language Model is Secretly a Reward Model](2305.18290.md)
- [RLHF Workflow: From Reward Modeling to Online RLHF](2405.07863.md)
- [From $r$ to $Q^*$: Your Language Model is Secretly a Q-Function](2404.12358.md)
- [Value-Incentivized Preference Optimization: A Unified Approach to Online and Offline RLHF](2405.19320.md)
- [FP8-LM: Training FP8 Large Language Models](2310.18313.md)
- [Bootstrapping Language Models with DPO Implicit Rewards](2406.09760.md)
