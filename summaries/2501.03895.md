# LLaVA-Mini: Efficient Image and Video Large Multimodal Models with One Vision Token
## TL;DR
## Summary
- [https://arxiv.org/pdf/2501.03895.pdf](https://arxiv.org/pdf/2501.03895.pdf)

1. PDF 파일 섹션별 요약:

- **서론**  
  이 논문은 효율적인 대규모 멀티모달 모델(LMM)인 LLaVA-Mini를 소개합니다. 이 모델은 적은 비전 토큰을 사용하여 이미지와 비디오 이해에서 높은 효율성을 발휘합니다. 주된 혁신은 운영 효율성을 높이고 메모리 사용을 줄이는 것이며, 실제 멀티모달 상호작용을 실현하는 데 유리합니다.

- **모델 이론 및 구현**  
  LLaVA-Mini는 기존 멀티모달 파이프라인에 쉽게 추가할 수 있는 압축 모듈과 프리퓨전 모듈을 사용하여 기존의 구조를 변화시키지 않고 고유의 효율성을 달성합니다. 문제 해결을 위한 주요 접근법은 비전 토큰 수를 줄이고 필수 정보를 보존하면서 비전 토큰을 적응형으로 압축하는 것입니다.

- **실험 및 평가**  
  이 모델은 11개의 이미지 및 7개의 비디오 벤치마크에서 성능을 테스트하였으며, 기존 모델들과 비교하여 뛰어난 성능을 보여주었습니다. LLaVA-Mini는 구체적인 이미지와 스크린샷 등 다양한 타입의 이미지를 분석하는 데 효과적으로 주의력을 분배합니다.

- **결론**  
  실시간 멀티모달 상호작용을 최적화하기 위해 LLaVA-Mini가 도입되었고, 이 모델은 효율적인 이미지 및 비디오 이해 가능성을 보여주었습니다.

2. 전체 요약:
LLaVA-Mini 모델은 멀티모달 인공지능 상호작용의 효율성을 높이기 위해 개발되었습니다. 이 모델의 주요 기여는 적은 비전 토큰을 사용하면서도 높은 이미지 및 비디오 이해 능력을 발휘하는 것입니다. 이를 통해 운영 효율성과 메모리 사용을 최적화하여 효과적인 실시간 적용을 가능하게 합니다.