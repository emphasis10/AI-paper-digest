# Discriminative Fine-tuning of LVLMs
## TL;DR
## Summary
- [https://arxiv.org/pdf/2412.04378.pdf](https://arxiv.org/pdf/2412.04378.pdf)

1. 각 섹션의 주요 내용을 요약:

- **서론**: 본 논문은 대조적으로 훈련된 Vision-Language Models(VLMs)의 제한된 언어 이해 능력을 극복하기 위해 Large Vision-Language Models(LVLMs)를 변형하여 더 강력한 이미지-텍스트 판별능력을 제공하는 새로운 훈련 방법을 제안합니다.

- **관련 연구**: CLIP과 같은 대조적 VLM들이 여러 비전-언어 작업에서 성과를 올리지만, 언어 이해, 합성 이해 등에서 제한적입니다. LVLMs는 강력한 시각-언어 이해를 가능케 할 수 있습니다.

- **LVLMs의 판별적 미세 조정**: 기존의 생성적 LVLM을 판별적으로 변환하기 위해, 여러 가지 길이와 세분성을 가진 이미지-텍스트 쌍을 이용하여 훈련하는 최적화 프레임워크를 제안합니다. 이에 따라 대조적 손실과 다음 토큰 예측 손실을 결합한 새로운 프레임워크가 제시됩니다.

- **실험**: 제안된 방법은 기존의 VLMs와 비교하여 이미지-텍스트 검색, 합성성 벤치마크에서 성능 향상을 보이며, 특히 Winoground와 같은 어려운 대상을 포함한 데이터셋에서 문자적으로 진보적인 결과를 보였습니다.

- **결론**: 본 논문의 방법론은 LVLM의 내재된 이미지-텍스트 판별 및 언어 이해를 향상시킵니다. 프레임워크를 통해 대조적 및 다음 토큰 예측 손실을 함께 사용하며, 이것이 모델의 성능 향상을 이끌어냅니다.

2. 전체 요약:

이 연구는 기존 대조적 VLMs의 언어 이해 한계를 극복하기 위해, 생성적 LVLM을 판별적 모델로 변환하는 새로운 훈련 접근법을 제안하고 있습니다. 이러한 혁신적인 접근 방식은 이미지 텍스트 판별 및 합성 능력을 크게 향상시키며, 특히 미세 조정이 필요한 관련 작업에 대해 두드러진 성과를 보여줍니다. 이를 통해 생성적이었던 LVLM의 본래 특징을 유지하면서도, 강력한 언어 이해 능력을 갖춘 인간같은 모델 개발에 이바지하고 있습니다.