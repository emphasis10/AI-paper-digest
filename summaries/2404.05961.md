# LLM2Vec: Large Language Models Are Secretly Powerful Text Encoders
## TL;DR
## Summary
- [https://arxiv.org/pdf/2404.05961.pdf](https://arxiv.org/pdf/2404.05961.pdf)

이 문서는 "LLM2Vec: Large Language Models Are Secretly Powerful Text Encoders"이라는 제목으로, 대규모 언어 모델(대규모 디코더 전용 언어 모델)이 최신 NLP(자연언어 처리) 작업에서 우수한 성능을 보이고 있음에도 불구하고, 텍스트 임베딩 작업(복잡한 문맥을 표현하는 작업)에는 덜 활용되고 있다고 언급합니다. 특히, 이 연구에서는 LLM2Vec라는 간단하지만 효과적인 비지도 방식을 소개하여, 모든 디코더 전용 대규모 언어 모델을 강력한 텍스트 인코더로 전환할 수 있는 방법을 제시합니다. LLM2Vec는 크게 세 단계로 구성되어 있으며, 이를 통해 언어 모델을 조정하여 높은 성능의 텍스트 임베딩 모델을 구축함을 입증합니다. 이 방법은 텍스트의 '전방 및 후방 문맥'을 고려하여 풍부하고 복잡한 문맥 정보를 포착할 수 있도록 돕습니다. 연구에서는 각기 다른 파라미터 크기(1.3B에서 7B)를 가지는 세 가지 대규모 언어 모델에 LLM2Vec 방법을 적용하고, 단어 및 문장 수준 작업에 대한 모델의 성능을 평가합니다. 결과적으로, 이 연구방법은 기존 인코더 전용 모델을 능가하는 새로운 비지도 성능 기준을 설정하고, 공개된 데이터만을 사용한 모델 중 최상의 성능을 달성함을 보여줍니다.

### 1. 서론(Introduction)
   - 텍스트 임베딩 모델은 자연어 텍스트의 의미적 내용을 벡터 표현으로 인코딩하며, 이는 다양한 NLP 작업에 활용됩니다.
   - 대부분의 현재 텍스트 임베딩 모델은 양방향 인코더를 기반으로 하고 있으며, 최근에는 단방향 LLM이 사용되기 시작했습니다.

### 2. 관련 연구 (Related Work)
- **감독된 텍스트 인코더**: 자연어 추론이나 문장 유사도 같은 작업을 통해 BERT와 같은 모델을 학습하여 문장 임베딩을 생성함.
- **비감독 텍스트 인코더**: 정렬되지 않은 문장집합만을 사용하는 비감독 방식의 텍스트 임베딩 도구들에 대해 논의. 이러한 접근 방식은 대조 학습을 위해 같은 문장의 두 가지 다른 표현을 생성함.
- **디코더 전용 LLM을 텍스트 인코더로 변환**: 기존 연구들은 LLM의 마지막 토큰의 최종 숨겨진 상태를 문장 임베딩으로 사용함. 본 연구와 유사한 최근 연구들은 LLMs의 본질적 제한을 뛰어넘고자 하였다.

### 3. LLM2Vec 방법론(Methodology of LLM2Vec)
- LLM2Vec는 세 가지 간단한 단계로 구성됩니다: 양방향 주의(Bidirectional Attention) 활성화, 마스킹된 다음 토큰 예측(Masked Next Token Prediction), 비지도 대조 학습(Unsupervised Contrastive Learning).
- 이 방법은 레이블이 필요 없으며, 데이터와 매개변수 효율이 높습니다.

### 4. 실험 및 결과(Experiments and Results)
- 다양한 크기의 LLM에 LLM2Vec을 적용하고, 영어 단어 및 시퀀스 레벨 작업에서 평가했습니다.
- LLM2Vec 변환 모델은 기존의 인코더 전용 모델보다 뛰어난 성능을 보였으며, 비지도 학습 상태에서 새로운 최고 성능을 달성했습니다.

### 5. 결론 (Conclusion)
- 본 논문은 디코더 전용 대규모 언어 모델(LLM)을 (범용) 텍스트 임베더로 변환하는 강력한 비감독 방식인 **LLM2Vec**을 제안함.
- 단어 및 시퀀스 레벨 작업에서 **LLM2Vec**의 효과성을 광범위하게 평가하고, 비감독 및 감독된 설정에서의 효율성을 입증함.
- 결과적으로, **LLM2Vec**은 MTEB에서 비감독 접근법 중 새로운 최고 성능을 달성함과 동시에, 공개적으로 이용 가능한 데이터만을 사용하여 훈련된 감독 방식 중에서 또한 최고 성능을 보임.
- LLM2Vec은 LLM을 범용 텍스트 임베딩 모델로 효과적으로 변환할 수 있는 간단하고 효율적인 방법을 제공함.
- 이 방법은 계산 및 샘플 효율성 측면에서 매우 유망하며, 저자원 및 계산 제약 환경에서 특히 유용할 수 있음.

### 전체 요약
이 논문은 대규모 언어 모델을 효율적으로 텍스트 인코더로 활용하는 새로운 방법론, **LLM2Vec**에 초점을 맞추고 있습니다. 이 방법은 단방향 언어 모델의 한계를 극복하고, 텍스트 임베딩에 필요한 풍부한 컨텍스트 정보를 포함시킬 수 있게 도와줍니다. 이 방법론은 수동 라벨 부여나 복잡한 학습 파이프라인 없이도, 단순하면서도 계산적으로 효율적인 조정을 통해 파워풀한 텍스트 임베딩 능력을 대규모 언어 모델에 부여합니다. **LLM2Vec**의 도입은 자연어 처리(NLP) 분야에서 텍스트의 깊은 문맥을 이해하고 표현하는 방법에 혁신을 가져올 뿐만 아니라, 저자원 및 계산 제약 환경에서의 활용 가능성을 열어줍니다. 이 연구는 비감독 및 감독된 학습 방식 모두에서 새로운 성능 기준을 제시하며, 특히 공개 데이터를 사용하여 훈련된 모델 중 최고의 성능을 달성함으로써, 향후 연구 및 개발 방향에 중요한 영향을 미칠 것으로 기대됩니다.