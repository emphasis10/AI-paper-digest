# Accurate Knowledge Distillation with n-best Reranking
## TL;DR
## Summary
- [https://arxiv.org/pdf/2305.12057.pdf](https://arxiv.org/pdf/2305.12057.pdf)

### 논문 요약

#### 1. 각 섹션 요약 및 주요 기여와 혁신점

##### 서론
이 논문은 시퀀스 수준 지식증류(Knowledge Distillation; KD)의 성능을 향상시키기 위해 n-베스트 리스트 재랭킹(n-best reranking)을 사용하는 방법을 제안합니다. 이를 통해 더 높은 품질의 가설을 라벨로 사용하는 다양한 모델을 활용하여 학생 모델의 정확도를 크게 향상시킬 수 있습니다.

##### 배경: 시퀀스 수준 지식증류
기존의 시퀀스 수준 KD 방법들은 동일한 데이터셋과 동일한 어휘와 네트워크 아키텍처를 공유하는 모델들을 사용해 모델 앙상블을 활용합니다. 이는 교사 모델의 다양성을 제한해 성능 향상에 한계를 둡니다.

##### n-베스트 재랭킹을 이용한 증류
논문에서 제안하는 n-베스트 재랭킹은 다양한 모델을 사용하여 더 다양한 유형의 모델들이 제공하는 가설을 재평가하고 높은 품질의 라벨을 선택합니다. 이 방법은 특히 학습 단계에서의 컴퓨팅 비용을 증가시키지만, 예측 단계에서는 영향을 주지 않습니다.

##### 실험 결과
독일어↔영어 및 중국어↔영어 번역 작업에서 n-베스트 재랭킹을 통한 가성 라벨이 학생 모델의 정확도를 크게 향상시키는 것을 실험적으로 검증하였습니다. 특히, n-베스트 재랭킹을 통해 자동 생성된 라벨을 사용한 학생 모델이 매우 큰 번역 모델(약 47억 파라미터)과 유사한 성능을 발휘하지만, 사용한 파라미터 수는 더 적습니다.

##### 결론 및 미래 연구
논문은 n-베스트 재랭킹 방법이 시퀀스 수준의 KD의 정확도를 크게 향상시킬 수 있음을 보여줍니다. 미래 연구에서는 더 강력한 대형 언어 모델들을 활용하고, 성별이나 수의 일치 등 세부 현상을 더 명확히 포착할 수 있는 모델들을 통합할 계획입니다. 또한, 효율성을 높이기 위해 점수 매기기 과정을 가속화할 방법을 조사할 것입니다.

#### 2. 전체 요약
이 논문은 지식증류(KD) 방법을 향상시키기 위해 n-베스트 리스트 재랭킹을 적용하는 새로운 방법을 제안하고, 이를 통해 다양한 모델들의 가설을 비교하여 더 높은 품질의 라벨을 선택합니다. 이를 통해 학생 모델의 정확도를 높이고, 대규모 언어 모델과 유사한 성능을 발휘할 수 있게 합니다. 논문에서 제안한 방법은 독일어↔영어 및 중국어↔영어 번역 작업에서 실험적으로 검증되었습니다. 이번 연구의 결과는 다양한 모델을 활용함으로써 번역 모델의 성능을 대폭 향상시킬 수 있는 가능성을 제시합니다.

## Similar Papers
- [PLaD: Preference-based Large Language Model Distillation with Pseudo-Preference Pairs](2406.02886.md)
- [Depth-Adaptive Transformer](1910.10073.md)
- [Fast Inference from Transformers via Speculative Decoding](2211.17192.md)
- [Attention Is All You Need](1706.03762.md)
- [DistiLLM: Towards Streamlined Distillation for Large Language Models](2402.03898.md)
- [SPEED: Speculative Pipelined Execution for Efficient Decoding](2310.12072.md)
- [Zero-Shot Tokenizer Transfer](2405.07883.md)
- [Applying RLAIF for Code Generation with API-usage in Lightweight LLMs](2406.20060.md)
- [Sentence-wise Speech Summarization: Task, Datasets, and End-to-End Modeling with LM Knowledge Distillation](2408.00205.md)
