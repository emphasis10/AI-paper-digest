# ThinK: Thinner Key Cache by Query-Driven Pruning
## TL;DR
## Summary
- [https://arxiv.org/pdf/2407.21018.pdf](https://arxiv.org/pdf/2407.21018.pdf)

### 섹션별 중요 내용 요약

#### 1. 들어가며
논문은 대형 언어 모델(LLM)이 자연어 처리 분야에서 도약적인 성과를 거두고 있다는 사실을 강조합니다. 특히, 모델 크기와 시퀀스 길이가 증가함에 따라 LLM의 성능도 향상된다는 스케일링 법칙을 설명합니다. 그러나 모델 크기와 시퀀스 길이의 증가에 따른 계산 및 메모리 비용도 함께 증가한다는 문제를 지적합니다. 이를 해결하기 위해 LLM의 효율적인 실행 방안을 논의하며, 특히 긴 시퀀스 처리에 관련된 메모리 사용량을 줄이는 방법에 중점을 둡니다.

#### 2. 관찰
이 섹션에서는 KV-캐시의 채널 차원이 불균형한 크기를 갖고 있으며, 주의 메커니즘에서 낮은 순위 구조를 보인다는 중요한 발견을 다룹니다. 이러한 관찰을 바탕으로, 채널 차원에 불필요한 중복이 존재함을 가정하고, 이를 줄이기 위한 방법을 모색합니다.

#### 3. ThinK
- **기초 연구**: KV 캐시 최적화를 위한 사전 연구를 진행하여 다양한 접근 방식을 검토합니다.
- **쿼리 기반 가지치기 방법**: 이 섹션은 쿼리에 따라 불필요한 채널을 가지치기하는 방법에 대해 설명합니다. 이는 주의 가중치 손실을 최소화하기 위해 채널의 중요도를 평가하고, 가장 중요한 채널을 선택하는 최적화 문제로 정의됩니다.
- **구현**: 제안된 방법을 다양한 모델에 적용하는 방법을 세부적으로 설명합니다.

#### 4. 실험 결과
- **설정**: 실험을 위한 환경 설정을 설명합니다.
- **LongBench 및 Needle-in-a-Haystack 결과**: 제안된 ThinK 방법이 기존 방법과 비교하여 메모리 사용량을 20% 이상 줄이면서도 유사하거나 더 나은 성능을 보여줍니다.
- **가지치기 연구**: 쿼리 기반 가지치기가 l1 및 l2 노름 기반 방법보다 더 효과적임을 보여줍니다.

#### 5. 관련 연구
기존의 KV 캐시 압축 및 정량화 방법들을 종합적으로 비교 분석합니다. 이러한 연구들은 시스템 수준에서의 최적화와 알고리즘 수준에서의 최적화 방안을 포함합니다.

#### 6. 결론 및 향후 연구 방향
논문은 채널의 크기가 불균형하다는 점과 키 캐시가 본질적으로 낮은 순위를 갖는다는 점을 바탕으로, 쿼리에 따라 채널을 가지치기하는 ThinK 방법을 제안합니다. 향후 연구에서는 더 높은 가지치기 비율을 설정하면서도 성능을 유지할 수 있는 방법과 값 캐시 가지치기 기술을 탐색할 계획이라고 합니다.

### 전체 요약
이 논문은 대형 언어 모델(LLM)의 키-값(KV) 캐시 메모리 사용량을 줄이기 위한 새로운 방법인 ThinK를 소개합니다. 제안된 방법은 쿼리의 중요도에 따라 불필요한 채널을 가지치기하여, 모델의 정확도를 유지하거나 향상시키면서 메모리 사용량을 20% 이상 줄이는 데 성공하였습니다. 이는 주의 메커니즘 내에서 채널 크기의 불균형과 낮은 순위 구조를 기반으로 채널 가지치기를 수행하는 방식으로, LLaMA3 및 Mistral 모델에서 다양한 긴 시퀀스 데이터셋을 사용한 실험을 통해 그 효과를 입증하였습니다. 향후 연구에서는 가지치기 비율을 더 높이면서 성능을 유지하는 방법과 값 캐시 가지치기 기술을 탐색할 계획입니다.

이 논문은 효율적인 LLM 배치를 위한 새롭고 혁신적인 방법을 제시하며, 메모리와 계산 비용을 줄이는 데 기여하고 있습니다.