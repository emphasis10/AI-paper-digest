# LIMO: Less is More for Reasoning
## TL;DR
## Summary
- [https://arxiv.org/pdf/2502.03387.pdf](https://arxiv.org/pdf/2502.03387.pdf)

### 섹션별 요약

1. **서론**
   이 논문에서는 LIMO(.less is more for reasoning)이라는 모델을 제안하며, 복잡한 사고 능력이 적은 예제만으로도 발휘될 수 있다는 것을 보여줍니다. 817개의 정제된 훈련 샘플만으로 AIME에서 57.1%, MATH에서 94.8%의 정확도를 달성한 점이 강조됩니다. 이는 기존의 SFT(지속적 훈련) 기반 모델들보다 더 나은 성과를 나타내며, 데이터 요구 사항에 대한 기존의 가정을 도전합니다.

2. **LIMO 가설**
   LIMO 가설은 목적지식이 충분히 인코딩된 기초 모델에서 적은 수의 예제를 통해 복잡한 추론 능력이 발휘될 수 있다고 주장합니다. 이는 모델의 지식 기반이 얼마나 잘 구성되어 있는지와 훈련 후 예제가 얼마나 효과적으로 인지 과정을 보여주느냐에 따라 달라진다고 설명됩니다.

3. **LIMO 데이터셋**
   데이터셋 구성은 문제 및 해답의 질을 유지하는 것을 목표로 하며, 고품질 문제를 선정하여 모델의 내재적 사고 능력을 효과적으로 유도하는 과정을 기술합니다.

4. **방법론**
   LIMO 모델은 Qwen2.5-32B-Instruct를 사용하여 지도식 훈련을 수행합니다. 이는 효율적인 모델 훈련을 위해 DeepSpeed와 FlashAttention 등의 기술을 활용합니다.

5. **실험**
   LIMO는 다양한 벤치마크에서 기존의 경쟁 모델들을 압도하는 성능을 보입니다. 특히, LIMO는 최소한의 예제(817개)로도 우수한 성과를 달성하며, 나머지 모델보다 데이터 효율성이 높았습니다.

6. **미래 연구 방향**
   LIMO의 원리를 다른 도메인으로 확장하고, 이론적인 기초를 개발하며, 자동화된 평가 도구를 만드는 방향으로 연구를 진행할 계획입니다. 또한, 실제 사례에서의 적용 가능성도 탐구할 것입니다.

### 전체 요약
이 논문은 적은 양의 훈련 데이터(817개)로도 복잡한 수학적 사고 능력을 이끌어낼 수 있는 LIMO 모델을 소개합니다. 기존의 수많은 데이터 요구 사항에 대한 가정을 반박하고, 뛰어난 성과(예: AIME 57.1%, MATH 94.8%)를 보여줍니다. LIMO의 성공은 충분히 인코딩된 도메인 지식과 효과적인 인지 과정의 시연에 기초하며, 이는 인공지능 연구에 중요한 영향을 미칠 것으로 예상됩니다. будущих исследований в этом направлении.