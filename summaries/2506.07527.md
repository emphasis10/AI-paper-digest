# Learning What Reinforcement Learning Can't: Interleaved Online Fine-Tuning for Hardest Questions
## TL;DR
## Summary
- [https://arxiv.org/pdf/2506.07527.pdf](https://arxiv.org/pdf/2506.07527.pdf)

1. 요약:

- **서론**: 이 연구는 대규모 언어 모델(LLM)의 추론 능력 향상을 위해 고안되었습니다. 최근 강화 학습(강화학습)과 검증 가능한 보상을 통해 LLM의 복잡한 추론 작업을 개선하는 방법이 부각되고 있습니다.

- **문제 정의 및 관련 연구**: 본 논문에서는 강화 학습(RL)과 지도형 미세 조정(SFT)의 상호 보완적인 강점에 집중하여 LLM의 학습을 개선하는 방법을 제시합니다. RL은 기존 모델의 지식 기반에서 작업하며, 주로 쉬운 질문 학습에 효과적입니다. 반면, SFT는 고품질의 데이터가 필요하지만 더 어려운 문제를 해결하는데 도움이 됩니다.

- **ReLIFT 제안 방법**: ReLIFT(Reinforcement Learning Interleaved with Online Fine-Tuning)는 RL과 SFT를 번갈아 사용하여 특히 어려운 질문에 대한 ft 단계를 통해 모델의 추론 능력을 향상시키는 기법입니다. 이 방법은 적응적으로 SFT 빈도를 조절하여 모델의 성능을 지속적으로 향상시킵니다.

- **실험 및 결과**: ReLIFT는 수학적 추론 기준에서 우수한 성능을 입증하였고, 특히 다른 RL 및 SFT 접근 방식을 능가하는 결과를 보였습니다. 이는 5개의 경쟁 수준의 대회에서 평균 51.1%의 정확도를 기록하며, 짧은 결합 반응을 통해 효율성을 높였습니다.

- **결론**: ReLIFT은 적은 양의 상세한 데이터로도 높은 성능을 발휘하며, SFT와 RL의 장점을 통합하여 LLM 추론을 개선하는 데 있어 효과적인 방법임을 입증했습니다.

2. 전체 요약:

이 논문은 대규모 언어 모델의 추론 능력을 강화하기 위해 ReLIFT라는 새로운 방식을 제시합니다. ReLIFT는 강화 학습과 온라인 미세 조정(SFT)을 번갈아 사용함으로써 LLM이 어려운 질문을 더 잘 해결할 수 있게 합니다. 실험 결과, ReLIFT는 기존 방법에 비해 더 적은 데이터로도 높은 성능을 기록하며, 효율적이고 정확한 추론을 할 수 있게 합니다. 이 연구는 LLM 추론 능력을 개선할 새로운 가능성을 열었습니다.