# Knowledge Composition using Task Vectors with Learned Anisotropic Scaling
## TL;DR
## Summary
- [https://arxiv.org/pdf/2407.02880.pdf](https://arxiv.org/pdf/2407.02880.pdf)

### 1. 각 섹션 요약

#### Introduction
이 논문은 사전 훈련된 모델의 가중치 차이를 나타내는 태스크 벡터를 이용해 복잡한 학습 문제를 해결하는 새로운 알고리즘인 aTLAS를 소개합니다. 태스크 벡터는 사전 학습된 모델과 미세 조정된 모델 간의 가중치 차이를 나타내며, 다양한 태스크 표현을 결합하는 데 사용될 수 있습니다. 주요 목표는 태스크 벡터의 조합을 통해 지식 전달을 최적화하는 것입니다. 이 방법은 주어진 라벨 데이터가 적거나 없는 상황에서 특히 유용합니다. 주된 기여는 태스크 아리어메틱(Task Arithmetic), 소수 샷(Shot) 인식, 테스트 시간 적응 등의 문제에서 성능을 입증하는 것입니다.

#### Models and Task Vectors
여기서는 태스크 벡터의 개념과 이를 학습에 적용하는 방법을 설명합니다. 특히, CLIP 모델을 사용하여 태스크 벡터를 획득하고, 모델의 파라미터 블록별로 독립적인 스케일링 계수를 학습합니다. 이러한 비등방성 스케일링은 태스크 벡터의 모듈성을 활용하여 지식 조합을 가능하게 합니다. 이는 라벨 데이터가 희소한 상황에서 특히 효과적입니다.

#### Learning Task Vector Compositions
aTLAS는 주어진 태스크 벡터의 선형 결합을 학습하여 비등방성 스케일링을 통해 모델의 성능을 향상시킵니다. 이 방법은 매우 적은 수의 학습 가능한 파라미터로 높은 성능을 낼 수 있도록 합니다. 특히, 태스크 벡터의 모듈성을 활용하여 지식 조합을 최적화하고, 데이터가 적은 상황에서도 강한 일반화 능력을 보입니다.

#### Few-Shot Adaptation and Test-Time Adaptation
소수 샷 학습과 테스트 시간 적응에서도 aTLAS는 높은 효율성을 보입니다. 실험 결과, 소수 샷 학습에서 기존 방법보다 높은 성능을 나타내며, 테스트 시간 적응에서도 효과적인 성능을 입증했습니다. 즉, 데이터가 부족한 경우에도 태스크 벡터의 조합을 통해 높은 성능을 유지할 수 있습니다.

#### Conclusion
이 논문은 aTLAS 알고리즘의 효용성을 입증하며, 사전 훈련된 모델의 낮은 내재적 차원을 직접 활용하여 매우 적은 수의 학습 가능한 파라미터로 높은 성능을 이끌어냅니다. 태스크 벡터를 여러 태스크에서 조합하여 효율적인 지식 전달과 일반화를 가능하게 합니다. 동시에, 기존 방법과의 결합을 통해 성능을 더욱 향상시킬 수 있습니다.

### 2. 전체 요약

논문은 사전 학습된 모델의 가중치 차이를 나타내는 태스크 벡터를 활용한 새로운 알고리즘인 aTLAS를 소개합니다. 태스크 벡터는 다양한 태스크의 표현을 결합하는 데 사용될 수 있습니다. aTLAS는 태스크 벡터 조합을 통해 고차원 문제를 보다 적은 학습 가능한 파라미터로 해결할 수 있도록 설계되었습니다. 이 방법은 주로 태스크 아리어메틱, 소수 샷 인식, 테스트 시간 적응과 같은 문제에서 높은 성능을 보이며, 라벨 데이터가 충분하지 않은 상황에서도 효율적입니다. 주요 기여는 태스크 벡터의 비등방성 스케일링을 통해 학습 가능한 파라미터를 줄이고, 다양한 태스크에서 효율적인 지식 전달과 일반화를 가능하게 하는 것입니다. 결론적으로, aTLAS는 적은 수의 파라미터로도 높은 성능을 유지할 수 있는 강력한 방법론임을 증명합니다.

## Similar Papers
- [Scaling (Down) CLIP: A Comprehensive Analysis of Data, Architecture, and Training Strategies](2404.08197.md)
- [An accurate detection is not all you need to combat label noise in web-noisy datasets](2407.05528.md)
- [Sparser is Faster and Less is More: Efficient Sparse Attention for Long-Range Transformers](2406.16747.md)
- [MobileCLIP: Fast Image-Text Models through Multi-Modal Reinforced Training](2311.17049.md)
- [MultiLoRA: Democratizing LoRA for Better Multi-Task Learning](2311.11501.md)
- [MoDE: CLIP Data Experts via Clustering](2404.16030.md)
- [Mixture of Nested Experts: Adaptive Processing of Visual Tokens](2407.19985.md)
- [CatLIP: CLIP-level Visual Recognition Accuracy with 2.7x Faster Pre-training on Web-scale Image-Text Data](2404.15653.md)
- [Safety Arithmetic: A Framework for Test-time Safety Alignment of Language Models by Steering Parameters and Activations](2406.11801.md)
