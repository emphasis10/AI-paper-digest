# XLand-100B: A Large-Scale Multi-Task Dataset for In-Context Reinforcement Learning
## TL;DR
## Summary
- [https://arxiv.org/pdf/2406.08973.pdf](https://arxiv.org/pdf/2406.08973.pdf)

### 1. 각 섹션의 요약

#### 서론
In-context 학습은 새로운 작업을 학습할 때 예제만을 사용해 무게 업데이트 없이 수행하는 능력입니다. 최근 연구에서 대형 언어 모델뿐만 아니라 작은 트랜스포머 모델도 이러한 학습이 가능함이 밝혀졌습니다. 이러한 학습은 언어 모델링 외에도 이미지 생성 등 다양한 분야에서도 발견되고 있습니다. 그러나 강화 학습(RL)에서는 최근에서야 이 능력이 나타났습니다. 이는 수만 개의 고유한 작업에서 학습하는 것이 필요하기 때문에 작업 수집의 어려움과 맞물려 있었습니다.

#### 배경
In-context 강화 학습(RL)은 다양한 방법론이 연구되고 있으며, 특히 Algorithm Distillation(AD)와 Decision-Pretrained Transformer(DPT)이 주요한 방법으로 사용됩니다. AD는 과거 행동의 이력을 기반으로 예측하는 방식으로, DPT는 특정 작업 상황에서 최적의 행동을 예측하는 방식입니다. 두 방법 모두 sim-경험 및 온라인 RL 알고리즘의 근사치를 달성할 수 있습니다.

#### XLand-100B 데이터셋
XLand-100B는 XLand-MiniGrid 환경을 기반으로 한 대규모 데이터셋으로, 약 30,000개의 작업 이력과 1000억 개의 전환을 포함합니다. 이 데이터셋은 in-context RL 연구를 민주화하고 대규모로 확장하는 것을 목표로 합니다. 데이터 수집 과정과 필터링, 리레이블링 과정이 상세히 설명되어 있으며, 실험 결과는 아직 개선의 여지가 있음을 보여줍니다.

#### 실험 및 결과
AD와 DPT 방법을 통해 다양한 작업 복잡도에서 학습이 가능함을 확인했습니다. AD는 간단한 작업에서는 효과적이었으나 복잡한 작업에서는 성능이 저하되는 현상을 보였습니다. DPT는 일부 상황에서는 학습 성능이 낮았습니다. 이 결과는 더욱 효율적인 아키텍처 연구가 필요함을 시사합니다.

#### 한계 및 향후 연구 방향
본 연구의 한계점으로는 데이터의 도메인이 다양하지 않다는 점, 모든 작업이 동일한 구조를 가진다는 점이 지적되었습니다. 또, 사전 훈련된 체크포인트에서 파인 튜닝(fine-tuning)하는 효과에 대해 더 많은 연구가 필요합니다. 이러한 한계점을 보완하기 위해 더 다양한 벤치마크 생성기를 도입하고 RL 데이터 수집 방식을 개선할 계획입니다.

### 2. 전체 요약
이 논문은 in-context 강화 학습을 위한 대규모 데이터셋인 XLand-100B를 소개합니다. 이는 다양한 작업 이력과 전환을 포함하며, AD와 DPT 방법을 통해 다양한 작업 복잡도에서의 학습 성능을 평가했습니다. AD는 간단한 작업에 효과적이나, 더욱 복잡한 작업에서는 한계가 드러났습니다. 연구는 데이터의 도메인 다양성 부족 등 몇 가지 한계점이 있었으나, 향후 지속적인 개선과 더 많은 연구를 통해 이러한 문제를 해결하고자 합니다. 본 논문의 주된 기여는 대규모 데이터를 통해 in-context RL 연구를 확대하고, 민주화하는 기반을 제공하는 것입니다.