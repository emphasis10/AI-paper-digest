# Compact Language Models via Pruning and Knowledge Distillation
## TL;DR
## Summary
- [https://arxiv.org/pdf/2407.14679.pdf](https://arxiv.org/pdf/2407.14679.pdf)

### 1. 각 섹션별 요약 

#### 초록 (Abstract)
이 논문은 기존의 대규모 언어 모델(LLMs)을 대상 규모와 크기에 맞게 재훈련하는 것이 아닌, 이미 학습된 LLM을 가지치기(pruning) 후 일부 데이터(전체의 3% 미만)로 재훈련하여 다양한 모델 변형을 만드는 방법을 탐구합니다. 이를 통해 최소한의 데이터로도 높은 성능을 유지하면서 모델을 압축할 수 있는 실용적인 최적화 기법을 제시합니다.

#### 서론 (Introduction)
훈련 데이터와 자원의 비용 절감을 목표로, 한 가지 큰 모델을 학습한 후 가지치기와 재훈련을 통해 더 작은 모델들을 생성하는 방법을 제안합니다. 이를 통해 Nemotron-4 모델을 압축하여 MINITRON 모델군을 생성하고, 다양한 언어 모델링 작업에서 성능을 비교합니다. 이 방법은 기존의 모델 학습 비용을 크게 절감하면서도 높은 성능을 유지할 수 있습니다.

#### 가지치기 방법론 (Pruning Methodology)
단층 인지 신경망과 다중 헤드 주의 메커니즘을 사용하여 각 레이어, 뉴런, 헤드, 임베딩의 중요도를 계산하고 정렬하여 가지치기 모델을 얻는 과정을 설명합니다. 기존의 무게 크기를 이용한 중요도 계산은 효율적이지 않으며, 최근 연구에서는 그래디언트, 코사인 유사도 및 당혹도와 같은 메트릭을 사용해야 한다고 강조합니다.

#### 실험 (Experiments)
다양한 가지치기 축(뉴런, 헤드, 임베딩, 깊이)을 통해 최적의 압축 아키텍처를 찾기 위한 실험을 수행합니다. 실험 결과, 처음에는 뉴런과 헤드 가지치기가 우수하였으나 재훈련 후에는 임베딩 가지치기가 더 나았고, 폭 가지치기가 깊이 가지치기보다 효율적이라는 등의 비직관적인 통찰을 제공합니다.

#### 결과 (Results)
압축된 MINITRON 모델은 기존보다 최대 40배 적은 훈련 토큰으로도 높은 성능을 유지합니다. Nemotron-4 모델을 가지치기하여 생성된 MINITRON 8B 모델은 Nemotron-3 8B, LLaMa-2 7B 등과 비교해 더 좋은 성능을 보였습니다.

#### 결론 (Conclusion)
구조화된 가지치기와 재훈련을 통한 LLM 압축 기법을 제시하며, 이는 매우 비용 효율적이고 높은 성능을 유지할 수 있음을 실험적으로 입증했습니다. 학습 데이터 비용을 크게 줄이면서도 여러 커뮤니티 모델들과 경쟁력 있는 성능을 달성하는 MINITRON 모델을 제안합니다.

### 2. 전체 요약
이 논문은 LLM을 더 작은 변형 모델로 만들기 위한 비용 효율적인 방법을 제안합니다. 기존 모델을 거대한 데이터로 반복 학습하는 대신, 이미 학습된 모델을 가지치기하고, 그 후 부분적인 데이터로 재훈련하여 성능을 유지하는 방식을 채택합니다. 이를 통해 생산 비용을 크게 절감하면서도 높은 성능을 유지하는 최적화 기법을 구축했습니다. 주요 기여로는 Nemotron-4 모델을 가지치기하여 생성한 MINITRON 모델군을 제안하며, 이 모델들은 기존의 다양한 모델들과 비교해 우수한 성능을 보였고, 비용 절감에도 크게 기여합니다. 결과적으로, 이 연구는 구조화된 가지치기와 지식 증류를 통한 데이터 효율적 재훈련의 실질적인 모범 사례를 수립했습니다.