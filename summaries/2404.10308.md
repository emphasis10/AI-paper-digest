# Hierarchical Context Merging: Better Long Context Understanding for Pre-trained LLMs
## TL;DR
## Summary
- [https://arxiv.org/pdf/2404.10308](https://arxiv.org/pdf/2404.10308)

이 논문에서는 대규모 언어 모델(LLMs)의 긴 문맥 이해를 개선하기 위해 계층적 컨텍스트 통합(Hierarchical Context Merging, HOMER) 기법을 제안합니다. 이 기법은 트랜스포머의 계층을 통해 입력을 관리 가능한 청크로 나누고, 이 청크들을 계층적으로 병합하는 과정을 통해 긴 입력을 효율적으로 처리할 수 있도록 설계되었습니다. 주요 내용은 다음과 같습니다.

1. **서론**:
   - 대규모 언어 모델들은 다양한 자연어 처리 작업에서 뛰어난 성능을 보이지만, 처리할 수 있는 토큰 수에는 한계가 있습니다.
   - 이 연구는 기존 모델의 문맥 한계를 확장하고자 하는 새로운 접근 방식인 HOMER를 소개합니다.

2. **관련 작업**:
   - 긴 범위 트랜스포머에 대한 기존 연구들은 주로 자기 주의 메커니즘의 계산 비용을 줄이는 데 집중해왔습니다.
   - HOMER는 이러한 기존 접근 방식과 다르게 입력을 여러 청크로 나누고 계층적으로 통합하는 방식을 제안합니다.

3. **계층적 컨텍스트 통합 (HOMER)**:
   - 입력을 여러 청크로 나누고, 이 청크들을 트랜스포머의 계층을 통해 점진적으로 통합합니다.
   - 각 청크는 병합 과정을 거치기 전에 토큰 축소 기술을 사용하여 길이를 줄입니다.

4. **실험 및 결과**:
   - 다양한 다운스트림 작업에서 HOMER의 효과를 검증하며, 특히 긴 문맥을 필요로 하는 작업에서 우수한 성능을 보입니다.
   - 문서의 펄플렉시티 측정을 통해 HOMER가 긴 문맥에서도 유창함을 유지함을 보여줍니다.

5. **결론**:
   - HOMER는 기존 LLMs의 문맥 한계를 효과적으로 확장하며, 메모리 요구량을 로그 급수적으로 줄일 수 있습니다.
   - 이 기법은 특히 자원이 제한된 환경에서 긴 입력을 처리할 때 유용합니다.

이 논문은 긴 문맥을 효율적으로 처리할 수 있는 새로운 기술을 소개하며, 대규모 언어 모델의 활용 범위를 넓히는 데 기여할 것으로 기대됩니다.

## Similar Papers
- [You Only Cache Once: Decoder-Decoder Architectures for Language Models](2405.05254.md)
- [SliceGPT: Compress Large Language Models by Deleting Rows and Columns](2401.15024.md)
- [LLM in a flash: Efficient Large Language Model Inference with Limited Memory](2312.11514.md)
- [Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention](2404.07143.md)
- [Never Miss A Beat: An Efficient Recipe for Context Window Extension of Large Language Models with Consistent "Middle" Enhancement](2406.07138.md)
- [Megalodon: Efficient LLM Pretraining and Inference with Unlimited Context Length](2404.08801.md)
- [Retrieval-Enhanced Machine Learning: Synthesis and Opportunities](2407.12982.md)
- [Layer-Condensed KV Cache for Efficient Inference of Large Language Models](2405.10637.md)
- [From Artificial Needles to Real Haystacks: Improving Retrieval Capabilities in LLMs by Finetuning on Synthetic Data](2406.19292.md)
