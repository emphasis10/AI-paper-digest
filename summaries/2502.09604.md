# SelfCite: Self-Supervised Alignment for Context Attribution in Large Language Models
## TL;DR
## Summary
- [https://arxiv.org/pdf/2502.09604.pdf](https://arxiv.org/pdf/2502.09604.pdf)

I'm unable to process the entire document thoroughly due to the constraints in place. However, I can give you a brief summary and main contributions based on excerpts I have parsed from the document.

1. **논문 섹션 요약**:
   - **도입**: 대형 언어 모델(LLM)은 사용자들이 정보를 모으고 지식을 습득하는 데 많은 도움을 주고 있습니다. 하지만 흔히 환각(hallucination)이라는 문제로, 설득력은 있지만 실제로는 허구인 내용을 생성하는 경우가 많습니다.
   
   - **자기 감독 방식 (Self-Supervised Setting)**: 문맥 속에서 인용군을 생략하여 생성된 이전 응답과 새롭게 생성된 응답을 서로 비교하는 방식으로 인용의 정확성을 높인다는 새로운 접근 방식을 제시하였습니다.

   - **기술적 혁신**: 어떻게 필요성과 충분성 점수를 결합해 최종 보상을 산출하는지 설명합니다. 필요 점수는 문장에서 인용된 모든 콘텐츠를 제거할 때의 생성 확률 감소 측정을 통한 것입니다.

   - **결론**: SelfCite는 LLM이 보다 정확하고 미세 조정된 인용을 생성하도록 도우며, 사람이 주석이나 검토 없이 학습을 진행할 수 있도록 합니다. 그러나 이것은 제한점이 있으며 앞으로의 연구가 더욱 필요함을 언급합니다.

2. **논문 전체 요약**:
   - 이 논문은 LLM에서 발생할 수 있는 비현실적 정보 생성의 문제를 해결하고자 하는 접근방식을 소개하고 있습니다. SelfCite 방식과 같은 자기 감독 체계와 최적화 기법을 활용하여 모델이 자주 사용하는 콘텐츠와 인용 출처 간의 정확한 연결을 통해 보다 신뢰성 있는 결과를 제안합니다.

이러한 방식은 인용의 정확성을 크게 개선하고, 외부 도움 없이도 모델 자체가 인용의 품질을 측정할 수 있도록 합니다. 이는 LLM이 좀 더 검증 가능한 방향으로 발전하도록 도울 것이며, 이 연구는 추후 참고 자료나 검토가 필요 없는 학습의 큰 진전을 보여줍니다.