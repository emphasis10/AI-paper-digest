# PowerInfer-2: Fast Large Language Model Inference on a Smartphone
## TL;DR
## Summary
- [https://arxiv.org/pdf/2406.06282.pdf](https://arxiv.org/pdf/2406.06282.pdf)

### 각 섹션 요약

#### 1. 소개 (Introduction)
이 논문에서는 PowerInfer-2라는 프레임워크를 소개합니다. 이 시스템은 스마트폰에서 매우 큰 언어 모델(LLM)을 고속으로 추론할 수 있도록 설계되었습니다. 기존의 LLM 추론은 대규모 행렬 계산에 의존했지만, PowerInfer-2는 이를 더 세밀한 신경군(cluster) 계산으로 변환하여 스마트폰의 이기종 컴퓨팅 자원을 효율적으로 활용합니다.

#### 2. 배경 및 동기 (Background and Motivation)
이 섹션에서는 LLM 추론의 기본 원리와 FFN(Feed-Forward Network) 블록의 동작을 설명합니다. 최근의 연구들은 FFN 블록의 동작이 예측 가능하다는 점을 이용해, 일부 뉴런만 활성화함으로써 계산량을 줄이는 기술을 개발했습니다. 이 기술을 활용하면, 스마트폰처럼 메모리가 제한적인 장치에서도 효율적으로 LLM을 실행할 수 있습니다.

#### 3. PowerInfer-2 개요 (PowerInfer-2 Overview)
PowerInfer-2는 스마트폰의 하드웨어 특성에 맞추어 세밀한 신경군 계산 전략을 사용합니다. 이를 통해 CPU, GPU, NPU 같은 이기종 컴퓨팅 자원을 최적화하고, I/O 지연을 최소화합니다. 또한, 메모리 사용을 줄이기 위해 세분화된 캐시 관리 및 파이프라인 기법을 도입했습니다. 이러한 설계를 통해 PowerInfer-2는 기존 시스템 대비 최대 29.2배의 성능 향상을 이뤘으며, TurboSparse-Mixtral-47B 모델을 최초로 스마트폰에서 효율적으로 실행할 수 있었습니다.

#### 4. 신경군-유도 런타임 추론 (Neuron-Aware Runtime Inference)
PowerInfer-2는 다형성 신경 엔진을 도입하여 LLM 추론 단계와 이기종 XPU의 다양한 계산 특성을 효율적으로 활용합니다. NPU가 대규모 행렬 계산에 뛰어난 점을 이용해, 입력 문장 전체를 동시에 처리해야 하는 prefill 단계에서는 NPU 중심의 워크플로우를 사용하고, 디코딩 단계에서는 CPU 중심의 워크플로우를 사용하여 전반적인 성능을 향상시킵니다.

#### 5. 캐시 및 I/O 최적화 (In-Memory Neuron Cache and I/O Optimization)
PowerInfer-2는 활성화 뉴런을 효과적으로 캐시하기 위해 다양한 데이터 유형에 맞춘 세그먼트 캐시 설계를 도입합니다. 또한, 미리 읽기 및 LRU(Least Recently Used) 기반의 동적 축출 전략을 사용하여 I/O 오버헤드를 최소화합니다. 이를 통해, 캐시 히트율을 높이고 추론 성능을 높일 수 있습니다.

#### 6. 성능 평가 (Evaluation)
PowerInfer-2는 OnePlus 12와 Ace 2 스마트폰에서 다양한 모델을 사용해 평가되었습니다. 평가 결과, PowerInfer-2는 기존 시스템보다 최대 29.2배 빠르며, 메모리 사용량도 약 40% 절감할 수 있었습니다. 이는 모델의 활성화 희소성을 효과적으로 활용한 덕분입니다. TurboSparse-Mixtral-47B 모델의 경우, 기존 시스템 대비 21.2배 빠른 속도를 달성했습니다.

### 전체 요약
PowerInfer-2는 스마트폰에서 대형 언어 모델을 고속으로 추론하기 위한 혁신적 프레임워크입니다. 기존의 대규모 행렬 계산을 세밀한 신경군 계산으로 변환하여, 스마트폰의 다양한 하드웨어 자원을 효율적으로 활용합니다. 이를 통해 최대 29.2배 빠른 성능을 달성하고 메모리 사용량도 약 40% 절감할 수 있습니다. 특히, TurboSparse-Mixtral-47B 모델을 최초로 스마트폰에서 효율적으로 실행할 수 있게 되었습니다. PowerInfer-2는 이기종 컴퓨팅 자원의 특성을 최적화하고 I/O 지연을 최소화함으로써, 스마트폰에서도 대규모 언어 모델을 효과적으로 사용할 수 있도록 돕습니다.