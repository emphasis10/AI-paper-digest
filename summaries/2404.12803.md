# TextSquare: Scaling up Text-Centric Visual Instruction Tuning
## TL;DR
## Summary
- [https://arxiv.org/pdf/2404.12803.pdf](https://arxiv.org/pdf/2404.12803.pdf)

### 논문 요약

#### 1. 서론
이 연구는 대규모 멀티모달 언어 모델(MLLM)을 통한 텍스트 중심의 시각적 질문 응답(VQA)을 탐구하며, 텍스트 중심 VQA 데이터셋, Square-10M을 개발하여 기존 오픈소스 모델과 최신 상용 모델 간의 성능 격차를 해소합니다. 이 데이터셋은 질문 생성, 답변, 추론, 평가의 네 단계를 통해 생성됩니다.

#### 2. Square-10M 데이터셋
Square-10M은 다양한 시나리오에서 텍스트가 풍부한 이미지를 수집하여 고도로 구조화된 질문-답변 쌍을 생성하는 과정을 통해 구축되었습니다. 이는 인공지능이 이미지 내 텍스트를 분석하고 관련 질문을 생성하며, 그에 대한 답변과 그 이유를 제공합니다.

#### 3. 실험 및 결과
TextSquare 모델은 Square-10M을 사용하여 훈련되었고, 다양한 벤치마크에서 상용 최고 성능 모델들과 경쟁하는 결과를 보였습니다. 또한, 텍스트 중심의 VQA 시나리오에서 모델의 성능을 크게 향상시키는 데 중요한 역할을 하는 추론 데이터의 중요성을 입증했습니다.

#### 4. 결론
Square-10M 데이터셋과 TextSquare 모델은 텍스트 중심 VQA에서의 인스트럭션 튜닝을 위한 새로운 표준을 설정합니다. 이 연구는 멀티모달 언어 모델이 복잡한 시각적 질문에 대해 더 정확하고 자세하게 답변할 수 있도록 돕는 방법론을 제시합니다.

### 종합적인 요약
이 논문은 텍스트 중심 VQA를 위한 대규모 고품질 데이터셋인 Square-10M을 소개하고, 이를 통해 훈련된 TextSquare 모델이 다양한 벤치마크에서 뛰어난 성능을 보여줌으로써, 최신 상용 모델들과 경쟁할 수 있음을 보여줍니다. 이 연구는 특히 VQA에서의 추론 데이터가 모델의 성능 향상에 얼마나 중요한지를 강조하며, 텍스트 중심 VQA의 새로운 방향을 제시합니다. 이는 더 나은 텍스트 분석과 질문 응답 능력을 갖춘 인공지능 개발로 이어질 수 있습니다.