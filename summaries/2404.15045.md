# Multi-Head Mixture-of-Experts
## TL;DR
## Summary
- [https://arxiv.org/pdf/2404.15045.pdf](https://arxiv.org/pdf/2404.15045.pdf)

이 논문은 다중 헤드 혼합 전문가 시스템(Multi-Head Mixture-of-Experts, MH-MoE)을 소개하고 있습니다. 이는 입력 토큰을 여러 서브 토큰으로 분할하고 이를 다양한 전문가(expert)에게 할당하여 처리하는 시스템입니다. 이 방식은 토큰의 세밀한 분석을 가능하게 하며, 각기 다른 전문가들이 동시에 다양한 정보를 처리함으로써 더 깊은 컨텍스트 이해를 가능하게 합니다. 주요 내용은 다음과 같습니다:

1. **서론 및 배경**: 대용량 모델의 성능 향상을 위해 모델의 파라미터 수를 늘리는 것이 일반적이지만, 이로 인해 추론 속도가 저하되고 실용성이 제한됩니다. MH-MoE는 스파스 혼합 전문가 모델(Sparse Mixture of Experts, SMoE)을 기반으로, 입력 토큰을 여러 서브 토큰으로 분할하고 이를 다양한 전문가에게 할당하여 처리하는 방식을 제안합니다.

2. **MH-MoE 시스템**: 각 입력 토큰은 여러 서브 토큰으로 나뉘고, 이 서브 토큰들은 다양한 전문가에게 할당되어 병렬로 처리됩니다. 이렇게 함으로써 전문가 활성화를 증가시키고, 더 섬세한 정보 처리가 가능해집니다. 특히, 이 시스템은 기존 SMoE 프레임워크와 쉽게 통합될 수 있어 성능을 개선하는 데 유리합니다.

3. **실험 결과**: MH-MoE는 영어 중심 언어 모델링, 다국어 언어 모델링, 마스킹된 멀티모달 모델링 작업 등 세 가지 주요 작업에서 효과적임을 보여줍니다. 실험 결과 MH-MoE는 기존 모델들보다 낮은 perplexity를 보이며, 특히 전문가 활성화 면에서 높은 비율을 달성합니다.

4. **결론**: MH-MoE는 간단하지만 효과적인 방법으로 다양한 전문가의 활성화를 증가시키고, 토큰의 미세한 이해를 향상시킬 수 있습니다. 이는 언어 및 이미지 데이터에 대한 깊은 이해를 가능하게 하며, 향후 대규모 언어 모델의 활용에 중요한 기여를 할 것입니다.

이 논문은 모델의 성능을 향상시키기 위한 새로운 접근 방법을 제시하며, 이는 다양한 언어 처리 작업에 큰 잠재력을 가지고 있습니다.