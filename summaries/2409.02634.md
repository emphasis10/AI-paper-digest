# Loopy: Taming Audio-Driven Portrait Avatar with Long-Term Motion Dependency
## TL;DR
## Summary
- [https://arxiv.org/pdf/2409.02634.pdf](https://arxiv.org/pdf/2409.02634.pdf)

### 요약

논문의 중요한 내용들을 요약하였습니다. 각 섹션별 요약과 그 결과를 바탕으로 한 전체 요약을 제공하겠습니다.

#### 1. 서론 (Introduction)

최근 GAN과 확산 모델 기술의 발전으로 인해 인간 비디오 합성의 품질이 실용적인 수준에 도달했습니다. 특히, 제로샷 오디오 기반 초상화 합성에 대한 연구가 활발히 이루어지고 있습니다. 기존 방법들은 오디오와 초상화 움직임 간의 약한 상관성 때문에 추가적인 공간 조건을 도입하여 합성 비디오의 시간적 안정성을 보장합니다. 그러나 이러한 방법들은 움직임의 표현력을 감소시킬 수 있습니다.

이번 논문에서는 이러한 문제를 해결하기 위해 오직 오디오 만으로 조건화된 초상화 확산 모델인 'Loopy'를 제안합니다. 이 모델은 데이터에서 자연스러운 움직임 패턴을 학습하고 오디오-초상화 움직임의 상관성을 개선하는 inter/intra-clip 시간 모듈과 오디오-라텐트 모듈을 설계하였습니다.

#### 2. 관련 연구 (Related Works)

오디오 기반의 초상화 비디오 합성은 주로 GAN 기반 방법과 확산 기반 방법으로 나뉩니다. GAN 기반 방법은 오디오-모션 모델과 모션-비디오 모델의 두 가지 주요 컴포넌트로 구성됩니다. 최근에는 확산 기술이 도입되어 보다 생동감 있는 합성 결과를 보여주고 있습니다. EMO Portrait, Hallo, EchoMimic, VExpress와 같은 끝단-끝단(end-to-end) 방법들은 생생한 비디오를 생성할 수 있지만, 여전히 공간적 조건 모듈을 필요로 하여 실제 응용에서의 모션 모델의 다양성을 제한합니다.

#### 3. 방법론 (Methodology)

Loopy의 주요 기여는 다음과 같습니다:
- **Inter/Intra-clip Temporal Modules**: 모션 프레임과 노이즈 프레임을 각각 처리하여 시간적 의존성을 개선.
- **Audio-to-Latents Module**: 오디오와 얼굴 모션 관련 피쳐를 라텐트로 변환하여 오디오-초상화 움직임의 상관성을 강화.

이 방법은 기존 방법들이 사용하던 얼굴 추적기와 속도층( Speed Layer)등의 공간 조건을 제거하고, 더 유연하고 자연스러운 움직임을 생성할 수 있습니다.

#### 4. 실험 (Experiments)

Loopy는 공개 데이터셋을 이용한 광범위한 실험에서 기존 방법들보다 더 생동감 있고 안정적인 합성 결과를 보였습니다. 특히, 움직임 스타일 정보를 장기적 시간 의존성을 통해 학습함으로써 표현력과 시간적 안정성이 크게 개선되었습니다.

#### 5. 결론 (Conclusion)

이 논문에서는 오디오 기반 초상화 비디오 생성 프레임워크인 'LOOPY'를 제안합니다. 이 방법은 공간 조건 없이 장기적인 시간 의존성을 활용하여 데이터에서 자연스러운 움직임 패턴을 학습할 수 있습니다. 광범위한 실험을 통해 시간적 안정성, 움직임 다양성 및 전체 비디오 품질에서 기존 방법들보다 큰 개선을 보여주었습니다.

---

### 전체 요약

이 논문은 현재 GAN 및 확산 모델 기술을 사용하여 오디오 기반 초상화 비디오 생성 분야에서의 최신 연구를 소개하고, Loopy라는 혁신적인 모델을 제안합니다. Loopy는 inter/intra-clip 시간 모듈과 오디오-라텐트 모듈을 통해 오직 오디오 신호만을 사용한 자연스러운 움직임 패턴을 학습합니다. 이를 통해 기존 모델들이 필요로 했던 얼굴 추적기와 같은 공간 조건을 제거하고, 더 유연하고 자연스러운 초상화 움직임을 생성할 수 있습니다. 광범위한 실험 결과, Loopy는 시간적 안정성, 움직임 다양성, 및 전체적인 비디오 품질에서 뛰어난 성능을 보였습니다.