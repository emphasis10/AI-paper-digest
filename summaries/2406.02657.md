# Block Transformer: Global-to-Local Language Modeling for Fast Inference
## TL;DR
## Summary
- [https://arxiv.org/pdf/2406.02657.pdf](https://arxiv.org/pdf/2406.02657.pdf)

### 논문의 주요 내용 요약

#### 1. 서론
이 논문은 기존의 자기 회귀(transformer 기반) 언어 모델에서 발생하는 추론 병목 문제를 해결하기 위해 Block Transformer 아키텍처를 제안합니다. 자기 회귀 모델에서 모든 이전 토큰을 참조하는 자기 주의 메커니즘이 추론 속도를 저하시키는데, 이를 해결하기 위해 Block Transformer는 계층적 전역-지역 모델링 방식을 도입하여 성능을 향상시킵니다. 이 아키텍처는 전역 모델링의 비용을 줄이기 위해 입력 토큰을 고정 크기 블록으로 집계하고, 블록 단위로 자기 주의를 적용합니다.

#### 2. Block Transformer 아키텍처
Block Transformer는 크게 임베더(embedder), 블록 디코더(block decoder), 토큰 디코더(token decoder)로 구성됩니다.
- **임베더**: 입력 토큰을 고정 크기 블록으로 집계하여 임베딩합니다.
- **블록 디코더**: 블록 단위로 자기 주의를 적용하여 컨텍스트 임베딩을 생성합니다.
- **토큰 디코더**: 컨텍스트 임베딩을 사용하여 다음 블록의 토큰을 디코딩합니다. 전역 주의 없이 빠른 지역 모델링을 수행합니다.

#### 3. 실험
실험 결과, Block Transformer는 기존의 transformer 모델에 비해 추론 속도가 10~20배 향상되었습니다. 이는 전역-지역 모델링을 통해 추론 병목을 해소하고, 하드웨어 자원의 효율적인 활용을 가능하게 한 결과입니다.

#### 4. 논의 및 결론
Block Transformer는 기존의 transformer 모델에 비해 추론 효율성을 크게 개선하였으며, 특히 대규모 언어 모델에서 더욱 두드러진 성능 향상을 보였습니다. 이 논문은 전역-지역 모델링 방식을 통해 언어 모델의 추론 병목을 해결하는 새로운 접근 방식을 제안하였으며, 향후 연구 및 실용적인 응용에 중요한 기여를 할 것으로 기대됩니다.

### 전체 요약
이 논문은 자기 회귀 transformer 기반 언어 모델의 추론 병목 문제를 해결하기 위해 Block Transformer 아키텍처를 제안하고, 이를 통해 기존 모델에 비해 추론 속도를 10~20배 향상시켰습니다. Block Transformer는 입력 토큰을 고정 크기 블록으로 집계하고, 블록 단위로 자기 주의를 적용하여 전역 주의의 비용을 줄입니다. 실험 결과, Block Transformer는 대규모 언어 모델에서 특히 두드러진 성능 향상을 보였으며, 전역-지역 모델링을 통해 추론 병목을 해소하는 새로운 접근 방식을 제시합니다. 이 연구는 향후 언어 모델의 추론 효율성을 높이는 데 중요한 기여를 할 것입니다.

## Similar Papers
- [Towards Fast Multilingual LLM Inference: Speculative Decoding and Specialized Drafters](2406.16758.md)
- [Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache Consumption](2407.18003.md)
- [In-Context Example Selection via Similarity Search Improves Low-Resource Machine Translation](2408.00397.md)
- [Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention](2404.07143.md)
- [You Only Cache Once: Decoder-Decoder Architectures for Language Models](2405.05254.md)
- [CoverBench: A Challenging Benchmark for Complex Claim Verification](2408.03325.md)
- [TriForce: Lossless Acceleration of Long Sequence Generation with Hierarchical Speculative Decoding](2404.11912.md)
- [Parrot: Multilingual Visual Instruction Tuning](2406.02539.md)
- [Large Language Models Are Reasoning Teachers](2212.10071.md)
