# Block Transformer: Global-to-Local Language Modeling for Fast Inference
## TL;DR
## Summary
- [https://arxiv.org/pdf/2406.02657.pdf](https://arxiv.org/pdf/2406.02657.pdf)

### Block Transformer: Global-to-Local Language Modeling for Fast Inference
#### 요약 및 설명

이 논문은 Block Transformer라는 아키텍처를 도입하여 자기 회귀 변환기의 추론 병목 현상을 완화하고자 합니다. 여기서는 각 섹션의 주요 내용을 요약하고, 중요한 기여와 혁신적인 부분을 설명하겠습니다.

---

#### 1. Introduction
**요약**:
Transformer 기반의 자기 회귀 언어 모델(LM)은 자기 주의 메커니즘 때문에 많은 비용이 듭니다. 모든 이전 토큰에 대해 키-값(KV) 캐시를 로드하고, 이는 배치 추론 시 주요 병목 현상이 됩니다. 기존의 방법들은 이러한 비용을 줄이기 위해 여러 기술을 제안했지만, 여전히 효과적인 변환기 기반 LM 아키텍처 개발은 어려운 과제로 남아 있습니다.

**주요 기여 및 혁신**:
- 새로운 글로벌-로컬 모델링 구조를 도입하여 KV 캐시의 병목 현상을 최소화.
- 블록 단위의 글로벌 모델링과 로컬 블록 내부에서 빠른 로컬 모델링을 통해 추론 성능을 크게 개선.

---

#### 2. Block Transformer Architecture
**요약**:
Block Transformer는 글로벌 및 로컬 주의 메커니즘을 결합한 계층적 패러다임을 사용합니다. 글로벌 컨텍스트는 하위 계층에서 코스 블록 레벨의 세분성으로 캡처되고, 로컬 종속성은 상위 계층에서 개별 토큰을 디코딩하는 방식으로 처리됩니다. 주요 구성 요소는 Embedder, Block Decoder, Token Decoder로 구성됩니다.

**주요 기여 및 혁신**:
- Embedder는 각 블록을 하나의 임베딩으로 집계.
- Block Decoder는 전체 블록 시퀀스에 대해 자기 주의를 적용하여 글로벌 종속성을 모델링.
- Token Decoder는 블록 내에서 로컬 종속성을 처리하고 개별 토큰을 디코딩.

---

#### 3. Inference Efficiency
**요약**:
Block Transformer는 벽 시간 병목 현상을 최소화하기 위해 설계되었습니다. 기존의 트랜스포머는 모든 이전 토큰과의 글로벌 자기 주의 때문에 배치 디코딩 처리량이 크게 저하됩니다. 코스-그레인드 글로벌 모델링은 KV 캐시 병목 현상을 완화하고, 로컬 디코딩은 프리필 및 KV 캐시 오버헤드를 거의 제거합니다.

**주요 기여 및 혁신**:
- 글로벌 모델링의 병목 현상을 하위 계층에 격리하여 비용을 줄임.
- 로컬 모델링은 높은 수준의 병렬 처리를 가능하게 하여 추론 처리량을 크게 향상.

---

### Overall Summary
Block Transformer는 기존 트랜스포머의 병목 현상을 해결하기 위해 글로벌 및 로컬 주의 메커니즘을 결합한 혁신적인 아키텍처입니다. 주요 기여는 글로벌 종속성을 처리하는 하위 계층과 로컬 종속성을 처리하는 상위 계층의 효율적인 결합입니다. 이 구조는 다양한 병목 현상을 완화하여 추론 효율성을 크게 향상시키며, 벽 시간 병목 현상을 최소화하면서도 높은 처리량을 유지합니다.

이 논문은 언어 모델 추론의 핵심 병목 현상을 해결하는 새로운 아키텍처를 제안하며, 이는 AI와 머신러닝 분야의 큰 발전을 의미합니다.