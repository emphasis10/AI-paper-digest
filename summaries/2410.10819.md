# DuoAttention: Efficient Long-Context LLM Inference with Retrieval and Streaming Heads
## TL;DR
## Summary
- [https://arxiv.org/pdf/2410.10819.pdf](https://arxiv.org/pdf/2410.10819.pdf)

1. 각 섹션의 중요 내용 요약

   - **서론**: 대형 언어 모델(LLM)의 발전으로 인해 대량의 문맥 토큰을 처리해야 하는 자연어 처리 및 비주얼 언어 응용 프로그램이 증가하고 있습니다. 이러한 문맥 처리 요구는 매우 큰 메모리와 계산 자원을 필요로 하며, DuoAttention은 효율적인 메모리 관리와 계산을 통해 이러한 문제를 해결하고자 합니다.
   
   - **DuoAttention 소개**: Retrieval Heads와 Streaming Heads로 구분하여 메모리 사용량과 지연 시간을 줄이는 새로운 프레임워크입니다. Retrieval Heads는 모든 토큰에 대해 완전한 주의를 필요로 하므로 전체 Key-Value(KV) 캐시를 사용하지만, Streaming Heads는 최근 토큰에만 집중하여 경량 캐시를 사용합니다.
   
   - **Retrieval과 Streaming Heads**: Retrieval Heads는 장문맥 처리를 위해 필수적이며 모든 토큰에 주의를 요합니다. 반면, Streaming Heads는 최신 토큰과 '주의 깔때기'에 집중할 수 있습니다.
   
   - **최적화 기반의 Retrieval Heads 식별**: DuoAttention은 경량화된 최적화 기반 절차를 통해 Retrieval Heads를 식별합니다. 이는 attention 패턴 분석에 의존하는 기존 방법보다 더 높은 압축률과 배포 효율성을 제공합니다.
   
   - **DuoAttention을 사용한 LLM 배포**: 이 방법은 메모리 사용을 극적으로 줄이고 LLM의 디코딩 속도를 향상시킵니다. Llama-3-8B와 같은 모델에서 큰 장문맥을 처리할 수 있는 능력을 가집니다.

2. 전체 요약

   DuoAttention은 메모리 사용량과 계산 효율성을 크게 향상시킬 수 있는 프레임워크로, LLM의 장문맥 응용 프로그램에서 특히 유용합니다. Retrieval Heads와 Streaming Heads의 차별화를 통해 메모리 사용량을 줄이고, 디코딩과 사전 충전의 속도를 개선하며, 최소한의 정확도 손실로 여러 모델에서 성능을 향상시킵니다. 이 접근법은 LLM이 백만 단위의 문맥을 처리할 수 있는 새로운 가능성을 열어줍니다.