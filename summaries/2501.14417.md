# DeepFlow: Serverless Large Language Model Serving at Scale
## TL;DR
## Summary
- [https://arxiv.org/pdf/2501.14417.pdf](https://arxiv.org/pdf/2501.14417.pdf)

### 1. 각 섹션의 주요 내용 요약 (한국어)

**1. 서론**
이 논문에서는 DEEPFLOW라는 AI 플랫폼을 소개한다. DEEPFLOW는 클라우드 환경에서 대규모 언어 모델(LLM)을 효율적으로 제공하기 위해 설계된 서버리스 플랫폼이다. Generative AI의 발전으로 LLM 서빙이 중요해졌으며, DEEPFLOW는 리소스 할당, 서빙 효율성 및 콜드 스타트 지연 같은 문제들을 해결하고자 한다. 

**2. 배경**
LLM과 Ascend NPU 칩에 대한 설명이 포함된다. LLM의 생성은 입력 프롬프트를 처리하고 키-값 벡터를 생성하는 "프리필" 단계와 새로운 토큰을 반복적으로 생성하는 "디코드" 단계로 나뉜다. Ascend NPU는 AI 모델을 위해 디자인된 칩이다.

**3. DEEPFLOW: 서버리스 AI 플랫폼**
DEEPFLOW는 사용자가 HTTP 요청을 통해 작업을 생성하고 이를 관리하는 서버리스 추상화를 기반으로 한다. 이를 통해 AI 워크로드를 동적으로 확장하고 리소스를 효율적으로 공유할 수 있게 된다. DEEPFLOW의 아키텍처는 작업 실행기(JE), 작업 실행기(TE), 클러스터 관리자 등으로 구성된다.

**4. FLOWSERVE: 효율적인 서빙 시스템**
FLOWSERVE는 DEEPFLOW의 서빙 엔진으로, 마이크로 커널 기반 설계와 NPU 중심 실행 등을 적용하여 효율적으로 LLM을 서빙한다. FLOWSERVE는 토크나이저, 모델 실행, 스케줄러, 메모리 및 캐싱 관리 기능을 포함한다.

**5. 분산 스케줄링**
여기서는 프리픽스 캐싱과 분리형 구성으로 인한 문제들을 해결하기 위한 스케줄링 알고리즘이 설명된다. 주로 지역 인식, PD 인식 및 결합 스케줄링 알고리즘이 제안된다.

**6. 빠른 스케일링**
DEEPFLOW는 리소스 할당과 스케일링을 위해 여러 가지 최적화 기법을 사용하여 64개의 인스턴스를 초에 호출할 수 있는 능력을 보여준다.

**7. 관련 연구**
서버리스 컴퓨팅, 서빙 엔진, 스케줄링 및 LLM 워크로드 확장을 위한 연구들을 검토한다.

**8. 결론**
DEEPFLOW는 클라우드 AI 플랫폼으로서 AI 워크로드 관리를 용이하게 한다. FLOWSERVE 엔진의 설계 원리와 효율적인 요청 처리를 위한 스케줄링을 설명하며, 빠른 스케일링 기술에 대한 분석도 포함되어 있다.

### 2. 전체 요약 (한국어)
이 논문은 클라우드 환경에서 대규모 언어 모델을 효율적으로 제공하기 위해 설계된 서버리스 AI 플랫폼인 DEEPFLOW를 소개한다. DEEPFLOW는 서버리스 추상화를 통해 다양한 AI 워크로드를 관리하고 동적으로 리소스를 할당하여 효율성을 높인다. 주요 기술로는 FLOWSERVE라는 서빙 엔진을 통해 NPU 중심의 설계 원리를 적용하고, 분산 스케줄링 알고리즘을 통해 성능을 극대화하며, 빠른 스케일링을 통해 최대 64개의 인스턴스를 초에 확장할 수 있는 능력을 갖춘 점이 특징이다. 이 플랫폼은 AI 클라우드 성능을 극대화하고, 서비스 수준 목표를 충족하는 데 중점을 두고 있다.