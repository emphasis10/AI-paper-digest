# ZeroQuant: Efficient and Affordable Post-Training Quantization for Large-Scale Transformers
## TL;DR
## Summary
- [https://arxiv.org/pdf/2206.01861.pdf](https://arxiv.org/pdf/2206.01861.pdf)

### 1. 요약
ZeroQuant는 대규모 트랜스포머 모델을 위한 효율적이고 저렴한 사후 훈련 양자화(post-training quantization, PTQ) 기법을 제안합니다. 이 논문은 BERT 및 GPT-3 스타일 모델에 대한 양자화 방법을 통해 모델의 메모리 사용량을 줄이고, 연산 효율성을 향상시키는 방법을 설명합니다.

### 2. 주요 기여 및 혁신
1. **하드웨어 친화적인 세분화 양자화(Fine-grained Hardware-friendly Quantization)**:
    - 모델의 가중치와 활성화를 INT8로 양자화하여 메모리 사용량과 연산 속도를 향상시킵니다.
    - 이로 인해 BERT 및 GPT-3 스타일 모델에서 FP16 추론에 비해 최대 5.19배/4.16배의 속도 향상을 이룹니다.

2. **저렴한 레이어별 지식 증류 알고리즘(Layer-by-layer Knowledge Distillation, LKD)**:
    - 원래 훈련 데이터를 사용하지 않고도 각 레이어별로 모델을 양자화할 수 있는 알고리즘을 개발했습니다.
    - 이를 통해 INT4 가중치와 INT8 활성화로 모델을 양자화하여 메모리 사용량을 3배 감소시킵니다.

3. **최적화된 양자화 시스템 백엔드**:
    - 양자화 및 디양자화 연산의 오버헤드를 제거하여 INT8 텐서 코어에서의 추론 지연 시간을 단축시킵니다.

### 3. 논문 각 섹션 요약

#### 1. 서론
대규모 자연어 모델은 다양한 응용 분야에서 널리 사용되며, BERT와 GPT-3 스타일 모델이 대표적입니다. 그러나 모델 크기의 증가로 인해 메모리와 연산 비용이 큰 문제로 대두되고 있습니다. 이 문제를 해결하기 위한 양자화 기술이 주목받고 있습니다.

#### 2. 관련 연구
모델 압축 및 양자화에 대한 기존 연구들을 검토하고, ZeroQuant가 기존 방법들과 차별화되는 점을 설명합니다. 대부분의 양자화 연구는 모델 재훈련이 필요하지만, ZeroQuant는 사후 훈련 양자화를 통해 재훈련 없이도 모델을 효과적으로 압축합니다.

#### 3. 배경 및 도전 과제
트랜스포머 모델과 양자화의 기본 개념을 설명하고, BERT 및 GPT-3 스타일 모델에 사후 훈련 양자화를 적용하는 데 있어 발생하는 주요 도전 과제를 다룹니다.

#### 4. 방법론
1. **세분화된 하드웨어 친화적인 양자화**:
    - 가중치 행렬과 활성화 토큰의 다양한 수치 범위를 세분화하여 양자화 정확도를 높입니다.
    - PyTorch와 같은 딥러닝 프레임워크에서 토큰별 양자화의 비용 문제를 해결하기 위해 최적화된 추론 백엔드를 구축했습니다.
  
2. **레이어별 지식 증류(LKD)**:
    - 원래 훈련 데이터에 접근하지 않고도 모델의 각 레이어를 순차적으로 양자화하는 방법입니다.
  
3. **양자화 최적화 트랜스포머 커널**:
    - 다양한 연산을 결합하여 데이터 이동 비용을 줄이고, 양자화 연산의 효율성을 높였습니다.

#### 5. 결과
1. **BERT 모델 결과**:
    - ZeroQuant는 BERTbase 모델에서 0.2포인트의 성능 저하만으로 INT8로 양자화할 수 있음을 보여줍니다.
  
2. **GPT-3 스타일 모델 결과**:
    - ZeroQuant는 GPT-3350M 모델에서 최대 4.16배의 추론 속도 향상을 이룹니다.

3. **BERT 및 GPT-3 스타일 모델의 지연 시간 감소**:
    - BERTbase와 BERTlarge 모델에서 최대 5.19배, GPT-3350M에서 최대 4.06배의 속도 향상을 보여줍니다.

4. **GPT-J6B 및 GPT-NeoX20B 사례 연구**:
    - ZeroQuant는 GPT-J6B 모델에서 FP16 대비 3.67배의 속도 향상을, GPT-NeoX20B에서 5.2배의 시스템 효율성 향상을 달성합니다.

#### 6. 결론
ZeroQuant는 대규모 트랜스포머 모델에 대한 효율적이고 저렴한 양자화 솔루션을 제공합니다. 이 방법은 모델의 메모리 사용량을 줄이고 연산 속도를 크게 향상시킵니다. BERT와 GPT-3 스타일 모델에서의 실험 결과는 이 접근법의 효과를 입증합니다.

### 전체 요약
ZeroQuant는 대규모 트랜스포머 모델을 효율적으로 압축하고 추론 성능을 향상시키기 위한 혁신적인 사후 훈련 양자화 방법을 제안합니다. 세분화된 하드웨어 친화적인 양자화와 레이어별 지식 증류 알고리즘을 통해, BERT와 GPT-3 스타일 모델에서 메모리 사용량을 줄이고 연산 속도를 크게 향상시킵니다. 이는 원래 훈련 데이터에 접근하지 않고도 가능한 방법으로, 실용적인 대규모 모델 배포에 큰 기여를 할 수 있습니다.