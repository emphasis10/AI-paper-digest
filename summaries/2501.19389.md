# Federated Sketching LoRA: On-Device Collaborative Fine-Tuning of Large Language Models
## TL;DR
## Summary
- [https://arxiv.org/pdf/2501.19389.pdf](https://arxiv.org/pdf/2501.19389.pdf)

### 1. 섹션 요약 및 주요 기여

#### 1.1 서론
최근 모바일 장치에서 대형 언어 모델(LLM)을 조정하는 데 대한 관심이 높아지고 있습니다. 기존의 방법들에서는 LoRA(저차원 적응) 기법이 연합 학습과 통합되어 데이터 부족 문제를 해결하고자 했습니다. 그러나, 장치의 다양성으로 인해 LoRA의 계수 범위가 제한되며, 이에 대한 효율적인 해결책이 필요합니다.

#### 1.2 기여
이 논문은 FSLoRA라는 새로운 프레임워크를 제안합니다. FSLoRA는 선택적으로 하위 행렬을 업데이트할 수 있는 스케치 방식의 업데이트를 도입합니다. 이 방법은 각 장치의 통신 및 계산 제약에 따라 조정될 수 있는 스케치 비율을 사용합니다. 이러한 기전에 따라, 리소스를 효율적으로 사용하면서도 더 높은 성능을 유지합니다.

#### 1.3 방법론
FSLoRA는 스케치 매트릭스를 통해 각 장치가 자원에 맞게 조정할 수 있는 점을 강조합니다. 이를 통해 최적의 조정 결과를 얻을 수 있는 방법을 제시하며, 다양한 장치의 연산능력 차이를 고려한 설계를 필요로 합니다.

#### 1.4 실험
FSLoRA는 다양한 데이터셋에서 성능을 평가받았으며, 기존의 방법들과 비교하여 성능 개선이 이루어졌음을 보여주었습니다. 특히, RoBERTa 및 LLaMA-3.2-3B 모델에서 모든 비교 기준에서 우수한 성과를 보였습니다.

### 2. 전체 요약
이번 논문은 자원 제약이 있는 장치들에서 대형 언어 모델을 효율적으로 조정하기 위한 FSLoRA 프레임워크를 제안합니다. 이 프레임워크는 스케치 매트릭스를 이용하여 각 장치가 업데이트할 하위 행렬을 선택적으로 조정할 수 있게 하여, 통신 및 계산 비용을 줄이는 동시에 성능 저하 없이 유연성을 제공합니다. 기존의 방법들과 비교할 때 FSLoRA는 통신의 효율성을 높이고 수렴 분석을 통해 사용자에게 보다 이론적으로 뒷받침된 결과를 제공합니다. 이로 인해 FSLoRA는 리소스가 제한된 환경에서도 보다 적응력 있는 조정이 가능한 파라미터 기반의 방법론이라고 할 수 있습니다.