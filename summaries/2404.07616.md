# Audio Dialogues: Dialogues dataset for audio and music understanding
## TL;DR
## Summary
- [https://arxiv.org/pdf/2404.07616.pdf](https://arxiv.org/pdf/2404.07616.pdf)

논문 "Audio Dialogues"는 오디오와 음악 이해를 위한 다중 턴 대화 데이터셋을 소개하며, 기존의 단일 턴 대화 데이터셋의 한계를 극복하고자 합니다. 이 데이터셋은 일반적인 소리와 음악을 포함한 163.8k의 샘플을 바탕으로 구성되어 있으며, 오디오 증강된 대규모 언어 모델의 복잡성과 적용 가능성을 시험하기 위해 사용됩니다.

### 1. 서론
오디오는 인간의 의사소통과 상호작용에서 필수적인 요소로, 음성, 음악, 일반적인 및 주변 소리 등 다양한 정보를 포함합니다. 이를 이해하기 위한 모델 개발은 오디오 및 사운드 모니터링, 음성 인식, 음악 추천 시스템 개발, 청각 장애인 지원 등 여러 분야에서 중요한 역할을 합니다.

### 2. 관련 연구
기존의 오디오 이해 모델은 대규모 언어 모델과의 통합을 통해 강화되었습니다. 이러한 통합은 LLM의 지식 유지, 추론 및 작업 수행 능력을 활용하는 데 있어 매우 유용합니다. 다중 턴 대화 능력을 오디오 증강된 LLM에 확장하는 것이 필요합니다.

### 3. 데이터 생성 파이프라인
"Audio Dialogues" 데이터셋은 AudioSet과 MusicCaps에서 제공하는 캡션 주석을 바탕으로 GPT-4를 사용하여 생성되었습니다. 이는 프롬프트 기반 접근 방식을 사용하고 데이터 필터링 전략을 적용하여 신뢰할 수 있는 대화만을 유지합니다.

### 4. 오디오 대화 데이터셋
이 데이터셋은 훈련 및 평가 분할을 포함하여 다중 턴 대화 데이터셋을 제공합니다. 복잡한 맥락과 강한 라운드 간 상관관계를 가진 대화를 생성하여 오디오 샘플에 대한 더 나은 이해와 대응을 가능하게 합니다.

### 5. 실험
최근 오디오 이해 모델을 이용하여 "Audio Dialogues" 데이터셋에서의 성능을 평가했습니다. 이는 모델이 사용자와의 상호작용을 통해 보다 정교하고 뉘앙스 있는 반응을 생성할 수 있음을 보여줍니다.

### 6. 토론
이 논문은 오디오 이해를 개선하기 위한 다중 턴 대화 커버리지를 제공합니다. 프롬프트 기반 접근과 기존 데이터셋의 캡션 주석을 활용하여 품질이 높은 대화를 생성하는 것이 주요 목표입니다. 미래 연구 방향으로는 오디오에 대한 답변을 시간적으로 더 잘 구체화하는 방법 등이 있습니다.

## Similar Papers
- [PicoAudio: Enabling Precise Timestamp and Frequency Controllability of Audio Events in Text-to-audio Generation](2407.02869.md)
- [Task Me Anything](2406.11775.md)
- [Associative Recurrent Memory Transformer](2407.04841.md)
- [OmniCorpus: A Unified Multimodal Corpus of 10 Billion-Level Images Interleaved with Text](2406.08418.md)
- [Vibravox: A Dataset of French Speech Captured with Body-conduction Audio Sensors](2407.11828.md)
- [Video-MME: The First-Ever Comprehensive Evaluation Benchmark of Multi-modal LLMs in Video Analysis](2405.21075.md)
- [PLLaVA : Parameter-free LLaVA Extension from Images to Videos for Video Dense Captioning](2404.16994.md)
- [Speech-MASSIVE: A Multilingual Speech Dataset for SLU and Beyond](2408.03900.md)
- [MiniGPT4-Video: Advancing Multimodal LLMs for Video Understanding with Interleaved Visual-Textual Tokens](2404.03413.md)
