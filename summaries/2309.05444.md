# Pushing Mixture of Experts to the Limit: Extremely Parameter Efficient MoE for Instruction Tuning
## TL;DR
## Summary
- [https://arxiv.org/pdf/2309.05444.pdf](https://arxiv.org/pdf/2309.05444.pdf)

이 논문은 "Pushing Mixture of Experts to the Limit: Extremely Parameter Efficient MoE for Instruction Tuning"이라는 제목의 연구로, "Mixture of Experts(MoE)"라는 신경망 아키텍처를 활용하여, 특화된 하위 모델들의 앙상블이 전체 성능을 최적화하면서도 상수의 계산 비용을 유지할 수 있도록 하는 연구입니다. 기존의 MoE 모델들이 규모가 클 때 메모리에 모든 전문가를 저장해야 한다는 문제점을 해결하기 위해, 매우 파라미터 효율적인 MoE를 제안하며, 이를 통해 표준 파라미터 효율적인 미세조정(PEFT) 방법들을 능가하고 전체 미세조정과 비슷한 성능을 달성합니다.

### 1. 도입

- MoE는 다양한 입력 유형에 특화된 하위 모듈(전문가)을 통해 조건부 계산을 강조합니다. 이는 고정된 추론 비용과 같은 중요한 효율성 이점을 제공합니다. 본 연구에서는 MoE를 지시사항 미세조정 설정에 적용 가능한지를 탐구합니다.

### 2. 방법론

- 이 연구는 (IA)³와 LORA 어댑터를 활용한 파라미터 효율적 미세조정과, 매우 파라미터 효율적인 MoE에 대해 설명합니다. (IA)³와 LORA는 기존 사전 학습된 모델에 소량의 파라미터를 추가하여 효율적으로 미세조정을 가능하게 합니다.

### 3. 실험

- 공개 프롬프트 모음인 P3 데이터셋을 사용하여 지시사항 튜닝 실험을 진행했습니다. T5 모델의 다양한 크기(770M에서 11B에 이르는)에 대한 실험을 수행했습니다.

### 4. 결과 및 토론

- MoE 변형은 표준 PEFT 방법들과 전체 미세조정 대비 뛰어난 성능을 보였습니다. 특히, MoV와 MoLORA는 보다 적은 파라미터 업데이트로 전체 미세조정과 유사하거나 더 우수한 성능을 달성했습니다. 또한, 파라미터 효율적인 MoE는 기본 모델 크기와 전문가의 수에 따라 어떻게 확장되는지에 대한 통찰력을 제공했습니다.

### 5. 결론

- 본 연구는 매우 파라미터 효율적인 MoE를 통해 지시사항 기반 미세조정이 큰 모델에서도 효율적으로 수행될 수 있음을 보였습니다. 이는 파라미터 효율적인 기술과 결합될 때, 메모리 요구 사항을 크게 줄이면서도 우수한 성능을 달성할 수 있음을 시사합니다.

### 전반적인 요약

이 연구는 MoE 아키텍처를 효율적으로 활용하여, 큰 규모의 모델에서도 파라미터 효율적인 미세조정이 가능하게 하는 새로운 방법을 제안합니다. 특히, (IA)³와 LORA 어댑터를 활용하여, 매우 적은 수의 파라미터만을 업데이트하면서도 기존의 PEFT 방법을 능가하는 성능을 보여줍니다. 이는 AI와 머신러닝 분야에서 큰 모델을 효율적으로 활용할 수 있는 새로운 가능성을 열어줍니다.

## Similar Papers
- [Lory: Fully Differentiable Mixture-of-Experts for Autoregressive Language Model Pre-training](2405.03133.md)
- [Back to Basics: Revisiting REINFORCE Style Optimization for Learning from Human Feedback in LLMs](2402.14740.md)
- [AdaMix: Mixture-of-Adaptations for Parameter-efficient Model Tuning](2205.12410.md)
- [Efficient LLM Inference on CPUs](2311.00502.md)
- [Mixture of A Million Experts](2407.04153.md)
- [RouteLLM: Learning to Route LLMs with Preference Data](2406.18665.md)
- [TIES-Merging: Resolving Interference When Merging Models](2306.01708.md)
- [MoRA: High-Rank Updating for Parameter-Efficient Fine-Tuning](2405.12130.md)
- [Mixture of Nested Experts: Adaptive Processing of Visual Tokens](2407.19985.md)
