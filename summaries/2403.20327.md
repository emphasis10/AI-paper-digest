# Gecko: Versatile Text Embeddings Distilled from Large Language Models
## TL;DR
## Summary
- [https://arxiv.org/pdf/2403.20327.pdf](https://arxiv.org/pdf/2403.20327.pdf)

### 1. 섹션별 요약 및 주요 내용

#### 1. 소개 (Introduction)
이 논문에서는 LLM(대형 언어 모델)의 광범위한 세계 지식을 바탕으로 다기능 텍스트 임베딩 모델인 Gecko를 소개합니다. 이 모델은 질의-응답 쌍을 생성하고, 긍정 예제와 부정 예제를 LLM을 통해 리랭킹하여 최적의 텍스트 임베딩을 만듭니다. Gecko는 다양한 작업에서 강력한 성능을 보이며 여러 작업을 지원합니다.

#### 2. 합성 데이터 생성 (Synthetic Data Generation)
텍스트 임베딩 모델을 새로운 작업과 도메인에 적용하기 위해, 몇 샷 프롬프트를 통한 LLM을 이용해 합성 데이터를 생성합니다. 이 작업은 특정 도메인에 적용하는 대신, 일반적인 LLM의 지식을 텍스트 임베딩 모델로 증류하여 MTEB 벤치마크에서 강력한 성과를 냅니다.

#### 3. Gecko 훈련 레시피 (Training Recipe for Gecko)
Gecko는 사전 학습된 1.2B 매개변수 트랜스포머 언어 모델을 기반으로 하며 두 번의 추가 훈련 단계를 거칩니다. 먼저, 자가 지도 학습 작업을 포함하여 광범위한 텍스트 다양성을 노출합니다. 그 후, LLM 증류기를 통해 생성된 질의에 대해 긍정 예제와 어려운 부정 예제를 식별하여 다양한 다운스트림 작업용 데이터 세트를 만듭니다.

#### 4. 실험 (Experiments)
Gecko는 다양한 작업에서 다재다능한 텍스트 임베딩 모델을 만들기 위해 MTEB(대규모 텍스트 임베딩 벤치마크)을 평가합니다. 다양한 컴포넌트들이 Gecko와 FRet의 성능에 어떻게 기여하는지 분석하여 이질적인 텍스트 임베딩 모델을 구축하는 데 있어서의 통찰력을 제공합니다. Gecko는 비슷하거나 더 큰 모델보다 더 나은 성능을 보이며, 특히 다중 언어 작업에서도 우수한 성능을 보입니다.

#### 5. 결론 (Conclusion)
Gecko는 LLM을 통해 생성된 합성 데이터 세트인 FRet을 이용하여 훈련된 다기능 텍스트 임베딩 모델입니다. LLM을 활용하여 긍정 및 부정 예제를 식별하여 성능을 크게 향상시켰음을 보여줍니다. 이 모델은 제로 샷 벤치마크에서도 강력한 성과를 냅니다.

### 2. 전체 요약
Gecko는 LLM의 지식을 활용하여 다목적 텍스트 임베딩 모델을 만든 혁신적인 접근 방식을 제시합니다. 이는 두 번의 LLM 증류를 통해 생성된 합성 데이터 세트를 활용하여 다양한 작업에서 우수한 성능을 발휘합니다. Gecko의 주요 기여는 다양한 작업을 하나의 모델로 지원하는 것이며, 특히 데이터 생성의 질을 개선함으로써 제로 샷 학습에서도 강력한 성과를 보여줍니다. 이 모델은 텍스트 임베딩의 새로운 기준을 세우며 향후 AI 연구에 큰 기여를 할 것입니다.