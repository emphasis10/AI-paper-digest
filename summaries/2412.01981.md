# Free Process Rewards without Process Labels
## TL;DR
## Summary
- [https://arxiv.org/pdf/2412.01981.pdf](https://arxiv.org/pdf/2412.01981.pdf)

### 섹션별 요약:

1. **서론 (Introduction)**:
   이 논문은 AI에서 주로 사용되는 두 가지 모델, 즉 결과 보상 모델(ORM)과 과정 보상 모델(PRM)을 소개합니다. ORM은 완성된 응답을 평가하는 데 반해, PRM은 중간 과정을 평가하여 더 세부적인 보상을 제공할 수 있습니다. PRM의 문제점은 모든 중간 단계에 라벨을 지정해야 하는 어려움입니다.

2. **이론적 배경 (Theoretical Background)**:
   PRM은 로그-우도로 보상 함수가 매개 변수화되어, 데이터를 수집하거나 추가적인 라벨링 작업 없이 ORM을 훈련하는 것만으로 학습될 수 있습니다. 이론적으로 PRM은 최적화 목표와 상관없이 잘 작동할 수 있음을 보여줍니다.

3. **방법론 (Methodology)**:
   저자들은 ORM을 사용하여 PRM을 암묵적으로 학습하는 방법을 제안합니다. 이 방법은 과정을 매개 변수화하여 응답 수준의 데이터만으로 PRM을 효과적으로 생성할 수 있도록 합니다.

4. **실험 결과 (Experimental Results)**:
   다양한 학습 목표를 통해 암묵적인 PRM을 MATH 데이터셋에서 테스트한 결과, 효율성과 성능에서 기존의 강력한 기준 모델보다 나은 성과를 보였습니다. 특히, 응답의 수가 증가할수록 성과가 개선되는 점을 관찰했습니다.

5. **결론 (Conclusion)**:
   이 논문은 PRM을 훈련하기 위해 방대한 양의 데이터를 필요로 하지 않는 새로운 접근법을 제안합니다. PRM의 성능을 대규모 샘플링과 결합하여 더욱 향상시킬 수 있습니다. 이러한 결과는 최종적으로 데이터 획득과 훈련의 오버헤드를 크게 줄인 것을 보여줍니다.

### 논문의 주요 기여와 혁신:

이 연구는 PRM을 훈련하기 위한 전통적 방법의 문제점을 해결하기 위해 ORM을 활용하여 PRM을 암묵적으로 학습할 수 있는 방법론을 제안합니다. 이는 기존 방법들보다 훨씬 효율적이며, 데이터 수집과 처리에 드는 비용을 크게 절감할 수 있는 혁신적인 접근법입니다. 이는 AI 훈련 모델의 접근성을 높입니다.

### 종합 요약:

이 논문은 과정 보상 모델(PRM)을 데이터 효율적으로 훈련하기 위한 새로운 접근을 제시합니다. 이는 PRM을 결과 보상 모델(ORM)을 활용하여 암묵적으로 훈련함으로써, 중간 단계 라벨링의 어려움을 해결합니다. 다양한 실험을 통해 이 접근법이 기존 모델들보다 성능이 우수하고 데이터 획득에 드는 비용도 절감할 수 있음을 입증했습니다. 따라서, 이 연구는 AI 및 머신러닝 분야에서 모델의 학습 가능성과 효율성을 상당히 향상시킬 수 있는 중요한 기여를 합니다.