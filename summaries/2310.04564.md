# ReLU Strikes Back: Exploiting Activation Sparsity in Large Language Models
## TL;DR
## Summary
- [https://arxiv.org/pdf/2310.04564.pdf](https://arxiv.org/pdf/2310.04564.pdf)

### 1. **서론 및 배경**
연구의 배경에서는 대규모 언어 모델(Large Language Models, LLMs)이 인공 지능 응용 프로그램을 혁신적으로 변화시켰지만, 추론 단계에서의 높은 계산 요구 사항이 자원 제한 장치에서의 배포를 어렵게 만든다고 설명합니다. 이 연구는 ReLU(Rectified Linear Unit) 활성화 함수를 재조명하며, 이를 통해 메모리 및 계산 효율성을 크게 향상시키는 방법을 제시합니다.

### 2. **ReLU 활성화 함수의 장점**
ReLU를 사용하면 활성화 패턴에서의 희소성(sparsity)이 증가하며, 이는 대규모 모델에서 추론 계산을 크게 줄일 수 있습니다. 이 연구에서는 다양한 LLM에 ReLU를 적용하여 기존의 GELU나 SiLU 등 다른 활성화 함수를 사용할 때보다 효율적인 결과를 도출합니다. ReLU는 특히 토큰 생성시 재사용되는 활성화된 뉴런을 통해 새로운 토큰을 생성하는 데 있어서 중요한 역할을 합니다.

### 3. **성능 평가 및 응용**
ReLU를 적용한 모델은 다양한 추론 및 읽기 이해 작업에서 빠르게 원래 성능을 회복함을 보여줍니다. 또한, 추가적인 ReLU 층을 삽입하여 추론 단계에서의 FLOPS(초당 부동 소수점 연산)를 최대 세 배까지 줄일 수 있는 방법을 제시합니다.

### **혁신적인 부분**
이 연구의 혁신적인 부분은 기존의 활성화 함수를 ReLU로 대체하여 활성화 패턴의 희소성을 이용하고, 이를 통해 메모리 및 계산 효율성을 크게 향상시킨다는 점입니다. 이러한 접근 방식은 특히 메모리가 제한된 장치에서 LLMs의 배포를 용이하게 하며, 실시간 처리가 필요한 응용 프로그램에 매우 유용할 수 있습니다.

이 연구는 큰 규모의 언어 모델의 효율성을 개선하는 데 중요한 발전을 나타내며, 향후 이 분야의 연구에 좋은 영향을 미칠 것으로 기대됩니다.