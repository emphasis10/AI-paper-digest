# FeedbackEval: A Benchmark for Evaluating Large Language Models in Feedback-Driven Code Repair Tasks
## TL;DR
## Summary
- [https://arxiv.org/pdf/2504.06939.pdf](https://arxiv.org/pdf/2504.06939.pdf)

1. 각 섹션의 요약:

- **서론**: 연구의 배경과 목표를 설명합니다. 주로 코드 수정을 위한 대규모 언어 모델(LLMs)의 활용 가능성을 탐색합니다.

- **LLMs 기반의 코드 생성 및 수정**: 최근 LLMs의 발전은 코드 생성 및 수정을 크게 향상시켰습니다. 코드 LLMs는 코드 중심 작업을 위해 설계되었으며, 테스트 생성, 취약점 탐지 등 다양한 응용 프로그램에 사용됩니다.

- **FeedbackEval 벤치마크**: 이 벤치마크는 LLM의 피드백 기반 코드 수리 능력을 평가하는 데 중점을 둡니다. 다양한 오류 유형과 피드백 시나리오를 포함하여 LLMs가 어떻게 피드백을 처리하고 적응하는지를 체계적으로 평가합니다.

- **실증 연구**: 다섯 가지 최첨단 LLMs를 사용하여 피드백 기반 코드 수리 작업 수행 능력을 평가합니다. 피드백의 단일 및 반복 사용 시 효과를 분석하였습니다.

- **결론**: FeedbackEval 벤치마크의 주요 기여는 다양한 오류 유형을 포함하는 포괄적인 벤치마크 제공, 적응성 및 피드백 이해에 대한 새로운 통찰력 제시, 코드 수리 작업 시 최적화된 프롬프트 기법 권장.

2. 전체 요약:

이 문서는 플록 기반 코드 수리 작업에서 LLMs의 성능을 평가하기 위한 완전한 벤치마크인 FeedbackEval을 개발하여 LLM의 피드백 이해 및 적응 능력을 체계적으로 평가합니다. 주요 연구 결과는 테스트 피드백이 가장 높은 성과를 내며, 단순 피드백로도 LLMs가 적절한 수정 방향을 파악할 수 있다는 점을 강조합니다. 이 연구는 LLMs가 구조화된 피드백을 통해 오류 수정 작업을 더욱 효과적으로 수행할 수 있음을 보여줍니다.