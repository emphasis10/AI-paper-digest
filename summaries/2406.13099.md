# Sampling 3D Gaussian Scenes in Seconds with Latent Diffusion Models
## TL;DR
## Summary
- [https://arxiv.org/pdf/2406.13099.pdf](https://arxiv.org/pdf/2406.13099.pdf)

### 1. 섹션별 요약

#### Abstract
이 논문에서는 2D 이미지 데이터를 사용하여 3D 장면을 생성하는 잠재 확산 모델(latent diffusion model)을 제안합니다. 이 모델은 자동 인코더(autoencoder)를 사용하여 여러 시점의 이미지를 3D Gaussian splats로 변환하고, 이러한 splats의 압축된 잠재 표현을 생성합니다. 그런 다음, 다중 시점 확산 모델을 사용하여 잠재 공간에서 효율적인 생성 모델을 학습합니다. 이 접근법은 깊이(depth)나 객체 마스크가 필요하지 않으며, 다양한 카메라 위치에서 복잡한 장면을 처리할 수 있습니다. 이 논문은 MVImgNet과 RealEstate10K 두 대규모 데이터셋에서 실험을 통해, 초당 3D 장면 생성이 가능하며, 단일 시점 또는 드문 시점의 입력으로도 높은 품질의 3D 장면을 생성할 수 있음을 보여줍니다.

#### Introduction
3D 세계의 분포를 학습하는 생성 모델은 지능형 에이전트 개발뿐만 아니라 게임 및 시각 효과 같은 여러 응용 분야에 유용합니다. 그러나 대규모 포토리얼리스틱 3D 장면 데이터셋이 부족하여, 주로 이미지 기반의 데이터셋을 활용하는 방법론이 필요합니다. 이 논문에서는 다중 시점 이미지 데이터셋을 직접적으로 사용해 3D 생성 모델을 개발하는 것이 체계적이고 효율적임을 제안합니다. 기존 방법들은 희소 시점에서의 3D 장면 생성을 위해 많은 자원이 소모되는 반면, 이 연구는 압축된 잠재 공간에서의 확산 과정을 통해 이를 최적화합니다.

#### Method
제안된 방법은 가우시안 스플랫(Gaussian Splat)을 사용하여 3D 장면을 표현합니다. 이 방법은 높은 재구성 품질과 학습/렌더링 속도 사이의 균형을 제공합니다. 먼저 다중 시점 이미지를 통해 자동 인코더를 학습하여 3D 장면을 잠재 공간으로 압축한 후, 이 잠재 공간에서 확산 모델을 학습합니다. 이는 조건부/비조건부 생성 모두를 효과적으로 수행할 수 있습니다.

#### Results and Discussions
제안된 모델은 이전의 3D 생성 모델보다 20배 빠르게 3D 장면을 생성할 수 있으며, 다양한 작업에 활용 가능합니다: 무조건 생성, 단일 이미지 3D 재구성, 드문 시점의 3D 재구성 등. 이 모델은 깊이 정보나 세그멘테이션이 필요 없으며, 단순히 다중 시점 이미지만으로 학습할 수 있습니다. 또한, 제안된 방법은 제한된 자원 내에서 더 나은 결과를 제공합니다.

#### Conclusion
이 연구의 핵심 공헌은 다중 시점 이미지에서 추출한 압축된 잠재 표현을 통해 가우시안 스플랫을 사용하여 3D 장면을 낮은 비용으로 빠르게 생성할 수 있는 첫 번째 생성 모델을 제안한 것입니다. 중요한 기술적 기여는 새로운 3D 인식 자동 인코더 아키텍처와 잠재 공간에서의 확산 모델 학습 방법입니다.

### 2. Overall Summary
이 논문은 다중 시점 이미지 데이터를 사용하여 3D 장면을 빠르고 효율적으로 생성할 수 있는 새로운 잠재 확산 모델을 제안합니다. 여러 시점의 이미지를 3D Gaussian Splats로 변환하고 이를 압축된 잠재 표현으로 변환하는 자동 인코더를 사용합니다. 이 모델은 다양한 작업에 활용될 수 있으며, 기존 모델보다 최대 20배 빠른 속도로 3D 장면을 생성할 수 있습니다. 깊이 정보나 객체 마스크 없이 학습이 가능하며, 단일 시점 또는 드문 시점에서도 높은 품질의 결과를 제공합니다. 전반적으로 이 연구는 3D 세계의 분포를 학습하고 샘플링하는 새로운 방법론을 제시하여, 기계 학습 및 컴퓨터 비전 분야에 중요한 기여를 합니다.