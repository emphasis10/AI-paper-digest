# Learning to Refuse: Towards Mitigating Privacy Risks in LLMs
## TL;DR
## Summary
- [https://arxiv.org/pdf/2407.10058.pdf](https://arxiv.org/pdf/2407.10058.pdf)

### 섹션 요약

#### 1. 서론 (Introduction)
AI와 머신러닝 모델은 텍스트 이해 및 생성 능력에서 탁월한 성과를 보이지만, 민간 정보를 기억하기 때문에 프라이버시 침해 우려가 있습니다. 이를 해결하기 위해 전부 재학습하는 대신 머신 언러닝(MU) 방법을 사용하는 것이 제안되었습니다.

#### 2. 관련 연구 (Related Work)
기존 연구들은 LLM의 민간 정보 기억 문제를 다양한 방법으로 해결하려고 노력했지만, 이 중 많은 방법은 하이퍼파라미터에 민감하거나, 모델 성능을 크게 떨어뜨릴 수 있습니다.

#### 3. RETURN: 실세계 개인 데이터 언러닝 (Real-world pErsonal daTa UnleaRNing)
RETURN 데이터셋은 위키피디아와 GPT-4를 이용해 만든 2,492명의 개인 정보와 20개의 Q&A 쌍으로 구성됩니다. 이 데이터셋을 통해 모델이 특정 개인의 정보를 보호하면서도 다른 정보를 정상적으로 처리하는 능력을 평가할 수 있습니다.

#### 4. LLM을 위한 머신 언러닝 (Machine Unlearning for LLMs)
네 가지 주요 MU 방법이 포함됩니다: Gradient Ascent, Negative Preference Optimization, Relabeled Gradient Descent, Relabeled Direct Preference Optimization. 이 방법들은 Forget Set과 Retain Set을 사용해 균형적인 프라이버시 보호와 모델 성능 유지를 목표로 합니다. 그 중 Name-Aware Unlearning Framework (NAUF)는 이름 인식 거부 응답과 대조 데이터 증식을 포함하여 프라이버시 보호 성능을 개선합니다.

#### 5. 평가 (Evaluation)
모델의 Forget Score와 Retain Score를 계산해 개인 정보 보호 능력과 일반 성능을 평가합니다. 연구 결과 NAUF가 가장 높은 평균 언러닝 점수를 기록하며, GB 정규화를 통해 최고의 성능 균형을 달성했습니다.

#### 6. 결론 (Conclusion)
NAUF는 프라이버시 보호를 위한 강력한 방법이며, RETURN과 함께 사용 시 높은 성능을 보입니다. 그러나 특정 정보에 대한 정밀한 보호는 불가능하므로, 향후 연구에서는 인간 판단을 반영한 모델 개선을 목표로 합니다.

### 전체 요약

이 논문은 대형 언어 모델(LLM)이 특정 개인의 정보를 저장함으로써 발생하는 프라이버시 문제를 해결하기 위한 새로운 접근법을 제안합니다. RETURN 데이터셋과 NAUF 프레임워크를 도입해 LLM이 특정 개인의 프라이버시를 보호하면서도 다른 정보 처리 성능을 유지할 수 있도록 설계되었습니다. NAUF는 이름 인식 거부 응답과 대조 데이터 증식을 포함하며, 기존의 방법보다 더 나은 성능을 보였습니다. 이 연구는 LLM의 민간 정보 보호를 위한 새로운 가능성을 열어줍니다.

## Similar Papers
- [To Forget or Not? Towards Practical Knowledge Unlearning for Large Language Models](2407.01920.md)
- [AutoRAG-HP: Automatic Online Hyper-Parameter Tuning for Retrieval-Augmented Generation](2406.19251.md)
- [RichRAG: Crafting Rich Responses for Multi-faceted Queries in Retrieval-Augmented Generation](2406.12566.md)
- [Practical Unlearning for Large Language Models](2407.10223.md)
- [UnUnlearning: Unlearning is not sufficient for content regulation in advanced generative AI](2407.00106.md)
- [Red Teaming GPT-4V: Are GPT-4V Safe Against Uni/Multi-Modal Jailbreak Attacks?](2404.03411.md)
- [A Tale of Trust and Accuracy: Base vs. Instruct LLMs in RAG Systems](2406.14972.md)
- [Privacy Preserving Prompt Engineering: A Survey](2404.06001.md)
- [Explore the Limits of Omni-modal Pretraining at Scale](2406.09412.md)
