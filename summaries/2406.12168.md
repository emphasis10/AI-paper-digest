# BPO: Supercharging Online Preference Learning by Adhering to the Proximity of Behavior LLM
## TL;DR
## Summary
- [https://arxiv.org/pdf/2406.12168.pdf](https://arxiv.org/pdf/2406.12168.pdf)

### 1. 섹션별 요약 및 주요 기여 내용

#### 1. 서론(Introductory Section)
- **요약**: 이 논문에서는 RLHF(인간 피드백을 통한 강화 학습)를 통해 대형 언어 모델(LLM)을 인간의 선호도에 맞추는 방법을 연구합니다. 전통적인 RLHF 방법은 보상 모델(RM) 학습을 포함하는 두 단계의 학습 과정으로 매우 복잡하고 불안정합니다.
- **주요 기여**: 이 작업은 모델을 직접 선호 데이터에서 맞춤화할 수 있는 Direct Preference Optimization(DPO) 등의 방법을 제안하며, 이를 통해 보상 모델 학습 단계가 필요하지 않게 합니다.

#### 2. 관련 연구(Related Work)
- **요약**: 기존 연구에서는 RLHF와 이를 위한 다양한 방법을 다룹니다. Direct Alignment from Preferences (DAP) 방법은 정적, 사전 수집된 선호 데이터셋에서 학습하여 보상 모델을 배제하고 학습 안정성을 높입니다. 해당 논문에서는 다양한 DAP 방법의 문제점을 지적하고 개선된 방안을 제시합니다.

#### 3. 배경(Preliminaries)
- **요약**: DAP 방법에서는 상벌학습을 통해 대형 언어 모델을 학습시키기 위해 인간의 선호 데이터셋을 사용합니다. 또한, 학습 중간에 생성된 새로운 데이터 샘플을 통해 학습하는 온라인 학습과 사전 수집된 정적 데이터셋을 사용하는 오프라인 학습의 차이를 설명합니다.
- **주요 기여**: 학습 데이터가 더 정교할수록 모델의 성능이 향상된다는 점을 입증합니다. 이로 인해 모델 학습 간 차이 이해를 돕습니다.

#### 4. 온라인 DAP 향상을 위한 신뢰 구역 구축(Improving Online DAP by Constructing a Better Trust Region)
- **요약**: 본 논문에서는 온라인 DAP 방법의 문제점을 해결하기 위해 중요한 참조 모델을 사용한 신뢰 구역 구성 방법을 제안합니다.
- **주요 기여**: KL 발산을 최소화하여 학습과 참조 모델 사이의 차이를 줄임으로써 학습 안정성을 높였습니다.

#### 5. 실험(Experiments)
- **요약**: 다양한 데이터셋(TL;DR, Helpfulness, Harmlessness)을 사용하여 실험을 진행했습니다. 온라인 DPO와 오프라인 DPO 모두에 대해 다양한 상황에서의 성능을 비교 분석했으며, 여러 LoRA 가중치를 사용하여 학습 안정성을 높였습니다.
- **주요 기여**: 실험 결과 BPO(DPO)가 기존의 온라인 및 오프라인 DAP와 비교하여 더 높은 성능을 보였음을 보여주었습니다.

#### 6. 결론(Conclusion)
- **요약**: BPO는 학습 모델과 행위 모델 사이의 발산을 최소화하여 온라인 DAP의 학습 안정성과 성능을 높였습니다. 다양한 선호도 주석 빈도에 따라서도 높은 성능을 유지합니다.
- **주요 기여**: 새로운 DAP 방법이 안정성과 성능을 동시에 제공함을 입증하였습니다.

#### 7. 한계(Limitations)
- **요약**: 이 연구에서는 Low-Rank Adaptations(LoRAs)의 앙상블을 사용한 학습 안정화에 집중했으며, 미래 연구에서는 이러한 방법 외에 추가적인 안정화 기술을 연구할 필요가 있습니다.
- **주요 기여**: BPO의 학습 안정성을 추가적인 방법을 통해 개선할 가능성을 제안합니다.

### 2. 전체 요약 (전반적 개요)
이 논문은 인간의 선호도에 따르는 대형 언어 모델(LLM)의 학습 방법을 다루며, 기존의 복잡하고 불안정한 RLHF 방법을 개선하기 위한 새로운 Direct Preference Optimization(DPO) 방법을 제안합니다. 주요 기여는 학습 중 인간의 선호도를 직접적으로 반영하여 모델의 학습 효율성과 안정성을 높이는 것입니다. 다양한 데이터셋을 활용한 실험 결과, 제안된 BPO(DPO) 방법이 기존의 DAP 방법에 비해 우수한 성능을 보였으며, 학습 과정에서의 안정성도 증가시켰음을 확인하였습니다.