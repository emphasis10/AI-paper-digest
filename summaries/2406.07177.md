# TernaryLLM: Ternarized Large Language Model
## TL;DR
## Summary
- [https://arxiv.org/pdf/2406.07177.pdf](https://arxiv.org/pdf/2406.07177.pdf)

### 1. 각 섹션의 요약

#### 서론 (Introduction)
대형 언어 모델(LLM)은 다양한 언어 작업에서 놀라운 성능을 보여주었지만, 큰 연산비용과 메모리 요구사항 때문에 실용적인 적용에 제한이 있습니다. 이 논문은 메모리 사용량을 줄이고 에너지 효율적인 부동 소수점 연산을 가능하게 하는 3진 양자화(ternarization)에 대해 다룹니다. 3진 양자화를 적용하기 위해, 우리는 비대칭적인 이상치와 비영점 평균 분포를 해결하는 듀얼 학습 가능한 3진 양자화(DLT)와 매우 낮은 비트 양자화로 인한 정보 손실을 회복하는 이상치 친화적 특성 지식 증류(OFF)를 제안합니다.

#### 관련 연구 (Related Work)
양자화는 모델 추론 속도를 가속화하기 위해 널리 응용되어 왔습니다. 최근에는 LLM에서도 양자화가 널리 사용되며, 이는 가중치-활성화 양자화와 가중치만 양자화로 구분됩니다. 가중치-활성화 양자화는 부동 소수점 가중치와 활성화를 낮은 비트 정수로 양자화하는 반면, 가중치만 양자화는 가중치만을 낮은 비트로 변환하고 활성화는 부동 소수점으로 유지합니다.

#### 3진 양자화 (Motivation and Background)
3진 양자화는 부동 소수점 가중치를 -1, 0, 1의 세 값으로 변환하는 과정입니다. 이전 방법들은 비대칭적인 이상치와 비영점 평균 분포를 처리하는 데 적합하지 않으며, 매우 낮은 비트 양자화는 중요한 정보 손실을 초래합니다. 따라서 이 논문에서는 비대칭적 가중치 분포를 학습하여 조정할 수 있는 DLT와 이상치 친화적 OFF를 사용하여 이러한 문제를 해결하고자 합니다.

#### 듀얼 학습 가능한 3진 양자화 (Dual Learnable Ternarization)
DLT는 그룹 내부에서 비대칭적으로 분포된 가중치의 규모와 이동을 학습할 수 있도록 설계된 맞춤형 3진 양자화 방법입니다. 이는 LLM에서 비대칭적인 가중치 분포를 더 정확하게 반영하고, 학습 가능한 스케일과 쉬프트를 도입함으로써 더 나은 표현력을 제공합니다.

#### 이상치 친화적 특성 지식 증류 (Outlier-Friendly Feature Knowledge Distillation)
OFF는 부동 소수점 모델과 양자화된 모델 사이의 상호정보를 최대화하여 양자화 과정에서 손실된 정보를 회복하는 방법입니다. 코사인 유사도를 사용하여 이상치에 덜 민감하게 만들고, 이를 통해 학습의 불안정을 줄입니다. 이 방법은 텍스트 생성 및 제로샷 작업에서 이전 포스트 트레이닝 및 양자화 인식 방법들을 능가합니다.

#### 실험 결과 (Experimental Results)
다양한 벤치마크를 사용하여 OPT와 LLaMA 모델 군을 대상으로 실험을 수행했습니다. 우리의 방법은 텍스트 생성과 제로샷 작업에서 이전의 모든 저비트 양자화 방법들을 능가하였습니다. 특히 LLaMA-3 모델의 경우, 우리의 방법(W1.58A16)은 이전 방법(W2A16) 대비 평균 perplexity에서 5.8만큼 향상되었고, 제로샷 작업에서 평균 정확도가 8.2% 향상되었습니다.

### 2. 전체 요약
이 논문은 대형 언어 모델의 효율적인 3진 양자화 기법인 듀얼 학습 가능한 3진 양자화(DLT)와 이상치 친화적 특성 지식 증류(OFF)를 소개합니다. 기존의 저비트 양자화 기법은 비대칭적 이상치를 처리하는 데 한계가 있었고, 매우 낮은 비트 양자화로 인해 정보 손실이 발생하는 문제가 있었습니다. DLT는 그룹 내 비대칭 가중치를 학습할 수 있도록 설계되었으며, OFF는 양자화 과정에서 손실된 정보를 회복하기 위해 코사인 유사도를 활용하여 이상치에 덜 민감하게 만들었습니다. 실험 결과, 제안된 방법은 OPT와 LLaMA 모델 군에서 텍스트 생성 및 제로샷 작업에서 이전 방법들보다 뛰어난 성능을 보였습니다. 이러한 혁신적인 접근법은 대형 언어 모델의 효율적인 활용을 가능하게 하여, 메모리 사용량을 줄이고 에너지 효율성을 증대시킬 수 있습니다.

## Similar Papers
- [OneBit: Towards Extremely Low-bit Large Language Models](2402.11295.md)
- [Codecfake: An Initial Dataset for Detecting LLM-based Deepfake Audio](2406.08112.md)
- [T-MAC: CPU Renaissance via Table Lookup for Low-Bit LLM Deployment on Edge](2407.00088.md)
- [ShiftAddLLM: Accelerating Pretrained LLMs via Post-Training Multiplication-Less Reparameterization](2406.05981.md)
- [AutoRAG-HP: Automatic Online Hyper-Parameter Tuning for Retrieval-Augmented Generation](2406.19251.md)
- [AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration](2306.00978.md)
- [OmniQuant: Omnidirectionally Calibrated Quantization for Large Language Models](2308.13137.md)
- [Extreme Compression of Large Language Models via Additive Quantization](2401.06118.md)
- [Turbo Sparse: Achieving LLM SOTA Performance with Minimal Activated Parameters](2406.05955.md)
