# Why Does the Effective Context Length of LLMs Fall Short?
## TL;DR
## Summary
- [https://arxiv.org/pdf/2410.18745.pdf](https://arxiv.org/pdf/2410.18745.pdf)

1. 각 섹션의 요약:

   - **서론**: 현재 대형 언어 모델(LLM)의 발전과 그 한계에 대해 논의하고, 특히 컨텍스트 창 길이를 확대하는데 있어서의 어려움을 설명합니다. 이 연구는 LLM의 효과적인 컨텍스트 길이가 기대 이하로 짧다는 문제에 중점을 두고 있습니다.

   - **왼쪽으로 기울어진 위치 빈도 분포**: 이 섹션에서는 LLM의 사전 훈련 및 후속 훈련에서의 상대적인 위치 빈도의 불균형 분포가 모델의 장거리 의존성 수집 능력을 저해한다고 설명합니다.

   - **STRING 방법론**: 새롭게 제안된 온라인 학습 방법인 ShifTed Rotray position embeddING (STRING)은 잘 훈련된 위치를 이용해 장거리 정보를 효과적으로 수집하도록 설계되었습니다. 추가 훈련 없이 기존 모델의 성능을 향상시킵니다.

   - **STRING의 주요 결과**: 다양한 롱컨텍스트 벤치마크에서 STRING의 성능을 평가하고, 이를 통해 새로운 최첨단 성능 기준을 설정한 결과를 보여줍니다.

   - **결론**: STRING은 기존 공개 소스 LLM의 장거리 의존성 모델링 능력을 크게 향상시켰으며, 이를 통해 LLM의 컨텍스트 활용 효과를 극대화할 가능성을 제시합니다.

2. 전체 요약:

   이 논문은 대형 언어 모델(LLM)의 컨텍스트 창 길이를 늘리려는 기존의 접근 방식에서 벗어나, 효율적인 컨텍스트 길이 사용의 한계를 탐구합니다. 주된 기여는 왼쪽으로 기울어진 위치 빈도 분포가 LLM의 장거리 의존성 모델링을 저해한다는 점을 밝혀내고, 이를 해결하기 위한 STRING 방법론을 제안한 것입니다. STRING은 잘 훈련된 위치를 사용하여 비효율적인 위치를 대체하고, 이를 통해 추가적인 훈련 없이도 모델의 장거리 정보 수집 능력을 향상시킵니다. 이 접근법은 여러 롱컨텍스트 벤치마크에서 뛰어난 성능을 보여주며, 공개 소스 LLM의 새로운 성능 기준을 세웠습니다. 이러한 연구 결과는 향후 LLM 설계 시 롱컨텍스트 처리 효과를 개선하기 위한 새로운 접근 방식을 고무할 수 있습니다.