# NeRF-XL: Scaling NeRFs with Multiple GPUs
## TL;DR
## Summary
- [https://arxiv.org/pdf/2404.16221.pdf](https://arxiv.org/pdf/2404.16221.pdf)

이 논문에서는 NeRF-XL이라는 새로운 방법을 제안하여 여러 GPU를 이용한 Neural Radiance Fields(NeRF)의 효율적인 분산 훈련과 렌더링을 가능하게 합니다. 이 방법은 대규모 장면의 네트워크 훈련을 위해 여러 GPU에 걸쳐 NeRF 파라미터를 할당하고 공동으로 훈련합니다. 기존 방법들과 비교하여 향상된 구조 품질과 렌더링 속도를 달성하는 것이 주요 목표입니다. 다음은 각 섹션의 주요 내용 요약입니다.

1. **서론 및 관련 작업**:
   - 기존의 단일 GPU NeRF 방법들은 큰 규모의 장면을 처리하는 데 한계가 있습니다.
   - 여러 GPU를 사용하는 기존 방법들은 독립적인 NeRF를 훈련하여 각 GPU가 전체 공간을 모델링하도록 합니다. 이는 모델 용량의 중복을 증가시키고 시각적 품질을 저하시킵니다.

2. **NeRF-XL 방법론**:
   - NeRF 파라미터를 여러 GPU에 분산시켜 공동으로 훈련함으로써 모델 용량의 중복을 제거하고 효율성을 증가시킵니다.
   - 전달 단계에서만 GPU 간 정보 동기화가 필요하며, 뒤로 전달 단계에서는 필요하지 않습니다.
   - 볼륨 렌더링 방정식과 관련 손실 항을 재작성하여 GPU 간 데이터 전송을 줄입니다.

3. **실험 및 결과**:
   - 다양한 유형의 데이터셋에서 NeRF-XL의 효과를 시연하고 기존 방법들과 비교하여 시각적 품질과 렌더링 속도에서 개선된 결과를 보여줍니다.
   - 특히 대규모 장면에서 더 많은 GPU를 할당할수록 PSNR 및 렌더링 속도가 향상됩니다.

4. **결론 및 한계**:
   - NeRF-XL은 대규모 NeRF 성능을 향상시키기 위해 여러 GPU를 효율적으로 활용하는 원리적인 알고리즘을 제시합니다.
   - 동기화 및 통신 오버헤드로 인해 훈련 속도가 다소 느려질 수 있으며, 이는 향후 연구에서 해결해야 할 문제입니다.

이 연구는 NeRF의 확장성과 효율성을 크게 향상시키는 새로운 접근 방식을 제시함으로써, 큰 규모의 3D 장면을 보다 실시간에 가깝게 처리할 수 있는 가능성을 열어줍니다.