# RetrievalAttention: Accelerating Long-Context LLM Inference via Vector Retrieval
## TL;DR
## Summary
- [https://arxiv.org/pdf/2409.10516.pdf](https://arxiv.org/pdf/2409.10516.pdf)

### 섹션별 요약 및 주요 기여

**1. Introduction**
이 논문은 장문 텍스트를 처리하는 대형 언어 모델(LLM)의 계산 효율성을 높이기 위해 "RetrievalAttention"이라는 방법을 제안합니다. 이 방법은 주의 집중 메커니즘에서 동적 희소성(dynamic sparsity)을 활용하여 중요한 토큰만을 선택적으로 처리합니다. 이로 인해 GPU 메모리 사용량을 크게 줄이고, 계산 속도를 높일 수 있습니다.

**2. Background and Motivation**
주의 집중(Attention) 메커니즘은 쿼리 벡터와 키 벡터 간의 점곱을 통해 처리합니다. 이 과정은 시간 복잡도가 O(n^2)로 매우 비싸긴 하지만, 대부분의 토큰은 쿼리와 상호작용하지 않기 때문에 동적으로 희소한 특성을 보입니다. 이는 처리 비용을 줄일 수 있는 가능성을 열어줍니다.

**3. Expensive Long-Context Serving**
망간의 시간 복잡도로 인해 긴 문맥을 처리하는 것은 비용이 매우 큽니다. 많은 양의 GPU 메모리가 필요하며, KV 캐시를 사용해도 약 1백만 토큰을 처리하는 데 많은 시간이 소요됩니다. 이를 극복하기 위해서는 적은 메모리와 시간으로 중요한 토큰을 정확하게 찾을 수 있는 방법이 필요합니다.

**4. Dynamic and Sparse Attention**
실제로 생성 정확도에 영향을 미치는 토큰은 매우 적습니다. 예를 들어, Llama-2-7B 모델에서는 64,000개의 토큰 중 상위 500개의 토큰이 주의 집중 점수에 주요한 영향을 미칩니다. 이러한 특성을 활용해 계산 비용을 줄이는 방법을 모색합니다.

**5. Challenges of Off-the-shelf Vector Search**
기존의 근사 최근접 탐색(ANNS) 알고리즘은 주의 집중 메커니즘에서 잘 작동하지 않습니다. 이것은 쿼리 벡터와 키 벡터가 같은 분포에서 나오지 않기 때문에 발생하는 문제입니다. 여기에서는 이 문제를 해결하기 위해 주의 집중에 맞춘 벡터 검색 알고리즘을 구성합니다.

**6. RetrievalAttention Design**
RetrievalAttention은 CPU와 GPU를 함께 사용하여 효율적으로 장문 텍스트를 처리합니다. 중요도에 따라 일부 KV 벡터는 GPU에, 나머지는 CPU에 저장하며, 필요한 토큰만을 선택적으로 검색하여 처리합니다. 이를 통해 메모리 사용량을 최소화하고 처리 속도를 높입니다.

**7. Evaluation**
실험 결과에 따르면, RetrievalAttention은 정확도를 유지하면서도 기존의 방법들보다 최대 4.9배 빠른 디코딩 속도를 보입니다. 이 방법은 24GB의 GPU 메모리로 128K 토큰을 처리할 수 있는 유일한 솔루션입니다.

**8. Conclusion**
RetrievalAttention은 동적 희소 주의 집중 메커니즘을 활용하여 장문 텍스트를 효율적으로 처리하는 방법을 제시합니다. 이는 메모리 사용량을 줄이고, 처리 속도를 대폭 개선할 수 있습니다.

### 전체 요약

이 논문은 주의 집중 메커니즘의 동적 희소성을 활용한 "RetrievalAttention"라는 혁신적인 방법을 제안합니다. 이를 통해 장문 텍스트를 처리하는 대형 언어 모델의 계산 비용과 메모리 사용량을 크게 줄일 수 있습니다. RetrievalAttention은 쿼리 벡터와 키 벡터 간의 점곱 문제를 해결하며, CPU와 GPU를 효율적으로 활용해 필요한 토큰만을 선택적으로 검색하여 처리 속도를 높입니다. 실험 결과, 이 방법은 기존의 방법들보다 최대 4.9배 빠른 디코딩 속도를 보이며, 정확도를 유지하면서도 24GB의 GPU 메모리로 128K 토큰을 처리할 수 있습니다. 

이 논문은 주의 집중 메커니즘에서 발생하는 비용 문제를 효율적으로 해결하는 방법을 제시하며, 이는 장문 텍스트를 처리하기 위한 대형 언어 모델의 효율성을 크게 향상시킬 수 있습니다.