# Mini-Omni: Language Models Can Hear, Talk While Thinking in Streaming
## TL;DR
## Summary
- [https://arxiv.org/pdf/2408.16725.pdf](https://arxiv.org/pdf/2408.16725.pdf)

### 요약: AI 및 기계 학습 논문

#### 1. 각 섹션의 중요한 내용 요약
##### Introduction
최근 성능이 향상된 대형 언어 모델들이 개발되었으며, GPT-4가 뛰어난 실시간 다중모달 상호작용을 할 수 있음에도 불구하고 폐쇄형이라는 한계가 있다. 대부분의 모델은 텍스트 생성 후 텍스트-음성 변환(TTS)에 의존해 음성을 생성하여 지연이 발생한다. 이러한 문제를 해결하기 위해 Mini-Omni를 소개하고, 텍스트 지시에 따른 음성 생성 방법과 배치-병렬 추론 전략을 제안하여 퍼포먼스를 향상시켰다.

##### Related Work
기존 연구들은 주로 텍스트를 입력받아 TTS를 통해 음성을 생성하는 방식으로 구성되었다. Whisper나 Beats와 같은 모델들이 오디오 특징 엔코더로 사용되며, 여전히 텍스트 출력 중심이다. 최신 연구들은 오디오 토큰화를 통해 오디오와 텍스트의 간극을 해소하려고 한다.

##### Mini-Omni
Mini-Omni는 오픈 소스 최초의 실시간 음성 상호작용이 가능한 대형 모델로 음성 입력 및 출력을 포함한 다중모달 기능을 제공한다. 텍스트-음성 병렬 생성 방식을 통해 오디오 추론 출력을 텍스트 기능과 정렬시켜 빠르고 일관된 음성 응답을 제공한다.

##### Audio Language Modeling
오디오 토큰화를 통해 연속적인 오디오 신호를 이산적인 오디오 토큰으로 변환함으로써 대형 언어 모델이 연산 및 교차 모달 상호작용을 수행하도록 한다. 이 접근법은 텍스트를 통해 오디오 토큰 생성을 빠르게 하여 실시간 응답을 가능하게 한다.

##### Decoding Strategies
Mini-Omni는 텍스트와 오디오 토큰을 동시에 출력하는 병렬 디코딩 방식을 사용해 실시간 오디오 출력을 달성한다. 텍스트 지시에 따른 방법과 배치 병렬 디코딩을 통해 모델의 추론 능력을 강화했다.

##### Any Model Can Talk
모델의 텍스트 출력을 오디오 모달로 빠르게 확장할 수 있는 방법론을 제안한다. 이 접근법은 "모든 모델은 말할 수 있다"는 개념을 기반으로 하며, 최소한의 추가 데이터로 음성 출력 기능을 향상시킨다.

##### Experiments
ASR, 텍스트-질의응답(textQA), 자동 음성 인식(ASR), 텍스트-음성 응답 및 음성-질의응답(speechQA) 등 주요 멀티모달 태스크에서 Mini-Omni의 성능을 평가했다. 결과는 모델이 각 분야에서 우수한 능력을 보임을 나타낸다. 또한 실험 결과 경쟁 모델에 비해 우수한 음성 인식 성능을 보여줬다.

##### Conclusion
Mini-Omni는 실시간 음성 대화를 위한 최초의 종단간(end-to-end) 오픈 소스 모델이다. 새로운 병렬 생성 방식을 통해 다양한 다중모달 상호작용을 빠르게 수행할 수 있으며, "Any Model Can Talk" 방법론을 통해 다른 모델들에서도 음성 상호작용 기능을 쉽게 통합할 수 있도록 했다.

#### 2. 전체 요약
이 논문은 실시간 음성 상호작용이 가능한 첫 번째 오픈 소스 모델인 Mini-Omni를 소개한다. 이 모델은 텍스트와 오디오를 동시에 생성하는 병렬 디코딩 방식을 사용해, 기존 모델의 성능을 유지하면서 빠르고 일관되게 실시간 대화를 수행할 수 있다. 또한 "Any Model Can Talk" 방법론을 통해 최소한의 추가 데이터로 다른 모델에서도 음성 상호작용 기능을 쉽게 통합할 수 있게 한다. 이 연구는 다양한 멀티모달 태스크에서 Mini-Omni의 우수한 성능을 입증했으며, 차세대 AI 연구에 중요한 기여를 할 것으로 기대된다.