# SEED-Story: Multimodal Long Story Generation with Large Language Model
## TL;DR
## Summary
- [https://arxiv.org/pdf/2407.08683.pdf](https://arxiv.org/pdf/2407.08683.pdf)

### 1. 각 섹션 요약 (중요 내용 및 혁신적인 부분)

#### 1.1 도입부 (Introduction)
이 논문은 텍스트와 이미지를 통합하여 내러티브를 생성하는 "다중 모드 스토리 생성"을 탐구합니다. 최근에 이미지 생성과 텍스트 생성의 발전으로 인해 텍스트-이미지 내용을 생성하는 데 많은 관심이 집중되고 있습니다. SEED-Story는 다중 모드 대형 언어 모델(MLLM)을 활용하여 일관되고 스토리텔링에 적합한 이미지를 생성하며, 특히 장편 스토리를 생성하는 데 중점을 둡니다.

#### 1.2 관련 연구 (Related Work)
사건 GAN(StoryGAN)에서 시작된 스토리 시각화 연구는 이미지 품질, 내러티브 흐름, 캐릭터 일관성을 크게 개선한 다양한 모델들을 통해 발전했습니다. SEED-Story는 이러한 연구를 기반으로 더욱 풍부하고 몰입감 있는 이야기 생성으로 나아갑니다.

#### 1.3 방법론 (Method)
SEED-Story는 사전 학습된 비전 트랜스포머(ViT)와 확산 모델을 사용하여 이미지를 토큰화하고 디토큰화합니다. SEED-Story 모델은 이미지와 텍스트를 모두 이해하고 생성해야 하므로, 이미지 특징을 입력으로 받아 일관된 이미지를 생성할 수 있는 방식으로 설계되었습니다. 또한, 이 모델은 "멀티모달 어텐션 싱크" 메커니즘을 도입하여 장편 스토리를 효율적으로 생성할 수 있습니다.

#### 1.4 실험 및 결과 (Experiments and Results)
실험 결과, SEED-Story는 기존의 장편 스토리 인코딩 방식보다 효율성이 높고, 이미지 일관성과 스타일 세부 사항에서 뛰어난 성능을 보여줬습니다. 새로운 데이터셋 "스토리스트림"을 통해 모델의 성능을 검증하고 평가하였습니다. 이 데이터셋은 높은 해상도와 더 긴 시퀀스를 자랑하며, 기존 데이터셋의 한계를 극복합니다.

#### 1.5 결론 (Conclusion)
SEED-Story는 혁신적인 장편 스토리 생성 모델로, 텍스트와 이미지의 내러티브를 일관되게 통합할 수 있는 새로운 메커니즘을 도입했습니다. 이로 인해 장편 스토리의 생성 효율성이 크게 향상되었으며, 새로운 데이터셋을 통해 다양한 응용 프로그램에서 높은 성능을 입증했습니다.

### 2. 전체 요약
이 논문은 SEED-Story라는 다중 모드 스토리 생성 모델을 소개합니다. 이 모델은 대형 언어 모델(MLLM)을 활용하여 텍스트-이미지 내러티브를 효율적으로 생성할 수 있습니다. 이를 위해, 사전 학습된 비전 트랜스포머와 확산 모델을 사용하여 이미지 특징을 디코딩하며, "멀티모달 어텐션 싱크" 메커니즘을 통해 장편 스토리 생성 문제를 해결합니다. 새로운 대규모 데이터셋인 "스토리스트림"을 통해 모델을 검증했으며, 실험 결과 SEED-Story는 이미지 일관성, 내러티브 흐름에서 뛰어난 성능을 보여줍니다. 이로 인해 교육 및 엔터테인먼트 분야에서 더 풍부하고 몰입감 있는 스토리텔링을 가능하게 합니다.