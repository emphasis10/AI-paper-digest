# Aligning Large Language Models via Self-Steering Optimization
## TL;DR
## Summary
- [https://arxiv.org/pdf/2410.17131.pdf](https://arxiv.org/pdf/2410.17131.pdf)

### 1. 요약

#### 소개 및 배경
논문은 대형 언어 모델(LLM)의 정렬(alignment)을 자동화하여 인간의 개입을 최소화하는 방법을 제시합니다. 여기서 주요 과제로는 학습 가능한 정확한 선호 신호(preference signals)를 사람의 주석 없이 제공하는 것입니다. 최근 연구자들이 자동 정렬로 초점을 이동하면서 고품질 정렬 시스템을 개발하고 있으며, 이 논문에서는 자가 조정 최적화(Self-Steering Optimization, SSO)를 통해 이러한 과제를 극복하고 있습니다.

#### 주요 기여 및 혁신
이번 연구의 주요 기여는 SSO라는 방법론의 제안에 있습니다. SSO는 정책 모델의 학습 용량을 충족시키기 위해 선택된 응답과 거부된 응답 간의 일관된 격차를 유지하면서 정책 모델의 학습을 최적화합니다. 이러한 방식을 통해 인류의 주석이나 외부 모델 없이도 다양한 주관적 및 객관적 벤치마크에서 성능을 크게 향상시킬 수 있었습니다.

#### SSO의 구성 및 작동 원리
SSO는 정책 모델을 비정책적 응답에서 온정책(on-policy) 응답으로 유도하는 두 가지 단계로 구성되어 있습니다. 첫 번째 단계에서는 대조적 프롬프트를 작성하여 응답을 샘플링하고, 두 번째 단계에서는 셋의 선호 쌍(preference pairs)을 사용하여 모델을 학습합니다.

#### 실험 결과
SSO의 효과는 여러 벤치마크에서 증명되었습니다. 실험에서는 Qwen2 및 Llama3.1 백본을 사용하여 SSO가 학습 기간 동안 학습 가능한 자동 신호를 생성하는 능력을 입증했습니다. 결과적으로 SSO는 인간의 주석이나 외부 모델 없이도 성능을 향상시키는 데 성공하였습니다.

#### 한계 및 미래 작업
SSO 방법론은 아직 몇 가지 한계를 가지고 있습니다. 예를 들어, 설계의 단순함이 때때로 불필요한 계산 비용을 초래할 수 있습니다. 미래의 작업에서는 자동화된 SSO를 구현하고, 새로운 가중치 함수(W) 및 손실 함수(G)를 설계하여 정렬의 효율성을 더욱 향상시키려 합니다.

### 2. 전체 요약
이번 논문은 AI와 기계 학습에서 사람의 개입을 최소화하면서도 학습 가능한 정확한 선호 데이터를 생성할 수 있는 새로운 접근 방식인 자가 조정 최적화(SSO)를 소개합니다. SSO는 정책 모델의 학습 용량을 고려하여, 오프라인 및 온라인 학습 모두에 유효한 자동화된 정렬 시스템을 가능케 합니다. 다양한 실험을 통해 SSO는 기존의 방법론만큼이나 강력하며, 특히 인간의 주석이 없는 환경에서도 높은 성과를 기록했습니다. 이는 AI의 자동 정렬을 추진하고 보다 효율적이고 효과적인 방식으로 나아가는 중요한 발걸음이 될 것입니다.