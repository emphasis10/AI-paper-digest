# LLMLingua-2: Data Distillation for Efficient and Faithful Task-Agnostic Prompt Compression
## TL;DR
## Summary
- [https://arxiv.org/pdf/2403.12968.pdf](https://arxiv.org/pdf/2403.12968.pdf)

### 요약

#### 1. 각 섹션 요약

##### 1.1 서론
이 논문은 자연어 처리(NLP)에서 등장하는 거대 언어 모델(LLM)의 프롬프트를 효율적으로 압축하는 방법을 제안합니다. 기존 접근 방식들은 프롬프트 압축을 위해 정보 엔트로피 등의 지표를 사용했지만, 이는 최적의 압축 지표가 아닐 수 있습니다. 본 논문은 GPT-4의 지식을 활용해 중요한 정보를 잃지 않고 원문을 압축하는 데이터를 생성하는 방법을 소개합니다.

##### 1.2 관련 연구
프롬프트 압축 방법은 크게 작업 인지형과 작업 비인지형으로 나뉩니다. 기존 연구들에서는 주로 정보 엔트로피 등의 지표를 활용하여 작업과 상관없이 프롬프트를 압축했습니다. 하지만 이는 전체적인 맥락을 포착하지 못할 수도 있습니다.

##### 1.3 데이터셋 구축
본 연구에서는 GPT-4를 이용해 MeetingBank 데이터셋에서 중요한 정보를 유지하면서 텍스트를 압축하는 데이터를 구축했습니다. 이를 위해 텍스트의 각 단어에 보존 여부를 라벨링하고, 낮은 품질의 샘플을 필터링하는 품질 관리 지표를 제안했습니다.

##### 1.4 제안한 방법
프롬프트 압축을 토큰 분류 문제로 보고, 각 토큰의 보존 여부를 예측하는 모델을 사용합니다. 이 접근법은 정확도와 낮은 지연 시간을 보장하며, Transformer 인코더를 사용하여 양방향 문맥에서 중요한 정보를 추출합니다.

##### 1.5 실험 결과
MeetingBank와 LongBench 등 여러 데이터셋에서의 실험 결과, 제안한 모델은 기존의 강력한 Baselines보다 뛰어난 성능을 보였습니다. 특히 GPT-3.5-Turbo 에서 Mistral-7B까지 다양한 LLM에서 우수한 일반화 능력을 보였으며, 속도 면에서도 기존 방법보다 3배에서 6배 빠르게 프롬프트를 압축했습니다.

##### 1.6 결론과 한계
논문에서는 작업 비인지형 프롬프트 압축의 문제점을 해결하기 위해 제안한 방법의 유효성을 강조하였습니다. 하지만 본 연구에서 사용된 텍스트 압축 데이터셋이 MeetingBank에만 국한되어 있어, 다른 도메인에서도 일반화 가능한지에 대한 의문이 남아 있습니다. 향후 연구에서는 더 다양한 도메인에서의 성능 검증이 필요합니다.

#### 2. 전체 요약
이 논문은 거대 언어 모델(LLM)의 프롬프트 압축 문제를 다루고 있습니다. 특히, GPT-4의 지식을 활용하여 중요한 정보를 유지하면서 원문을 압축하는 데이터셋을 구축하고, 이를 바탕으로 토큰 분류 모델을 훈련시켜 프롬프트를 효율적으로 압축하는 방법을 제안하였습니다. 이를 통해 프롬프트의 길이를 줄이면서도 중요한 정보를 보존할 수 있었습니다. 다양한 데이터셋에서의 실험 결과, 제안된 방법은 기존의 강력한 Baselines보다 뛰어난 성능을 보였으며, 다른 도메인에서도 우수한 일반화 능력을 나타냈습니다. 하지만, 본 연구가 특정 데이터셋에 한정되어 있는 만큼 다른 도메인에서도 유사한 성능을 보일 수 있는지는 추가 연구가 필요합니다.

## Similar Papers
- [MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention](2407.02490.md)
- [Finch: Prompt-guided Key-Value Cache Compression](2408.00167.md)
- [Context Embeddings for Efficient Answer Generation in RAG](2407.09252.md)
- [Parrot: Efficient Serving of LLM-based Applications with Semantic Variable](2405.19888.md)
- [AutoRAG-HP: Automatic Online Hyper-Parameter Tuning for Retrieval-Augmented Generation](2406.19251.md)
- [Characterizing Prompt Compression Methods for Long Context Inference](2407.08892.md)
- [Compressing LLMs: The Truth is Rarely Pure and Never Simple](2310.01382.md)
- [Judging the Judges: Evaluating Alignment and Vulnerabilities in LLMs-as-Judges](2406.12624.md)
- [Diffree: Text-Guided Shape Free Object Inpainting with Diffusion Model](2407.16982.md)
