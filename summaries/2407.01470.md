# DogeRM: Equipping Reward Models with Domain Knowledge through Model Merging
## TL;DR
## Summary
- [https://arxiv.org/pdf/2407.01470.pdf](https://arxiv.org/pdf/2407.01470.pdf)

### 1. 각 섹션 요약

**1. 서론 (Introduction)**  
이 논문은 인간 피드백을 통한 강화 학습(RLHF)을 사용하여 대형 언어 모델(LLMs)을 인간의 선호에 맞추는 방법을 논의합니다. RLHF의 핵심 구성 요소는 보상 모델(RMs)입니다. 이 보상 모델은 정책 모델이 생성한 문장을 평가하고, 그 결과를 기반으로 정책 모델의 매개변수를 조정합니다. 그러나 도메인 특화된 선호 데이터를 수집하는 것은 비용이 많이 들고 시간이 촉박한 작업입니다.

**2. 관련 연구 (Related Work)**  
기존 연구들은 다양한 보상 모델과 RL 알고리즘을 사용하여 언어 모델을 인간의 선호에 맞추려고 했습니다. 또한, 여러 오픈 소스 선호 데이터셋이 존재하지만, 대부분의 데이터셋은 도메인 특화되어 있지 않습니다. 따라서 이 논문에서는 도메인 특화된 언어 모델과 보상 모델을 병합하는 방법을 제안합니다.

**3. 방법론 (Methodology)**  
보상 모델 훈련을 위해, 트랜스포머 기반의 사전 훈련된 언어 모델의 디코딩 레이어를 선형 회귀 레이어로 대체합니다. 이 새로운 레이어는 트랜스포머의 마지막 레이어에서 나온 로그도형(logits)을 보상값으로 변환합니다. 또한, 도메인 별 특화된 언어 모델을 활용해 도메인 지식을 통합합니다.

**4. 결과 (Results)**  
라마(LLAMA)-2 보상 모델과 MetaMath-7B 및 MAmmoTH-7B의 병합은 RewardBench에서 수학 성능을 각각 11.4% 및 17% 향상시켰으며, 코딩 성능도 각각 5.2% 및 5.8% 향상되었습니다. 또한, Auto-J Eval에서도 유사한 향상이 관찰되었습니다. 도메인 특화 모델을 사용한 병합 방법이 모델의 재정렬 성능을 개선시키는 것을 확인했습니다.

**5. 분석 (Analysis)**  
λ 가중치가 보상 모델의 성능에 어떻게 영향을 미치는지 분석했습니다. λ 값이 0에서 1로 변함에 따라 RewardBench의 점수가 어떻게 달라지는지 관찰했으며, 보상 신호의 값과 차이를 평가하였습니다. 도메인별 특화 모델과의 병합이 성능에 미치는 영향을 심도 있게 분석했습니다.

**6. 결론 (Conclusion)**  
도메인 지식을 통합한 보상 모델(DogeRM)은 도메인 특화된 언어 모델과의 병합을 통해 기존 보상 모델의 성능을 극대화합니다. 이 방법은 특히 전문적인 도메인에서 시간과 비용을 절감하는 데 큰 효과가 있습니다. DogeRM의 잠재력을 입증하는 상세한 실험 결과를 제공했습니다.

### 2. 전체 요약

이 논문은 도메인 지식을 통합한 보상 모델(DogeRM)을 제안합니다. 인간 피드백을 통한 강화 학습(RLHF)을 사용하여 대형 언어 모델(LLMs)을 인간의 선호에 맞추기 위한 방법론이 주요 내용입니다. 도메인 특화된 언어 모델과 보상 모델의 병합을 통해, 기존의 시간과 비용 문제가 큰 도메인 특화된 데이터를 효과적으로 다룹니다. 실험 결과, 다양한 도메인에서 DogeRM의 성능 향상을 입증하였으며, 이 방법은 특히 수학 및 코딩과 같은 전문적인 분야에서 두드러진 성과를 보였습니다. 이 논문은 도메인 특화된 지식이 필요한 경우, DogeRM이 실용적이고 비용 효율적인 해결책이 될 수 있음을 강조합니다.

## Similar Papers
- [Advancing LLM Reasoning Generalists with Preference Trees](2404.02078.md)
- [Multi-property Steering of Large Language Models with Dynamic Activation Composition](2406.17563.md)
- [WPO: Enhancing RLHF with Weighted Preference Optimization](2406.11827.md)
- [Chat Vector: A Simple Approach to Equip LLMs with Instruction Following and Model Alignment in New Languages](2310.04799.md)
- [Self-Training with Direct Preference Optimization Improves Chain-of-Thought Reasoning](2407.18248.md)
- [Large Language Monkeys: Scaling Inference Compute with Repeated Sampling](2407.21787.md)
- [Understanding the performance gap between online and offline alignment algorithms](2405.08448.md)
- [Never Miss A Beat: An Efficient Recipe for Context Window Extension of Large Language Models with Consistent "Middle" Enhancement](2406.07138.md)
- [Is DPO Superior to PPO for LLM Alignment? A Comprehensive Study](2404.10719.md)
