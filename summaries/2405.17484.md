# Bridging The Gap between Low-rank and Orthogonal Adaptation via Householder Reflection Adaptation
## TL;DR
## Summary
- [https://arxiv.org/pdf/2405.17484.pdf](https://arxiv.org/pdf/2405.17484.pdf)

### 논문 요약 (한글)

#### 1. 섹션별 요약

##### 서론
현대 AI 연구는 대규모 기초 모델의 크기를 계속 증가시키고 있으며, 이는 모델의 이해, 생성, 추론, 일반화 능력을 크게 향상시키는 반면, 모델 적응 비용도 거대해지는 문제를 야기하고 있습니다. 효율적인 파인 튜닝을 통해 이러한 대규모 기초 모델을 다양한 다운스트림 작업에 적응시키는 것이 큰 도전 과제로 떠오르고 있습니다. PEFT(파라미터 효율적 파인 튜닝) 방법은 적은 학습 가능한 파라미터와 메모리 소비를 통해 모델 적응 성능을 유지하거나 향상시키려는 해결책입니다.

##### 주요 기여 및 혁신 부분
본 논문은 Householder Reflection을 기반으로 한 새로운 적응 방법을 제안합니다. 이 방법은 HRs(학습 가능한 하우스홀더 반사) 체인을 사용하여 사전 훈련된 모델의 각 층을 미세 조정합니다. 이 HR 기반의 정규화를 통해 모델 용량과 규칙성을 조절할 수 있으며, 이는 적은 학습 가능한 파라미터를 사용하여 더 우수한 성능을 달성할 수 있게 합니다. 이 방법은 저차원 적응과 정규화된 적응의 장점을 결합하여 다양한 다운스트림 작업에서 뛰어난 성능을 보여줍니다.

##### 결론
Householder Reflection Adaptation (HRA)은 저차원 적응과 정규화된 적응의 간극을 효과적으로 연결하며, 다양한 작업에서 상태 최첨단 방법보다 적은 학습 가능한 파라미터로 더 나은 성능을 보여줍니다. 자연어 이해와 수학적 추론 작업에서 HRA는 특히 뛰어난 성능을 발휘합니다.

#### 2. 전반적인 요약

이 논문은 발달된 기초 모델의 효율적인 파인 튜닝을 목표로 Householder Reflection을 사용한 새로운 적응 방법을 제안합니다. 제안된 HRA 방법은 저차원 적응과 정규화된 적응의 장점을 결합하여, 적은 데이터량과 메모리 사용으로도 뛰어난 성능을 보여줍니다. 자연어 이해와 수학적 추론, 이미지 생성 등 다양한 작업에서 HRA는 상태 최첨단 방법을 능가하는 결과를 나타내며, 현대 AI 연구의 중요한 발전을 이룰 수 있는 가능성을 보여줍니다. 

종합적으로 이 논문은 효율적인 파인 튜닝 방법론으로서, 학습 가능한 파라미터를 줄이면서도 성능을 유지하거나 향상시키고자 하는 AI 연구자들에게 큰 도움이 될 것입니다. HRA는 특히 대규모 모델의 적응성과 적용 범위를 넓히는 데 기여할 수 있으며, 이는 AI의 진보에 중요한 역할을 할 것입니다.

## Similar Papers
- [PiSSA: Principal Singular Values and Singular Vectors Adaptation of Large Language Models](2404.02948.md)
- [On the Minimal Degree Bias in Generalization on the Unseen for non-Boolean Functions](2406.06354.md)
- [Conditioned Language Policy: A General Framework for Steerable Multi-Objective Finetuning](2407.15762.md)
- [MoRA: High-Rank Updating for Parameter-Efficient Fine-Tuning](2405.12130.md)
- [Understand What LLM Needs: Dual Preference Alignment for Retrieval-Augmented Generation](2406.18676.md)
- [Why Larger Language Models Do In-context Learning Differently?](2405.19592.md)
- [SHERL: Synthesizing High Accuracy and Efficient Memory for Resource-Limited Transfer Learning](2407.07523.md)
- [Parameter-Efficient Orthogonal Finetuning via Butterfly Factorization](2311.06243.md)
- [On Computationally Efficient Multi-Class Calibration](2402.07821.md)
