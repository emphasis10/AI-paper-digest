# LLMs Know More Than They Show: On the Intrinsic Representation of LLM Hallucinations
## TL;DR
## Summary
- [https://arxiv.org/pdf/2410.02707.pdf](https://arxiv.org/pdf/2410.02707.pdf)

먼저 각 섹션의 주요 내용을 요약해 드리겠습니다.

### 1. 서론
서론에서는 AI와 머신러닝에 대한 연구의 중요성을 강조합니다. 특히, 대형 언어 모델(LLM)의 오류 감지에 대한 필요성을 제시하며, 기존 연구들이 특정 과제에 집중된 방법을 사용해왔음을 설명합니다.

### 2. 배경
이 섹션에서는 LLM 오류를 정의하고 특징을 설명합니다. '환각(hallucination)'이라는 용어가 여러 분야에서 각기 다른 의미로 사용되고 있으며, 구체적으로 LLM에서의 오류가 어떻게 발생하는지 설명합니다. 연구자들이 이를 '오류', '허위 생성' 등으로 차별화하고 있음을 소개합니다.

### 3. 오류 감지 향상
이 부분에서는 LLM의 내부 연산을 활용해 오류를 감지하는 방법에 대해 실험을 수행합니다. 특히, 토큰 선택이 오류 감지에 미치는 영향을 분석하고, 이를 통해 다른 방법들보다 나은 오류 탐지 방법론을 소개합니다.

### 4. 실험 결과
실험 결과에 따르면, 내부 표상(중간 단계)에서 추출한 정보가 정확성 있게 오류를 감지할 수 있음을 확인했습니다. 이 과정에서 다양한 데이터셋과 과제를 사용하여 성능을 평가하고, LLM이 내재적으로 올바른 답을 알고 있더라도 외부 행동에서 이를 잘못 표현할 수 있음을 시사합니다.

### 전체 요약
이 논문은 AI와 머신러닝의 주요 부분인 대형 언어 모델의 오류 감지를 중심으로 합니다. 모델 내부의 계산을 분석하여 오류의 근원을 파악하고 개선하려는 시도를 하였으며, 특히 모델 내부 표현이 외부 행동과 일치하지 않을 수 있음을 보여줍니다. 이를 통해 보다 신뢰할 수 있는 AI 모델 개발에 기여할 수 있는 지침을 제시하고 있습니다.

논문의 혁신적인 부분은 LLM의 내부 연산을 중심으로 오류 감지를 향상하는 방법을 제시한 점이며, 모델이 외부적으로 보여주는 결과가 반드시 내재된 능력을 반영하지 않을 수 있다는 점을 강조하여 향후 연구 방향을 제시합니다.