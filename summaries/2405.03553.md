# AlphaMath Almost Zero: process Supervision without process
## TL;DR
## Summary
- [https://arxiv.org/pdf/2405.03553.pdf](https://arxiv.org/pdf/2405.03553.pdf)

### 주요 내용 요약

1. **서론 및 배경**:
   - 이 연구는 대규모 언어 모델(LLM)을 사용하여 복잡한 수학적 추론 과제에서 이러한 모델의 추론 능력을 향상시키는 방법을 탐구합니다. 수학적 추론에서의 주요 제한은 "환각" 문제로, LLM이 숫자 계산에서 오류를 범하는 경향이 있습니다. 이를 해결하기 위해 외부 코드 인터프리터를 통합하는 방법이 개발되었습니다.

2. **몬테카를로 트리 탐색(MCTS) 평가**:
   - MCTS 프레임워크를 통합하여 높은 품질의 훈련 데이터를 생성하는 알고리즘이 개발되었습니다. 이 접근 방식은 보드 게임 분야에서 광범위하게 조사되었으며, 특히 AlphaGo Zero와 같은 모델이 인간의 지식 없이도 지속적으로 진화할 수 있음을 보여줍니다.

3. **반복 훈련**:
   - MCTS를 사용하여 생성된 데이터를 바탕으로 정책 모델과 가치 모델을 반복적으로 훈련시킵니다. 이 과정은 문제 해결 능력을 점진적으로 향상시키며, 가치 모델은 효과적인 해결 경로를 탐색하는 데 중요한 역할을 합니다.

4. **추론**:
   - MCTS 추론 과정은 계산적으로 집약적이며 시간이 많이 소요되기 때문에, 생산 환경에서는 보다 간소화된 탑-다운 버전을 사용합니다. 이는 단계별 빔 탐색을 통해 보다 실용적으로 구현됩니다.

### 혁신적인 부분
이 연구의 혁신적인 점은 LLM의 수학적 추론 능력을 크게 향상시킬 수 있는 새로운 훈련 및 추론 방법을 개발한 것입니다. 특히, MCTS와 결합된 가치 모델을 사용하여 복잡한 수학 문제를 해결하는 데 필요한 고품질의 훈련 데이터를 자동으로 생성할 수 있습니다. 이러한 접근 방식은 인간의 주석이 필요 없으며, 모델이 독립적으로 수학적 추론 과정을 파악하고 개선할 수 있게 합니다.

이 연구는 수학적 문제 해결에 있어 LLM의 가능성을 크게 확장시키며, 향후 이러한 기술이 실제 응용에 어떻게 도입될 수 있을지에 대한 통찰을 제공합니다.

## Similar Papers
- [Accessing GPT-4 level Mathematical Olympiad Solutions via Monte Carlo Tree Self-refine with LLaMa-3 8B](2406.07394.md)
- [Improve Mathematical Reasoning in Language Models by Automated Process Supervision](2406.06592.md)
- [Iterative Reasoning Preference Optimization](2404.19733.md)
- [Toward Self-Improvement of LLMs via Imagination, Searching, and Criticizing](2404.12253.md)
- [Self-Training with Direct Preference Optimization Improves Chain-of-Thought Reasoning](2407.18248.md)
- [THOUGHTSCULPT: Reasoning with Intermediate Revision and Search](2404.05966.md)
- [LiteSearch: Efficacious Tree Search for LLM](2407.00320.md)
- [Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters](2408.03314.md)
- [Tree of Thoughts: Deliberate Problem Solving with Large Language Models](2305.10601.md)
