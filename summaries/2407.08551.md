# Autoregressive Speech Synthesis without Vector Quantization
## TL;DR
## Summary
- [https://arxiv.org/pdf/2407.08551.pdf](https://arxiv.org/pdf/2407.08551.pdf)

## 논문 요약

### 1. 각 섹션 요약

#### **Introduction (서론)**
논문은 MELLE라는 새로운 연속값 기반 텍스트-음성 합성(TTS) 모델을 소개합니다. MELLE는 텍스트 조건에 기반하여 멜-스펙트로그램을 직접 생성하며, 전통적인 벡터 양자화를 필요로 하지 않습니다. 이 접근법은 VALL-E와 같은 기존 모델의 복잡한 다단계 코덱 코드 예측 과정을 방지하고, Mel-spectrograms을 이용하여 더 자연스러운 음성을 생성합니다.

#### **Related Work (관련 연구)**
전통적인 TTS 시스템은 접합 TTS, 파라메트릭 TTS, 그리고 종단형 신경망 TTS로 구분됩니다. 기존의 VALL-E 모델은 텍스트-음성 합성 작업을 하나의 디코더-온리 접근 방식으로 처리하지만, MELLE는 이를 간소화하고 향상된 월드 모델 성능을 통해 더 높은 품질의 음성을 제공합니다.

#### **MELLE Architecture (MELLE 아키텍처)**
MELLE는 몇 가지 주요 컴포넌트로 구성됩니다: 프리-넷, AR 트랜스포머 디코더, 잠재 샘플링 모듈, Stop prediction 레이어, 그리고 Post-Net입니다. 이 구성 요소들은 텍스트를 서브워드 토큰으로 변환하고, 멜-스펙트로그램을 추출하며, 이를 통해 자연스럽고 고품질의 음성을 생성합니다.

#### **Experimental Setup (실험 설정)**
MELLE는 Libriheavy 데이터셋에서 훈련되었습니다. 이 데이터셋은 6,736명의 화자로부터 50,000시간의 음성을 포함하고 있습니다. 텍스트는 BPE로 토크나이징되었고, 음성은 멜-스펙트로그램으로 변환되었습니다. 이러한 설정을 통해 MELLE는 다양한 음성 합성 시나리오에서 높은 성능을 발휘할 수 있습니다.

#### **Results and Discussion (결과 및 논의)**
MELLE는 객관적 메트릭에서 VALL-E와 유사한 성능을 보이며, 화자 유사성을 포함한 주관적 메트릭에서는 이를 능가합니다. MELLE는 WER에서 47.9% 상대적 감소를 달성하였으며, 주관적 평가에서는 인간 청취자들로부터 더 높은 평가를 받았습니다.

#### **Conclusion (결론)**
논문은 연속 음향 표현 기반의 언어 모델링 접근법을 제안하며, 이를 통해 벡터 양자화를 제거하고, 멜-스펙트로그램 예측을 통해 더 빠르고 효율적인 TTS 모델을 구축하는 방법을 설명합니다. MELLE는 다양한 샘플링 및 손실 함수를 통해 더 다양하고 강력한 예측을 생성할 수 있습니다.

### 2. 전체 요약
본 논문은 MELLE라는 새로운 연속값 기반의 텍스트-음성 합성 모델을 제안합니다. MELLE는 멜-스펙트로그램을 직접 예측하여 텍스트에 기반한 고품질 음성을 생성하며, 전통적인 벡터 양자화를 사용하지 않고도 더 자연스럽고 효율적인 음성 합성 작업을 수행합니다. 주요 혁신으로는 Latent Sampling Module, Stop Prediction Layer, 그리고 Post-Net을 포함한 여러 개선된 방법들이 있습니다. MELLE는 기존의 VALL-E 모델보다 더 향상된 성능을 보여주며, 특히 화자 유사성과 주관적 평가에서 우수한 결과를 나타냈습니다. 

이를 통해 MELLE는 TTS 연구에 있어 중요한 기여를 할 수 있으며, 향후 다양한 멀티링구얼 시나리오 및 대규모 음성 데이터셋에 적용될 가능성을 제시합니다.

---

## Similar Papers
- [E2 TTS: Embarrassingly Easy Fully Non-Autoregressive Zero-Shot TTS](2406.18009.md)
- [VALL-E 2: Neural Codec Language Models are Human Parity Zero-Shot Text to Speech Synthesizers](2406.05370.md)
- [LiveSpeech: Low-Latency Zero-shot Text-to-Speech via Autoregressive Modeling of Audio Discrete Codes](2406.02897.md)
- [Enhancing CTC-based speech recognition with diverse modeling units](2406.03274.md)
- [Naturalistic Music Decoding from EEG Data via Latent Diffusion Models](2405.09062.md)
- [RALL-E: Robust Codec Language Modeling with Chain-of-Thought Prompting for Text-to-Speech Synthesis](2404.03204.md)
- [Codecfake: An Initial Dataset for Detecting LLM-based Deepfake Audio](2406.08112.md)
- [Rho-1: Not All Tokens Are What You Need](2404.07965.md)
- [MusiConGen: Rhythm and Chord Control for Transformer-Based Text-to-Music Generation](2407.15060.md)
