# NVIDIA Tensor Core Programmability, Performance & Precision
## TL;DR
## Summary
- [https://arxiv.org/pdf/1803.04014.pdf](https://arxiv.org/pdf/1803.04014.pdf)

## 요약

### I. 서론
AI 기반 데이터 분석 및 딥러닝 애플리케이션의 증가로 인해 고성능 텐서 연산을 지원하는 하드웨어의 개발이 가속화되고 있습니다. NVIDIA의 Volta 마이크로아키텍처는 텐서 연산을 위한 특수 유닛인 Tensor Core를 도입하여 4x4 행렬의 곱셈 및 누산 연산을 클럭 사이클당 수행합니다. 이 논문에서는 Tensor Core 프로그래밍 접근법, 성능, 혼합 정밀도 계산으로 인한 정밀도 손실을 조사합니다.

### II. 관련 연구
딥러닝 애플리케이션의 증가로 텐서 연산을 지원하는 소프트웨어와 하드웨어가 급격히 발전했습니다. Google의 TPU, Microsoft의 Catapult 시스템, Movidius의 Myriad 2 Vision Processing Unit 등이 대표적입니다. 이와 유사하게, NVIDIA의 Tensor Core는 혼합 부동 소수점 정밀도를 사용하여 높은 처리량을 제공합니다.

### III. NVIDIA Volta 아키텍처
Volta 아키텍처는 GV100 GPU를 포함하며, 이는 640개의 Tensor Core를 포함합니다. 각 Tensor Core는 혼합 정밀도 모드에서 64개의 부동 소수점 곱셈 및 누산 연산을 클럭 사이클당 수행합니다. Tesla V100 가속기는 이론적으로 최대 125 Tflops/s의 성능을 제공합니다.

### IV. NVIDIA Tensor Core 프로그래밍
NVIDIA Tensor Core는 주로 행렬 곱셈 및 누산 연산을 수행합니다. 이를 프로그래밍하는 세 가지 주요 방법이 있습니다:
1. CUDA 9 Warp Matrix Multiply and Accumulate (WMMA) API
2. CUDA Templates for Linear Algebra Subroutines (CUTLASS)
3. cuBLAS GEMM 라이브러리

### IV-A. 행렬 곱셈
CUDA 9 WMMA API는 기본 행렬 곱셈을 수행하는 직접적인 방법을 제공합니다. CUTLASS는 다양한 타일링 전략과 소프트웨어 파이프라이닝을 활용하여 성능을 최적화합니다. cuBLAS는 BLAS 연산을 수행하는 고성능 라이브러리로, Tensor Core를 사용하여 성능을 극대화할 수 있습니다.

### IV-B. 배치 행렬 곱셈
많은 HPC 애플리케이션은 여러 개의 작은 행렬 곱셈을 병렬로 수행합니다. CUDA 9 WMMA를 사용하여 배치 행렬 곱셈을 구현하면 성능 향상을 기대할 수 있습니다.

### V. 정밀도 손실
Tensor Core는 혼합 정밀도 연산을 수행하기 때문에, 반 정밀도 입력으로 인한 정밀도 손실이 발생할 수 있습니다. 이를 해결하기 위해 정밀도 보정 기법을 제안하며, 이는 추가 연산 비용이 필요합니다.

### VI. 실험 설정
Tesla V100 가속기와 Intel E5-2690v3 호스트를 사용하여 성능을 측정했습니다. GEMM 연산의 성능을 주요 지표로 사용했으며, 여러 크기의 행렬을 테스트했습니다.

### VII. 결과
Tensor Core를 사용한 GEMM 연산은 최대 83 Tflops/s의 성능을 기록했습니다. 배치 행렬 곱셈의 경우 최대 4 Tflops/s의 성능을 달성했습니다. 정밀도 보정 기법을 적용하면 정밀도 손실을 효과적으로 줄일 수 있음을 확인했습니다.

### VII-A. 성능
최대 성능은 cuBLAS GEMM에서 기록되었으며, CUTLASS는 큰 행렬 크기에서 더 나은 성능을 보였습니다. 단순 WMMA 구현은 성능 향상을 가져오지 않았지만, 공유 메모리를 사용하면 성능이 크게 향상되었습니다.

### VII-B. 정밀도 및 보정
혼합 정밀도 연산으로 인한 정밀도 손실은 매트릭스 크기가 증가함에 따라 증가했습니다. 제안된 정밀도 보정 기법을 적용하면 정밀도 손실을 효과적으로 줄일 수 있음을 확인했습니다.

### VIII. 논의 및 결론
NVIDIA Tensor Core는 HPC 애플리케이션에서 큰 성능 향상을 제공할 수 있으며, 혼합 정밀도 계산의 정밀도 손실을 줄이기 위한 추가 연구가 필요합니다. Tensor Core의 프로그래밍 인터페이스는 지속적으로 발전할 것으로 예상되며, 이는 HPC 애플리케이션에 큰 혜택을 제공할 것입니다.

---

### 전체 요약
이 논문은 NVIDIA Tensor Core의 프로그래밍 가능성, 성능 및 정밀도 손실을 조사합니다. Tensor Core는 고성능 텐서 연산을 지원하며, AI 및 딥러닝 애플리케이션에서 큰 성능 향상을 제공합니다. 혼합 정밀도 계산으로 인한 정밀도 손실은 정밀도 보정 기법을 통해 효과적으로 줄일 수 있습니다. HPC 애플리케이션은 Tensor Core를 활용하여 큰 성능 향상을 기대할 수 있으며, 이를 위한 프로그래밍 인터페이스는 지속적으로 발전할 것입니다.