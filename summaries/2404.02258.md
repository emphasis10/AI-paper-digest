# Mixture-of-Depths: Dynamically allocating compute in transformer-based language models
## TL;DR
## Summary
- [https://arxiv.org/pdf/2404.02258.pdf](https://arxiv.org/pdf/2404.02258.pdf)

### 섹션별 요약

#### 1. 도입 (Introduction)
도입에서는 Transformer 기반 언어 모델이 입력 시퀀스에 동일한 연산량(FLOPs)을 할당하는 문제를 다룹니다. 이 연구는 Transformer가 각 토큰의 연산량을 동적으로 조정하도록 학습할 수 있음을 보여줍니다. 이를 통해 특정 레이어에서 일부 토큰에만 연산을 집중시켜 전체 연산량을 줄일 수 있습니다. 이 방식은 총 연산량을 예측 가능하게 유지하면서도 개별 토큰 수준에서 동적이고 문맥에 따라 조정됩니다. 이러한 모델은 기본 모델과 동일한 성능을 유지하면서도 각 예측 시 더 적은 연산을 필요로 하며, 훈련 후 샘플링 시 최대 50% 더 빠를 수 있습니다.

#### 2. 배경 (Background)
Transformer 아키텍처는 인공지능 발전에 중요한 역할을 했지만, 연산 비용이 높습니다. 이를 개선하기 위해 조건부 연산이 제안되었습니다. 조건부 연산은 필요할 때만 연산을 사용하도록 학습 메커니즘을 도입하여 전체 연산량을 줄이는 방법입니다. 이 논문에서는 기존 하드웨어와 호환되는 정적 연산 그래프를 사용하는 방법을 제안합니다. 이 방식은 하드웨어 효율성을 극대화하면서도 성능을 유지할 수 있습니다.

#### 3. Mixture-of-Depths Transformer 구현 (Implementing Mixture-of-Depths Transformers)
이 섹션에서는 Mixture-of-Depths (MoD) Transformer's 고수준 전략을 설명합니다.
- 정해진 연산 예산 설정: 각 레이어에서 연산에 참여할 토큰의 수를 제한하여 연산 예산을 설정합니다.
- 라우터 사용: 각 토큰에 대해 연산 참여 여부를 결정하는 스칼라 가중치를 발행합니다.
- 상위 k개의 토큰 선택: 각 블록에서 연산에 참여할 토큰을 선택하여 정적 연산 그래프를 유지합니다.
- 훈련 후 샘플링 시 복잡성 논의: 훈련 후 예측 시 발생할 수 있는 문제를 논의합니다.

#### 4. 결과 (Results)
훈련 및 성능 비교에서 MoD Transformer's가 isoFLOP 최적 베이스라인보다 더 나은 성능을 보였으며, 더 작은 모델에서도 동일한 성능을 유지하면서 빠르게 훈련되었습니다. MoD Transformer는 연산 비용을 줄이면서도 성능 저하 없이 모델 크기를 키우거나 더 오래 훈련할 수 있습니다.

#### 5. 논의 (Discussion)
MoD Transformer's는 FLOPs를 절약하면서도 성능을 유지할 수 있는 가능성을 보여줍니다. 이를 통해 주어진 훈련 예산 내에서 더 나은 성능의 모델을 훈련할 수 있습니다. MoD Transformer's는 다양한 연산 유형을 동적으로 선택할 수 있어, 다른 조건부 연산 기법과 통합할 수 있는 가능성을 제공합니다.

### 전체 요약
이 논문은 Transformer 기반 언어 모델에서 연산량을 동적으로 조정하는 Mixture-of-Depths (MoD) 방식을 제안합니다. 기존 모델이 모든 토큰에 동일한 연산량을 할당하는 것과 달리, MoD 방식은 각 토큰에 필요한 연산량을 학습하여 동적으로 할당합니다. 이를 통해 전체 연산량을 줄이면서도 성능을 유지할 수 있습니다. MoD Transformer's는 정적 연산 그래프를 유지하여 하드웨어 효율성을 극대화하며, 훈련 후 샘플링 시에도 성능 저하 없이 더 빠르게 작동할 수 있습니다. 이 방식은 특히 조건부 연산 기법과 통합할 수 있는 가능성을 제공하여, 향후 더 다양한 연산 요구를 충족시킬 수 있는 잠재력을 가지고 있습니다.