# AlphaMaze: Enhancing Large Language Models' Spatial Intelligence via GRPO
## TL;DR
## Summary
- [https://arxiv.org/pdf/2502.14669.pdf](https://arxiv.org/pdf/2502.14669.pdf)

### 1. 논문의 각 섹션 요약

**소개 (Introduction):**
큰 언어 모델(LLM)은 자연어 처리에서 뛰어난 성능을 보였으나, 시각적 공간 추론, 특히 미로 탐색과 같은 작업에서는 부족함이 있었다. 논문은 이러한 시각적 추론 능력을 향상시키기 위해, 큰 언어 모델을 대상으로 한 새로운 2단계 교육 프레임워크를 제안했다. 첫 번째 단계인 감독된 미세조정(SFT)을 통해 미로의 시각적 표현을 토큰화하여 학습한다. 그 후, GRPO(Group Relative Policy Optimization)를 적용하여 모형의 추론 능력을 향상시키고, 체인-오브-생각(chain-of-thought) 유형의 추론 행동을 유도한다.

**관련 연구 (Related Work):**
지금까지 대형 언어 모델의 추론 능력을 개선하기 위한 여러 접근법들이 존재했으며, 그 중 연쇄적 사고 유도(CoT prompting)와 감독된 미세조정 및 강화학습(GRPO)이 주로 사용된다. 이러한 기법들은 각각 추론 과정의 중간 단계 생성과, 인식된 시각 정보를 바탕으로 한 추론 성능 향상에 기여한다.

**감독된 미세조정 (Supervised Fine-Tuning):**
SFT는 토큰화된 시각적 입력과 움직임 토큰을 예측하는 능력을 교육하는 첫 번째 단계로, 미로 탐색의 기초 능력을 강화한다. 이 과정은 각 단계에서 다음의 움직임 토큰을 예측하는 방식으로 진행되어, 단계별 추론을 학습하게 한다.

**강화학습 및 GRPO:**
GRPO는 그룹 상대 정책 최적화를 활용하여 강화학습을 통한 추론 성능을 향상시키며, 설계된 보상 함수를 기반으로 정확성, 적합한 움직임 순서, 그리고 정해진 포맷의 출력을 장려한다. 이 과정을 통해 모형의 시각적 추론 능력을 보다 세련된 방향으로 유도한다.

**실험 및 결과 (Experiments and Results):**
새롭게 제시한 MazeBench 벤치마크를 통해 제안된 모형의 성능을 평가하였다. AlphaMaze-SFT 모델은 86%의 정확성을 보였고, GRPO가 추가된 AlphaMaze 모델은 93%로 성능이 상승했다. 이로 인해, GRPO 과정이 정말로 추론 능력을 강화하며 더 세련된 생각 패턴을 유도함을 알 수 있었다.

**결론 (Conclusion):**
제안된 접근법은 미로 탐색을 위한 시각적 추론 능력을 표준 대형 언어 모델에 부여하는 새로운 방법을 제시하며, 적절한 훈련 방법론과 시각적 토큰화 표현의 조합이 언어 모델과 시각 AI 사이의 간극을 메워줄 수 있음을 시사한다. 이는 로봇공학, 자율 탐색 등 여러 도메인에 효율적인 시각적 추론을 제공할 가능성을 열어준다.

### 2. 전체 요약

본 논문에서는 시각적 공간 추론 능력을 향상시키기 위해 대형 언어 모델을 대상으로 한 새로운 2단계 교육 프레임워크를 제안하였습니다. 이 접근법은 감독된 미세조정 및 그룹 상대 정책 최적화를 통해 미로 탐색과 같은 시각적 작업의 정확성을 향상시켰습니다. AlphaMaze 모델의 학습 결과, 감독된 미세조정을 통해 정확성이 86%에 도달하였으며, GRPO를 적용하여 93%의 성능을 달성하였습니다. 이러한 연구는 언어 모델과 시각 AI의 통합 가능성을 보여주며, 다양한 분야에서 활용될 수 있는 기반을 마련합니다.