# DeMo: Decoupled Momentum Optimization
## TL;DR
## Summary
- [https://arxiv.org/pdf/2411.19870.pdf](https://arxiv.org/pdf/2411.19870.pdf)

1. 각 섹션의 요약 및 주요 기여와 혁신 부분:

   - **서론**: 대규모 신경망, 특히 언어 모델은 많은 수의 파라미터를 가지고 있으며, 이러한 모델을 훈련하기 위해서는 여러 가속기(예: GPU, TPU)가 필요합니다. 현재 일반적인 훈련 전략으로는 가속기 간의 통신이 필수적이며 이는 고속의 특수 네트워크 연결을 요구합니다. 이 논문은 이러한 통신의 필요성을 줄이는 DeMo(Demoupled Momentum Optimization)라는 최적화 알고리즘을 제안합니다.

   - **배경 및 관련 연구**: 통신 오버헤드를 줄이는 다양한 전략이 존재하며, 주로 양자화 및 희소화, 저랭크 프로젝션, 페더레이션 평균화 등의 방법이 있습니다. 이러한 방법들은 각각의 장단점을 가지고 있으며, 본 연구에서는 이들 중 중앙 집중적인 방법에 집중하고 있습니다.

   - **방법론**: 기존 알고리즘의 점진적인 수정보다는, 가속기 간의 다르게 작동하는 최적화 상태를 유지하면서 통신 요구를 최소화하는 새로운 분리된 모멘텀 최적화 알고리즘(Demoupled Momentum Optimization)을 제안합니다. 이 알고리즘은 빠르게 움직이는 모멘텀 성분을 효율적으로 추출하여 최소한의 통신으로 동기화합니다.

   - **결론**: DeMo는 기존 AdamW와 비교하여 느린 속도 감소 없이 대규모 언어 모델의 훈련 시 통신 요구를 획기적으로 줄일 수 있습니다. DeMo의 다른 변수인 'signum' 변형은 메모리를 더 효율적으로 사용하며, 사전 계산된 작은 DCT 전환 행렬을 사용하면 계산 오버헤드가 거의 없습니다.

2. 전반적인 요약:

   이 연구는 대규모 신경망 학습에서 필요한 통신 오버헤드를 줄이기 위한 새로운 최적화 알고리즘, 'Decoupled Momentum Optimization (DeMo)'를 제안하고 평가하였습니다. DeMo는 가속기 간 통신 중복성을 줄여 주며, AdamW와 비교하여 성능 감소 없이 통신 요건을 획기적으로 줄일 수 있음을 실험적으로 증명하였습니다.