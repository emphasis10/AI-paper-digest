# Scaling Granite Code Models to 128K Context
## TL;DR
## Summary
- [https://arxiv.org/pdf/2407.13739.pdf](https://arxiv.org/pdf/2407.13739.pdf)

### 논문의 요약

#### 1. 각 섹션별 요약
1. **서론 (Introduction)**
   논문은 코드 언어 모델을 위한 긴 컨텍스트 길이의 중요성을 강조하며, 현재 공개된 대부분의 코드 언어 모델들이 상대적으로 짧은 컨텍스트 길이를 지원하는 문제를 제기한다. 이에 대해 3B와 8B의 긴 컨텍스트 Granite 코드 모델을 소개한다. 이 모델들은 최대 128K 토큰의 컨텍스트 길이를 지원하며, 이로 인해 실제 소프트웨어 개발에서 더 실용적이다.

2. **긴 컨텍스트 모델링 (Long Context Modeling)**
   긴 컨텍스트 모델링을 위해, 계속적인 사전 학습와 명령어 튜닝 단계를 거친다. 기본 가설은 대규모 사전 학습을 통해 이미 획득한 정보를 충분히 활용할 수 있다는 점이다. 이를 통해 원래의 사전 학습 기간 동안 본 4K 컨텍스트를 128K로 확장할 수 있다.

3. **계속적인 사전 학습 (Continual Pretraining)**
   계속적인 사전 학습은 sequence parallelism을 사용하며, 점진적으로 RoPE 베이스 주파수를 증가시킨다. 이 과정에서는 주로 파이썬, C, C++, 고(Go), 자바, 자바스크립트, 타입스크립트 등의 프로그래밍 언어로 작성된 자료를 사용하며, 리포지토리 단위로 파일을 패킹하여 컨텍스트 길이를 업샘플링한다.

4. **명령어 튜닝 (Instruction Tuning)**
   긴 컨텍스트 명령어 모델들을 훈련시키기 위해서는, 기존 Granite 코드 명령어 모델의 데이터를 계속 사용하며, 길이를 증가시키기 위해 신속하게 생성된 코드 명령어 데이터셋도 포함한다. 각 샘플에 대해 멀티턴 손실 마스크를 사용하고, 모델의 응답 후 EOS 토큰을 추가하여 생성 중 도망치는 문제를 방지한다.

5. **결과 (Results)**
   다양한 벤치마크를 통해 Granite 코드 모델들을 평가한다. HumanEvalPack, Long Code Completion, RepoBench-P, RepoQA, Key Retrieval 등의 태스크에서 긴 컨텍스트 모델들이 눈에 띄게 개선된 성과를 보인다. 특히, 4K 이상의 토큰 길이에서 원래의 베이스 모델보다 높은 성과를 자랑한다.

#### 2. 전체 요약
이 논문은 기존 코드 언어 모델의 컨텍스트 길이 한계를 극복하기 위해 3B와 8B의 긴 컨텍스트 Granite 코드 모델을 소개한다. 이 모델들은 최대 128K 토큰의 유효 컨텍스트 길이를 지원하며, 이를 달성하기 위한 두 가지 주요 접근 방식을 논의한다. 첫째, 계속적인 사전 학습을 통해 기존 베이스 모델의 컨텍스트 길이를 점진적으로 확장한다. 둘째, 길이가 증가된 신속한 생성 명령어 데이터셋을 사용하여 모델을 추가 튜닝한다. 이러한 접근 방식으로, 긴 컨텍스트 모델은 짧은 컨텍스트 태스크에서도 성능 저하 없이 우수한 성과를 보인다. 이 논문은 긴 컨텍스트 Granite 코드 모델들이 실제 코딩 작업에서 더 실용적일 수 있음을 실험적으로 입증하며, 모든 긴 컨텍스트 모델을 Apache 2.0 라이선스 하에 공개한다.