# Compressing LLMs: The Truth is Rarely Pure and Never Simple
## TL;DR
## Summary
- [https://arxiv.org/pdf/2310.01382.pdf](https://arxiv.org/pdf/2310.01382.pdf)

이 문서는 대규모 언어 모델(Large Language Models, LLMs)의 압축에 관한 연구를 다루고 있습니다. 연구의 핵심은 대규모 언어 모델의 계산 및 메모리 요구 사항을 줄이기 위한 방법을 탐구하고, 이를 위한 새로운 평가 프로토콜인 지식 집약적 압축 LLM 벤치마크(LLM-KICK)를 소개하는 것입니다.

### 1. 서론 및 요약
- 대규모 언어 모델(LLMs)은 자연어 처리(NLP), 컴퓨터 비전, 그래프 신경망 등 다양한 분야에서 뛰어난 성능을 보이고 있습니다. 하지만, 이들 모델은 수십억 개의 파라미터를 포함하며, 이에 따른 고도의 계산 및 메모리 요구 사항으로 인해 널리 사용되는 것이 제한됩니다. 예를 들어, GPT-175B 모델은 단순히 모델 가중치를 로드하는 데 325GB의 GPU 메모리가 필요합니다.
- 이 연구는 LLMs의 네트워크 프루닝(가중치 삭제)과 가중치 양자화(낮은 비트 수준 표현으로 변환)를 포함한 압축 방법을 탐구하고 있는데, 이전 연구들은 50-60%의 희소성을 달성하고 비트 폭을 3~4비트로 줄여도 성능 저하가 거의 없음을 보여 주었습니다. 그러나 이러한 연구들 대부분은 복잡도를 제대로 반영하지 못하는 단순한 지표(예: 복잡도 표현이 문제가 될 수 있는 'perplexity')에 의존하고 있습니다.
- 이 논문은 LLM-KICK을 통해 압축된 LLMs의 평가 프로토콜을 재정의하고, 언어 이해, 추론, 생성, 맥락 검색, 맥락 요약 등의 능력을 포괄적으로 평가하고자 합니다. 이는 기존 압축 전략이 실제로 성능을 얼마나 유지하는지에 대한 체계적인 이해를 가능하게 할 것입니다.

### 번역 및 요약
- 대규모 언어 모델은 우리 생활 곳곳에 영향을 미치고 있으나, 이러한 모델들은 운영 비용이 매우 높습니다. 특히, 대모델들은 GPU 메모리를 많이 사용하므로, 이를 해결하기 위한 방법 중 하나가 모델의 크기를 줄이는 것입니다. 이 연구는 모델의 크기를 줄이기 위한 '네트워크 프루닝'과 '가중치 양자화'라는 두 가지 방법을 집중적으로 다룹니다.
- 이 연구의 가장 큰 기여는 LLM-KICK, 즉 압축된 대규모 언어 모델의 성능을 평가하기 위한 새로운 프로토콜을 개발한 것입니다. 이 평가 도구는 다양한 언어 관련 태스크를 포함하여 모델의 압축이 성능에 미치는 영향을 다각도에서 평가할 수 있게 합니다.
- 이 연구는 압축 기술이 진정으로 모델의 성능을 어떻게 유지하고 있는지에 대한 정교하고 포괄적인 평가를 제공함으로써, 더 나은 압축 방법의 개발을 촉진시킬 수 있을 것으로 기대됩니다.

이 논문은 대규모 언어 모델의 높은 운영 비용을 해결하기 위한 압축 기술의 현재 상태를 평가하고, 이 분야의 연구를 발전시키기 위해 새로운 평가 도구인 LLM-KICK을 제안합니다. LLM-KICK을 통해 복잡도 표현이나 기타 언어 태스크에서의 미묘한 성능 변화를 포착할 수 없는 기존 평가 방법의 한계를 극복하고자 합니다. 이 연구는 압축된 대규모 언어 모델의 진정한 가능성과 한계를 탐구하는 데 중요한 기여를 하고 있습니다.