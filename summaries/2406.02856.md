# Xmodel-LM Technical Report
## TL;DR
## Summary
- [https://arxiv.org/pdf/2406.02856.pdf](https://arxiv.org/pdf/2406.02856.pdf)


### 1. 부분별 요약 및 설명

#### 1.1 서론

이 논문은 Xmodel-LM이라는 소형이지만 효율적인 1.1B 언어 모델을 소개합니다. 이 모델은 2조 개 이상의 토큰으로 사전 학습되었으며, 중국어와 영어 코퍼스를 균형 있게 사용하여 다양한 자연어 처리 작업에서 뛰어난 성능을 보여줍니다. 모델의 체크포인트와 코드는 모두 GitHub에서 공개됩니다.

##### 기여와 혁신
- **작은 규모**: 기존의 큰 모델들에 비해 크기가 작지만, 성능은 비슷하거나 더 우수합니다.
- **효율적인 학습 데이터 사용**: 자체 구축한 데이터셋 'Xdata'를 사용하여 최적화된 결과를 얻었습니다.

#### 1.2 사전 학습

여기서는 Xmodel-LM의 사전 학습 과정을 다룹니다. 데이터 소싱, 전처리 방법, 토크나이저 구성, 모델 아키텍처 및 학습 매개변수 설정이 포함됩니다.

##### 주요 내용
- **데이터 소싱**: 다양한 LLM 데이터셋에서 데이터를 수집하여 학습의 품질과 다양성을 보장했습니다.
- **데이터 전처리**: 5-gram Kneser-Ney 모델을 이용해 데이터 품질을 향상시키고, SimHash 기반의 중복 제거를 통해 데이터 중복을 최소화했습니다.

#### 1.3 토크나이저

단일어 알고리즘을 사용하여 데이터 토크나이징을 수행하였습니다. 중국어와 영어가 혼합된 코퍼스를 기반으로 32,000개의 작은 어휘를 사용하여 높은 압축률을 구현했습니다.

##### 핵심 포인트
- **작은 어휘 크기**: 32,000개의 어휘 크기로도 충분한 압축을 구현했습니다.
- **수치 데이터 처리**: 숫자를 개별 숫자로 분리하여 인코딩 효율성을 높였습니다.

#### 1.4 모델 아키텍처

LLama 2 모델 구조를 채택하였고, 주요 특징은 다음과 같습니다:
- **Rotary Positional Embeddings (RoPE)**: 각 레이어에 RoPE를 통합하여 학습 성능을 향상시켰습니다.
- **SwiGLU 활성화 함수**: 전통적인 ReLU 대신 SwiGLU 활성화 함수를 활용하여 성능을 최적화했습니다.

#### 1.5 학습

단일 노드에서 8개의 H800 GPU를 사용하여 학습을 진행했습니다. 학습 효율을 높이기 위해 Distributed Data Parallel과 FlashAttention-V2를 사용하였고, AdamW 옵티마이저를 적용했습니다.

#### 1.6 결과

여러 인공지능 모델과의 비교 평가를 통해, Xmodel-LM의 우수한 성능을 입증했습니다. 다양한 벤치마크 테스트에서 기존의 모델들을 능가하는 성과를 보여주었습니다.

### 2. 전체 요약

Xmodel-LM은 작은 크기에도 불구하고 높은 성능을 발휘하는 혁신적인 언어 모델입니다. 자체 구축한 데이터셋과 고유의 데이터 전처리 및 토크나이저 방식을 통해 효율적으로 학습되었으며, LLama 2의 모델 구조를 활용하여 안정적이고 높은 성능을 보장합니다. 다양한 벤치마크에서 기존의 모델과 비교했을 때 뛰어난 성능을 나타내며, 다양한 자연어 처리 작업에서의 적용 가능성을 보여줍니다.

이 논문은 작은 규모의 모델이 큰 규모의 모델과 유사하거나 더 뛰어난 성능을 보일 수 있음을 입증하였으며, 학습 과정의 효율적인 데이터 처리 및 최적화 기술을 소개함으로써 자연어 처리 분야의 중요한 기여를 제공합니다. 