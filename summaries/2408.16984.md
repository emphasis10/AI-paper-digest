# Beyond Preferences in AI Alignment
## TL;DR
## Summary
- [https://arxiv.org/pdf/2408.16984.pdf](https://arxiv.org/pdf/2408.16984.pdf)

### 섹션별 요약 및 주요 기여 요약

#### 1. 서론
서론에서는 AI 시스템의 능력과 사회적 채택이 증가함에 따라 인간의 가치에 맞지 않는 AI 시스템이 가져올 영향에 대한 우려가 증가하고 있음을 설명합니다. 특히, AI 정렬(Alignment) 분야의 중요성과 이를 위해 인간의 선호를 정확히 파악하고 정렬시키는 문제를 강조합니다. 주요 기여로는 인간의 선호를 넘어서는 AI 정렬의 필요성을 강조하며, 기존 접근법의 한계점을 지적합니다.

#### 2. 선행 연구
선행 연구 섹션에서는 기존의 AI 정렬 방법, 특히 인간의 선호를 기반으로 한 접근법의 한계에 대해 설명합니다. 또한, 선호가 단지 표면적인 가치나 기준을 반영하는데 그치며, 이를 기반으로 한 AI 정렬이 가져올 수 있는 문제점을 지적합니다.

#### 3. 방법론
이 섹션에서는 AI 시스템에서 사용되는 보상 함수와 그 한계에 대해 논의하며, 특히 문맥에 따른 선호의 변화와 이를 AI 시스템에 적용하는 방법에 대해 설명합니다. 주요 기여는 기존 보상 함수의 한계를 극복하기 위해 문맥에 민감한 보상 모델을 제안하는 것입니다.

#### 4. 결과
결과 섹션에서는 문맥 민감한 보상 모델을 적용한 사례와 그 한계성을 다룹니다. 또한, 일반 목적의 AI 시스템에서도 효과적으로 적용될 수 있는지에 대한 평가 결과를 제시합니다. 주요 기여는 이러한 보상 모델이 실제로 AI 시스템의 정렬 문제를 해결하는 데 얼마나 효과적인지를 실험적으로 입증하려는 시도입니다.

#### 5. 논의 및 결론
논의에서는 AI가 인간의 가치와 선호에 맞추어 정렬되기 위해 해결해야 하는 추가적인 과제들, 특히 다중 주체 정렬의 문제를 제기하며, 기존 접근법이 어떻게 이를 해결하지 못했는지 설명합니다. 결론에서는 AI 시스템이 단순히 선호를 따르는 것에서 벗어나, 각각의 사회적 역할에 맞는 규범적 기준을 준수해야 한다고 강조합니다.

### 전체 요약
이 논문은 AI 시스템이 인간의 선호를 따르는 접근법의 한계를 지적하고, AI 시스템이 인간의 가치와 규범에 더 잘 맞도록 정렬하기 위해 새로운 방법론을 제안합니다. 선호를 기준으로 하는 기존의 정렬 방법은 제한적이며, 특히 문맥에 따라 변화하는 인간의 선호를 제대로 반영하지 못합니다. 따라서, 논문은 문맥 민감한 보상 모델을 제안하며, 이를 바탕으로 AI 시스템의 정렬을 개선하려는 시도를 합니다. 또한, 다중 주체 정렬의 필요성을 강조하며, AI 시스템이 특정 작업이나 역할에 맞는 규범적 기준을 따르는 것이 중요하다고 주장합니다. 

이 논문은 AI 정렬 연구의 중요한 방향성을 제시하며, 더 발전된 AI 시스템의 개발에 기여할 수 있는 이론적 및 실질적 기반을 마련합니다.