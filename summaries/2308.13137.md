# OmniQuant: Omnidirectionally Calibrated Quantization for Large Language Models
## TL;DR
## Summary
- [https://arxiv.org/pdf/2308.13137.pdf](https://arxiv.org/pdf/2308.13137.pdf)

1. 서론 및 관련 연구

이 논문은 큰 언어 모델(LLM)의 실용적 배치를 어렵게 만드는 엄청난 메모리와 계산 요구 사항을 다루기 위해, 다양한 양자화 설정에서 좋은 성능을 유지하면서 PTQ의 계산 효율성을 유지하는 “Omnidirectionally Calibrated Quantization (OmniQuant)” 기술을 소개합니다. OmniQuant는 Learnable Weight Clipping (LWC)과 Learnable Equivalent Transformation (LET)을 포함한 두 가지 혁신적인 구성 요소로 구성됩니다 .

1. OmniQuant
    - 블록별 양자화 오류 최소화: 모델의 각 변압기 블록의 양자화 파라미터를 순차적으로 최적화함으로써, 가중치 전용 및 가중치-활성화 양자화 모두에 효과적인 방법입니다.
    - 가중치 클리핑 학습: 가중치의 극단 값을 조절하여 양자화를 용이하게 하는 동시에 최적의 클리핑 임계값을 결정합니다.
    - 학습 가능한 동등 변환: 활성화의 이상치를 다루기 위해 활성화에서 가중치로 양자화의 도전을 전환합니다.
2. 실험 결과
    - 가중치 전용 양자화 결과: OmniQuant는 다양한 LLM 가족(OPT, LLaMA-1, LLaMA-2) 및 다양한 양자화 구성에서 기존 LLM 가중치 전용 양자화 방법을 일관되게 능가합니다.
    - 가중치-활성화 양자화 결과: 양자화된 모델들은 다양한 양자화 설정에서 우수한 성능을 보여주며, 특히 낮은 비트 양자화(예: W2A16, W4A4)에서 두드러집니다.
3. 결론

이 연구는 큰 언어 모델의 양자화 과정을 효율적으로 최적화하는 OmniQuant 방법을 제시하며, 다양한 양자화 설정에서 우수한 성능을 입증하고 실제 장치에서의 추론 속도 및 메모리 절감 효과를 보여줍니다.

전체 요약

이 논문은 큰 언어 모델(LLM)의 계산 효율성을 개선하기 위한 새로운 양자화 기법, OmniQuant를 제안합니다. 이 기법은 Learnable Weight Clipping (LWC)과 Learnable Equivalent Transformation (LET)이라는 두 가지 혁신적인 접근 방식을 통해, 양자화 과정에서 가중치와 활성화의 이상치를 효과적으로 관리하고 최적화합니다. 다양한 양자화 설정과 모델 크기에 걸쳐 우수한 성능을 입증하였으며, 이는 낮은 비트 설정에서도 높은 성능을 유지할 수 있음을 보여줍니다. 실험 결과는 OmniQuant가 기존 PTQ 방식을 상당히 개선하고, 실제 장치에서의 추론 속도 및 메모리 사용을 현저히 줄일 수 있음을 보여줍니다.