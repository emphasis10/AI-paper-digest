# Layer Skip: Enabling Early Exit Inference and Self-Speculative Decoding
## TL;DR
## Summary
- [https://arxiv.org/pdf/2404.16710.pdf](https://arxiv.org/pdf/2404.16710.pdf)


## 1. 소개
- **대규모 언어 모델(LLM)**의 높은 계산 및 메모리 요구사항은 GPU 서버에서의 배포 시 높은 비용을 초래.
- **LayerSkip**은 LLM의 추론 속도를 향상시키기 위한 솔루션으로, 훈련 시 레이어 드롭아웃과 조기 종료 손실을 적용하고, 추론 시 조기 종료를 통해 정확성을 높임.
- 또한, **자체 추측 디코딩(self-speculative decoding)**을 도입하여 메모리 풋프린트를 줄이고 공유 계산 및 활성화를 활용하여 속도를 높임.

## 2. 동기
- LLM의 레이어별 출력을 분석하여 조기 종료가 가능한 레이어를 찾아내는 것이 목표.
- 조기 종료를 통해 계산 비용을 줄이기 위해 훈련 시 레이어 드롭아웃과 조기 종료 손실을 적용.
- 최종 출력의 정확성을 유지하기 위해 훈련 시 레이어별 조기 종료 손실을 적용하여 모델이 모든 레이어에서 정확한 출력을 예측하도록 유도.

## 3. 관련 연구
- **Dropout**: 과적합 방지를 위해 레이어의 일부 출력을 무작위로 제거하는 기술.
- **Layer Dropout**: 훈련 시 레이어를 무작위로 스킵하여 모델이 나중 레이어에 덜 의존하도록 유도.
- **Early Exit**: CNN 및 언어 모델에서 조기 종료를 도입하여 계산 비용을 줄이는 방법.
- **Speculative Decoding**: 빠른 모델을 사용하여 토큰을 생성하고, 더 큰 모델로 이를 검증 및 수정하는 기술.

## 4. 제안된 솔루션
### 4.1 훈련: Layer Dropout 및 Early Exit Loss
#### 4.1.1 Layer Dropout
- **Layer Dropout**: 레이어 드롭아웃은 훈련 중에 특정 레이어의 출력을 무작위로 제거하는 기술입니다. 이를 통해 모델이 나중 레이어에 과도하게 의존하지 않도록 합니다. 각 레이어에서 드롭아웃을 적용하여, 모델이 모든 레이어에서 고르게 학습되도록 합니다.
- **드롭아웃 비율 설정**: 레이어별 드롭아웃 비율은 점진적으로 증가하도록 설정합니다. 초기 레이어에서는 드롭아웃 비율이 낮고, 마지막 레이어로 갈수록 드롭아웃 비율이 높아집니다. 이를 통해 모델이 나중 레이어에 덜 의존하고, 초기에 더 많은 정보를 학습하도록 유도합니다.
  - 수식: \(p_{l,t} = S(t)D(l)p_{max}\)
  - 여기서 \(p_{max}\)는 최대 드롭아웃 비율, \(D(l)\)은 레이어별 스케일링 함수, \(S(t)\)는 시간별 스케일링 함수입니다.

#### 4.1.2 Early Exit Loss
- **Early Exit Loss**: 조기 종료 손실은 모델이 각 레이어에서 정확한 출력을 예측하도록 하는 추가적인 손실 함수입니다. 이를 통해 모든 레이어가 독립적으로 정확한 출력을 생성할 수 있도록 합니다.
- **손실 함수**: 각 레이어에서 출력된 값을 최종 출력과 비교하여 손실을 계산합니다. 이를 통해 모든 레이어가 독립적으로 학습되며, 최종 출력의 정확도를 높일 수 있습니다.
  - 수식: \(J(X, Y, t) = \sum_{l=0}^{L-1} e(t, l) \cdot J_{BCE}(g(x_{l+1}), Y)\)
  - 여기서 \(e(t, l)\)은 각 레이어의 손실 가중치입니다.

### 4.2 추론: Early Exit
- **조기 종료 추론**: 추론 시, 모델의 모든 레이어를 통과하지 않고, 일정 레이어까지만 실행한 후 조기 종료하여 최종 출력을 생성합니다. 이를 통해 계산 비용을 크게 줄일 수 있습니다.
- **레이어 수 조정**: 조기 종료를 통해 몇 개의 레이어까지만 실행할지 조정하여, 성능과 속도 사이의 균형을 맞춥니다. 

### 4.3 추론: Self-Speculative Decoding
#### 4.3.1 Self-Drafting
- **Self-Drafting**: 자체 추측 디코딩의 첫 번째 단계로, 모델의 초기 레이어를 사용하여 초안 토큰을 생성합니다. 조기 종료를 통해 생성된 초안 토큰은 나중에 검증 단계에서 사용됩니다.
- **조기 종료 설정**: 초안 토큰을 생성할 때, 특정 레이어까지만 실행하여 초안을 생성합니다. 이를 통해 초안을 생성하는 속도를 높입니다.

#### 4.3.2 Self-Verification
- **Self-Verification**: 자체 추측 디코딩의 두 번째 단계로, 초안 토큰을 검증하고 필요한 경우 수정합니다. 이를 위해 모델의 나머지 레이어를 사용하여 최종 출력을 생성합니다.
- **검증 단계**: 초안 토큰을 검증할 때, 첫 번째 단계에서 사용한 레이어의 출력을 재사용하여 계산 비용을 줄입니다. 이를 통해 검증 속도를 높이고 메모리 사용량을 줄입니다.

#### 4.3.3 Reusing the Cache
- **캐시 재사용**: 자체 추측 디코딩의 효율성을 높이기 위해, 초안 생성 단계에서 사용된 캐시를 검증 단계에서도 재사용합니다. 이를 통해 메모리 사용량을 줄이고 계산 속도를 높입니다.
- **KV 캐시 및 Exit Query 캐시**: 초안 생성 단계에서 사용된 키-값 쌍 캐시(KV 캐시)와 조기 종료 레이어의 쿼리 캐시를 저장하고, 검증 단계에서 이를 재사용하여 계산 비용을 절감합니다.

## 5. 실험
- 다양한 훈련 유형(초기 훈련, 지속적 훈련, 코드 데이터에 대한 미세 조정, 특정 작업에 대한 미세 조정)에서 LayerSkip을 평가.
- **LayerSkip**이 적용된 모델은 다양한 평가 과제에서 기존 모델보다 빠른 속도와 유사한 정확도를 보임.

## 6. 결과
- **조기 종료 추론 결과**: 다양한 과제에서 조기 종료를 통해 정확도를 유지하면서 속도를 높임.
- **자체 추측 디코딩 결과**: 자체 추측 디코딩을 통해 속도를 높이고 정확도를 유지함.

## 전체 요약
- **훈련**: 레이어 드롭아웃과 조기 종료 손실을 적용하여 모델이 모든 레이어에서 정확한 출력을 예측하도록 훈련합니다.
- **추론**: 조기 종료를 통해 계산 비용을 줄이고, 자체 추측 디코딩을 통해 정확도를 유지하면서 속도를 높입니다.
- **효율성**: 캐시 재사용을 통해 메모리 사용량과 계산 비용을 절감합니다.


LayerSkip은 대규모 언어 모델(LLM)의 추론 속도를 높이기 위한 솔루션으로, 훈련 시 레이어 드롭아웃과 조기 종료 손실을 적용하고, 추론 시 조기 종료와 자체 추측 디코딩을 도입하여 메모리 풋프린트를 줄이고 계산 속도를 높입니다. 이를 통해 다양한 훈련 유형에서 모델의 성능을 유지하면서도 빠른 추론을 가능하게 합니다.

## Similar Papers
- [Is Flash Attention Stable?](2405.02803.md)
- [Accelerating Large Language Model Decoding with Speculative Sampling](2302.01318.md)
- [Nearest Neighbor Speculative Decoding for LLM Generation and Attribution](2405.19325.md)
- [EAGLE-2: Faster Inference of Language Models with Dynamic Draft Trees](2406.16858.md)
- [Draft & Verify: Lossless Large Language Model Acceleration via Self-Speculative Decoding](2309.08168.md)
- [Fast Inference from Transformers via Speculative Decoding](2211.17192.md)
- [Kangaroo: Lossless Self-Speculative Decoding via Double Early Exiting](2404.18911.md)
- [Distributed Speculative Inference of Large Language Models](2405.14105.md)
- [TriForce: Lossless Acceleration of Long Sequence Generation with Hierarchical Speculative Decoding](2404.11912.md)
