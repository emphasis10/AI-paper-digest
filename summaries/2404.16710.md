# Layer Skip: Enabling Early Exit Inference and Self-Speculative Decoding
## TL;DR
## Summary
- [https://arxiv.org/pdf/2404.16710.pdf](https://arxiv.org/pdf/2404.16710.pdf)

이 논문에서는 특정 훈련 단계에서의 레이어를 동적으로 건너뛰는 "LayerSkip" 기법을 통해 대규모 언어 모델(LLMs)의 추론 속도를 향상시키는 새로운 접근법을 제안합니다. 이 기법은 추론 과정에서 초기 레이어에서의 빠른 출구와 자체 추측적 디코딩을 가능하게 합니다. 다음은 각 섹션의 주요 내용 요약입니다.

1. **서론**:
   - 대규모 언어 모델은 다양한 응용 분야에 배포되었으나, 고비용의 계산 및 메모리 요구로 인해 GPU 서버에서의 배포 비용이 높습니다.
   - 이러한 문제를 해결하기 위해, 이 논문은 레이어 스킵(LayerSkip)과 초기 출구 손실(Early Exit Loss)을 통합한 훈련 접근법을 소개합니다.

2. **LayerSkip 방법론**:
   - 추론 시 초기 레이어에서 조기에 출구를 실행할 수 있도록 하는 레이어 드롭아웃과 초기 출구 손실을 조합합니다.
   - 이 기법은 복잡한 모델에서도 높은 정확성을 유지하면서 계산 효율성을 증가시킬 수 있습니다.

3. **실험 및 결과**:
   - 여러 라마 모델 크기에 대한 다양한 유형의 훈련(스크래치에서의 사전 훈련, 지속적인 사전 훈련, 특정 데이터 도메인 및 작업에 대한 파인튜닝)을 통해 이 기법의 효과를 검증합니다.
   - CNN/DM 문서 요약, 코딩, TOPv2 의미 파싱 작업에서 최대 2.16배의 속도 향상을 보여줍니다.

4. **결론**:
   - LayerSkip은 LLMs의 추론 속도를 크게 향상시키며, 메모리 및 계산 비용을 절감할 수 있는 유망한 접근 방식을 제공합니다.
   - 이 방법은 특히 복잡하고 비용이 많이 드는 LLMs의 배포를 위한 효율적인 솔루션을 제공합니다.

이 논문은 대규모 언어 모델의 효율성을 크게 개선하고, 빠른 추론을 가능하게 하는 새로운 기술을 제안함으로써 중요한 기여를 합니다.