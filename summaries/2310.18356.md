# LoRAShear: Efficient Large Language Model Structured Pruning and Knowledge Recovery
## TL;DR
## Summary
- [https://arxiv.org/pdf/2310.18356.pdf](https://arxiv.org/pdf/2310.18356.pdf)

### 요약

#### 1. 소개 (Introduction)
이 논문은 LoRAShear라는 대형 언어 모델(LLM)의 구조적 프루닝 및 지식 회복을 위한 효율적인 방법을 제안합니다. LLM의 거대한 크기는 계산 비용 측면에서 상당한 도전을 제기하며, LoRAShear는 이러한 모델을 구조적으로 프루닝하여 크기를 줄이고 지식을 보존하는 것을 목표로 합니다.

#### 2. 배경 (Background)
LLM은 수십억 개의 매개변수를 가지며 자연어 처리뿐만 아니라 다양한 도메인에서 중요한 역할을 하고 있습니다. 그러나, 이러한 모델의 막대한 계산 및 메모리 요구 사항은 현실적인 배포에서 큰 도전 과제가 됩니다. 구조적 프루닝은 중복된 구조를 식별하고 제거하여 압축된 DNN을 제공하는 효과적인 방법입니다.

#### 3. 방법론 (Methodology)
LoRAShear는 다음과 같은 주요 단계로 구성됩니다:
1. **최소 제거 구조 발견 (Minimally Removal Structure Discovery)**: LoRA 모듈을 포함한 LLM의 의존성 그래프를 생성하고 분석하여 최소한의 제거 구조를 발견합니다.
2. **진행적 구조적 프루닝 (Progressive Structured Pruning)**: LHSPG(LoRA Half-Space Projected Gradient)를 사용하여 프루닝을 진행하며, 중요한 구조에 지식을 전달하여 지식을 보존합니다.
3. **동적 지식 회복 (Dynamic Knowledge Recovery)**: 프루닝 후 손실된 지식을 회복하기 위해 동적 데이터 어댑터를 사용하여 다단계 미세 조정을 수행합니다.

#### 4. 실험 (Experiments)
실험 결과, LoRAShear는 20% 프루닝된 LLAMAv1 모델에서 1%의 성능 저하만 발생시키면서 모델의 크기를 효과적으로 줄였습니다. 50% 프루닝된 모델은 82%의 성능을 유지하였습니다. 이는 기존 최첨단 방법보다 월등히 우수한 결과입니다.

#### 5. 결론 (Conclusion)
LoRAShear는 LLM의 구조적 프루닝과 지식 회복을 효율적으로 수행하는 새로운 프레임워크를 제안합니다. 이 방법은 최소 제거 구조를 자동으로 발견하고, LHSPG를 통해 구조적 프루닝을 진행하며, 프루닝 후 동적 지식 회복을 통해 성능을 회복합니다. 실험 결과, LoRAShear는 20% 프루닝에서 1%의 성능 저하만 발생시키며, 50% 프루닝에서는 82%의 성능을 유지하여 기존 방법보다 뛰어난 성능을 입증했습니다.

### 전체 요약
LoRAShear는 대형 언어 모델의 효율적인 구조적 프루닝과 지식 회복을 위한 새로운 방법을 제안합니다. 이 방법은 의존성 그래프를 생성하고 분석하여 최소 제거 구조를 발견하고, LHSPG를 사용하여 프루닝을 진행하며, 동적 지식 회복을 통해 프루닝 후 손실된 지식을 회복합니다. 실험 결과, LoRAShear는 20% 프루닝에서 1%의 성능 저하만 발생시키고, 50% 프루닝된 모델은 82%의 성능을 유지하여 기존 방법보다 우수한 성능을 보여줍니다. 이 논문은 대형 언어 모델의 효율적인 압축과 지식 보존을 위한 새로운 가능성을 제시합니다.