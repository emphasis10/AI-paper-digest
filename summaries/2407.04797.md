# Revealing the Utilized Rank of Subspaces of Learning in Neural Networks
## TL;DR
## Summary
- [https://arxiv.org/pdf/2407.04797.pdf](https://arxiv.org/pdf/2407.04797.pdf)

### 요약

#### 1. 도입 (Introduction)
이 논문에서는 대규모, 심층 신경망에서 ‘용량’ 개념이 불분명해지며, 이를 해결하기 위해 네트워크의 복잡성과 데이터셋의 상호작용을 캡처하는 ‘활용도’라는 개념을 정의하고 있습니다. 학습된 가중치들이 전체 공간을 사용할 것처럼 보이지만, 실제로는 데이터와 상호작용하는 일부분만 사용함을 밝혀냈습니다. 이를 통해 입력과 출력의 저차원 부분 공간에 투영하여 네트워크의 효율성을 증대시키는 방법을 제안하고 있습니다.

#### 2. 방법론 (Methodology)
방법론 섹션에서는 입력과 출력 부분 공간을 정의하고 투영 행렬을 통해 가중치 매트릭스를 변환하는 과정을 설명합니다. 이 변환은 층 매핑을 유지하면서 저차원 구조를 드러냅니다. 또한, 각 층의 사용된 랭크를 계산하고, 이를 통한 메모리와 계산비용의 절감 효과를 논의합니다.

#### 3. 결과와 토론 (Results and Discussion)
다양한 네트워크 아키텍처와 데이터셋에 대해 실험을 수행한 결과, 대부분의 네트워크가 제공된 가중치 공간을 완전히 활용하지 않음을 보여줍니다. 예를 들어, ViT 변형 모델들은 평균 20%~35%의 층 활용도를 보이며, 이를 통해 초기 크기 대비 25~48%로 매개변수를 감소시킬 수 있습니다. 또한, 자가 지도 학습이 여러 다운스트림 작업에 적합하다는 것을 발견했습니다.

#### 4. 결론 (Conclusion)
이 연구는 신경망이 특정 데이터셋을 학습하는 효율성을 평가하기 위한 '평균 층 활용도'라는 데이터 종속적인 메트릭을 제안합니다. 이 메트릭은 입력 및 출력 활성화에 의해 지시된 컴팩트한 저차원 부분 공간으로 가중치를 변환함으로써 네트워크의 용량을 재정의합니다. 마지막으로, 이러한 부분 공간으로 층을 분해함으로써 파라미터를 크게 줄이면서도 성능을 유지할 수 있는 방법을 제안합니다.

### 종합 요약
이 논문은 신경망의 '용량' 개념을 재정립하고, 데이터와 네트워크의 상호작용을 캡처하는 '활용도' 개념을 소개합니다. 이를 통해 네트워크 가중치를 효율적으로 투영하고, 파라미터와 계산비용을 줄이면서 성능을 유지하는 방법을 제시합니다. 다양항 실험을 통해 ViT, ResNet 등의 모델에서 이 접근법의 유효성을 확인하였으며, 자가 지도 학습을 통한 모델의 활용도가 높아짐을 발견했습니다. 이 연구는 신경망의 효율성 개선과 더불어, 낮은 랭크 분해를 통해 계산 자원을 절감하는 새로운 방법을 제시합니다.

## Similar Papers
- [Q-GaLore: Quantized GaLore with INT4 Projection and Layer-Adaptive Low-Rank Gradients](2407.08296.md)
- [MultiLoRA: Democratizing LoRA for Better Multi-Task Learning](2311.11501.md)
- [Adapting LLaMA Decoder to Vision Transformer](2404.06773.md)
- [LoRA: Low-Rank Adaptation of Large Language Models](2106.09685.md)
- [LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models](2310.08659.md)
- [Personalized Residuals for Concept-Driven Text-to-Image Generation](2405.12978.md)
- [MoRA: High-Rank Updating for Parameter-Efficient Fine-Tuning](2405.12130.md)
- [Your Transformer is Secretly Linear](2405.12250.md)
- [Reasoning in Large Language Models: A Geometric Perspective](2407.02678.md)
