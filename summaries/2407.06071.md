# From Loops to Oops: Fallback Behaviors of Language Models Under Uncertainty
## TL;DR
## Summary
- [https://arxiv.org/pdf/2407.06071.pdf](https://arxiv.org/pdf/2407.06071.pdf)

## 논문 요약 (Summary in Korean)

### 1. 논문의 주요 기여와 혁신 부분 요약 (Sections Summary)

#### 서론 (Introduction)
이 논문은 대형 언어 모델(LLM)이 불확실성 상황에서 보이는 행동, 특히 반복된 텍스트 생성과 허구(text degeneration and hallucinations)의 문제를 다룹니다. 기존 연구에서 제안된 해결책들이 이러한 문제를 충분히 해결하지 못함을 지적하고, 모델 강화가 이러한 문제를 악화시킬 수 있음을 제시합니다.

#### 대체 행동 (Fallback Behaviors)
LLM이 불확실성 상황에서 나타내는 대표적인 행동으로, 반복(sequence repetition), 퇴행(degenerate text), 허구(hallucinations) 등이 있습니다. 모델의 강도가 이러한 행동에 미치는 영향을 체계적으로 분석합니다. 모델이 강해질수록 허구가 더 많이 발생하고 반복은 줄어드는 경향이 나타납니다.

#### 실험 설계 (Experimental Setting)
실험에서는 다양한 데이터셋을 사용하여 모델이 불확실성 상황에서 어떻게 행동하는지를 분석합니다. 데이터셋으로는 퀴즈 형태의 TRIVIAFACTS, 바이오그래피 생성 BIOGENERATION 등이 사용되었습니다.

#### 실험 결과 (Results)
모델의 크기와 디코딩 전략이 결과에 미치는 영향을 실험을 통해 검토하였습니다. 모델이 강해질수록, 더 많은 허구가 생성되며, 이는 내부 불확실성을 더 잘 다루지 못하는 결과로 나타납니다. 즉, 모델이 자체적으로 불확실성을 표현할 수 있는지가 중요한 요인으로 작용합니다.

#### 관련 연구 (Related Work)
기존 연구에서는 사전 확률(calibration)과 각종 디코딩 전략(예: 핵심 샘플링, 제약 샘플링 등)이 텍스트 퇴행 문제를 완화할 수 있다고 제안하였으나, 근본적인 문제 해결에는 크게 기여하지 못했습니다. 이 논문에서는 이러한 기존 연구를 확장하여 내부 메커니즘을 심층 분석합니다.

#### 논문의 주요 기여 (Contributions)
1. 모델이 불확실한 상황에서 보이는 행동을 체계적으로 분석하였습니다.
2. 강한 모델일수록 허구가 더 발생하는 경향을 실증하였습니다.
3. 기존의 텍스트 퇴행 문제를 줄이는 방법들이 오히려 허구를 증가시킬 수 있음을 보여주었습니다.

#### 결론 (Conclusion)
모델의 강도가 증가할수록 정확성은 높아지지만, 불확실한 상황에서의 허구 생성 문제는 여전히 해결되지 않고 악화될 수 있습니다. 디코딩 전략과 같은 표면적인 해결법은 반복된 텍스트 문제를 완화할 수 있으나, 허구를 증가시키는 결과를 낳을 수 있습니다. 따라서, 내부 불확실성 표현과 같은 근본적인 문제 해결 방법을 모색해야 합니다.

### 2. 전체 요약 (Overall Summary)

이 논문은 대형 언어 모델(LLM)이 불확실한 상황에서 보이는 반복과 허구 생성 문제를 다루고 있습니다. 모델이 강해질수록 이러한 문제가 더 두드러지며, 기존의 텍스트 퇴행 문제를 줄이는 방법들이 오히려 허구를 증가시킬 수 있음을 실증적으로 보여줍니다. 따라서 모델의 내부 불확실성 표현과 같은 새로운 접근 방법이 필요함을 강조합니다. 주요 기여로는, 모델이 불확실한 상황에서 보이는 행동을 체계적으로 분석하고, 허구 문제의 근본 원인을 규명한 점이 있습니다. 이러한 분석을 바탕으로, 새로운 해결책을 모색함으로써 AI 연구를 진전시킬 수 있는 기초 자료를 제공합니다.