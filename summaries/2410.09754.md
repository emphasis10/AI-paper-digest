# SimBa: Simplicity Bias for Scaling Up Parameters in Deep Reinforcement Learning
## TL;DR
## Summary
- [https://arxiv.org/pdf/2410.09754.pdf](https://arxiv.org/pdf/2410.09754.pdf)

**1. 섹션별 요약**

**도입부**
- 이 연구는 강화 학습(Deep RL)에서 매개변수의 규모를 확장하는 새로운 아키텍처 "SimBa"를 소개합니다. 기존에는 컴퓨터 비전(CV)과 자연어 처리(NLP) 분야에서 많이 사용된 이러한 확장 기법이 강화 학습에서는 충분히 탐구되지 않았습니다.

**SimBa 아키텍처**
- SimBa는 세 가지 주요 구성 요소를 포함합니다: (1) 입력을 정상화하는 관찰 노멀라이제이션 (2) 선형 경로를 유지하는 잔차 피드포워드 블록, (3) 출력 레이어 이전에 적용되는 레이어 노멀라이제이션입니다. 이 구성 요소들은 모델이 높은 매개변수 설정에서도 과적합을 피할 수 있게 합니다.

**Simplicity Bias의 증대**
- SimBa는 강화 학습에서의 단순성 편향을 강조합니다. 이는 Fourier 감쇠 방식으로 측정되며, SimBa는 기존의 MLP 아키텍처보다 더 높은 단순성 점수를 기록했습니다.

**실험 결과**
- SimBa는 다양한 강화 학습 알고리즘과 환경에서 놀라운 성능을 보여주었습니다. 특히, SimBa를 사용하는 SAC(Soft Actor-Critic)는 매개변수 수를 확장할 때 성능 악화 없이 꾸준히 향상되어, 다른 최첨단 RL 방법들을 능가하거나 동등한 성능을 보였습니다.

**결론**
- SimBa는 간단한 변경만으로도 RL 아키텍처의 확장성과 성능을 크게 향상시킬 수 있음을 보여주었습니다. 특히 다양한 입출력 차원을 가진 환경에서도 SimBa의 이점이 두드러졌습니다.

**2. 종합 요약**

이 논문은 강화 학습에서 매개변수를 효과적으로 확장하는 SimBa라는 새로운 아키텍처를 제안하였습니다. SimBa는 입력 정상화, 잔차 피드포워드 블록, 그리고 레이어 노멀라이제이션을 통해 모델의 단순성 편향을 극대화하며 과적합을 피합니다. 실험 결과, SimBa는 다양한 강화 학습 알고리즘과 환경에서 일관되게 높은 성능을 나타냈으며, 이는 특히 고차원 입력 환경에서 두드러졌습니다. 이러한 결과는 복잡한 구성 요소의 추가 없이도 아키텍처 설계를 통해 간단하게 성능을 극대화할 수 있음을 시사합니다.