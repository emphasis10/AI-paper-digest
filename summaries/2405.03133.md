# Lory: Fully Differentiable Mixture-of-Experts for Autoregressive Language Model Pre-training
## TL;DR
## Summary
- [https://arxiv.org/pdf/2405.03133.pdf](https://arxiv.org/pdf/2405.03133.pdf)

### 주요 내용 요약

1. **서론 및 배경**:
   - 이 논문은 **Lory**라는 완전 미분 가능(MoE) 아키텍처를 사용하여 자기 회귀 언어 모델을 사전 훈련하는 방법을 소개합니다. 기존의 MoE 모델은 라우터 네트워크의 비미분 가능한, 이산적인 목표를 최적화하는 데 어려움이 있었으나, Lory는 이를 극복하기 위해 설계되었습니다. 두 가지 주요 기술을 도입하여 전문가 병합 작업을 효율적으로 수행하면서도 언어 모델의 자기 회귀 특성을 유지합니다.

2. **방법론**:
   - **인과 세그먼트 라우팅**: Lory는 인과 세그먼트 라우팅 전략을 사용하여 전문가 병합 작업의 효율성을 높입니다. 입력 토큰 시퀀스를 여러 세그먼트로 나누고 이전 세그먼트의 정보를 사용하여 현재 세그먼트를 처리합니다.
   - **유사성 기반 데이터 배치**: 문서의 의미적 유사성을 기준으로 데이터를 배치하여 유사한 문서를 함께 그룹화합니다. 이를 통해 전문가가 특정 도메인 또는 주제에 특화될 수 있도록 유도합니다.

3. **실험**:
   - Lory 모델을 150B 토큰으로 사전 훈련하여, 최대 32명의 전문가와 30B(활성 파라미터 1.5B) 파라미터를 가진 모델을 포함합니다. 실험 결과, Lory 모델은 동일한 데이터로 훈련된 동등한 크기의 밀집 모델에 비해 퍼플렉시티와 다양한 다운스트림 작업에서 뛰어난 성능을 보여줍니다. 특히, 전문가 병합 전략을 통해 도메인 수준의 특화를 달성하였습니다.

### 혁신적인 부분
Lory의 혁신성은 기존의 MoE 아키텍처에서 비미분 가능하고 이산적인 라우팅 문제를 해결하기 위해 완전 미분 가능 구조를 도입한 데 있습니다. 이 방법은 인과 세그먼트 라우팅과 유사성 기반 데이터 배치 기법을 통해 언어 모델의 사전 훈련 효율성을 크게 향상시킵니다. 또한, 다양한 도메인에서 전문가가 특화될 수 있도록 유도하여, 일반적인 MoE 모델의 문제점을 해결하고 성능을 개선합니다.

## Similar Papers
- [Skywork-MoE: A Deep Dive into Training Techniques for Mixture-of-Experts Language Models](2406.06563.md)
- [Sheared LLaMA: Accelerating Language Model Pre-training via Structured Pruning](2310.06694.md)
- [Who Says Elephants Can't Run: Bringing Large Scale MoE Models into Cloud Scale Production](2211.10017.md)
- [Scaling Diffusion Transformers to 16 Billion Parameters](2407.11633.md)
- [Mixture of A Million Experts](2407.04153.md)
- [Mixture-of-Depths: Dynamically allocating compute in transformer-based language models](2404.02258.md)
- [Pushing Mixture of Experts to the Limit: Extremely Parameter Efficient MoE for Instruction Tuning](2309.05444.md)
- [MoMa: Efficient Early-Fusion Pre-training with Mixture of Modality-Aware Experts](2407.21770.md)
- [Turbo Sparse: Achieving LLM SOTA Performance with Minimal Activated Parameters](2406.05955.md)
