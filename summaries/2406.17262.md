# D2LLM: Decomposed and Distilled Large Language Models for Semantic Search
## TL;DR
## Summary
- [https://arxiv.org/pdf/2406.17262.pdf](https://arxiv.org/pdf/2406.17262.pdf)

### 초록 및 기여
**요약:** 이 논문은 효율성과 정확성을 겸비한 시맨틱 검색 모델을 제안합니다. 전통적으로 사용되는 BERT 스타일의 바이인코더는 효율적이지만, 미묘한 의미를 놓치는 경우가 많습니다. 반면, GPT 스타일의 LLM은 정확하지만 계산 비용이 많이 들기 때문에 실시간 응용에는 적합하지 않습니다. 이 논문은 이러한 문제를 해결하기 위해 D2LLM(Distilled and Decomposed LLMs for Semantic Search)이라는 새로운 프레임워크를 제안합니다. 이는 LLM 기반의 크로스인코더를 바이인코더와 상호작용 모듈로 분해하고, 효율성 및 사전 계산 가능성을 향상시킵니다. 결과적으로, D2LLM은 다양한 메트릭에서 기존 모델들을 능가하며, 특히 NLI 과제에서 최소 6.45%의 성능 향상을 보였습니다.

### 1. 소개
**요약:** 시맨틱 검색은 사용자 쿼리에 가장 관련 있는 구문을 찾기 위해 방대한 텍스트를 탐색하는 작업을 말합니다. 이는 TF-IDF 및 BM25와 같은 비시맨틱 기술을 능가하며, 질문 응답, 대화 시스템, 항목 추천, 사실 확인 등 다양한 분야에서 중요한 역할을 합니다. 그러나 시맨틱 검색의 주요 과제는 높은 정확도와 효율성을 동시에 갖춘 모델을 개발하는 것입니다. 현재 사용되는 BERT 스타일의 바이인코더는 효율적이지만 정확도에서 한계를 보이며, GPT 스타일의 크로스인코더는 정확하지만 계산 비용이 많이 듭니다.

### 2. 관련 연구
**요약:** 전통적인 시맨틱 검색 기술은 보통 바이인코더, 크로스인코더, 그리고 크로스인코더에서 증류된 바이인코더로 나뉩니다. 바이인코더는 효율적이지만 정확도에서 한계를 가지며, 크로스인코더는 높은 정확도를 보이지만 실시간 응용에는 적합하지 않습니다. 가장 관련이 깊은 방법은 크로스인코더의 효과를 바이인코더로 증류하는 것입니다. 이전 연구들은 주로 간단한 증류 전략을 사용했으며, D2LLM은 이러한 접근방식을 개선하여 학생 모델을 정제하는 과정을 포함합니다.

### 3. D2LLM의 제안
**요약:** D2LLM은 LLM 기반의 크로스인코더를 바이인코더와 상호작용 모듈로 분해한 새로운 시맨틱 검색 솔루션입니다. 이를 통해 속도와 정확도를 모두 향상시키며, 대칭 및 비대칭 검색 작업을 위해 두 개의 전용 분기를 가진 상호작용 모듈을 도입했습니다. 또한 대조 모방, 순위 모방 및 피처 모방 기술을 통해 교사의 언어적 전문 지식을 학생 모델로 전이합니다. D2LLM은 다섯 가지 선도적인 방법론을 능가하며, 특히 NLI 과제에서 6.45% 이상의 개선을 보입니다.

### 4. 실험 및 결과
**요약:** 실험 결과 D2LLM은 NLI, STS, IR 작업 모두에서 우수한 성능을 보였습니다. 특히 대칭 및 비대칭 관계를 모두 잘 다루며, 각 작업에 대해 기존 방법들을 뛰어넘는 성능을 보여줍니다.

### 5. 결론
**요약:** 이 논문은 시맨틱 검색의 효율성과 정확성을 동시에 달성하기 위해 D2LLM을 제안합니다. LLM 기반의 크로스인코더를 바이인코더와 상호작용 모듈로 분해하고, 다양한 증류 기법을 사용하여 학생 모델을 정제합니다. 결과적으로 D2LLM은 기존의 다섯 가지 방법론을 모두 능가하는 성능을 보입니다.

### 전체 요약
D2LLM은 시맨틱 검색 기술의 효율성과 정확성을 대폭 향상시키는 새로운 프레임워크입니다. LLM 기반의 크로스인코더를 바이인코더와 상호작용 모듈로 분해하고, 다양한 증류 기법을 통해 학생 모델을 정제하여 더 나은 성능을 달성합니다. 실험 결과, D2LLM은 NLI, STS, IR 작업 모두에서 기존 방법들을 능가하는 뛰어난 성능을 보였습니다. 이러한 접근은 더 효율적이고 정확한 시맨틱 검색 모델을 개발하는 데 중요한 기여를 합니다.

## Similar Papers
- [JaColBERTv2.5: Optimising Multi-Vector Retrievers to Create State-of-the-Art Japanese Retrievers with Constrained Resources](2407.20750.md)
- [Can Small Language Models Help Large Language Models Reason Better?: LM-Guided Chain-of-Thought](2404.03414.md)
- [Enhancing LLM's Cognition via Structurization](2407.16434.md)
- [PLaD: Preference-based Large Language Model Distillation with Pseudo-Preference Pairs](2406.02886.md)
- [Poisoned LangChain: Jailbreak LLMs by LangChain](2406.18122.md)
- [Can LLMs Learn by Teaching? A Preliminary Study](2406.14629.md)
- [CodexGraph: Bridging Large Language Models and Code Repositories via Code Graph Databases](2408.03910.md)
- [Value-Incentivized Preference Optimization: A Unified Approach to Online and Offline RLHF](2405.19320.md)
- [Direct Preference Knowledge Distillation for Large Language Models](2406.19774.md)
