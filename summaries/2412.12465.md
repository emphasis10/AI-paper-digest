# Core Context Aware Attention for Long Context Language Modeling
## TL;DR
## Summary
- [https://arxiv.org/pdf/2412.12465.pdf](https://arxiv.org/pdf/2412.12465.pdf)

### 1. 섹션 요약

#### 서론
이 논문에서는 거대한 자연어 모델(Large Language Models, LLMs)의 문제점, 특히 긴 문맥을 처리할 때의 연산 복잡성과 메모리 사용량 문제를 집중적으로 다루고 있습니다. 이러한 문제를 해결하기 위해 Core Context Aware Attention(CCA) 메커니즘을 제안하고 있습니다. 이 메커니즘은 불필요한 문맥 정보를 줄이고 중요한 토큰 간의 의존성을 효과적으로 포착하여 성능을 향상시키는 것을 목표로 합니다.

#### 관련 연구
기존의 연구들은 긴 문맥을 효과적으로 모델링하기 위해 다양한 주의 메커니즘을 제안했습니다. 이 연구들은 주로 토큰 간 상호작용을 제한하거나 계산을 효율적으로 하기 위한 다양한 방법론을 택하고 있지만, 토큰 간의 도달 가능성을 유지하는 데는 부족한 점이 있습니다.

#### Core Context Aware Attention
CCA 메커니즘은 두 가지 주요 컴포넌트를 가지고 있습니다: 
1. **Globality-pooling Attention**: 입력 토큰을 그룹으로 나누고, 각 그룹의 중요도를 기반으로 핵심 토큰만을 유지하여 연산 비용을 줄입니다.
2. **Locality-preserved Attention**: 이웃 토큰들을 결합하여 지역적 문맥 정보를 포착합니다.

#### 성능
CCA의 효율성은 실험적으로 입증되었습니다. 특히 긴 문맥 처리에서 뛰어난 성능과 더불어 64K 토큰을 처리할 때 5.7배 더 빠른 추론 속도를 보입니다. 이는 전통적인 주의 메커니즘보다 더 나은 효율성을 나타냅니다.

### 2. 종합 요약
이 논문은 긴 문맥을 효과적으로 처리하기 위한 새로운 주의 메커니즘을 제안합니다. 제안된 Core Context Aware Attention (CCA) 메커니즘은 중요도 기반의 핵심 토큰을 사용하여 연산 복잡성을 줄이면서 글로벌 및 로컬 의존성을 포착합니다. 실험을 통해 CCA는 더 적은 계산 비용으로 뛰어난 인공지능 모델링 능력을 보이며 기존의 효율적인 주의 메커니즘들을 능가하는 성능을 보였습니다.