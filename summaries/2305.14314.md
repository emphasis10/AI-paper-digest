# QLoRA: Efficient Finetuning of Quantized LLMs
## TL;DR
## Summary
- [https://arxiv.org/pdf/2305.14314.pdf](https://arxiv.org/pdf/2305.14314.pdf)

### 요약

#### 1. Introduction (소개)
이 논문에서는 QLORA라는 효율적인 미세 조정 방법을 제안합니다. 이 방법은 메모리 사용량을 줄여 65억 매개변수 모델을 단일 48GB GPU에서 완전한 16비트 미세 조정 성능을 유지하면서도 조정할 수 있게 합니다. QLORA는 동결된 4비트 양자화된 사전 학습 언어 모델을 통해 저랭크 어댑터(LoRA)에 그래디언트를 역전파합니다. 저자들은 최적의 성능을 제공하는 Guanaco라는 모델 가족을 개발했으며, 이 모델은 24시간의 미세 조정만으로 ChatGPT의 99.3% 성능을 달성합니다.

#### 2. Background (배경)
대형 언어 모델(LLM)을 미세 조정하는 것은 매우 효과적이지만, 매우 큰 모델을 미세 조정하는 것은 비용이 많이 듭니다. 기존의 양자화 방법은 메모리 풋프린트를 줄이는 데 효과적이지만, 훈련 중에는 제대로 작동하지 않습니다. QLORA는 4비트 양자화를 통해 이러한 문제를 해결하고, 메모리 스파이크를 관리하기 위해 페이지드 옵티마이저를 도입합니다.

#### 3. Methodology (방법론)
QLORA는 두 가지 주요 기술을 사용하여 높은 충실도의 4비트 미세 조정을 달성합니다:
1. 4-bit NormalFloat (NF4) 양자화: 일반적으로 분포된 가중치에 대해 정보 이론적으로 최적의 데이터 유형을 사용합니다.
2. Double Quantization: 양자화 상수를 다시 양자화하여 평균 메모리 풋프린트를 줄입니다.

이 논문에서는 QLORA를 사용하여 1,000개 이상의 모델을 미세 조정하고, 다양한 명령어 데이터셋과 모델 유형, 모델 크기를 분석합니다. 또한 GPT-4 평가를 통해 인간 평가와 비교한 챗봇 성능 분석도 제공합니다.

#### 4. Experiments (실험)
실험 결과는 QLORA가 16비트 미세 조정 성능과 일치하며, 학술 벤치마크에서 높은 성능을 보여줍니다. NF4는 FP4보다 효과적이며, 더블 양자화는 성능 저하 없이 메모리 효율성을 극대화합니다. 이 논문은 QLORA의 효율성 덕분에 작은 고품질 데이터셋으로도 최첨단 결과를 얻을 수 있음을 보여줍니다.

#### 5. Conclusion (결론)
QLORA는 대형 언어 모델의 4비트 미세 조정에서 16비트 성능을 유지하면서도 메모리 사용량을 크게 줄이는 데 성공했습니다. 이 논문은 QLORA가 다양한 명령어 데이터셋에서 우수한 성능을 보이며, ChatGPT와 경쟁할 수 있는 챗봇 모델인 Guanaco를 개발하는 데 사용되었음을 보여줍니다.

---

### 전체 요약
이 논문은 QLORA라는 효율적인 미세 조정 방법을 제안하여, 대형 언어 모델의 메모리 사용량을 줄이면서도 높은 성능을 유지할 수 있게 합니다. QLORA는 4비트 양자화와 저랭크 어댑터를 사용하여 65억 매개변수 모델을 단일 48GB GPU에서 미세 조정할 수 있습니다. 실험 결과, QLORA는 16비트 미세 조정 성능과 일치하며, 작은 고품질 데이터셋으로도 최첨단 결과를 얻을 수 있습니다. 이 논문은 QLORA의 효율성과 성능을 입증하며, ChatGPT와 경쟁할 수 있는 챗봇 모델을 개발하는 데 성공했습니다.

## Similar Papers
- [SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression](2306.03078.md)
- [HelpSteer2: Open-source dataset for training top-performing reward models](2406.08673.md)
- [VeLoRA: Memory Efficient Training using Rank-1 Sub-Token Projections](2405.17991.md)
- [GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection](2403.03507.md)
- [LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale](2208.07339.md)
- [OpenBezoar: Small, Cost-Effective and Open Models Trained on Mixes of Instruction Data](2404.12195.md)
- [Grass: Compute Efficient Low-Memory LLM Training with Structured Sparse Gradients](2406.17660.md)
- [LLaMA-NAS: Efficient Neural Architecture Search for Large Language Models](2405.18377.md)
- [LQ-LoRA: Low-rank Plus Quantized Matrix Decomposition for Efficient Language Model Finetuning](2311.12023.md)
