# Foundational Autoraters: Taming Large Language Models for Better Automatic Evaluation
## TL;DR
## Summary
- [https://arxiv.org/pdf/2407.10817.pdf](https://arxiv.org/pdf/2407.10817.pdf)

### 요약

1. **소개(Introduction)**
   이 논문에서는 대형 언어 모델(LLM)의 출력 평가를 자동화하기 위해 FLAMe(Foundational Large Autorater Models)를 도입합니다. FLAMe는 다양한 평가 작업을 위해 크고 다양한 데이터셋(100+ 퀄리티 평가 작업, 5M+ 인간 판단)에 기반하여 훈련되었습니다. 이는 GPT-4, Claude-3 등의 모델을 여러 평가 작업에서 능가합니다.

2. **관련 연구(Related Work)**
   이 섹션에서는 기존 자동 평가 메트릭스와 LLM-as-a-Judge 모델을 다룹니다. 전통적인 메트릭스(예: BLEU, ROUGE) 와 사전 훈련된 모델을 사용한 접근 등이 논의됩니다. FLAMe는 다양한 세부적인 퀄리티 평가 작업에서 훈련되어 이와 차별화됩니다.

3. **방법론(Methodology)**
   FLAMe는 다양한 인간 평가 데이터셋을 표준화하여 사용합니다. 다목적 LLM 자동 평가 모델로서의 FLAMe와 타겟 애플리케이션을 위한 최적화 버전(FLAMe-RM, FLAMe-Opt-RM)을 훈련합니다. 이 모델들은 모든 유명한 LLM-as-a-Judge 모델을 여러 평가 벤치마크에서 능가합니다.

4. **실험(Experiments)**
   FLAMe 모델들은 다양한 평가 벤치마크(RewardBench, LLM-AggreFact)에서 테스트되어, GPT-4 등과 비교 시 뛰어난 성능을 보입니다. 특히 코딩 벤치마크 HumanEval에서 FLAMe를 사용하여 코드 샘플을 재순위화하면 성능이 크게 개선됨을 보여줍니다.

5. **결론(Conclusion)**
   FLAMe는 많은 평가 작업에서 높은 일반화 능력을 보이며, 인간 평가 데이터셋에 기반한 강력한 시작점을 제공합니다. FLAMe는 타 언어 모델보다 편향이 적고 코딩 응답 식별에서도 우수한 성능을 보입니다. 이를 통해 AI 평가의 접근성과 효율성을 크게 개선할 수 있습니다.

### 전체 요약
FLAMe는 대형 언어 모델의 출력을 평가하는 강력한 자동 평가 모델군입니다. 다양한 인간 평가 데이터셋에 훈련되어, GPT-4, Claude-3 등을 능가하는 성능을 보여줍니다. FLAMe는 허가된 데이터셋에서만 훈련된 최초의 모델로서, 데이터 접근성이 제한된 상황에서도 우수한 평가 성능을 제공합니다. 이를 통해 FLAMe는 인간 평가를 대체할 잠재력을 가지며, 평가의 효율성과 공정성을 크게 향상시킬 수 있습니다.