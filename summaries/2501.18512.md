# Streaming DiLoCo with overlapping communication: Towards a Distributed Free Lunch
## TL;DR
## Summary
- [https://arxiv.org/pdf/2501.18512.pdf](https://arxiv.org/pdf/2501.18512.pdf)

1. 각 섹션별 주요 내용 요약:

- **서론**: 딥러닝의 발전은 큰 성과를 이루었지만, 대형 언어 모델(LLM)을 훈련할 때 수천 개의 가속기를 필요로 하며, 이로 인해 물리적 인프라가 복잡해지고 비용이 상승한다. 기존 방법들은 모든 가속기의 동기화가 필요했으며, 이는 큰 대역폭과 시간 지연을 초래했다. 

- **모델 구성**: 제안된 방법은 DiLoCo를 개선한 "Streaming DiLoCo"이다. 중요한 혁신은 다음과 같다:
  1. **동기화 부분**: 모든 파라미터를 동기화하는 대신 일부 파라미터만 동기화하여 대역폭을 크게 줄인다.
  2. **계산 중 통신 중첩**: 계산 중에 동기화를 겹쳐 진행하여 전체적인 시간을 줄인다.
  3. **양자화**: 교환되는 데이터를 압축하여 대역폭을 추가로 줄인다.

- **결론 및 향후 연구 방향**: 본 연구는 동기화 방식, 통신 중첩, 고정밀도 통신을 통해 훈련 품질을 유지하면서도 대역폭을 400배 줄일 수 있음을 보여주었다. 이러한 방법은 배급 시스템의 품질을 유지하면서 대역폭을 최소화하는 "분산 무료 점심"의 첫걸음으로 간주된다.

주요 기여와 혁신적인 부분: "Streaming DiLoCo"는 기존의 모든 파라미터를 동기화하는 방식에서 벗어나 부분 동기화 및 통신 중첩을 통해 대역폭을 극적으로 감소시켰으며, 이로써 대규모 모델 훈련을 더욱 효율적이게 만드는 방법이다.

2. 전체 요약:
이 논문은 대형 언어 모델 훈련 시 필요한 통신 대역폭을 줄이기 위한 새로운 접근 방식을 제안한다. "Streaming DiLoCo"는 기존의 DiLoCo 모델을 개선하여 동기화되는 파라미터의 수를 최소화하고, 계산 중에도 통신을 병행할 수 있도록 하여 훈련 시간을 줄인다는 점에서 획기적이다. 이러한 혁신을 통해 400배 적은 대역폭으로도 비슷한 성능을 유지할 수 있음을 실험적으로 입증하였으며, 이는 향후 분산 시스템에서 AI 훈련의 효율성을 크게 향상시킬 수 있는 첫 발걸음으로 평가된다.