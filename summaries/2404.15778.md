# BASS: Batched Attention-optimized Speculative Sampling
## TL;DR
## Summary
- [https://arxiv.org/pdf/2404.15778.pdf](https://arxiv.org/pdf/2404.15778.pdf)

이 논문은 대규모 언어 모델(LLMs)의 추론 속도 및 처리량을 개선하기 위한 새로운 기법인 Batched Attention-optimized Speculative Sampling (BASS)에 대해 소개합니다. BASS는 특히 여러 시퀀스를 동시에 처리할 수 있도록 예측 디코딩을 확장한 것으로, GPU 활용도와 생성 품질을 향상시키는 것이 주요 목표입니다. 다음은 각 섹션의 주요 내용 요약입니다.

1. **서론 및 배경**:
   - 대규모 언어 모델은 다양한 작업에서 높은 성능을 보여주고 있으나, 추론 단계에서의 고려해야 할 처리 시간과 계산 요구 사항이 증가하고 있습니다.
   - 기존의 자동 회귀 디코딩 방식은 GPU의 메모리 대역폭이 병목 현상을 일으키는 주요 원인입니다.
   - 예측 디코딩은 효율성을 높이기 위한 방법으로 등장했으나, 배치 처리 시 복잡성이 증가하는 문제가 있습니다.

2. **Batched Attention-optimized Speculative Sampling (BASS)**:
   - BASS는 복수의 시퀀스를 동시에 처리하여 GPU 활용을 극대화하는 방식입니다.
   - CUDA 커널을 사용하여 비정형 텐서를 관리하고, 동적으로 드래프트 토큰의 길이를 조절하여 효율성을 극대화합니다.

3. **실험 및 결과**:
   - 다양한 모델과 작업에서 BASS를 통한 추론 시간 단축 및 처리량 증가를 실험적으로 검증합니다.
   - BASS는 기존 방법보다 뛰어난 GPU 활용도와 더 낮은 지연 시간을 달성하며, 품질 면에서도 우수한 결과를 보여줍니다.

4. **결론**:
   - BASS는 대규모 언어 모델의 추론을 빠르고 효율적으로 처리할 수 있는 새로운 기법을 제공합니다.
   - 이 방법은 특히 실시간 응답이 중요한 애플리케이션에 유용할 것입니다.

이 논문은 대규모 언어 모델의 배치 처리 및 예측 디코딩을 최적화하여 처리 속도와 효율성을 개선하는 방법을 제시함으로써 중요한 기여를 합니다.