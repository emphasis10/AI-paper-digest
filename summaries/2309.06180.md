# Efficient Memory Management for Large Language Model Serving with PagedAttention
## TL;DR
## Summary
- [https://arxiv.org/pdf/2309.06180.pdf](https://arxiv.org/pdf/2309.06180.pdf)

### 요약본

#### 1. 논문의 개요
이 논문에서는 대규모 언어 모델을 처리할 때 발생하는 메모리 관리 문제에 대해 다루고 있습니다. 특히, 기존 시스템에서는 요청당 큰 키-값 캐시 메모리가 동적으로 변화하여 메모리 조각화와 중복으로 인해 비효율이 발생합니다. 이를 해결하기 위해, 저자들은 운영체제에서 사용되는 가상 메모리와 페이징 기술에서 영감을 받은 새로운 주목(attention) 알고리즘인 'PagedAttention'을 제안하며, 이를 기반으로 고성능 분산 언어 모델 서빙 시스템인 'vLLM'을 구축합니다.

#### 2. 관련 연구
기존의 대규모 언어 모델 서빙 시스템은 메모리를 효율적으로 관리하지 못해 요청 처리량을 제한하는 문제가 있었습니다. 이에 따라, 여러 연구에서는 이러한 문제를 해결하기 위한 다양한 접근 방식을 모색해 왔으나, 주로 연속적인 메모리 공간에 데이터를 저장하는 방식에 집중되어 왔습니다.

#### 3. PagedAttention 알고리즘
PagedAttention은 기존의 주목 메커니즘과 다르게 키와 값 벡터를 비연속적인 메모리 공간에 저장할 수 있게 합니다. 이를 통해 메모리 조각화를 크게 줄이고, 요청 간 및 요청 내에서 메모리를 공유할 수 있는 가능성을 열어줍니다.

#### 4. vLLM 시스템
vLLM은 PagedAttention을 기반으로 하는 고성능 분산 언어 모델 서빙 시스템입니다. 이 시스템은 메모리를 블록 단위로 관리하고, 요청 스케줄링을 사전에 수행하여 메모리 사용을 최적화합니다. 결과적으로, vLLM은 기존 시스템 대비 2-4배 높은 처리량을 달성하며, 메모리 사용을 효율적으로 관리합니다.

### 종합적인 요약
이 논문은 대규모 언어 모델의 서빙 시 메모리 관리의 비효율성을 해결하기 위해 PagedAttention 알고리즘과 vLLM 시스템을 제안합니다. 이들은 기존의 메모리 관리 방식에서 벗어나 비연속적인 메모리 할당을 통해 메모리 조각화 문제를 해결하고, 요청 간 메모리 공유를 최적화하여 처리량과 효율성을 대폭 향상시킵니다. 이 연구는 향후 대규모 언어 모델의 상용화와 연구에 중요한 기여를 할 것입니다.

## Similar Papers
- [S-LoRA: Serving Thousands of Concurrent LoRA Adapters](2311.03285.md)
- [SpecInfer: Accelerating Generative Large Language Model Serving with Tree-based Speculative Inference and Verification](2305.09781.md)
- [Fast Distributed Inference Serving for Large Language Models](2305.05920.md)
- [FlexGen: High-Throughput Generative Inference of Large Language Models with a Single GPU](2303.06865.md)
- [PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU](2312.12456.md)
- [Parrot: Efficient Serving of LLM-based Applications with Semantic Variable](2405.19888.md)
- [CLLMs: Consistency Large Language Models](2403.00835.md)
- [LLoCO: Learning Long Contexts Offline](2404.07979.md)
- [SARATHI: Efficient LLM Inference by Piggybacking Decodes with Chunked Prefills](2308.16369.md)
