# Arctic-SnowCoder: Demystifying High-Quality Data in Code Pretraining
## TL;DR
## Summary
- [https://arxiv.org/pdf/2409.02326.pdf](https://arxiv.org/pdf/2409.02326.pdf)

### 섹션별 요약

#### 1. 서론
최근 대규모 언어 모델(LLMs)의 사전 학습에 있어서, 고품질 데이터의 중요성이 강조되고 있습니다. 이 논문에서는 Python을 포함한 다양한 프로그래밍 언어에 대한 고품질 코드를 이용하여 단계적으로 데이터 품질을 향상시키는 새로운 사전 학습 방법론을 통해 개발된 Arctic-SnowCoder-1.3B 모델을 소개합니다.

#### 2. Arctic-SnowCoder
Arctic-SnowCoder의 훈련 방법론을 설명하며, 특히 다음과 같은 세 가지 단계적 사전 학습 과정을 강조합니다:
1. 일반 사전 학습: 500B의 저품질 코드를 기본 필터링 및 중복 제거 과정을 거쳐 학습합니다.
2. 고품질 데이터로 계속 학습: BERT 스타일의 품질 조정 도구를 사용하여 50B의 고품질 코드를 선택해 학습합니다.
3. 합성 데이터로 강화 학습: Llama-3.1-70B 모델을 사용한 5B 합성 데이터로 학습합니다.

#### 3. 실험
Arctic-SnowCoder와 최신 소형 언어 모델들을 비교한 결과, Arctic-SnowCoder는 HumanEval+, MBPP+, EvoEval 등 다양한 프로그래밍 벤치마크에서 우수한 성능을 보였습니다. 특히, 코딩 작업을 위해 오픈 소스 데이터에서 높은 성능을 보여주는 모델로 평가되었습니다.

#### 4. 관련 연구
코드 데이터의 사전 학습 코퍼스에 대한 다양한 연구들이 소개됩니다. 이러한 연구들은 주로 GitHub와 같은 공개 플랫폼에서 대규모 데이터를 수집하고 고품질 데이터를 선별하는 방법론을 강조합니다. Arctic-SnowCoder의 사례는 효율적인 전처리를 통해 고품질 데이터를 생성하고, 이를 통해 모델 성능을 극대화한 연구입니다.

#### 5. 결론
이 논문은 고품질 데이터가 모델 성능을 크게 향상시킬 수 있음을 강조하며, 코드 사전 학습에서의 데이터 품질의 중요성을 체계적으로 비교하고 분석합니다. 또한, 사전 학습 과정에서의 최적 설계 선택에 대한 실질적인 지침을 제공합니다.

### 전체 요약
Arctic-SnowCoder-1.3B는 단계적인 데이터 품질 향상을 통해 소형 코드 모델에서 우수한 성능을 달성한 연구입니다. 이 모델은 일반 사전 학습, 고품질 데이터로의 계속 학습, 합성 데이터로의 강화 학습의 세 가지 사전 학습 단계를 걸쳐 개발되었습니다. 연구 결과, 고품질 데이터와 합성 데이터가 모델 성능을 크게 향상시킬 수 있음을 입증했습니다. 이 논문은 코드 사전 학습에서 데이터 품질의 중요성을 강조하며, 향후 모델 개발을 위한 실질적인 지침을 제공합니다.