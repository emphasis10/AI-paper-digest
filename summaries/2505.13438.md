# Optimizing Anytime Reasoning via Budget Relative Policy Optimization
## TL;DR
## Summary
- [https://arxiv.org/pdf/2505.13438.pdf](https://arxiv.org/pdf/2505.13438.pdf)

1. 각 섹션 요약:

- **추상(Introduction):** 이 연구는 큰 언어 모델(LLM)의 연산 능력을 향상시키는 새로운 프레임워크 'AnytimeReasoner'를 소개합니다. 이 접근 방식은 토큰 효율성을 높이고 다양한 토큰 예산 제약 아래에서의 추론 유연성을 개선합니다. 이러한 목표를 달성하기 위해 사전 분포에서 샘플링된 토큰 예산에 맞추어 전체 사고 과정을 잘라내고, 모델이 각 잘라낸 추론 단계에서 최적의 해답을 요약하도록 유도합니다. 이 과정에서 확인 가능한 밀집 보상을 도입하여 강화 학습의 신용 할당을 개선합니다.

- **관련 작업(Related Works):** 강화 학습을 사용한 언어 모델에서의 추론 능력 강화는 주로 큰 고정된 토큰 예산 하에서 최종 성능을 최대화하는 방식으로 진행되었습니다. 이러한 기존 접근 방식의 비효율성을 극복하기 위해 AnytimeReasoner는 언제든지 중단할 수 있는 유연성을 제공하며, 각 중단점마다 최선의 해답을 제공할 수 있도록 설계되었습니다.

- **결론(Conclusion):** AnytimeReasoner는 생성-검증의 간극을 체계적으로 활용하며, 추가적인 보상 구조를 통해 RL 훈련의 안정성과 효율성을 크게 향상시켰습니다. 이 프레임워크는 표준 및 언제든지 추론 과제에서 모두 우수한 성능을 달성했습니다.

2. 전체 요약:

이 논문은 LLM의 추론 기능을 향상시키기 위한 AnytimeReasoner라는 새로운 프레임워크를 제안합니다. 주요 목표는 다양한 토큰 예산 하에서 추론의 유연성과 효율성을 높이는 것이며, 이를 통해 확인 가능한 밀집 보상 구조를 도입하여 강화 학습의 신용 할당을 개선합니다. 이 프레임워크는 특히 온라인 서비스에서 유용하며, 제한된 자원을 최대한 활용하여 더 나은 사용자 경험을 제공할 수 있습니다. AnytimeReasoner는 기존의 강화 학습 방법과 비교하여 매 시간 또는 최대 예산 하에서 모두 더 나은 성능을 보였습니다.