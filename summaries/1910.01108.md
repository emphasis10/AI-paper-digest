# DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter
## TL;DR
## Summary
- [https://arxiv.org/pdf/1910.01108.pdf](https://arxiv.org/pdf/1910.01108.pdf)

### 1. Section Summaries

#### Introduction
이 논문은 Natural Language Processing (NLP) 분야에서 큰 영향력을 발휘하고 있는 대규모 사전 학습 모델의 한계를 극복하기 위해 DistilBERT라는 경량화된 언어 표현 모델을 제안합니다. DistilBERT는 BERT 모델의 크기를 약 40% 줄이면서 97%의 성능을 유지하고, 속도는 60% 빠릅니다. 또한 이 모델은 모바일 기기에서도 효율적으로 작동할 수 있습니다.

#### Knowledge Distillation
지식 증류는 큰 모델(교사 모델)이 생성한 예측을 작은 모델(학생 모델)이 학습하는 과정을 포함합니다. 이 방법은 모델의 복잡도를 줄이면서도 성능을 유지하는 데 유용합니다. 지식 증류를 통해, 학생 모델은 교사 모델의 예측 분포를 복제하여 더 나은 일반화 성능을 얻을 수 있습니다. 이 논문에서는 distillation loss(증류 손실), masked language modeling loss(마스킹 언어 모델링 손실), cosine-distance loss(코사인 거리 손실)를 결합하여 최적의 결과를 도출했습니다.

#### DistilBERT: a Distilled Version of BERT
DistilBERT는 BERT 모델의 축소 버전으로, 레이어 수를 절반으로 줄이고, 토큰 타입 임베딩과 풀러(pooler)를 제거하여 효율성을 극대화했습니다. 초기화는 교사 모델에서 나온 가중치를 사용하여, 학생 모델이 빠르게 수렴하도록 했습니다.

#### Experiments
DistilBERT는 GLUE 벤치마크에서 다양한 태스크에 대해 BERT와 거의 동일한 성능을 보이며, 파라미터 수는 40% 적고 속도는 60% 빠릅니다. IMDb 감정 분류와 SQuAD v1.1 질의 응답 태스크에서도 BERT에 비해 약간의 성능 저하만 있었습니다. 추가적인 지식 증류 단계는 SQuAD 태스크에서 성능을 높이는 데 기여했습니다.

#### Conclusion and Future Work
DistilBERT는 효율적이고 경량화된 NLP 모델로, BERT의 97% 성능을 유지하면서도 크기와 속도를 대폭 줄일 수 있음을 입증했습니다. 향후 연구 방향으로는 더 다양한 태스크와 데이터셋에 대한 검증이 필요합니다.

### 2. Overall Summary

이 논문은 DistilBERT라는 경량화된 언어 모델을 소개합니다. BERT 모델의 크기를 줄이면서도 성능을 거의 유지하도록 지식 증류 기법을 사용했습니다. DistilBERT는 BERT 대비 40% 작고, 60% 더 빠르며, 모바일 기기에서도 효과적으로 작동합니다. 다양한 실험 결과에서 DistilBERT는 거의 동일한 성능을 보였으며, 추가적인 지식 증류 단계는 일부 태스크에서 성능을 향상시켰습니다. 이 연구는 큰 모델을 효율적으로 줄이면서도 성능을 유지하는 방법을 제안하여, 미래의 NLP 연구와 응용에 큰 기여를 할 수 있습니다.
