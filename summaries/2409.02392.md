# Building Math Agents with Multi-Turn Iterative Preference Learning
## TL;DR
## Summary
- [https://arxiv.org/pdf/2409.02392.pdf](https://arxiv.org/pdf/2409.02392.pdf)

### 1. 섹션별 요약과 주요 기여 및 혁신 부분

#### Introduction
AI와 머신러닝 모델, 특히 대형 언어 모델(LLMs)의 수학 문제 해결 능력 향상을 위해 코드 인터프리터 같은 외부 도구 통합과 다단계 Chain-of-Thought(CoT) 추론 기법이 사용되고 있습니다. 본 논문은 기존의 단일 턴 대화 특화된 알고리즘이 다단계 추론과 외부 도구 통합의 복잡성을 완전히 해결하지 못함에 따라 다단계 선호도 학습 프레임워크를 제시합니다. 이를 통해 다양한 언어 모델의 성능 향상을 입증합니다.

#### Algorithms Development
이 섹션에서는 계획 문제의 최적화 조건을 설정하고, 다단계 직접 정렬 알고리즘(M-DPO 및 M-KTO)을 개발합니다. 특히 각 학습 과정에서 불필요한 토큰을 마스킹하는 방법을 도입합니다. 또한 온라인 반복적인 변형 알고리즘이 효과적임을 입증하고, 이에 대한 평가를 위해 MATH와 GSM8K 벤치마크 데이터를 사용합니다.

#### RL에서 다단계 다중 턴 추론을 위한 학습 목표 및 프레임워크
이 섹션은 최적화 조건을 설정하고, 다단계 직접 정렬 알고리즘(M-DPO 및 M-KTO)을 개발합니다. 또한 학습 목표를 바탕으로 온라인 반복적인 변형 알고리즘이 효과적임을 입증합니다.

#### Conclusion, Limitation, and Future Research Direction
본 논문에서는 선호도 학습이 도구 통합 추론 LLM의 성능을 크게 향상시킬 수 있음을 입증했습니다. 향후 연구 방향으로는 더욱 세밀한 보상 신호 활용, 적응형 여유 및 길이 규제 등을 제안합니다. 또한 이 알고리즘이 일반적인 에이전트 학습에 적용될 수 있도록 확장할 계획입니다.

### 2. 전체 요약

본 논문은 대형 언어 모델(LLMs)의 수학 문제 해결 능력을 향상시키기 위해 다단계 직접 선호도 최적화 알고리즘(M-DPO 및 M-KTO)을 제안합니다. 이는 외부 도구와의 상호작용을 통한 다단계 추론의 복잡성을 해결하며, GSM8K 및 MATH 데이터셋을 사용한 실험 결과 기존의 단일 턴 알고리즘 대비 우수한 성능을 보였습니다. 논문에서는 최적화 조건 설정, 다양한 학습 목표 및 프레임워크 개발, 실험적 검증을 통한 모델 성능 향상을 다룹니다. 향후 연구는 더욱 정밀한 보상 신호 및 일반 에이전트 학습으로의 확장을 목표로 합니다.

---
이 요약을 바탕으로 발표 자료를 만드실 수 있습니다. 추가 문의사항이 있으시면 언제든지 말씀해 주세요!