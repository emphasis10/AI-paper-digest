# PLaD: Preference-based Large Language Model Distillation with Pseudo-Preference Pairs
## TL;DR
## Summary
- [https://arxiv.org/pdf/2406.02886.pdf](https://arxiv.org/pdf/2406.02886.pdf)

### 파일 요약

#### 1. 서론
이 연구는 대형 언어 모델(LLM)의 크기와 계산 요구 사항으로 인해 실제 환경에서의 배포에 어려움이 있다는 점에서 시작됩니다. 모델 압축 기술인 지식 증류(KD)를 사용하여 이러한 문제를 해결하고자 합니다. 여기서 PLaD라는 새로운 프레임워크를 소개합니다.

#### 2. 관련 연구
이 섹션에서는 기존의 KD 기법과, 인간의 피드백을 통한 강화 학습(RLHF), 그리고 최근의 LLM 증류 방법론에 대해 설명합니다. 이 논문은 기존 연구에서 발생했던 문제점을 지적하며 PLaD가 이를 어떻게 해결하는지 강조합니다.

#### 3. 초기 연구
자연어 생성 과제에서 학생 모델이 교사 모델의 출력을 모방하는 전통적인 교사 강요 방식의 한계를 논의합니다. 특히, 교사 모델과 학생 모델 간의 용량 차이와 이로 인한 분포 왜곡 문제를 다룹니다.

#### 4. PLaD 프레임워크
PLaD는 교사 모델과 학생 모델 간의 출력 샘플을 기반으로 가상의 선호 데이터(pseudo-preference pairs)를 생성합니다. 이 데이터를 사용하여 학생 모델의 출력을 재설정하고, 교사 모델의 내부 상태에 접근할 필요 없이 학생 모델이 상대적으로 출력 품질을 학습할 수 있도록 합니다.

#### 5. 실험
- **데이터셋:** Anthropic의 헬퍼 대화 생성과 Reddit TL;DR 요약 데이터를 사용합니다.
- **모델:** LLaMA와 GPT-Neo 모델을 주요 실험에 사용하며, PLaD가 다양한 모델 패밀리에서도 효과적임을 보여줍니다.
- **결과:** PLaD는 다른 최신 KD 방법에 비해 우수한 성능을 나타내며, 실험 결과로 생성길이에 따른 성능 평가도 시행합니다.
   
#### 6. 결론 및 미래 연구
PLaD는 효율적이고 질 높은 대형 언어 모델 증류를 가능하게 합니다. 이 논문은 PLaD가 교사-학생 모델의 상호 작용을 효율적으로 조정하며, 교사 모델의 내부 접근 없이도 질 높은 생성물을 보장할 수 있음을 실험 데이터를 통해 입증했습니다.

## 전체 요약
이 논문은 대형 언어 모델(LLM)의 실용적인 배포에서 발생하는 문제를 해결하기 위해 PLaD라는 새로운 프레임워크를 제안합니다. PLaD는 가상의 선호 데이터(pseudo-preference pairs)를 생성하여 학생 모델의 출력을 교사 모델 수준으로 재설정하는 방법을 사용합니다. 이를 통해 PLaD는 계산 자원을 덜 소모하면서도 질 높은 생성 결과를 보장할 수 있습니다. 실험을 통해 PLaD의 우수한 성능과 다양한 모델에서의 적용 가능성을 입증하였으며, 이로 인해 효율적이고 접근 가능한 AI 언어 시스템의 길을 열었습니다.

## Similar Papers
- [Finch: Prompt-guided Key-Value Cache Compression](2408.00167.md)
- [Direct Preference Knowledge Distillation for Large Language Models](2406.19774.md)
- [SeaLLMs 3: Open Foundation and Chat Multilingual Large Language Models for Southeast Asian Languages](2407.19672.md)
- [Value-Incentivized Preference Optimization: A Unified Approach to Online and Offline RLHF](2405.19320.md)
- [DistiLLM: Towards Streamlined Distillation for Large Language Models](2402.03898.md)
- [LLMLingua-2: Data Distillation for Efficient and Faithful Task-Agnostic Prompt Compression](2403.12968.md)
- [OneBit: Towards Extremely Low-bit Large Language Models](2402.11295.md)
- [DDK: Distilling Domain Knowledge for Efficient Large Language Models](2407.16154.md)
- [Can Small Language Models Help Large Language Models Reason Better?: LM-Guided Chain-of-Thought](2404.03414.md)
