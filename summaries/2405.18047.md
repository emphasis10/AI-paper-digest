# 2BP: 2-Stage Backpropagation
## TL;DR
## Summary
- [https://arxiv.org/pdf/2405.18047.pdf](https://arxiv.org/pdf/2405.18047.pdf)

### 1: 각 섹션 요약

**1. 서론 (Introduction)**:
딥 뉴럴 네트워크(DNN)의 크기와 복잡성이 증가함에 따라 단일 가속기의 메모리 용량을 초과하게 되어 모델 파라미터를 여러 가속기에 분산시키는 방법이 필요해졌습니다. 이를 위해 파이프라인 병렬 처리 방식이 자주 사용됩니다. 그러나 현재 대부분의 파이프라인 병렬 처리는 자동 미분 도구에 의해 병목현상을 겪고 있습니다. 이 논문에서는 백워드 프로세스를 두 단계로 나누어 컴퓨팅 자원의 비효율적 사용을 줄일 수 있는 2단계 백프로파게이션(2BP)을 제안합니다. 실험 결과, 2BP를 통해 기존 방법보다 최대 1.70배의 처리량 향상을 이루었습니다.

**2. 배경 및 관련 연구 (Background & Related Work)**:
GPipe와 같은 기존의 파이프라인 병렬 처리 기법은 미니 배치를 더욱 작게 나누어 여러 가속기에서 병렬로 처리하는 방식으로 효율성을 높였습니다. 그 외에도 다양한 모델 병렬 처리 알고리즘이 제안되었으며, 이러한 알고리즘들은 모두 '버블 비율'을 줄여 비효율적인 시간을 최소화하려고 합니다. 버블 비율이란 전체 계산 시간 중 비활성화된 시간의 비율을 의미합니다.

**3. 2단계 백프로파게이션 (2-Stage Backpropagation)**:
2BP는 백워드 프로세스를 두 단계로 분리하여 가속기의 사용률을 최적화합니다. 첫 번째 단계는 이전 레이어의 출력에 대한 손실의 기울기를 계산하고, 두 번째 단계는 레이어 파라미터에 대한 기울기를 계산합니다. 이를 통해 파이프라인의 비효율성을 줄일 수 있습니다. 

**4. 구현 (Implementation)**:
2BP는 PyTorch 상에서 구현되었지만 PyTorch의 자동 미분 엔진을 사용하지 않고 직접 백워드 패스를 수동으로 구현하였습니다. 각 모듈은 전방 및 후방-1 단계(foward, backward-p1)와 후방-2 단계(forward-p2) 함수를 가지고 있습니다.

**5. 평가 (Evaluation)**:
- **2BP 처리량(Throughput):** 4가지 모델 아키텍처를 통해 테스트한 결과, 2BP는 항상 처리량을 증가시켰으며 최대 1.70배의 향상을 보였습니다.
- **2BP 메모리 소비(Memory Consumption):** 2BP는 활성화 메모리를 증가시키는 단점이 있으며 이는 모델 아키텍처와 파이프라인 스케줄에 따라 다릅니다.
- **확장성(Scaling):** GPU 수가 증가할수록 버블 비율이 증가하지만 2BP를 사용하면 처리량이 더 큰 폭으로 증가합니다.
- **GPU 계산 자원 사용률(GPU Compute Occupancy):** 계산 자원의 사용률을 최적화하기 위해 메모리 내에서의 데이터 병합 및 CUDA 스트림을 통한 병렬 처리가 언급됩니다.

**6. 추가 작업 (Further Work)**:
2BP로 인해 증가하는 활성화 메모리를 줄이기 위한 방법으로 중간 결과 체크포인팅(intermediate derivative checkpointing)을 제안합니다. 또한, 데이터 병렬성과의 병합을 통해 메모리 및 처리량을 최적화하는 방법을 탐구할 계획입니다.

**7. 결론 및 논의 (Conclusion and Discussion)**:
본 연구는 4가지 DNN 모델 아키텍처에서 2BP를 적용하여 처리량 향상 효과를 입증하였습니다. 또한, PyTorch의 사용자 친화적인 기능 개선을 제안하였습니다.

---

### 2: 전체 요약

이 논문은 딥 뉴럴 네트워크(DNN) 훈련 시 가속기의 효율성을 높이기 위해 2단계 백프로파게이션(2BP) 기법을 제안합니다. 2BP는 기존의 백워드 프로세스를 두 단계로 나눠 계산 자원의 비활성 시간을 줄임으로써 처리량을 최대 1.70배까지 증가시켰습니다. 이를 위해 PyTorch 상에서 수동으로 백워드 패스를 구현하여 유연성을 높였으며, 다양한 모델 아키텍처에서 평가를 수행하여 그 효과를 검증하였습니다. 

추가적으로 활성화 메모리의 관리를 위해 중간 결과 체크포인팅 및 데이터 병렬성과의 병합 방법도 탐구할 계획입니다. 결론적으로, 2BP는 대규모 DNN 훈련 시 효율성을 크게 향상시킬 수 있는 혁신적인 접근 방식으로 평가됩니다.



## Similar Papers
- [Efficient Memory Management for Large Language Model Serving with PagedAttention](2309.06180.md)
- [Associative Recurrent Memory Transformer](2407.04841.md)
- [VeLoRA: Memory Efficient Training using Rank-1 Sub-Token Projections](2405.17991.md)
- [FlashDecoding++: Faster Large Language Model Inference on GPUs](2311.01282.md)
- [S-LoRA: Serving Thousands of Concurrent LoRA Adapters](2311.03285.md)
- [ByteTransformer: A High-Performance Transformer Boosted for Variable-Length Inputs](2210.03052.md)
- [Scalify: scale propagation for efficient low-precision LLM training](2407.17353.md)
- [FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning](2307.08691.md)
- [ByteCheckpoint: A Unified Checkpointing System for LLM Development](2407.20143.md)
