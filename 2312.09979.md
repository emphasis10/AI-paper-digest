# LoRAMoE: Alleviate World Knowledge Forgetting in Large Language Models via MoE-Style Plugin
## TL;DR
## Summary
- [https://arxiv.org/pdf/2312.09979.pdf](https://arxiv.org/pdf/2312.09979.pdf)

이 논문은 대규모 언어 모델(LLM)의 세계 지식을 보존하면서도 다양한 하류 작업에 대한 성능을 향상시키기 위한 새로운 프레임워크인 LoRAMoE를 제안합니다. 이 프레임워크는 저랭크 어댑터(LoRA)를 전문가로 도입하고 라우터 네트워크를 통해 이들을 통합하여 MoE(Mixture of Experts) 스타일의 플러그인으로 구성됩니다.

**1. 도입 및 동기 부여**

대규모 언어 모델의 지도 학습 세계 지식이 크게 증가하는 데이터로 인해 손상될 수 있음을 발견하였습니다. 이를 해결하기 위해 LoRAMoE 프레임워크를 제안하여 언어 모델이 세계 지식을 유지하면서도 다양한 작업에 대한 성능을 향상시킬 수 있도록 합니다.

**2. LoRAMoE**

LoRAMoE는 MoE 스타일의 플러그인으로, 저랭크 어댑터(LoRA)를 전문가로 사용하고 이를 라우터를 통해 통합합니다. 이 구조는 언어 모델의 기존 지식을 유지하면서도 특정 작업에 필요한 지식을 활용할 수 있도록 합니다. 특히, LoRAMoE는 세계 지식과 관련된 작업과 다른 하류 작업에 집중하는 전문가를 분리하여 세계 지식을 보존하고 다양한 작업에 대한 성능을 향상시킵니다.

**3. 실험 결과**

LoRAMoE는 다양한 하류 작업에 대해 기존 SFT(Supervised Fine-Tuning) 방법보다 더 우수한 성능을 보여줍니다. 특히 세계 지식을 평가하는 벤치마크에서도 기존 모델이 보여준 성능 저하 없이 성능을 유지하거나 향상시킵니다.

**4. 관련 작업**

이 연구는 MoE, 저랭크 어댑터, 그리고 다양한 파라미터 효율적인 미세조정 방법들과 관련이 있습니다. 특히, LoRAMoE는 이러한 기술을 통합하여 대규모 언어 모델의 세계 지식 손상 문제를 해결하고자 합니다.

**5. 결론**

LoRAMoE는 대규모 언어 모델의 세계 지식을 유지하면서도 다양한 하류 작업의 성능을 향상시킬 수 있는 새로운 방법을 제시합니다. 이 프레임워크는 언어 모델의 지식 보존과 다양한 작업에 대한 성능 향상 사이의 균형을 찾는데 기여할 수 있습니다.