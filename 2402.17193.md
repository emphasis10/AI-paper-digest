# When Scaling Meets LLM Finetuning: The Effect of Data, Model and Finetuning Method
## TL;DR
## Summary
- [https://arxiv.org/pdf/2402.17193.pdf](https://arxiv.org/pdf/2402.17193.pdf)

이 연구는 대규모 언어 모델(LLM)의 미세조정(finetuning) 성능에 대한 이해를 심화하기 위한 것입니다. 특히, LLM의 모델 크기, 사전 훈련 데이터 크기, 미세조정에 사용되는 새로운 매개변수 크기, 그리고 미세조정 데이터 크기와 같은 다양한 스케일링 요소가 미세조정 성능에 어떻게 영향을 미치는지 체계적으로 연구하였습니다. 이를 위해, 전체 모델 튜닝(Full-Model Tuning, FMT)과 매개변수 효율적 튜닝(Parameter-Efficient Tuning, PET)의 두 가지 미세조정 유형을 고려하였습니다.

### 주요 발견

1. **LLM 미세조정은 미세조정 데이터 크기와 각각의 스케일링 요소 사이에서 멀티플리커티브 조인트 스케일링 법칙을 따릅니다.**
2. **LLM 모델 스케일링이 사전 훈련 데이터 스케일링보다 미세조정에 더 큰 이점을 제공합니다.**
3. **PET 매개변수 스케일링은 LoRA와 Prompt 모두에 대해 일반적으로 효과가 없으며, LoRA가 더 나은 훈련 안정성을 보입니다.**
4. **최적의 미세조정 방법은 과제와 미세조정 데이터에 크게 의존적입니다.**

### 섹션별 요약

#### 1. 서론
이 섹션에서는 LLM의 사전 훈련된 모델을 다운스트림 애플리케이션에 전달하는 것이 최근의 성공적인 패러다임임을 소개합니다. 그러나 다양한 미세조정 방법의 스케일링 속성에 대한 이해는 여전히 제한적입니다.

#### 2. 설정
다운스트림 과제로는 기계 번역과 다국어 요약을 선택하였으며, 이는 높은 복잡성을 요구하는 과제입니다. 실험을 위해, 영어&독일어 및 영어&중국어로 사전 훈련된 두 세트의 양방향 LLM을 준비하였습니다.

#### 3. 왜 멀티플리커티브 조인트 스케일링 법칙인가?
이 섹션에서는 LLM 미세조정이 다양한 스케일링 요소와 미세조정 데이터 크기 사이에서 멀티플리커티브 조인트 스케일링 법칙을 따른다는 것을 발견했습니다. 

#### 4. LLM 미세조정의 스케일링 결과
LLM 모델 크기, 사전 훈련 데이터 크기, PET 매개변수 크기 스케일링에 대한 실험 결과를 제공합니다. LLM 모델 스케일링이 미세조정에 가장 큰 이점을 제공함을 발견했습니다.

#### 5. 토론
이 섹션에서는 주어진 과제에 대해 어떤 미세조정 방법을 적용해야 하는지에 대한 명확한 답이 없음을 지적합니다. 과제와 미세조정 데이터의 가용성에 기반한 최적의 방법 선택이 필요합니다.

#### 6. 관련 연구
LLM 미세조정 및 스케일링 법칙에 대한 이전 연구를 검토합니다.

#### 7. 결론 및 향후 작업
LLM 미세조정에 대한 멀티플리커티브 조인트 스케일링 법칙을 제안하고, LLM 모델 크기가 미세조정에 가장 중요한 요소임을 발견했습니다. 향후에는 다양한 미세조정 시나리오로 연구를 확장할 계획입니다.

### 종합 요약
이 연구는 다양한 스케일링 요소가 LLM 미세조정 성능에 미치는 영향을 체계적으로 분석합니다. 주요 발견은 LLM 모델 크기가 미세조정 성능에 가장 큰 영향을 미치며, 미세조정 방법의 선택은 과제와 미세조정 데이터에 따라 달라진다는 것입니다. 이러한 결과는 LLM 미세조정 방법의 선택과 개발을 지원하는 데 도움이 될 수 있습니다.