# The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits
## TL;DR
## Summary
- [https://arxiv.org/pdf/2402.17764.pdf](https://arxiv.org/pdf/2402.17764.pdf)

이 문서는 1비트 대형 언어 모델(Large Language Models, LLMs)에 관한 연구를 다루고 있습니다. 특히, 모든 파라미터(또는 가중치)가 삼진법 {-1, 0, 1}인 1비트 LLM 변형인 BitNet b1.58을 소개하고 있습니다. 이 모델은 기존의 전체 정밀도(예: FP16 또는 BF16) 트랜스포머 LLM과 비교해 동일한 모델 크기와 훈련 토큰을 사용하면서도, 복잡도와 최종 태스크 성능 측면에서 일치하는 성과를 보이며, 지연 시간, 메모리 사용량, 처리량 및 에너지 소비 측면에서 훨씬 비용 효율적입니다.

이 연구의 주요 내용은 다음과 같습니다:

1. **1비트 LLMs의 시대**:
    - 최근의 연구는 1비트 LLMs의 새로운 시대를 여는 중입니다. 이러한 모델들은 비용 효율적이면서도 높은 성능을 유지합니다.
    - BitNet b1.58은 모든 파라미터가 삼진법 {-1, 0, 1}을 사용하며, 이는 새로운 스케일링 법칙과 1비트 LLMs을 위한 특정 하드웨어 설계의 가능성을 열어줍니다.
2. **BitNet b1.58**:
    - BitNet b1.58은 BitNet 아키텍처를 기반으로 하며, 1.58비트 가중치와 8비트 활성화를 사용하여 처음부터 훈련됩니다.
    - 이 모델은 기존 BitNet에 몇 가지 변경 사항을 도입하여, 더 강력한 모델링 능력과 기능 필터링을 통한 성능 향상을 가능하게 합니다.
3. **성능 결과**:
    - BitNet b1.58은 다양한 크기의 LLaMA LLM과 비교하여 평가되었습니다. 이 모델은 3B 모델 크기부터 FP16 기준과 비교해 복잡도와 최종 태스크 성능 측면에서 일치하는 성과를 보였습니다.
    - 또한, 1.58비트 LLMs는 에너지 소비, 메모리 사용량 및 처리량 측면에서 기존 모델들에 비해 우수한 성능을 보입니다.
4. **토론 및 미래 작업**:
    - 1비트 LLMs는 모델의 배포와 적용에 있어서 고메모리 소비와 칩 간 통신 오버헤드를 줄일 수 있는 가능성을 제공합니다.
    - 이 연구는 또한 1.58비트 LLMs가 장기 시퀀스 처리와 에지 및 모바일 장치에서의 언어 모델 성능 향상에 기여할 수 있는 방법을 제시합니다.

이 연구는 1비트 LLMs의 새로운 시대를 여는 중요한 발전을 나타냅니다. BitNet b1.58은 비용 효율적이면서도 높은 성능을 제공하는 새로운 모델 아키텍처를 제시함으로써, 언어 모델의 스케일링과 배포에 있어 새로운 가능성을 열어줍니다.