# Chinese Tiny LLM: Pretraining a Chinese-Centric Large Language Model
## TL;DR
## Summary
- [https://arxiv.org/pdf/2404.04167.pdf](https://arxiv.org/pdf/2404.04167.pdf)

본 논문은 중국어에 초점을 맞춘 대규모 언어 모델(CT-LLM)을 소개하고 있습니다. 이 모델은 기존의 대규모 언어 모델이 영어 중심으로 개발된 것과 달리, 중국어 텍스트 데이터를 주로 활용하여 개발되었습니다. CT-LLM은 1,200억 토큰의 방대한 데이터셋을 기반으로 학습되었으며, 이 중 800억 개는 중국어 토큰입니다. 이를 통해 모델은 중국어 이해 및 처리 능력에서 뛰어난 성능을 보이며, 영어에 대해서도 유연한 대응이 가능한 다양한 언어 능력을 갖추었습니다.

또한, 본 논문에서는 CT-LLM의 학습 과정, 사용된 데이터셋, 모델 아키텍처, 그리고 다양한 평가 벤치마크를 통한 모델 성능 검증에 대해 상세히 설명합니다. 중국어 중심의 대규모 언어 모델 개발을 통해, 언어 다양성을 포용하고 다양한 언어 환경에서의 언어 모델의 활용 가능성을 확대하고자 하는 연구진의 노력이 돋보입니다.

**주요 내용 요약:**

1. **CT-LLM 소개:** 중국어에 초점을 맞춘 2B(20억) 파라미터를 갖는 대규모 언어 모델로, 1,200억 토큰의 대규모 데이터셋에서 학습됩니다. 이중 800억은 중국어 토큰입니다.
2. **데이터셋:** 모델 학습에는 중국어, 영어, 그리고 코드 토큰이 포함된 다양한 소스에서 수집된 데이터가 사용됩니다.
3. **모델 아키텍처:** 트랜스포머 기반의 디코더 모델로, 다양한 개선 사항을 포함하여 성능을 최적화합니다.
4. **성능 평가:** 여러 벤치마크를 통해 모델의 중국어 이해 및 처리 능력, 영어 처리 능력, 그리고 다양한 언어 태스크에서의 성능이 평가됩니다.
5. **오픈 소스 기여:** 학습 과정, 데이터 처리 절차, 그리고 중국어 하드 케이스 벤치마크(CHC-Bench) 등이 오픈 소스로 공개되어, 학계 및 산업계에서의 추가 연구와 혁신을 촉진합니다.

이 연구는 중국어를 포함한 비영어권 언어에 대한 대규모 언어 모델 개발의 가능성을 탐구하며, 언어 다양성에 대한 인식 확대와 다양한 언어 환경에서의 AI 응용을 촉진하는 데 기여합니다.