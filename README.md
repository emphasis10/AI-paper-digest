# Paper List
## 2404
#### [Sigma : Siamese Mamba Network for Multi-Modal Semantic Segmentation](2404.04256.md)
#### [Robust Gaussian Splatting](2404.04211.md)
#### [Social Skill Training with Large Language Models](2404.04204.md)
#### [Chinese Tiny LLM: Pretraining a Chinese-Centric Large Language Model](2404.04167.md)
#### [No “Zero-Shot” Without Exponential Data: Pretraining Concept Frequency Determines Multimodal Model Performance](2404.04125.md)
#### [CantTalkAboutThis: Aligning Language Models to Stay on Topic in Dialogues](2404.03820.md)
#### [Direct Nash Optimization: Teaching Language Models to Self-Improve with General Preferences](2404.03715.md)
#### [Stream of Search (SoS): Learning to Search in Language](2404.03683.md)
#### [RL for Consistency Models: Faster Reward Guided Text-to-Image Generation](2404.03673.md)
#### [AutoWebGLM: Bootstrap And Reinforce A Large Language Model-based Web Navigating Agent](2404.03648.md)
#### [Training LLMs over Neurally Compressed Text](2404.03626.md)
#### [ReFT: Representation Finetuning for Language Models](2404.03592.md)
#### [CodeEditorBench: Evaluating Code Editing Capability of Large Language Models](2404.03543.md)
#### [MiniGPT4-Video: Advancing Multimodal LLMs for Video Understanding with Interleaved Visual-Textual Tokens](2404.03413.md)
#### [Red Teaming GPT-4V: Are GPT-4V Safe Against Uni/Multi-Modal Jailbreak Attacks?](2404.03411.md)
#### [RALL-E: Robust Codec Language Modeling with Chain-of-Thought Prompting for Text-to-Speech Synthesis](2404.03204.md)
#### [LVLM-Intrepret: An Interpretability Tool for Large Vision-Language Models](2404.03118.md)
#### [Visual Autoregressive Modeling: Scalable Image Generation via Next-Scale Prediction](2404.02905.md)
#### [ChatGLM-Math: Improving Math Problem-Solving in Large Language Models with a Self-Critique Pipeline](2404.02893.md)
#### [Language Models as Compilers: Simulating Pseudocode Execution Improves Algorithmic Reasoning in Language Models](2404.02575.md)
#### [Mixture-of-Depths: Dynamically allocating compute in transformer-based language models](2404.02258.md)
#### [Advancing LLM Reasoning Generalists with Preference Trees](2404.02078.md)
#### [Long-context LLMs Struggle with Long In-context Learning](2404.02060.md)
#### [HyperCLOVA X Technical Report](2404.01954.md)
#### [Poro 34B and the Blessing of Multilinguality](2404.01856.md)
#### [Octopus v2: On-device language model for super agent](2404.01744.md)
#### [LLM-ABR: Designing Adaptive Bitrate Algorithms via Large Language Models](2404.01617.md)
#### [Are large language models superhuman chemists?](2404.01475.md)
#### [LLaVA-Gemma: Accelerating Multimodal Foundation Models with a Compact Language Model](2404.01331.md)
## 2403
#### [BLADE: Enhancing Black-box Large Language Models with Small Domain-Specific Models](2403.18365.md)
#### [Fully-fused Multi-Layer Perceptrons on Intel Data Center GPUs](2403.17607.md)
#### [Gemma: Open Models Based on Gemini Research and Technology](2403.08295.md)
#### [Learning to Decode Collaboratively with Multiple Language Models](2403.03870.md)
## 2402
#### [The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits](2402.17764.md)
#### [When Scaling Meets LLM Finetuning: The Effect of Data, Model and Finetuning Method](2402.17193.md)
#### [Training Neural Networks from Scratch with Parallel Low-Rank Adapters](2402.16828.md)
#### [More Agents Is All You Need](2402.05120.md)
## 2401
#### [Medusa: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads](2401.10774.md)
#### [Tuning Language Models by Proxy](2401.08565.md)
## 2312
#### [LoRAMoE: Alleviate World Knowledge Forgetting in Large Language Models via MoE-Style Plugin](2312.09979.md)
## 2311
#### [FlashDecoding++: Faster Large Langauge Model Inference on GPUs](2311.01282.md)
## 2310
#### [EELBERT: Tiny Models through Dynamic Embeddings](2310.20144.md)
## 2308
#### [Fast Feedforward Networks](2308.14711.md)
#### [EdgeMoE: Fast On-Device Inference of MoE-based Large Language Models](2308.14352.md)
## 2302
#### [Full Stack Optimization of Transformer Inference: a Survey](2302.14017.md)
## 2202
#### [cosFormer: Rethinking Softmax in Attention](2202.08791.md)
