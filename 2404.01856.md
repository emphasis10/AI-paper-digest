# Poro 34B and the Blessing of Multilinguality
## TL;DR
## Summary
이 문서는 다양한 언어로 사전 학습 데이터가 충분하지 않은 상황에서 멀티 언어 학습이 단일 언어 모델보다 우수할 수 있다는 주장과 Poro 34B 모델을 소개하고 있습니다. Poro 34B는 34억 개의 매개변수를 가진 모델로, 핀란드어, 영어, 프로그래밍 언어로 1조 토큰에 대해 학습되었습니다. 이 연구는 특히 작은 언어들을 위한 상태-읭-예술 모델을 개발하고자 하며, 멀티 언어 접근 방식에 대해 탐구하고 있습니다. 자세한 요약을 위해 각 섹션별로 요약을 시작하겠습니다.

1. **서론**
    - 신경 언어 모델, 특히 트랜스포머 아키텍처를 바탕으로 한 모델들의 연구와 발전이 비약적으로 증가했습니다. 이러한 모델들은 대용량 데이터에 대한 학습을 필요로 하며, 특히 일부 언어에 대해서는 충분한 훈련 데이터가 없는 상황입니다. 멀티 언어 학습을 통해 데이터 한계를 극복하려는 접근 방식이 제시되었으나, 대부분의 상태-읭-예술 모델들은 여전히 큰 언어들에 집중되어 있습니다. 핀란드어를 대상으로 한 Poro 34B 모델은 이러한 한계를 극복하고자 합니다.

2. **사전 학습 데이터**
    - Poro 34B의 사전 학습 데이터는 품질이 낮은 텍스트와 반복되는 텍스트를 제거하고, 독성이 있는 내용을 걸러내는 등의 전처리 과정을 거쳤습니다. 핀란드어 데이터는 주로 웹 크롤링, 뉴스 소스, 저작권이 없는 책 코퍼스 등에서 얻었으며, 전체 데이터의 대부분을 차지합니다.

각 섹션별 주요 내용 요약을 한국어로 제공하겠습니다.