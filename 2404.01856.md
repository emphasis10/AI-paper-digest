# Poro 34B and the Blessing of Multilinguality
## TL;DR
## Summary
이 문서는 "Poro 34B와 다국어의 축복"이라는 제목의 연구논문으로, 다양한 언어를 포함하는 것이 대규모 언어 모델의 사전 학습 데이터를 확보하는 명백한 방법임에도 불구하고, 많은 모델 학습 노력이 여전히 주로 개별 대형 언어에 집중되어 있다는 문제점에서 출발합니다. 연구진은 다국어를 활용하는 것이 소규모 언어들을 위한 단일 언어 모델보다 크게 개선될 수 있다고 주장하며, 이를 입증하기 위해 핀란드어, 영어, 그리고 프로그래밍 언어를 포함한 1조 토큰으로 학습된 340억 개의 파라미터를 가진 'Poro 34B' 모델을 소개합니다. 이 모델은 핀란드어에 대한 기존 모델들을 크게 앞지르는 성능을 보이며, 동시에 번역 작업에서도 뛰어난 성능을 보여줍니다.

아래는 각 섹션별 요약입니다.

### 1. 소개

- 다양한 언어를 처리할 수 있는 대규모 언어 모델의 필요성과 현재 이러한 모델들이 주로 대형 언어에 초점을 맞추고 있는 문제점을 지적합니다. 특히, 핀란드어와 같은 소규모 언어들을 위한 대규모 모델의 필요성을 강조하며, 다국어 학습을 통해 이러한 언어들에 대한 모델의 성능을 크게 개선할 수 있다고 주장합니다.

### 2. 사전 학습 데이터

- Poro 34B 모델의 학습에 사용된 데이터 세트는 핀란드어, 영어, 프로그래밍 언어를 포함합니다. 이 데이터들은 낮은 품질의 텍스트를 제거하고 중복을 제거하는 과정을 거쳐 선별되었습니다.

### 3. 방법론

- 모델 토큰화, 사전 학습 절차, 그리고 학습에 소요된 계산 비용에 대해 설명합니다. 특히, 다양한 언어와 프로그래밍 언어를 효과적으로 처리할 수 있는 새로운 토크나이저를 개발하였다고 언급합니다.

### 4. 평가

- Poro 34B 모델의 핀란드어, 영어, 프로그래밍 언어에 대한 성능을 평가하고, 이 모델이 기존 모델들을 능가함을 보여줍니다. 특히 핀란드어에 대한 모델의 성능이 크게 향상되었음을 강조합니다.

### 5. 토론 및 결론

- 연구진은 다국어 학습 접근 방식이 소규모 언어에 대한 대규모 언어 모델의 성능을 크게 향상시킬 수 있음을 보여줍니다. 또한, 이러한 접근 방식이 다른 소규모 언어에 대해서도 유용할 것이라고 기대합니다.

### 전반적 요약

이 연구는 다국어를 포함하는 사전 학습 데이터가 소규모 언어를 위한 언어 모델의 성능을 크게 개선할 수 있는 중요한 방법임을 보여줍니다. Poro 34B 모델은 핀란드어, 영어, 프로그래밍 언어를 포함한 다양한 언어의 처리에 뛰어난 성능을 보이며, 특히 핀란드어에 대한 성능 개선이 두드러집니다. 이 연구는 다국어 학습의 잠재력을 보여주며, 소규모 언어를 위한 더 크고 강력한 모델 개발에 대한 길을 제시합니다.